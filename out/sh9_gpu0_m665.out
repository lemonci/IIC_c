Config: Config: -----
batchnorm_track: True
data_mean: [0.5, 0.5, 0.5]
out_dir: /home/monica/IIC/datasets/iid_private/665
always_rot: False
dataset: MNIST
num_epochs: 3200
lr_schedule: []
mapping_test_partitions: [False]
crop_other: True
lamb: 1.0
save_freq: 10
no_jitter: False
lr_mult: 0.1
tf3_crop_sz: 0
input_sz: 24
dataloader_batch_sz: 70
eval_mode: orig
in_channels: 1
lr: 0.0001
gt_k: 10
no_flip: True
tf3_crop_diff: False
opt: Adam
restart_from_best: False
double_eval: False
output_k: 25
num_dataloaders: 5
tf2_crop: random
tf2_crop_szs: [16, 20, 24]
num_sub_heads: 5
crop_orig: True
mix_train: False
test_code: False
twohead: False
tf1_crop_sz: 20
mapping_assignment_partitions: [True]
per_img_demean: False
dataset_root: /home/monica/IIC/datasets/MNIST
arch: ClusterNet6c
train_partitions: [True]
restart: False
batch_sz: 350
tf1_crop: centre_half
out_root: /home/monica/IIC/datasets/iid_private
model_ind: 665
data_std: [0.5, 0.5, 0.5]
demean: False
mode: IID+
rot_val: 25.0
----------
selected centre_half crop for tf1
tf3 crop size is same as tf1
adding rotation option for imgs_tf: 25
not always_rot
selected random crop for tf2
adding crop size option for imgs_tf: 16
selected random crop for tf2
adding crop size option for imgs_tf: 20
selected random crop for tf2
adding crop size option for imgs_tf: 24
not adding flip
adding jitter
not demeaning data
not per image demeaning data
Making datasets with <class 'torchvision.datasets.mnist.MNIST'> and None
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Processing...
Done!
Creating auxiliary dataloader ind 0 out of 5 time 2020-06-25 17:17:26.202354
Creating auxiliary dataloader ind 1 out of 5 time 2020-06-25 17:17:26.218016
Creating auxiliary dataloader ind 2 out of 5 time 2020-06-25 17:17:26.232586
Creating auxiliary dataloader ind 3 out of 5 time 2020-06-25 17:17:26.246822
Creating auxiliary dataloader ind 4 out of 5 time 2020-06-25 17:17:26.261035
Length of datasets vector 6
Number of batches per epoch: 858
THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=663 error=11 : invalid argument
Pre: time 2020-06-25 17:17:38.337943: 
 	std: 0.03152745
	best_train_sub_head_match: [(0, 2), (1, 5), (2, 0), (3, 1), (4, 5), (5, 0), (6, 0), (7, 0), (8, 0), (9, 0), (10, 0), (11, 2), (12, 2), (13, 7), (14, 0), (15, 0), (16, 0), (17, 7), (18, 0), (19, 0), (20, 1), (21, 7), (22, 0), (23, 0), (24, 0)]
	test_accs: [0.1878, 0.1207, 0.1141, 0.1575, 0.1874]
	train_accs: [0.19056667, 0.11821666, 0.11348333, 0.15488334, 0.19445]
	best_train_sub_head: 4
	worst: 0.1141
	avg: 0.15349999
	best: 0.1874

Starting e_i: 1
Model ind 665 epoch 1 batch: 0 avg loss -0.000022 avg loss no lamb -0.000022 time 2020-06-25 17:17:38.485999
Model ind 665 epoch 1 batch: 1 avg loss -0.000085 avg loss no lamb -0.000085 time 2020-06-25 17:17:38.603178
Model ind 665 epoch 1 batch: 2 avg loss -0.000658 avg loss no lamb -0.000658 time 2020-06-25 17:17:38.719569
Model ind 665 epoch 1 batch: 3 avg loss -0.002417 avg loss no lamb -0.002417 time 2020-06-25 17:17:38.852436
Model ind 665 epoch 1 batch: 4 avg loss -0.004716 avg loss no lamb -0.004716 time 2020-06-25 17:17:38.956800
Model ind 665 epoch 1 batch: 5 avg loss -0.015597 avg loss no lamb -0.015597 time 2020-06-25 17:17:39.098887
Model ind 665 epoch 1 batch: 6 avg loss -0.016952 avg loss no lamb -0.016952 time 2020-06-25 17:17:39.256530
Model ind 665 epoch 1 batch: 7 avg loss -0.019150 avg loss no lamb -0.019150 time 2020-06-25 17:17:39.396658
Model ind 665 epoch 1 batch: 8 avg loss -0.033120 avg loss no lamb -0.033120 time 2020-06-25 17:17:39.511395
Model ind 665 epoch 1 batch: 9 avg loss -0.045800 avg loss no lamb -0.045800 time 2020-06-25 17:17:39.625847
Model ind 665 epoch 1 batch: 10 avg loss -0.045617 avg loss no lamb -0.045617 time 2020-06-25 17:17:39.764450
Model ind 665 epoch 1 batch: 11 avg loss -0.067410 avg loss no lamb -0.067410 time 2020-06-25 17:17:39.878592
Model ind 665 epoch 1 batch: 12 avg loss -0.066213 avg loss no lamb -0.066213 time 2020-06-25 17:17:39.992003
Model ind 665 epoch 1 batch: 13 avg loss -0.086180 avg loss no lamb -0.086180 time 2020-06-25 17:17:40.105892
Model ind 665 epoch 1 batch: 14 avg loss -0.126620 avg loss no lamb -0.126620 time 2020-06-25 17:17:40.221117
Model ind 665 epoch 1 batch: 15 avg loss -0.085642 avg loss no lamb -0.085642 time 2020-06-25 17:17:40.350634
Model ind 665 epoch 1 batch: 16 avg loss -0.124726 avg loss no lamb -0.124726 time 2020-06-25 17:17:40.465042
Model ind 665 epoch 1 batch: 17 avg loss -0.202553 avg loss no lamb -0.202553 time 2020-06-25 17:17:40.579792
Model ind 665 epoch 1 batch: 18 avg loss -0.226294 avg loss no lamb -0.226294 time 2020-06-25 17:17:40.694762
Model ind 665 epoch 1 batch: 19 avg loss -0.168602 avg loss no lamb -0.168602 time 2020-06-25 17:17:40.808853
Model ind 665 epoch 1 batch: 20 avg loss -0.115365 avg loss no lamb -0.115365 time 2020-06-25 17:17:40.953781
Model ind 665 epoch 1 batch: 21 avg loss -0.139922 avg loss no lamb -0.139922 time 2020-06-25 17:17:41.068194
Model ind 665 epoch 1 batch: 22 avg loss -0.107963 avg loss no lamb -0.107963 time 2020-06-25 17:17:41.182157
Model ind 665 epoch 1 batch: 23 avg loss -0.143702 avg loss no lamb -0.143702 time 2020-06-25 17:17:41.296802
Model ind 665 epoch 1 batch: 24 avg loss -0.162732 avg loss no lamb -0.162732 time 2020-06-25 17:17:41.429189
Model ind 665 epoch 1 batch: 25 avg loss -0.173185 avg loss no lamb -0.173185 time 2020-06-25 17:17:41.546873
Model ind 665 epoch 1 batch: 26 avg loss -0.260756 avg loss no lamb -0.260756 time 2020-06-25 17:17:41.669610
Model ind 665 epoch 1 batch: 27 avg loss -0.240027 avg loss no lamb -0.240027 time 2020-06-25 17:17:41.791790
Model ind 665 epoch 1 batch: 28 avg loss -0.309441 avg loss no lamb -0.309441 time 2020-06-25 17:17:41.942079
Model ind 665 epoch 1 batch: 29 avg loss -0.237087 avg loss no lamb -0.237087 time 2020-06-25 17:17:42.082609
Model ind 665 epoch 1 batch: 30 avg loss -0.261521 avg loss no lamb -0.261521 time 2020-06-25 17:17:42.197642
Model ind 665 epoch 1 batch: 31 avg loss -0.201838 avg loss no lamb -0.201838 time 2020-06-25 17:17:42.342054
Model ind 665 epoch 1 batch: 32 avg loss -0.187636 avg loss no lamb -0.187636 time 2020-06-25 17:17:42.470119
Model ind 665 epoch 1 batch: 33 avg loss -0.217980 avg loss no lamb -0.217980 time 2020-06-25 17:17:42.598344
Model ind 665 epoch 1 batch: 34 avg loss -0.240531 avg loss no lamb -0.240531 time 2020-06-25 17:17:42.723899
Model ind 665 epoch 1 batch: 35 avg loss -0.193616 avg loss no lamb -0.193616 time 2020-06-25 17:17:42.846837
Model ind 665 epoch 1 batch: 36 avg loss -0.279275 avg loss no lamb -0.279275 time 2020-06-25 17:17:42.970892
Model ind 665 epoch 1 batch: 37 avg loss -0.272897 avg loss no lamb -0.272897 time 2020-06-25 17:17:43.085193
Model ind 665 epoch 1 batch: 38 avg loss -0.290367 avg loss no lamb -0.290367 time 2020-06-25 17:17:43.220864
Model ind 665 epoch 1 batch: 39 avg loss -0.336366 avg loss no lamb -0.336366 time 2020-06-25 17:17:43.337700
Model ind 665 epoch 1 batch: 40 avg loss -0.253771 avg loss no lamb -0.253771 time 2020-06-25 17:17:43.455570
Model ind 665 epoch 1 batch: 41 avg loss -0.411661 avg loss no lamb -0.411661 time 2020-06-25 17:17:43.568688
Model ind 665 epoch 1 batch: 42 avg loss -0.267614 avg loss no lamb -0.267614 time 2020-06-25 17:17:43.681640
Model ind 665 epoch 1 batch: 43 avg loss -0.232799 avg loss no lamb -0.232799 time 2020-06-25 17:17:43.794902
Model ind 665 epoch 1 batch: 44 avg loss -0.375928 avg loss no lamb -0.375928 time 2020-06-25 17:17:43.908435
Model ind 665 epoch 1 batch: 45 avg loss -0.255934 avg loss no lamb -0.255934 time 2020-06-25 17:17:44.021997
Model ind 665 epoch 1 batch: 46 avg loss -0.198734 avg loss no lamb -0.198734 time 2020-06-25 17:17:44.140460
Model ind 665 epoch 1 batch: 47 avg loss -0.286524 avg loss no lamb -0.286524 time 2020-06-25 17:17:44.254938
Model ind 665 epoch 1 batch: 48 avg loss -0.397779 avg loss no lamb -0.397779 time 2020-06-25 17:17:44.370088
Model ind 665 epoch 1 batch: 49 avg loss -0.352301 avg loss no lamb -0.352301 time 2020-06-25 17:17:44.524451
Model ind 665 epoch 1 batch: 50 avg loss -0.357116 avg loss no lamb -0.357116 time 2020-06-25 17:17:44.639343
Model ind 665 epoch 1 batch: 51 avg loss -0.392475 avg loss no lamb -0.392475 time 2020-06-25 17:17:44.755681
Model ind 665 epoch 1 batch: 52 avg loss -0.312859 avg loss no lamb -0.312859 time 2020-06-25 17:17:44.870748
Model ind 665 epoch 1 batch: 53 avg loss -0.415749 avg loss no lamb -0.415749 time 2020-06-25 17:17:45.014907
Model ind 665 epoch 1 batch: 54 avg loss -0.349915 avg loss no lamb -0.349915 time 2020-06-25 17:17:45.131571
Model ind 665 epoch 1 batch: 55 avg loss -0.351633 avg loss no lamb -0.351633 time 2020-06-25 17:17:45.246148
Model ind 665 epoch 1 batch: 56 avg loss -0.432834 avg loss no lamb -0.432834 time 2020-06-25 17:17:45.385675
Model ind 665 epoch 1 batch: 57 avg loss -0.350657 avg loss no lamb -0.350657 time 2020-06-25 17:17:45.501244
Model ind 665 epoch 1 batch: 58 avg loss -0.375192 avg loss no lamb -0.375192 time 2020-06-25 17:17:45.617019
Model ind 665 epoch 1 batch: 59 avg loss -0.418101 avg loss no lamb -0.418101 time 2020-06-25 17:17:45.760235
Model ind 665 epoch 1 batch: 60 avg loss -0.335082 avg loss no lamb -0.335082 time 2020-06-25 17:17:45.875680
Model ind 665 epoch 1 batch: 61 avg loss -0.330344 avg loss no lamb -0.330344 time 2020-06-25 17:17:45.995601
Model ind 665 epoch 1 batch: 62 avg loss -0.296873 avg loss no lamb -0.296873 time 2020-06-25 17:17:46.112413
Model ind 665 epoch 1 batch: 63 avg loss -0.301504 avg loss no lamb -0.301504 time 2020-06-25 17:17:46.233071
Model ind 665 epoch 1 batch: 64 avg loss -0.425437 avg loss no lamb -0.425437 time 2020-06-25 17:17:46.353731
Model ind 665 epoch 1 batch: 65 avg loss -0.470028 avg loss no lamb -0.470028 time 2020-06-25 17:17:46.513598
Model ind 665 epoch 1 batch: 66 avg loss -0.437774 avg loss no lamb -0.437774 time 2020-06-25 17:17:46.632326
Model ind 665 epoch 1 batch: 67 avg loss -0.449781 avg loss no lamb -0.449781 time 2020-06-25 17:17:46.746677
Model ind 665 epoch 1 batch: 68 avg loss -0.468820 avg loss no lamb -0.468820 time 2020-06-25 17:17:46.885335
Model ind 665 epoch 1 batch: 69 avg loss -0.349409 avg loss no lamb -0.349409 time 2020-06-25 17:17:46.999732
Model ind 665 epoch 1 batch: 70 avg loss -0.475256 avg loss no lamb -0.475256 time 2020-06-25 17:17:47.114248
Model ind 665 epoch 1 batch: 71 avg loss -0.551143 avg loss no lamb -0.551143 time 2020-06-25 17:17:47.231420
Model ind 665 epoch 1 batch: 72 avg loss -0.431203 avg loss no lamb -0.431203 time 2020-06-25 17:17:47.366263
Model ind 665 epoch 1 batch: 73 avg loss -0.499007 avg loss no lamb -0.499007 time 2020-06-25 17:17:47.481191
Model ind 665 epoch 1 batch: 74 avg loss -0.464263 avg loss no lamb -0.464263 time 2020-06-25 17:17:47.596499
Model ind 665 epoch 1 batch: 75 avg loss -0.367073 avg loss no lamb -0.367073 time 2020-06-25 17:17:47.712230
Model ind 665 epoch 1 batch: 76 avg loss -0.440151 avg loss no lamb -0.440151 time 2020-06-25 17:17:47.840062
Model ind 665 epoch 1 batch: 77 avg loss -0.413627 avg loss no lamb -0.413627 time 2020-06-25 17:17:47.955219
Model ind 665 epoch 1 batch: 78 avg loss -0.460017 avg loss no lamb -0.460017 time 2020-06-25 17:17:48.069501
Model ind 665 epoch 1 batch: 79 avg loss -0.453702 avg loss no lamb -0.453702 time 2020-06-25 17:17:48.228225
Model ind 665 epoch 1 batch: 80 avg loss -0.485718 avg loss no lamb -0.485718 time 2020-06-25 17:17:48.343615
Model ind 665 epoch 1 batch: 81 avg loss -0.493452 avg loss no lamb -0.493452 time 2020-06-25 17:17:48.495893
Model ind 665 epoch 1 batch: 82 avg loss -0.325191 avg loss no lamb -0.325191 time 2020-06-25 17:17:48.638012
Model ind 665 epoch 1 batch: 83 avg loss -0.499146 avg loss no lamb -0.499146 time 2020-06-25 17:17:48.754340
Model ind 665 epoch 1 batch: 84 avg loss -0.450861 avg loss no lamb -0.450861 time 2020-06-25 17:17:48.869016
Model ind 665 epoch 1 batch: 85 avg loss -0.509701 avg loss no lamb -0.509701 time 2020-06-25 17:17:48.995718
Model ind 665 epoch 1 batch: 86 avg loss -0.587244 avg loss no lamb -0.587244 time 2020-06-25 17:17:49.110565
Model ind 665 epoch 1 batch: 87 avg loss -0.498218 avg loss no lamb -0.498218 time 2020-06-25 17:17:49.264106
Model ind 665 epoch 1 batch: 88 avg loss -0.591539 avg loss no lamb -0.591539 time 2020-06-25 17:17:49.404811
Model ind 665 epoch 1 batch: 89 avg loss -0.476263 avg loss no lamb -0.476263 time 2020-06-25 17:17:49.519348
Model ind 665 epoch 1 batch: 90 avg loss -0.489685 avg loss no lamb -0.489685 time 2020-06-25 17:17:49.667527
Model ind 665 epoch 1 batch: 91 avg loss -0.541350 avg loss no lamb -0.541350 time 2020-06-25 17:17:49.781947
Model ind 665 epoch 1 batch: 92 avg loss -0.397211 avg loss no lamb -0.397211 time 2020-06-25 17:17:49.896150
Model ind 665 epoch 1 batch: 93 avg loss -0.521115 avg loss no lamb -0.521115 time 2020-06-25 17:17:50.044847
Model ind 665 epoch 1 batch: 94 avg loss -0.468627 avg loss no lamb -0.468627 time 2020-06-25 17:17:50.196796
Model ind 665 epoch 1 batch: 95 avg loss -0.540857 avg loss no lamb -0.540857 time 2020-06-25 17:17:50.335570
Model ind 665 epoch 1 batch: 96 avg loss -0.506193 avg loss no lamb -0.506193 time 2020-06-25 17:17:50.452083
Model ind 665 epoch 1 batch: 97 avg loss -0.373103 avg loss no lamb -0.373103 time 2020-06-25 17:17:50.580638
Model ind 665 epoch 1 batch: 98 avg loss -0.475373 avg loss no lamb -0.475373 time 2020-06-25 17:17:50.695624
Model ind 665 epoch 1 batch: 99 avg loss -0.591039 avg loss no lamb -0.591039 time 2020-06-25 17:17:50.838068
Model ind 665 epoch 1 batch: 100 avg loss -0.560371 avg loss no lamb -0.560371 time 2020-06-25 17:17:50.952785
Model ind 665 epoch 1 batch: 101 avg loss -0.472887 avg loss no lamb -0.472887 time 2020-06-25 17:17:51.068686
Model ind 665 epoch 1 batch: 102 avg loss -0.496551 avg loss no lamb -0.496551 time 2020-06-25 17:17:51.184343
Model ind 665 epoch 1 batch: 103 avg loss -0.514896 avg loss no lamb -0.514896 time 2020-06-25 17:17:51.342087
Model ind 665 epoch 1 batch: 104 avg loss -0.523775 avg loss no lamb -0.523775 time 2020-06-25 17:17:51.459801
Model ind 665 epoch 1 batch: 105 avg loss -0.480556 avg loss no lamb -0.480556 time 2020-06-25 17:17:51.613239
Model ind 665 epoch 1 batch: 106 avg loss -0.621533 avg loss no lamb -0.621533 time 2020-06-25 17:17:51.727015
Model ind 665 epoch 1 batch: 107 avg loss -0.519587 avg loss no lamb -0.519587 time 2020-06-25 17:17:51.854827
Model ind 665 epoch 1 batch: 108 avg loss -0.569592 avg loss no lamb -0.569592 time 2020-06-25 17:17:51.968588
Model ind 665 epoch 1 batch: 109 avg loss -0.551473 avg loss no lamb -0.551473 time 2020-06-25 17:17:52.081817
Model ind 665 epoch 1 batch: 110 avg loss -0.614736 avg loss no lamb -0.614736 time 2020-06-25 17:17:52.194839
Model ind 665 epoch 1 batch: 111 avg loss -0.580318 avg loss no lamb -0.580318 time 2020-06-25 17:17:52.307552
Model ind 665 epoch 1 batch: 112 avg loss -0.488546 avg loss no lamb -0.488546 time 2020-06-25 17:17:52.421218
Model ind 665 epoch 1 batch: 113 avg loss -0.620716 avg loss no lamb -0.620716 time 2020-06-25 17:17:52.538601
Model ind 665 epoch 1 batch: 114 avg loss -0.585360 avg loss no lamb -0.585360 time 2020-06-25 17:17:52.656591
Model ind 665 epoch 1 batch: 115 avg loss -0.623655 avg loss no lamb -0.623655 time 2020-06-25 17:17:52.817977
Model ind 665 epoch 1 batch: 116 avg loss -0.598058 avg loss no lamb -0.598058 time 2020-06-25 17:17:52.932323
Model ind 665 epoch 1 batch: 117 avg loss -0.479586 avg loss no lamb -0.479586 time 2020-06-25 17:17:53.081918
Model ind 665 epoch 1 batch: 118 avg loss -0.428644 avg loss no lamb -0.428644 time 2020-06-25 17:17:53.195887
Model ind 665 epoch 1 batch: 119 avg loss -0.558237 avg loss no lamb -0.558237 time 2020-06-25 17:17:53.310468
Model ind 665 epoch 1 batch: 120 avg loss -0.579015 avg loss no lamb -0.579015 time 2020-06-25 17:17:53.466665
Model ind 665 epoch 1 batch: 121 avg loss -0.490025 avg loss no lamb -0.490025 time 2020-06-25 17:17:53.583151
Model ind 665 epoch 1 batch: 122 avg loss -0.689049 avg loss no lamb -0.689049 time 2020-06-25 17:17:53.701264
Model ind 665 epoch 1 batch: 123 avg loss -0.643071 avg loss no lamb -0.643071 time 2020-06-25 17:17:53.850318
Model ind 665 epoch 1 batch: 124 avg loss -0.533215 avg loss no lamb -0.533215 time 2020-06-25 17:17:53.986312
Model ind 665 epoch 1 batch: 125 avg loss -0.530396 avg loss no lamb -0.530396 time 2020-06-25 17:17:54.103459
Model ind 665 epoch 1 batch: 126 avg loss -0.430299 avg loss no lamb -0.430299 time 2020-06-25 17:17:54.250199
Model ind 665 epoch 1 batch: 127 avg loss -0.562671 avg loss no lamb -0.562671 time 2020-06-25 17:17:54.367401
Model ind 665 epoch 1 batch: 128 avg loss -0.676363 avg loss no lamb -0.676363 time 2020-06-25 17:17:54.483558
Model ind 665 epoch 1 batch: 129 avg loss -0.616079 avg loss no lamb -0.616079 time 2020-06-25 17:17:54.600077
Model ind 665 epoch 1 batch: 130 avg loss -0.595951 avg loss no lamb -0.595951 time 2020-06-25 17:17:54.716425
Model ind 665 epoch 1 batch: 131 avg loss -0.499429 avg loss no lamb -0.499429 time 2020-06-25 17:17:54.838065
Model ind 665 epoch 1 batch: 132 avg loss -0.683958 avg loss no lamb -0.683958 time 2020-06-25 17:17:54.979814
Model ind 665 epoch 1 batch: 133 avg loss -0.614570 avg loss no lamb -0.614570 time 2020-06-25 17:17:55.096688
Model ind 665 epoch 1 batch: 134 avg loss -0.584117 avg loss no lamb -0.584117 time 2020-06-25 17:17:55.234349
Model ind 665 epoch 1 batch: 135 avg loss -0.602598 avg loss no lamb -0.602598 time 2020-06-25 17:17:55.349081
Model ind 665 epoch 1 batch: 136 avg loss -0.671140 avg loss no lamb -0.671140 time 2020-06-25 17:17:55.470607
Model ind 665 epoch 1 batch: 137 avg loss -0.700413 avg loss no lamb -0.700413 time 2020-06-25 17:17:55.583388
Model ind 665 epoch 1 batch: 138 avg loss -0.697368 avg loss no lamb -0.697368 time 2020-06-25 17:17:55.696592
Model ind 665 epoch 1 batch: 139 avg loss -0.591527 avg loss no lamb -0.591527 time 2020-06-25 17:17:55.834353
Model ind 665 epoch 1 batch: 140 avg loss -0.493777 avg loss no lamb -0.493777 time 2020-06-25 17:17:55.951960
Model ind 665 epoch 1 batch: 141 avg loss -0.669144 avg loss no lamb -0.669144 time 2020-06-25 17:17:56.069352
Model ind 665 epoch 1 batch: 142 avg loss -0.656496 avg loss no lamb -0.656496 time 2020-06-25 17:17:56.186276
Model ind 665 epoch 1 batch: 143 avg loss -0.607543 avg loss no lamb -0.607543 time 2020-06-25 17:17:56.303270
Model ind 665 epoch 1 batch: 144 avg loss -0.706511 avg loss no lamb -0.706511 time 2020-06-25 17:17:56.420046
Model ind 665 epoch 1 batch: 145 avg loss -0.584048 avg loss no lamb -0.584048 time 2020-06-25 17:17:56.539207
Model ind 665 epoch 1 batch: 146 avg loss -0.632803 avg loss no lamb -0.632803 time 2020-06-25 17:17:56.656269
Model ind 665 epoch 1 batch: 147 avg loss -0.629807 avg loss no lamb -0.629807 time 2020-06-25 17:17:56.771025
Model ind 665 epoch 1 batch: 148 avg loss -0.866451 avg loss no lamb -0.866451 time 2020-06-25 17:17:56.884859
Model ind 665 epoch 1 batch: 149 avg loss -0.545541 avg loss no lamb -0.545541 time 2020-06-25 17:17:56.999089
Model ind 665 epoch 1 batch: 150 avg loss -0.773269 avg loss no lamb -0.773269 time 2020-06-25 17:17:57.117614
Model ind 665 epoch 1 batch: 151 avg loss -0.692675 avg loss no lamb -0.692675 time 2020-06-25 17:17:57.234584
Model ind 665 epoch 1 batch: 152 avg loss -0.638144 avg loss no lamb -0.638144 time 2020-06-25 17:17:57.349918
Model ind 665 epoch 1 batch: 153 avg loss -0.610023 avg loss no lamb -0.610023 time 2020-06-25 17:17:57.465670
Model ind 665 epoch 1 batch: 154 avg loss -0.747576 avg loss no lamb -0.747576 time 2020-06-25 17:17:57.582417
Model ind 665 epoch 1 batch: 155 avg loss -0.774257 avg loss no lamb -0.774257 time 2020-06-25 17:17:57.698147
Model ind 665 epoch 1 batch: 156 avg loss -0.762853 avg loss no lamb -0.762853 time 2020-06-25 17:17:57.845948
Model ind 665 epoch 1 batch: 157 avg loss -0.714247 avg loss no lamb -0.714247 time 2020-06-25 17:17:57.960942
Model ind 665 epoch 1 batch: 158 avg loss -0.696508 avg loss no lamb -0.696508 time 2020-06-25 17:17:58.076699
Model ind 665 epoch 1 batch: 159 avg loss -0.687388 avg loss no lamb -0.687388 time 2020-06-25 17:17:58.191461
Model ind 665 epoch 1 batch: 160 avg loss -0.783021 avg loss no lamb -0.783021 time 2020-06-25 17:17:58.331774
Model ind 665 epoch 1 batch: 161 avg loss -0.772624 avg loss no lamb -0.772624 time 2020-06-25 17:17:58.481668
Model ind 665 epoch 1 batch: 162 avg loss -0.771114 avg loss no lamb -0.771114 time 2020-06-25 17:17:58.595281
Model ind 665 epoch 1 batch: 163 avg loss -0.670593 avg loss no lamb -0.670593 time 2020-06-25 17:17:58.709170
Model ind 665 epoch 1 batch: 164 avg loss -0.739803 avg loss no lamb -0.739803 time 2020-06-25 17:17:58.834995
Model ind 665 epoch 1 batch: 165 avg loss -0.580616 avg loss no lamb -0.580616 time 2020-06-25 17:17:58.964388
Model ind 665 epoch 1 batch: 166 avg loss -0.656126 avg loss no lamb -0.656126 time 2020-06-25 17:17:59.078244
Model ind 665 epoch 1 batch: 167 avg loss -0.575706 avg loss no lamb -0.575706 time 2020-06-25 17:17:59.191282
Model ind 665 epoch 1 batch: 168 avg loss -0.557668 avg loss no lamb -0.557668 time 2020-06-25 17:17:59.327670
Model ind 665 epoch 1 batch: 169 avg loss -0.721152 avg loss no lamb -0.721152 time 2020-06-25 17:17:59.442360
Model ind 665 epoch 1 batch: 170 avg loss -0.650792 avg loss no lamb -0.650792 time 2020-06-25 17:17:59.556600
Model ind 665 epoch 1 batch: 171 avg loss -0.663676 avg loss no lamb -0.663676 time 2020-06-25 17:17:59.670280
Model ind 665 epoch 1 batch: 172 avg loss -0.792355 avg loss no lamb -0.792355 time 2020-06-25 17:17:59.786303
Model ind 665 epoch 1 batch: 173 avg loss -0.765668 avg loss no lamb -0.765668 time 2020-06-25 17:17:59.902076
Model ind 665 epoch 1 batch: 174 avg loss -0.713972 avg loss no lamb -0.713972 time 2020-06-25 17:18:00.016002
Model ind 665 epoch 1 batch: 175 avg loss -0.547382 avg loss no lamb -0.547382 time 2020-06-25 17:18:00.135392
Model ind 665 epoch 1 batch: 176 avg loss -0.760096 avg loss no lamb -0.760096 time 2020-06-25 17:18:00.254236
Model ind 665 epoch 1 batch: 177 avg loss -0.757223 avg loss no lamb -0.757223 time 2020-06-25 17:18:00.368897
Model ind 665 epoch 1 batch: 178 avg loss -0.663249 avg loss no lamb -0.663249 time 2020-06-25 17:18:00.486655
Model ind 665 epoch 1 batch: 179 avg loss -0.742098 avg loss no lamb -0.742098 time 2020-06-25 17:18:00.600609
Model ind 665 epoch 1 batch: 180 avg loss -0.772128 avg loss no lamb -0.772128 time 2020-06-25 17:18:00.714722
Model ind 665 epoch 1 batch: 181 avg loss -0.807707 avg loss no lamb -0.807707 time 2020-06-25 17:18:00.831957
Model ind 665 epoch 1 batch: 182 avg loss -0.800919 avg loss no lamb -0.800919 time 2020-06-25 17:18:00.948438
Model ind 665 epoch 1 batch: 183 avg loss -0.749203 avg loss no lamb -0.749203 time 2020-06-25 17:18:01.077458
Model ind 665 epoch 1 batch: 184 avg loss -0.649182 avg loss no lamb -0.649182 time 2020-06-25 17:18:01.191883
Model ind 665 epoch 1 batch: 185 avg loss -0.673784 avg loss no lamb -0.673784 time 2020-06-25 17:18:01.307414
Model ind 665 epoch 1 batch: 186 avg loss -0.459471 avg loss no lamb -0.459471 time 2020-06-25 17:18:01.423122
Model ind 665 epoch 1 batch: 187 avg loss -0.768714 avg loss no lamb -0.768714 time 2020-06-25 17:18:01.556936
Model ind 665 epoch 1 batch: 188 avg loss -0.682711 avg loss no lamb -0.682711 time 2020-06-25 17:18:01.673852
Model ind 665 epoch 1 batch: 189 avg loss -0.767032 avg loss no lamb -0.767032 time 2020-06-25 17:18:01.791010
Model ind 665 epoch 1 batch: 190 avg loss -0.674025 avg loss no lamb -0.674025 time 2020-06-25 17:18:01.907271
Model ind 665 epoch 1 batch: 191 avg loss -0.724077 avg loss no lamb -0.724077 time 2020-06-25 17:18:02.024623
Model ind 665 epoch 1 batch: 192 avg loss -0.790707 avg loss no lamb -0.790707 time 2020-06-25 17:18:02.170856
Model ind 665 epoch 1 batch: 193 avg loss -0.847457 avg loss no lamb -0.847457 time 2020-06-25 17:18:02.288005
Model ind 665 epoch 1 batch: 194 avg loss -0.873989 avg loss no lamb -0.873989 time 2020-06-25 17:18:02.404467
Model ind 665 epoch 1 batch: 195 avg loss -0.707955 avg loss no lamb -0.707955 time 2020-06-25 17:18:02.521330
Model ind 665 epoch 1 batch: 196 avg loss -0.684821 avg loss no lamb -0.684821 time 2020-06-25 17:18:02.639310
Model ind 665 epoch 1 batch: 197 avg loss -0.723911 avg loss no lamb -0.723911 time 2020-06-25 17:18:02.757628
Model ind 665 epoch 1 batch: 198 avg loss -0.544332 avg loss no lamb -0.544332 time 2020-06-25 17:18:02.873026
Model ind 665 epoch 1 batch: 199 avg loss -0.675936 avg loss no lamb -0.675936 time 2020-06-25 17:18:02.988757
Model ind 665 epoch 1 batch: 200 avg loss -0.787712 avg loss no lamb -0.787712 time 2020-06-25 17:18:03.104363
Model ind 665 epoch 1 batch: 201 avg loss -0.746888 avg loss no lamb -0.746888 time 2020-06-25 17:18:03.234907
Model ind 665 epoch 1 batch: 202 avg loss -0.753856 avg loss no lamb -0.753856 time 2020-06-25 17:18:03.352062
Model ind 665 epoch 1 batch: 203 avg loss -0.883641 avg loss no lamb -0.883641 time 2020-06-25 17:18:03.467058
Model ind 665 epoch 1 batch: 204 avg loss -0.745801 avg loss no lamb -0.745801 time 2020-06-25 17:18:03.581330
Model ind 665 epoch 1 batch: 205 avg loss -0.789308 avg loss no lamb -0.789308 time 2020-06-25 17:18:03.695261
Model ind 665 epoch 1 batch: 206 avg loss -0.686781 avg loss no lamb -0.686781 time 2020-06-25 17:18:03.819883
Model ind 665 epoch 1 batch: 207 avg loss -0.726901 avg loss no lamb -0.726901 time 2020-06-25 17:18:03.936102
Model ind 665 epoch 1 batch: 208 avg loss -0.819185 avg loss no lamb -0.819185 time 2020-06-25 17:18:04.054493
Model ind 665 epoch 1 batch: 209 avg loss -0.768780 avg loss no lamb -0.768780 time 2020-06-25 17:18:04.183943
Model ind 665 epoch 1 batch: 210 avg loss -0.744552 avg loss no lamb -0.744552 time 2020-06-25 17:18:04.299740
Model ind 665 epoch 1 batch: 211 avg loss -0.706338 avg loss no lamb -0.706338 time 2020-06-25 17:18:04.415254
Model ind 665 epoch 1 batch: 212 avg loss -0.829763 avg loss no lamb -0.829763 time 2020-06-25 17:18:04.531153
Model ind 665 epoch 1 batch: 213 avg loss -0.769362 avg loss no lamb -0.769362 time 2020-06-25 17:18:04.649415
Model ind 665 epoch 1 batch: 214 avg loss -0.820055 avg loss no lamb -0.820055 time 2020-06-25 17:18:04.779017
Model ind 665 epoch 1 batch: 215 avg loss -0.790464 avg loss no lamb -0.790464 time 2020-06-25 17:18:04.893013
Model ind 665 epoch 1 batch: 216 avg loss -0.794771 avg loss no lamb -0.794771 time 2020-06-25 17:18:05.013957
Model ind 665 epoch 1 batch: 217 avg loss -0.774093 avg loss no lamb -0.774093 time 2020-06-25 17:18:05.128142
Model ind 665 epoch 1 batch: 218 avg loss -0.829350 avg loss no lamb -0.829350 time 2020-06-25 17:18:05.273537
Model ind 665 epoch 1 batch: 219 avg loss -0.797619 avg loss no lamb -0.797619 time 2020-06-25 17:18:05.390783
Model ind 665 epoch 1 batch: 220 avg loss -0.648814 avg loss no lamb -0.648814 time 2020-06-25 17:18:05.506929
Model ind 665 epoch 1 batch: 221 avg loss -0.766103 avg loss no lamb -0.766103 time 2020-06-25 17:18:05.658984
Model ind 665 epoch 1 batch: 222 avg loss -0.780868 avg loss no lamb -0.780868 time 2020-06-25 17:18:05.776400
Model ind 665 epoch 1 batch: 223 avg loss -0.737757 avg loss no lamb -0.737757 time 2020-06-25 17:18:05.890628
Model ind 665 epoch 1 batch: 224 avg loss -0.744810 avg loss no lamb -0.744810 time 2020-06-25 17:18:06.021690
Model ind 665 epoch 1 batch: 225 avg loss -0.746669 avg loss no lamb -0.746669 time 2020-06-25 17:18:06.163740
Model ind 665 epoch 1 batch: 226 avg loss -0.716849 avg loss no lamb -0.716849 time 2020-06-25 17:18:06.278460
Model ind 665 epoch 1 batch: 227 avg loss -0.717371 avg loss no lamb -0.717371 time 2020-06-25 17:18:06.435031
Model ind 665 epoch 1 batch: 228 avg loss -0.687259 avg loss no lamb -0.687259 time 2020-06-25 17:18:06.548957
Model ind 665 epoch 1 batch: 229 avg loss -0.732506 avg loss no lamb -0.732506 time 2020-06-25 17:18:06.675101
Model ind 665 epoch 1 batch: 230 avg loss -0.704233 avg loss no lamb -0.704233 time 2020-06-25 17:18:06.820794
Model ind 665 epoch 1 batch: 231 avg loss -0.914523 avg loss no lamb -0.914523 time 2020-06-25 17:18:06.934844
Model ind 665 epoch 1 batch: 232 avg loss -0.989410 avg loss no lamb -0.989410 time 2020-06-25 17:18:07.053717
Model ind 665 epoch 1 batch: 233 avg loss -0.894917 avg loss no lamb -0.894917 time 2020-06-25 17:18:07.167232
Model ind 665 epoch 1 batch: 234 avg loss -0.799093 avg loss no lamb -0.799093 time 2020-06-25 17:18:07.280526
Model ind 665 epoch 1 batch: 235 avg loss -0.918805 avg loss no lamb -0.918805 time 2020-06-25 17:18:07.394296
Model ind 665 epoch 1 batch: 236 avg loss -0.917188 avg loss no lamb -0.917188 time 2020-06-25 17:18:07.507647
Model ind 665 epoch 1 batch: 237 avg loss -0.889277 avg loss no lamb -0.889277 time 2020-06-25 17:18:07.635473
Model ind 665 epoch 1 batch: 238 avg loss -0.953789 avg loss no lamb -0.953789 time 2020-06-25 17:18:07.752097
Model ind 665 epoch 1 batch: 239 avg loss -0.668057 avg loss no lamb -0.668057 time 2020-06-25 17:18:07.868309
Model ind 665 epoch 1 batch: 240 avg loss -0.745861 avg loss no lamb -0.745861 time 2020-06-25 17:18:07.983899
Model ind 665 epoch 1 batch: 241 avg loss -0.872264 avg loss no lamb -0.872264 time 2020-06-25 17:18:08.100673
Model ind 665 epoch 1 batch: 242 avg loss -0.750452 avg loss no lamb -0.750452 time 2020-06-25 17:18:08.240749
Model ind 665 epoch 1 batch: 243 avg loss -0.881217 avg loss no lamb -0.881217 time 2020-06-25 17:18:08.396114
Model ind 665 epoch 1 batch: 244 avg loss -0.780323 avg loss no lamb -0.780323 time 2020-06-25 17:18:08.529172
Model ind 665 epoch 1 batch: 245 avg loss -0.801809 avg loss no lamb -0.801809 time 2020-06-25 17:18:08.643759
Model ind 665 epoch 1 batch: 246 avg loss -0.944945 avg loss no lamb -0.944945 time 2020-06-25 17:18:08.787308
Model ind 665 epoch 1 batch: 247 avg loss -0.848287 avg loss no lamb -0.848287 time 2020-06-25 17:18:08.932621
Model ind 665 epoch 1 batch: 248 avg loss -0.877784 avg loss no lamb -0.877784 time 2020-06-25 17:18:09.048238
Model ind 665 epoch 1 batch: 249 avg loss -0.845940 avg loss no lamb -0.845940 time 2020-06-25 17:18:09.168922
Model ind 665 epoch 1 batch: 250 avg loss -0.761553 avg loss no lamb -0.761553 time 2020-06-25 17:18:09.315808
Model ind 665 epoch 1 batch: 251 avg loss -0.855070 avg loss no lamb -0.855070 time 2020-06-25 17:18:09.473050
Model ind 665 epoch 1 batch: 252 avg loss -0.847447 avg loss no lamb -0.847447 time 2020-06-25 17:18:09.589546
Model ind 665 epoch 1 batch: 253 avg loss -0.872190 avg loss no lamb -0.872190 time 2020-06-25 17:18:09.706012
Model ind 665 epoch 1 batch: 254 avg loss -0.895738 avg loss no lamb -0.895738 time 2020-06-25 17:18:09.834736
Model ind 665 epoch 1 batch: 255 avg loss -0.900562 avg loss no lamb -0.900562 time 2020-06-25 17:18:09.955761
Model ind 665 epoch 1 batch: 256 avg loss -0.878613 avg loss no lamb -0.878613 time 2020-06-25 17:18:10.111933
Model ind 665 epoch 1 batch: 257 avg loss -0.839077 avg loss no lamb -0.839077 time 2020-06-25 17:18:10.227087
Model ind 665 epoch 1 batch: 258 avg loss -0.858730 avg loss no lamb -0.858730 time 2020-06-25 17:18:10.343739
Model ind 665 epoch 1 batch: 259 avg loss -0.867486 avg loss no lamb -0.867486 time 2020-06-25 17:18:10.458120
Model ind 665 epoch 1 batch: 260 avg loss -0.835701 avg loss no lamb -0.835701 time 2020-06-25 17:18:10.598363
Model ind 665 epoch 1 batch: 261 avg loss -0.893912 avg loss no lamb -0.893912 time 2020-06-25 17:18:10.713006
Model ind 665 epoch 1 batch: 262 avg loss -0.961098 avg loss no lamb -0.961098 time 2020-06-25 17:18:10.866674
Model ind 665 epoch 1 batch: 263 avg loss -0.922182 avg loss no lamb -0.922182 time 2020-06-25 17:18:10.981100
Model ind 665 epoch 1 batch: 264 avg loss -0.957119 avg loss no lamb -0.957119 time 2020-06-25 17:18:11.133323
Model ind 665 epoch 1 batch: 265 avg loss -0.832879 avg loss no lamb -0.832879 time 2020-06-25 17:18:11.272163
Model ind 665 epoch 1 batch: 266 avg loss -0.961417 avg loss no lamb -0.961417 time 2020-06-25 17:18:11.418236
Model ind 665 epoch 1 batch: 267 avg loss -0.921817 avg loss no lamb -0.921817 time 2020-06-25 17:18:11.574809
Model ind 665 epoch 1 batch: 268 avg loss -0.863408 avg loss no lamb -0.863408 time 2020-06-25 17:18:11.690370
Model ind 665 epoch 1 batch: 269 avg loss -0.870639 avg loss no lamb -0.870639 time 2020-06-25 17:18:11.847050
Model ind 665 epoch 1 batch: 270 avg loss -1.041603 avg loss no lamb -1.041603 time 2020-06-25 17:18:11.963008
Model ind 665 epoch 1 batch: 271 avg loss -0.887505 avg loss no lamb -0.887505 time 2020-06-25 17:18:12.120282
Model ind 665 epoch 1 batch: 272 avg loss -0.945595 avg loss no lamb -0.945595 time 2020-06-25 17:18:12.252592
Model ind 665 epoch 1 batch: 273 avg loss -0.848614 avg loss no lamb -0.848614 time 2020-06-25 17:18:12.368424
Model ind 665 epoch 1 batch: 274 avg loss -0.784362 avg loss no lamb -0.784362 time 2020-06-25 17:18:12.488130
Model ind 665 epoch 1 batch: 275 avg loss -0.954664 avg loss no lamb -0.954664 time 2020-06-25 17:18:12.604010
Model ind 665 epoch 1 batch: 276 avg loss -0.922481 avg loss no lamb -0.922481 time 2020-06-25 17:18:12.719072
Model ind 665 epoch 1 batch: 277 avg loss -1.081040 avg loss no lamb -1.081040 time 2020-06-25 17:18:12.835829
Model ind 665 epoch 1 batch: 278 avg loss -0.948576 avg loss no lamb -0.948576 time 2020-06-25 17:18:12.955074
Model ind 665 epoch 1 batch: 279 avg loss -0.858922 avg loss no lamb -0.858922 time 2020-06-25 17:18:13.070733
Model ind 665 epoch 1 batch: 280 avg loss -0.983734 avg loss no lamb -0.983734 time 2020-06-25 17:18:13.186225
Model ind 665 epoch 1 batch: 281 avg loss -1.045077 avg loss no lamb -1.045077 time 2020-06-25 17:18:13.302083
Model ind 665 epoch 1 batch: 282 avg loss -1.009382 avg loss no lamb -1.009382 time 2020-06-25 17:18:13.455637
Model ind 665 epoch 1 batch: 283 avg loss -0.953518 avg loss no lamb -0.953518 time 2020-06-25 17:18:13.571268
Model ind 665 epoch 1 batch: 284 avg loss -0.977398 avg loss no lamb -0.977398 time 2020-06-25 17:18:13.712464
Model ind 665 epoch 1 batch: 285 avg loss -0.971611 avg loss no lamb -0.971611 time 2020-06-25 17:18:13.829682
Model ind 665 epoch 1 batch: 286 avg loss -0.978518 avg loss no lamb -0.978518 time 2020-06-25 17:18:13.948372
Model ind 665 epoch 1 batch: 287 avg loss -1.028427 avg loss no lamb -1.028427 time 2020-06-25 17:18:14.064667
Model ind 665 epoch 1 batch: 288 avg loss -0.971424 avg loss no lamb -0.971424 time 2020-06-25 17:18:14.180443
Model ind 665 epoch 1 batch: 289 avg loss -1.007256 avg loss no lamb -1.007256 time 2020-06-25 17:18:14.343081
Model ind 665 epoch 1 batch: 290 avg loss -0.827538 avg loss no lamb -0.827538 time 2020-06-25 17:18:14.481332
Model ind 665 epoch 1 batch: 291 avg loss -1.010096 avg loss no lamb -1.010096 time 2020-06-25 17:18:14.617865
Model ind 665 epoch 1 batch: 292 avg loss -1.115050 avg loss no lamb -1.115050 time 2020-06-25 17:18:14.734643
Model ind 665 epoch 1 batch: 293 avg loss -0.832913 avg loss no lamb -0.832913 time 2020-06-25 17:18:14.850917
Model ind 665 epoch 1 batch: 294 avg loss -0.961211 avg loss no lamb -0.961211 time 2020-06-25 17:18:14.985990
Model ind 665 epoch 1 batch: 295 avg loss -0.987812 avg loss no lamb -0.987812 time 2020-06-25 17:18:15.101740
Model ind 665 epoch 1 batch: 296 avg loss -0.941830 avg loss no lamb -0.941830 time 2020-06-25 17:18:15.215617
Model ind 665 epoch 1 batch: 297 avg loss -0.931903 avg loss no lamb -0.931903 time 2020-06-25 17:18:15.331835
Model ind 665 epoch 1 batch: 298 avg loss -0.978968 avg loss no lamb -0.978968 time 2020-06-25 17:18:15.473572
Model ind 665 epoch 1 batch: 299 avg loss -0.913458 avg loss no lamb -0.913458 time 2020-06-25 17:18:15.606717
Model ind 665 epoch 1 batch: 300 avg loss -1.059939 avg loss no lamb -1.059939 time 2020-06-25 17:18:15.734678
Model ind 665 epoch 1 batch: 301 avg loss -1.049798 avg loss no lamb -1.049798 time 2020-06-25 17:18:15.851754
Model ind 665 epoch 1 batch: 302 avg loss -1.109792 avg loss no lamb -1.109792 time 2020-06-25 17:18:15.995412
Model ind 665 epoch 1 batch: 303 avg loss -1.040585 avg loss no lamb -1.040585 time 2020-06-25 17:18:16.110425
Model ind 665 epoch 1 batch: 304 avg loss -1.040298 avg loss no lamb -1.040298 time 2020-06-25 17:18:16.225304
Model ind 665 epoch 1 batch: 305 avg loss -1.188781 avg loss no lamb -1.188781 time 2020-06-25 17:18:16.342348
Model ind 665 epoch 1 batch: 306 avg loss -1.119963 avg loss no lamb -1.119963 time 2020-06-25 17:18:16.483093
Model ind 665 epoch 1 batch: 307 avg loss -1.006660 avg loss no lamb -1.006660 time 2020-06-25 17:18:16.598157
Model ind 665 epoch 1 batch: 308 avg loss -1.011439 avg loss no lamb -1.011439 time 2020-06-25 17:18:16.715540
Model ind 665 epoch 1 batch: 309 avg loss -0.987988 avg loss no lamb -0.987988 time 2020-06-25 17:18:16.831948
Model ind 665 epoch 1 batch: 310 avg loss -1.030558 avg loss no lamb -1.030558 time 2020-06-25 17:18:16.949884
Model ind 665 epoch 1 batch: 311 avg loss -1.100289 avg loss no lamb -1.100289 time 2020-06-25 17:18:17.094386
Model ind 665 epoch 1 batch: 312 avg loss -1.050474 avg loss no lamb -1.050474 time 2020-06-25 17:18:17.231161
Model ind 665 epoch 1 batch: 313 avg loss -1.127385 avg loss no lamb -1.127385 time 2020-06-25 17:18:17.372725
Model ind 665 epoch 1 batch: 314 avg loss -1.070451 avg loss no lamb -1.070451 time 2020-06-25 17:18:17.527755
Model ind 665 epoch 1 batch: 315 avg loss -1.036610 avg loss no lamb -1.036610 time 2020-06-25 17:18:17.658771
Model ind 665 epoch 1 batch: 316 avg loss -1.122830 avg loss no lamb -1.122830 time 2020-06-25 17:18:17.792272
Model ind 665 epoch 1 batch: 317 avg loss -0.977457 avg loss no lamb -0.977457 time 2020-06-25 17:18:17.908212
Model ind 665 epoch 1 batch: 318 avg loss -1.000513 avg loss no lamb -1.000513 time 2020-06-25 17:18:18.068128
Model ind 665 epoch 1 batch: 319 avg loss -1.063614 avg loss no lamb -1.063614 time 2020-06-25 17:18:18.215587
Model ind 665 epoch 1 batch: 320 avg loss -1.063946 avg loss no lamb -1.063946 time 2020-06-25 17:18:18.333193
Model ind 665 epoch 1 batch: 321 avg loss -0.926994 avg loss no lamb -0.926994 time 2020-06-25 17:18:18.467705
Model ind 665 epoch 1 batch: 322 avg loss -0.978835 avg loss no lamb -0.978835 time 2020-06-25 17:18:18.618498
Model ind 665 epoch 1 batch: 323 avg loss -1.006767 avg loss no lamb -1.006767 time 2020-06-25 17:18:18.736991
Model ind 665 epoch 1 batch: 324 avg loss -1.027459 avg loss no lamb -1.027459 time 2020-06-25 17:18:18.872180
Model ind 665 epoch 1 batch: 325 avg loss -1.009178 avg loss no lamb -1.009178 time 2020-06-25 17:18:18.988669
Model ind 665 epoch 1 batch: 326 avg loss -1.047095 avg loss no lamb -1.047095 time 2020-06-25 17:18:19.105286
Model ind 665 epoch 1 batch: 327 avg loss -1.092997 avg loss no lamb -1.092997 time 2020-06-25 17:18:19.234820
Model ind 665 epoch 1 batch: 328 avg loss -1.011578 avg loss no lamb -1.011578 time 2020-06-25 17:18:19.370689
Model ind 665 epoch 1 batch: 329 avg loss -0.997788 avg loss no lamb -0.997788 time 2020-06-25 17:18:19.486042
Model ind 665 epoch 1 batch: 330 avg loss -1.104047 avg loss no lamb -1.104047 time 2020-06-25 17:18:19.602200
Model ind 665 epoch 1 batch: 331 avg loss -1.094208 avg loss no lamb -1.094208 time 2020-06-25 17:18:19.718445
Model ind 665 epoch 1 batch: 332 avg loss -1.142705 avg loss no lamb -1.142705 time 2020-06-25 17:18:19.836097
Model ind 665 epoch 1 batch: 333 avg loss -1.021867 avg loss no lamb -1.021867 time 2020-06-25 17:18:19.953762
Model ind 665 epoch 1 batch: 334 avg loss -1.132928 avg loss no lamb -1.132928 time 2020-06-25 17:18:20.073432
Model ind 665 epoch 1 batch: 335 avg loss -1.114460 avg loss no lamb -1.114460 time 2020-06-25 17:18:20.215173
Model ind 665 epoch 1 batch: 336 avg loss -1.115153 avg loss no lamb -1.115153 time 2020-06-25 17:18:20.351160
Model ind 665 epoch 1 batch: 337 avg loss -0.932444 avg loss no lamb -0.932444 time 2020-06-25 17:18:20.484317
Model ind 665 epoch 1 batch: 338 avg loss -1.046149 avg loss no lamb -1.046149 time 2020-06-25 17:18:20.600177
Model ind 665 epoch 1 batch: 339 avg loss -1.098750 avg loss no lamb -1.098750 time 2020-06-25 17:18:20.715369
Model ind 665 epoch 1 batch: 340 avg loss -1.038364 avg loss no lamb -1.038364 time 2020-06-25 17:18:20.831157
Model ind 665 epoch 1 batch: 341 avg loss -1.060823 avg loss no lamb -1.060823 time 2020-06-25 17:18:20.950366
Model ind 665 epoch 1 batch: 342 avg loss -1.112232 avg loss no lamb -1.112232 time 2020-06-25 17:18:21.103417
Model ind 665 epoch 1 batch: 343 avg loss -0.945022 avg loss no lamb -0.945022 time 2020-06-25 17:18:21.218184
Model ind 665 epoch 1 batch: 344 avg loss -1.195183 avg loss no lamb -1.195183 time 2020-06-25 17:18:21.333250
Model ind 665 epoch 1 batch: 345 avg loss -1.132321 avg loss no lamb -1.132321 time 2020-06-25 17:18:21.452982
Model ind 665 epoch 1 batch: 346 avg loss -1.143411 avg loss no lamb -1.143411 time 2020-06-25 17:18:21.587228
Model ind 665 epoch 1 batch: 347 avg loss -1.215225 avg loss no lamb -1.215225 time 2020-06-25 17:18:21.702082
Model ind 665 epoch 1 batch: 348 avg loss -1.097958 avg loss no lamb -1.097958 time 2020-06-25 17:18:21.817461
Model ind 665 epoch 1 batch: 349 avg loss -1.065688 avg loss no lamb -1.065688 time 2020-06-25 17:18:21.932961
Model ind 665 epoch 1 batch: 350 avg loss -1.137033 avg loss no lamb -1.137033 time 2020-06-25 17:18:22.050471
Model ind 665 epoch 1 batch: 351 avg loss -1.171518 avg loss no lamb -1.171518 time 2020-06-25 17:18:22.165989
Model ind 665 epoch 1 batch: 352 avg loss -1.084223 avg loss no lamb -1.084223 time 2020-06-25 17:18:22.281051
Model ind 665 epoch 1 batch: 353 avg loss -1.085415 avg loss no lamb -1.085415 time 2020-06-25 17:18:22.395904
Model ind 665 epoch 1 batch: 354 avg loss -1.171912 avg loss no lamb -1.171912 time 2020-06-25 17:18:22.511490
Model ind 665 epoch 1 batch: 355 avg loss -1.199282 avg loss no lamb -1.199282 time 2020-06-25 17:18:22.627891
Model ind 665 epoch 1 batch: 356 avg loss -1.364685 avg loss no lamb -1.364685 time 2020-06-25 17:18:22.744839
Model ind 665 epoch 1 batch: 357 avg loss -1.321211 avg loss no lamb -1.321211 time 2020-06-25 17:18:22.866983
Model ind 665 epoch 1 batch: 358 avg loss -1.401436 avg loss no lamb -1.401436 time 2020-06-25 17:18:23.021596
Model ind 665 epoch 1 batch: 359 avg loss -1.203489 avg loss no lamb -1.203489 time 2020-06-25 17:18:23.138979
Model ind 665 epoch 1 batch: 360 avg loss -1.226006 avg loss no lamb -1.226006 time 2020-06-25 17:18:23.291084
Model ind 665 epoch 1 batch: 361 avg loss -1.185910 avg loss no lamb -1.185910 time 2020-06-25 17:18:23.429282
Model ind 665 epoch 1 batch: 362 avg loss -1.240578 avg loss no lamb -1.240578 time 2020-06-25 17:18:23.577169
Model ind 665 epoch 1 batch: 363 avg loss -1.357772 avg loss no lamb -1.357772 time 2020-06-25 17:18:23.736536
Model ind 665 epoch 1 batch: 364 avg loss -1.330384 avg loss no lamb -1.330384 time 2020-06-25 17:18:23.853714
Model ind 665 epoch 1 batch: 365 avg loss -1.264916 avg loss no lamb -1.264916 time 2020-06-25 17:18:23.970902
Model ind 665 epoch 1 batch: 366 avg loss -1.207546 avg loss no lamb -1.207546 time 2020-06-25 17:18:24.100455
Model ind 665 epoch 1 batch: 367 avg loss -1.269073 avg loss no lamb -1.269073 time 2020-06-25 17:18:24.216057
Model ind 665 epoch 1 batch: 368 avg loss -1.374476 avg loss no lamb -1.374476 time 2020-06-25 17:18:24.332758
Model ind 665 epoch 1 batch: 369 avg loss -1.339661 avg loss no lamb -1.339661 time 2020-06-25 17:18:24.449697
Model ind 665 epoch 1 batch: 370 avg loss -1.114104 avg loss no lamb -1.114104 time 2020-06-25 17:18:24.592792
Model ind 665 epoch 1 batch: 371 avg loss -1.230095 avg loss no lamb -1.230095 time 2020-06-25 17:18:24.705692
Model ind 665 epoch 1 batch: 372 avg loss -1.305256 avg loss no lamb -1.305256 time 2020-06-25 17:18:24.819476
Model ind 665 epoch 1 batch: 373 avg loss -1.338874 avg loss no lamb -1.338874 time 2020-06-25 17:18:24.935231
Model ind 665 epoch 1 batch: 374 avg loss -1.154993 avg loss no lamb -1.154993 time 2020-06-25 17:18:25.071839
Model ind 665 epoch 1 batch: 375 avg loss -1.280481 avg loss no lamb -1.280481 time 2020-06-25 17:18:25.185060
Model ind 665 epoch 1 batch: 376 avg loss -1.148920 avg loss no lamb -1.148920 time 2020-06-25 17:18:25.298450
Model ind 665 epoch 1 batch: 377 avg loss -1.324587 avg loss no lamb -1.324587 time 2020-06-25 17:18:25.424595
Model ind 665 epoch 1 batch: 378 avg loss -1.302538 avg loss no lamb -1.302538 time 2020-06-25 17:18:25.547982
Model ind 665 epoch 1 batch: 379 avg loss -1.365764 avg loss no lamb -1.365764 time 2020-06-25 17:18:25.676773
Model ind 665 epoch 1 batch: 380 avg loss -1.280796 avg loss no lamb -1.280796 time 2020-06-25 17:18:25.792447
Model ind 665 epoch 1 batch: 381 avg loss -1.213761 avg loss no lamb -1.213761 time 2020-06-25 17:18:25.920507
Model ind 665 epoch 1 batch: 382 avg loss -1.358964 avg loss no lamb -1.358964 time 2020-06-25 17:18:26.037987
Model ind 665 epoch 1 batch: 383 avg loss -1.264257 avg loss no lamb -1.264257 time 2020-06-25 17:18:26.154517
Model ind 665 epoch 1 batch: 384 avg loss -1.332987 avg loss no lamb -1.332987 time 2020-06-25 17:18:26.297964
Model ind 665 epoch 1 batch: 385 avg loss -1.335635 avg loss no lamb -1.335635 time 2020-06-25 17:18:26.437881
Model ind 665 epoch 1 batch: 386 avg loss -1.269866 avg loss no lamb -1.269866 time 2020-06-25 17:18:26.579213
Model ind 665 epoch 1 batch: 387 avg loss -1.174358 avg loss no lamb -1.174358 time 2020-06-25 17:18:26.735486
Model ind 665 epoch 1 batch: 388 avg loss -1.213125 avg loss no lamb -1.213125 time 2020-06-25 17:18:26.879336
Model ind 665 epoch 1 batch: 389 avg loss -1.284010 avg loss no lamb -1.284010 time 2020-06-25 17:18:26.995863
Model ind 665 epoch 1 batch: 390 avg loss -1.333889 avg loss no lamb -1.333889 time 2020-06-25 17:18:27.113577
Model ind 665 epoch 1 batch: 391 avg loss -1.221319 avg loss no lamb -1.221319 time 2020-06-25 17:18:27.236381
Model ind 665 epoch 1 batch: 392 avg loss -1.407845 avg loss no lamb -1.407845 time 2020-06-25 17:18:27.384574
Model ind 665 epoch 1 batch: 393 avg loss -1.328692 avg loss no lamb -1.328692 time 2020-06-25 17:18:27.525791
Model ind 665 epoch 1 batch: 394 avg loss -1.266063 avg loss no lamb -1.266063 time 2020-06-25 17:18:27.643706
Model ind 665 epoch 1 batch: 395 avg loss -1.458433 avg loss no lamb -1.458433 time 2020-06-25 17:18:27.760751
Model ind 665 epoch 1 batch: 396 avg loss -1.402012 avg loss no lamb -1.402012 time 2020-06-25 17:18:27.877209
Model ind 665 epoch 1 batch: 397 avg loss -1.231434 avg loss no lamb -1.231434 time 2020-06-25 17:18:27.993529
Model ind 665 epoch 1 batch: 398 avg loss -1.363831 avg loss no lamb -1.363831 time 2020-06-25 17:18:28.109495
Model ind 665 epoch 1 batch: 399 avg loss -1.315282 avg loss no lamb -1.315282 time 2020-06-25 17:18:28.253629
Model ind 665 epoch 1 batch: 400 avg loss -1.457583 avg loss no lamb -1.457583 time 2020-06-25 17:18:28.371267
Model ind 665 epoch 1 batch: 401 avg loss -1.423390 avg loss no lamb -1.423390 time 2020-06-25 17:18:28.487217
Model ind 665 epoch 1 batch: 402 avg loss -1.268249 avg loss no lamb -1.268249 time 2020-06-25 17:18:28.604337
Model ind 665 epoch 1 batch: 403 avg loss -1.338830 avg loss no lamb -1.338830 time 2020-06-25 17:18:28.752561
Model ind 665 epoch 1 batch: 404 avg loss -1.268942 avg loss no lamb -1.268942 time 2020-06-25 17:18:28.896056
Model ind 665 epoch 1 batch: 405 avg loss -1.265457 avg loss no lamb -1.265457 time 2020-06-25 17:18:29.040254
Model ind 665 epoch 1 batch: 406 avg loss -1.373588 avg loss no lamb -1.373588 time 2020-06-25 17:18:29.191000
Model ind 665 epoch 1 batch: 407 avg loss -1.291656 avg loss no lamb -1.291656 time 2020-06-25 17:18:29.329380
Model ind 665 epoch 1 batch: 408 avg loss -1.359829 avg loss no lamb -1.359829 time 2020-06-25 17:18:29.475317
Model ind 665 epoch 1 batch: 409 avg loss -1.274164 avg loss no lamb -1.274164 time 2020-06-25 17:18:29.589847
Model ind 665 epoch 1 batch: 410 avg loss -1.161194 avg loss no lamb -1.161194 time 2020-06-25 17:18:29.705299
Model ind 665 epoch 1 batch: 411 avg loss -1.421153 avg loss no lamb -1.421153 time 2020-06-25 17:18:29.831825
Model ind 665 epoch 1 batch: 412 avg loss -1.381549 avg loss no lamb -1.381549 time 2020-06-25 17:18:29.975195
Model ind 665 epoch 1 batch: 413 avg loss -1.274057 avg loss no lamb -1.274057 time 2020-06-25 17:18:30.124539
Model ind 665 epoch 1 batch: 414 avg loss -1.238820 avg loss no lamb -1.238820 time 2020-06-25 17:18:30.260166
Model ind 665 epoch 1 batch: 415 avg loss -1.268391 avg loss no lamb -1.268391 time 2020-06-25 17:18:30.377368
Model ind 665 epoch 1 batch: 416 avg loss -1.188014 avg loss no lamb -1.188014 time 2020-06-25 17:18:30.493454
Model ind 665 epoch 1 batch: 417 avg loss -1.245433 avg loss no lamb -1.245433 time 2020-06-25 17:18:30.629941
Model ind 665 epoch 1 batch: 418 avg loss -1.370098 avg loss no lamb -1.370098 time 2020-06-25 17:18:30.747017
Model ind 665 epoch 1 batch: 419 avg loss -1.304298 avg loss no lamb -1.304298 time 2020-06-25 17:18:30.884480
Model ind 665 epoch 1 batch: 420 avg loss -1.470150 avg loss no lamb -1.470150 time 2020-06-25 17:18:31.000615
Model ind 665 epoch 1 batch: 421 avg loss -1.385057 avg loss no lamb -1.385057 time 2020-06-25 17:18:31.115737
Model ind 665 epoch 1 batch: 422 avg loss -1.405185 avg loss no lamb -1.405185 time 2020-06-25 17:18:31.230428
Model ind 665 epoch 1 batch: 423 avg loss -1.392057 avg loss no lamb -1.392057 time 2020-06-25 17:18:31.347894
Model ind 665 epoch 1 batch: 424 avg loss -1.316478 avg loss no lamb -1.316478 time 2020-06-25 17:18:31.484343
Model ind 665 epoch 1 batch: 425 avg loss -1.312546 avg loss no lamb -1.312546 time 2020-06-25 17:18:31.599733
Model ind 665 epoch 1 batch: 426 avg loss -1.206497 avg loss no lamb -1.206497 time 2020-06-25 17:18:31.716106
Model ind 665 epoch 1 batch: 427 avg loss -1.358702 avg loss no lamb -1.358702 time 2020-06-25 17:18:31.834333
Model ind 665 epoch 1 batch: 428 avg loss -1.394825 avg loss no lamb -1.394825 time 2020-06-25 17:18:31.967739
Model ind 665 epoch 1 batch: 429 avg loss -1.196901 avg loss no lamb -1.196901 time 2020-06-25 17:18:32.083286
Model ind 665 epoch 1 batch: 430 avg loss -1.257362 avg loss no lamb -1.257362 time 2020-06-25 17:18:32.199311
Model ind 665 epoch 1 batch: 431 avg loss -1.465079 avg loss no lamb -1.465079 time 2020-06-25 17:18:32.315401
Model ind 665 epoch 1 batch: 432 avg loss -1.410483 avg loss no lamb -1.410483 time 2020-06-25 17:18:32.433314
Model ind 665 epoch 1 batch: 433 avg loss -1.318611 avg loss no lamb -1.318611 time 2020-06-25 17:18:32.567954
Model ind 665 epoch 1 batch: 434 avg loss -1.364228 avg loss no lamb -1.364228 time 2020-06-25 17:18:32.708539
Model ind 665 epoch 1 batch: 435 avg loss -1.246994 avg loss no lamb -1.246994 time 2020-06-25 17:18:32.824378
Model ind 665 epoch 1 batch: 436 avg loss -1.279385 avg loss no lamb -1.279385 time 2020-06-25 17:18:32.949770
Model ind 665 epoch 1 batch: 437 avg loss -1.328264 avg loss no lamb -1.328264 time 2020-06-25 17:18:33.079402
Model ind 665 epoch 1 batch: 438 avg loss -1.315331 avg loss no lamb -1.315331 time 2020-06-25 17:18:33.208601
Model ind 665 epoch 1 batch: 439 avg loss -1.472264 avg loss no lamb -1.472264 time 2020-06-25 17:18:33.334802
Model ind 665 epoch 1 batch: 440 avg loss -1.427828 avg loss no lamb -1.427828 time 2020-06-25 17:18:33.466387
Model ind 665 epoch 1 batch: 441 avg loss -1.485671 avg loss no lamb -1.485671 time 2020-06-25 17:18:33.580927
Model ind 665 epoch 1 batch: 442 avg loss -1.414112 avg loss no lamb -1.414112 time 2020-06-25 17:18:33.695025
Model ind 665 epoch 1 batch: 443 avg loss -1.489461 avg loss no lamb -1.489461 time 2020-06-25 17:18:33.819157
Model ind 665 epoch 1 batch: 444 avg loss -1.371933 avg loss no lamb -1.371933 time 2020-06-25 17:18:33.947442
Model ind 665 epoch 1 batch: 445 avg loss -1.456146 avg loss no lamb -1.456146 time 2020-06-25 17:18:34.063715
Model ind 665 epoch 1 batch: 446 avg loss -1.355383 avg loss no lamb -1.355383 time 2020-06-25 17:18:34.178861
Model ind 665 epoch 1 batch: 447 avg loss -1.353191 avg loss no lamb -1.353191 time 2020-06-25 17:18:34.294091
Model ind 665 epoch 1 batch: 448 avg loss -1.451068 avg loss no lamb -1.451068 time 2020-06-25 17:18:34.438931
Model ind 665 epoch 1 batch: 449 avg loss -1.463563 avg loss no lamb -1.463563 time 2020-06-25 17:18:34.565794
Model ind 665 epoch 1 batch: 450 avg loss -1.464279 avg loss no lamb -1.464279 time 2020-06-25 17:18:34.682180
Model ind 665 epoch 1 batch: 451 avg loss -1.357459 avg loss no lamb -1.357459 time 2020-06-25 17:18:34.797800
Model ind 665 epoch 1 batch: 452 avg loss -1.333719 avg loss no lamb -1.333719 time 2020-06-25 17:18:34.913605
Model ind 665 epoch 1 batch: 453 avg loss -1.408375 avg loss no lamb -1.408375 time 2020-06-25 17:18:35.030013
Model ind 665 epoch 1 batch: 454 avg loss -1.445480 avg loss no lamb -1.445480 time 2020-06-25 17:18:35.147077
Model ind 665 epoch 1 batch: 455 avg loss -1.678836 avg loss no lamb -1.678836 time 2020-06-25 17:18:35.295073
Model ind 665 epoch 1 batch: 456 avg loss -1.444723 avg loss no lamb -1.444723 time 2020-06-25 17:18:35.434022
Model ind 665 epoch 1 batch: 457 avg loss -1.499376 avg loss no lamb -1.499376 time 2020-06-25 17:18:35.548877
Model ind 665 epoch 1 batch: 458 avg loss -1.565602 avg loss no lamb -1.565602 time 2020-06-25 17:18:35.678831
Model ind 665 epoch 1 batch: 459 avg loss -1.443799 avg loss no lamb -1.443799 time 2020-06-25 17:18:35.792357
Model ind 665 epoch 1 batch: 460 avg loss -1.394725 avg loss no lamb -1.394725 time 2020-06-25 17:18:35.919306
Model ind 665 epoch 1 batch: 461 avg loss -1.536739 avg loss no lamb -1.536739 time 2020-06-25 17:18:36.032429
Model ind 665 epoch 1 batch: 462 avg loss -1.412969 avg loss no lamb -1.412969 time 2020-06-25 17:18:36.146702
Model ind 665 epoch 1 batch: 463 avg loss -1.392274 avg loss no lamb -1.392274 time 2020-06-25 17:18:36.281772
Model ind 665 epoch 1 batch: 464 avg loss -1.432574 avg loss no lamb -1.432574 time 2020-06-25 17:18:36.394810
Model ind 665 epoch 1 batch: 465 avg loss -1.559228 avg loss no lamb -1.559228 time 2020-06-25 17:18:36.508393
Model ind 665 epoch 1 batch: 466 avg loss -1.436476 avg loss no lamb -1.436476 time 2020-06-25 17:18:36.636497
Model ind 665 epoch 1 batch: 467 avg loss -1.429748 avg loss no lamb -1.429748 time 2020-06-25 17:18:36.754575
Model ind 665 epoch 1 batch: 468 avg loss -1.403742 avg loss no lamb -1.403742 time 2020-06-25 17:18:36.893646
Model ind 665 epoch 1 batch: 469 avg loss -1.514831 avg loss no lamb -1.514831 time 2020-06-25 17:18:37.019639
Model ind 665 epoch 1 batch: 470 avg loss -1.555132 avg loss no lamb -1.555132 time 2020-06-25 17:18:37.133779
Model ind 665 epoch 1 batch: 471 avg loss -1.499772 avg loss no lamb -1.499772 time 2020-06-25 17:18:37.247967
Model ind 665 epoch 1 batch: 472 avg loss -1.401348 avg loss no lamb -1.401348 time 2020-06-25 17:18:37.362248
Model ind 665 epoch 1 batch: 473 avg loss -1.467981 avg loss no lamb -1.467981 time 2020-06-25 17:18:37.475105
Model ind 665 epoch 1 batch: 474 avg loss -1.468823 avg loss no lamb -1.468823 time 2020-06-25 17:18:37.600790
Model ind 665 epoch 1 batch: 475 avg loss -1.579978 avg loss no lamb -1.579978 time 2020-06-25 17:18:37.715712
Model ind 665 epoch 1 batch: 476 avg loss -1.580125 avg loss no lamb -1.580125 time 2020-06-25 17:18:37.831469
Model ind 665 epoch 1 batch: 477 avg loss -1.532178 avg loss no lamb -1.532178 time 2020-06-25 17:18:37.948374
Model ind 665 epoch 1 batch: 478 avg loss -1.456418 avg loss no lamb -1.456418 time 2020-06-25 17:18:38.095332
Model ind 665 epoch 1 batch: 479 avg loss -1.513254 avg loss no lamb -1.513254 time 2020-06-25 17:18:38.243654
Model ind 665 epoch 1 batch: 480 avg loss -1.578007 avg loss no lamb -1.578007 time 2020-06-25 17:18:38.396436
Model ind 665 epoch 1 batch: 481 avg loss -1.568403 avg loss no lamb -1.568403 time 2020-06-25 17:18:38.544388
Model ind 665 epoch 1 batch: 482 avg loss -1.459735 avg loss no lamb -1.459735 time 2020-06-25 17:18:38.685961
Model ind 665 epoch 1 batch: 483 avg loss -1.653270 avg loss no lamb -1.653270 time 2020-06-25 17:18:38.832758
Model ind 665 epoch 1 batch: 484 avg loss -1.607646 avg loss no lamb -1.607646 time 2020-06-25 17:18:38.979173
Model ind 665 epoch 1 batch: 485 avg loss -1.616839 avg loss no lamb -1.616839 time 2020-06-25 17:18:39.094079
Model ind 665 epoch 1 batch: 486 avg loss -1.511949 avg loss no lamb -1.511949 time 2020-06-25 17:18:39.220246
Model ind 665 epoch 1 batch: 487 avg loss -1.479150 avg loss no lamb -1.479150 time 2020-06-25 17:18:39.334513
Model ind 665 epoch 1 batch: 488 avg loss -1.698765 avg loss no lamb -1.698765 time 2020-06-25 17:18:39.449577
Model ind 665 epoch 1 batch: 489 avg loss -1.738867 avg loss no lamb -1.738867 time 2020-06-25 17:18:39.563297
Model ind 665 epoch 1 batch: 490 avg loss -1.515500 avg loss no lamb -1.515500 time 2020-06-25 17:18:39.676147
Model ind 665 epoch 1 batch: 491 avg loss -1.512664 avg loss no lamb -1.512664 time 2020-06-25 17:18:39.789495
Model ind 665 epoch 1 batch: 492 avg loss -1.500251 avg loss no lamb -1.500251 time 2020-06-25 17:18:39.902066
Model ind 665 epoch 1 batch: 493 avg loss -1.536256 avg loss no lamb -1.536256 time 2020-06-25 17:18:40.014827
Model ind 665 epoch 1 batch: 494 avg loss -1.574557 avg loss no lamb -1.574557 time 2020-06-25 17:18:40.127547
Model ind 665 epoch 1 batch: 495 avg loss -1.503017 avg loss no lamb -1.503017 time 2020-06-25 17:18:40.262069
Model ind 665 epoch 1 batch: 496 avg loss -1.565954 avg loss no lamb -1.565954 time 2020-06-25 17:18:40.376127
Model ind 665 epoch 1 batch: 497 avg loss -1.464988 avg loss no lamb -1.464988 time 2020-06-25 17:18:40.500412
Model ind 665 epoch 1 batch: 498 avg loss -1.628979 avg loss no lamb -1.628979 time 2020-06-25 17:18:40.627756
Model ind 665 epoch 1 batch: 499 avg loss -1.628336 avg loss no lamb -1.628336 time 2020-06-25 17:18:40.754296
Model ind 665 epoch 1 batch: 500 avg loss -1.693161 avg loss no lamb -1.693161 time 2020-06-25 17:18:40.876632
Model ind 665 epoch 1 batch: 501 avg loss -1.517862 avg loss no lamb -1.517862 time 2020-06-25 17:18:40.995958
Model ind 665 epoch 1 batch: 502 avg loss -1.614509 avg loss no lamb -1.614509 time 2020-06-25 17:18:41.118938
Model ind 665 epoch 1 batch: 503 avg loss -1.600978 avg loss no lamb -1.600978 time 2020-06-25 17:18:41.241543
Model ind 665 epoch 1 batch: 504 avg loss -1.656597 avg loss no lamb -1.656597 time 2020-06-25 17:18:41.389480
Model ind 665 epoch 1 batch: 505 avg loss -1.634554 avg loss no lamb -1.634554 time 2020-06-25 17:18:41.537464
Model ind 665 epoch 1 batch: 506 avg loss -1.508150 avg loss no lamb -1.508150 time 2020-06-25 17:18:41.654749
Model ind 665 epoch 1 batch: 507 avg loss -1.644454 avg loss no lamb -1.644454 time 2020-06-25 17:18:41.770748
Model ind 665 epoch 1 batch: 508 avg loss -1.548166 avg loss no lamb -1.548166 time 2020-06-25 17:18:41.884093
Model ind 665 epoch 1 batch: 509 avg loss -1.464505 avg loss no lamb -1.464505 time 2020-06-25 17:18:42.000570
Model ind 665 epoch 1 batch: 510 avg loss -1.580570 avg loss no lamb -1.580570 time 2020-06-25 17:18:42.113408
Model ind 665 epoch 1 batch: 511 avg loss -1.776353 avg loss no lamb -1.776353 time 2020-06-25 17:18:42.228618
Model ind 665 epoch 1 batch: 512 avg loss -1.594912 avg loss no lamb -1.594912 time 2020-06-25 17:18:42.377389
Model ind 665 epoch 1 batch: 513 avg loss -1.496858 avg loss no lamb -1.496858 time 2020-06-25 17:18:42.519841
Model ind 665 epoch 1 batch: 514 avg loss -1.544481 avg loss no lamb -1.544481 time 2020-06-25 17:18:42.635480
Model ind 665 epoch 1 batch: 515 avg loss -1.585143 avg loss no lamb -1.585143 time 2020-06-25 17:18:42.778641
Model ind 665 epoch 1 batch: 516 avg loss -1.511190 avg loss no lamb -1.511190 time 2020-06-25 17:18:42.895128
Model ind 665 epoch 1 batch: 517 avg loss -1.499196 avg loss no lamb -1.499196 time 2020-06-25 17:18:43.023949
Model ind 665 epoch 1 batch: 518 avg loss -1.817057 avg loss no lamb -1.817057 time 2020-06-25 17:18:43.148104
Model ind 665 epoch 1 batch: 519 avg loss -1.643918 avg loss no lamb -1.643918 time 2020-06-25 17:18:43.279596
Model ind 665 epoch 1 batch: 520 avg loss -1.603653 avg loss no lamb -1.603653 time 2020-06-25 17:18:43.393519
Model ind 665 epoch 1 batch: 521 avg loss -1.599337 avg loss no lamb -1.599337 time 2020-06-25 17:18:43.519844
Model ind 665 epoch 1 batch: 522 avg loss -1.505878 avg loss no lamb -1.505878 time 2020-06-25 17:18:43.638184
Model ind 665 epoch 1 batch: 523 avg loss -1.646835 avg loss no lamb -1.646835 time 2020-06-25 17:18:43.753028
Model ind 665 epoch 1 batch: 524 avg loss -1.468825 avg loss no lamb -1.468825 time 2020-06-25 17:18:43.886883
Model ind 665 epoch 1 batch: 525 avg loss -1.597989 avg loss no lamb -1.597989 time 2020-06-25 17:18:44.000313
Model ind 665 epoch 1 batch: 526 avg loss -1.583259 avg loss no lamb -1.583259 time 2020-06-25 17:18:44.114904
Model ind 665 epoch 1 batch: 527 avg loss -1.669155 avg loss no lamb -1.669155 time 2020-06-25 17:18:44.229277
Model ind 665 epoch 1 batch: 528 avg loss -1.723646 avg loss no lamb -1.723646 time 2020-06-25 17:18:44.345657
Model ind 665 epoch 1 batch: 529 avg loss -1.502221 avg loss no lamb -1.502221 time 2020-06-25 17:18:44.477799
Model ind 665 epoch 1 batch: 530 avg loss -1.602727 avg loss no lamb -1.602727 time 2020-06-25 17:18:44.624259
Model ind 665 epoch 1 batch: 531 avg loss -1.646328 avg loss no lamb -1.646328 time 2020-06-25 17:18:44.775329
Model ind 665 epoch 1 batch: 532 avg loss -1.561711 avg loss no lamb -1.561711 time 2020-06-25 17:18:44.917922
Model ind 665 epoch 1 batch: 533 avg loss -1.612293 avg loss no lamb -1.612293 time 2020-06-25 17:18:45.062463
Model ind 665 epoch 1 batch: 534 avg loss -1.611336 avg loss no lamb -1.611336 time 2020-06-25 17:18:45.203747
Model ind 665 epoch 1 batch: 535 avg loss -1.587406 avg loss no lamb -1.587406 time 2020-06-25 17:18:45.347698
Model ind 665 epoch 1 batch: 536 avg loss -1.660256 avg loss no lamb -1.660256 time 2020-06-25 17:18:45.486610
Model ind 665 epoch 1 batch: 537 avg loss -1.602720 avg loss no lamb -1.602720 time 2020-06-25 17:18:45.603996
Model ind 665 epoch 1 batch: 538 avg loss -1.587421 avg loss no lamb -1.587421 time 2020-06-25 17:18:45.720229
Model ind 665 epoch 1 batch: 539 avg loss -1.694468 avg loss no lamb -1.694468 time 2020-06-25 17:18:45.843134
Model ind 665 epoch 1 batch: 540 avg loss -1.695187 avg loss no lamb -1.695187 time 2020-06-25 17:18:45.965461
Model ind 665 epoch 1 batch: 541 avg loss -1.677685 avg loss no lamb -1.677685 time 2020-06-25 17:18:46.091252
Model ind 665 epoch 1 batch: 542 avg loss -1.696763 avg loss no lamb -1.696763 time 2020-06-25 17:18:46.224957
Model ind 665 epoch 1 batch: 543 avg loss -1.773356 avg loss no lamb -1.773356 time 2020-06-25 17:18:46.352907
Model ind 665 epoch 1 batch: 544 avg loss -1.710941 avg loss no lamb -1.710941 time 2020-06-25 17:18:46.473901
Model ind 665 epoch 1 batch: 545 avg loss -1.731525 avg loss no lamb -1.731525 time 2020-06-25 17:18:46.615175
Model ind 665 epoch 1 batch: 546 avg loss -1.645103 avg loss no lamb -1.645103 time 2020-06-25 17:18:46.749491
Model ind 665 epoch 1 batch: 547 avg loss -1.644621 avg loss no lamb -1.644621 time 2020-06-25 17:18:46.877198
Model ind 665 epoch 1 batch: 548 avg loss -1.575153 avg loss no lamb -1.575153 time 2020-06-25 17:18:46.992806
Model ind 665 epoch 1 batch: 549 avg loss -1.726496 avg loss no lamb -1.726496 time 2020-06-25 17:18:47.108865
Model ind 665 epoch 1 batch: 550 avg loss -1.723941 avg loss no lamb -1.723941 time 2020-06-25 17:18:47.224728
Model ind 665 epoch 1 batch: 551 avg loss -1.780612 avg loss no lamb -1.780612 time 2020-06-25 17:18:47.370480
Model ind 665 epoch 1 batch: 552 avg loss -1.694992 avg loss no lamb -1.694992 time 2020-06-25 17:18:47.485509
Model ind 665 epoch 1 batch: 553 avg loss -1.749397 avg loss no lamb -1.749397 time 2020-06-25 17:18:47.599824
Model ind 665 epoch 1 batch: 554 avg loss -1.761371 avg loss no lamb -1.761371 time 2020-06-25 17:18:47.714213
Model ind 665 epoch 1 batch: 555 avg loss -1.707620 avg loss no lamb -1.707620 time 2020-06-25 17:18:47.827672
Model ind 665 epoch 1 batch: 556 avg loss -1.814340 avg loss no lamb -1.814340 time 2020-06-25 17:18:47.972857
Model ind 665 epoch 1 batch: 557 avg loss -1.652138 avg loss no lamb -1.652138 time 2020-06-25 17:18:48.087727
Model ind 665 epoch 1 batch: 558 avg loss -1.761371 avg loss no lamb -1.761371 time 2020-06-25 17:18:48.204236
Model ind 665 epoch 1 batch: 559 avg loss -1.707970 avg loss no lamb -1.707970 time 2020-06-25 17:18:48.319542
Model ind 665 epoch 1 batch: 560 avg loss -1.759317 avg loss no lamb -1.759317 time 2020-06-25 17:18:48.436787
Model ind 665 epoch 1 batch: 561 avg loss -1.564510 avg loss no lamb -1.564510 time 2020-06-25 17:18:48.554928
Model ind 665 epoch 1 batch: 562 avg loss -1.622624 avg loss no lamb -1.622624 time 2020-06-25 17:18:48.671367
Model ind 665 epoch 1 batch: 563 avg loss -1.657858 avg loss no lamb -1.657858 time 2020-06-25 17:18:48.787018
Model ind 665 epoch 1 batch: 564 avg loss -1.737146 avg loss no lamb -1.737146 time 2020-06-25 17:18:48.902938
Model ind 665 epoch 1 batch: 565 avg loss -1.813621 avg loss no lamb -1.813621 time 2020-06-25 17:18:49.021731
Model ind 665 epoch 1 batch: 566 avg loss -1.698772 avg loss no lamb -1.698772 time 2020-06-25 17:18:49.139235
Model ind 665 epoch 1 batch: 567 avg loss -1.515675 avg loss no lamb -1.515675 time 2020-06-25 17:18:49.264665
Model ind 665 epoch 1 batch: 568 avg loss -1.595284 avg loss no lamb -1.595284 time 2020-06-25 17:18:49.402574
Model ind 665 epoch 1 batch: 569 avg loss -1.642336 avg loss no lamb -1.642336 time 2020-06-25 17:18:49.551402
Model ind 665 epoch 1 batch: 570 avg loss -1.601457 avg loss no lamb -1.601457 time 2020-06-25 17:18:49.667127
Model ind 665 epoch 1 batch: 571 avg loss -1.737303 avg loss no lamb -1.737303 time 2020-06-25 17:18:49.793212
Model ind 665 epoch 1 batch: 572 avg loss -1.767142 avg loss no lamb -1.767142 time 2020-06-25 17:18:49.921680
Model ind 665 epoch 1 batch: 573 avg loss -1.745259 avg loss no lamb -1.745259 time 2020-06-25 17:18:50.048053
Model ind 665 epoch 1 batch: 574 avg loss -1.666947 avg loss no lamb -1.666947 time 2020-06-25 17:18:50.163187
Model ind 665 epoch 1 batch: 575 avg loss -1.650271 avg loss no lamb -1.650271 time 2020-06-25 17:18:50.305980
Model ind 665 epoch 1 batch: 576 avg loss -1.726783 avg loss no lamb -1.726783 time 2020-06-25 17:18:50.420358
Model ind 665 epoch 1 batch: 577 avg loss -1.724277 avg loss no lamb -1.724277 time 2020-06-25 17:18:50.536869
Model ind 665 epoch 1 batch: 578 avg loss -1.694125 avg loss no lamb -1.694125 time 2020-06-25 17:18:50.689766
Model ind 665 epoch 1 batch: 579 avg loss -1.616209 avg loss no lamb -1.616209 time 2020-06-25 17:18:50.804043
Model ind 665 epoch 1 batch: 580 avg loss -1.794086 avg loss no lamb -1.794086 time 2020-06-25 17:18:50.920106
Model ind 665 epoch 1 batch: 581 avg loss -1.744791 avg loss no lamb -1.744791 time 2020-06-25 17:18:51.036191
Model ind 665 epoch 1 batch: 582 avg loss -1.801319 avg loss no lamb -1.801319 time 2020-06-25 17:18:51.176924
Model ind 665 epoch 1 batch: 583 avg loss -1.628076 avg loss no lamb -1.628076 time 2020-06-25 17:18:51.293085
Model ind 665 epoch 1 batch: 584 avg loss -1.785533 avg loss no lamb -1.785533 time 2020-06-25 17:18:51.417291
Model ind 665 epoch 1 batch: 585 avg loss -1.729479 avg loss no lamb -1.729479 time 2020-06-25 17:18:51.533449
Model ind 665 epoch 1 batch: 586 avg loss -1.758701 avg loss no lamb -1.758701 time 2020-06-25 17:18:51.651325
Model ind 665 epoch 1 batch: 587 avg loss -1.720955 avg loss no lamb -1.720955 time 2020-06-25 17:18:51.789711
Model ind 665 epoch 1 batch: 588 avg loss -1.653227 avg loss no lamb -1.653227 time 2020-06-25 17:18:51.929204
Model ind 665 epoch 1 batch: 589 avg loss -1.616700 avg loss no lamb -1.616700 time 2020-06-25 17:18:52.067990
Model ind 665 epoch 1 batch: 590 avg loss -1.684208 avg loss no lamb -1.684208 time 2020-06-25 17:18:52.224992
Model ind 665 epoch 1 batch: 591 avg loss -1.628050 avg loss no lamb -1.628050 time 2020-06-25 17:18:52.368972
Model ind 665 epoch 1 batch: 592 avg loss -1.615430 avg loss no lamb -1.615430 time 2020-06-25 17:18:52.492882
Model ind 665 epoch 1 batch: 593 avg loss -1.684879 avg loss no lamb -1.684879 time 2020-06-25 17:18:52.650909
Model ind 665 epoch 1 batch: 594 avg loss -1.665796 avg loss no lamb -1.665796 time 2020-06-25 17:18:52.777734
Model ind 665 epoch 1 batch: 595 avg loss -1.751278 avg loss no lamb -1.751278 time 2020-06-25 17:18:52.894028
Model ind 665 epoch 1 batch: 596 avg loss -1.634625 avg loss no lamb -1.634625 time 2020-06-25 17:18:53.038083
Model ind 665 epoch 1 batch: 597 avg loss -1.665712 avg loss no lamb -1.665712 time 2020-06-25 17:18:53.155030
Model ind 665 epoch 1 batch: 598 avg loss -1.631042 avg loss no lamb -1.631042 time 2020-06-25 17:18:53.269187
Model ind 665 epoch 1 batch: 599 avg loss -1.729973 avg loss no lamb -1.729973 time 2020-06-25 17:18:53.382344
Model ind 665 epoch 1 batch: 600 avg loss -1.671203 avg loss no lamb -1.671203 time 2020-06-25 17:18:53.496174
Model ind 665 epoch 1 batch: 601 avg loss -1.746486 avg loss no lamb -1.746486 time 2020-06-25 17:18:53.641715
Model ind 665 epoch 1 batch: 602 avg loss -1.752065 avg loss no lamb -1.752065 time 2020-06-25 17:18:53.755102
Model ind 665 epoch 1 batch: 603 avg loss -1.751314 avg loss no lamb -1.751314 time 2020-06-25 17:18:53.869193
Model ind 665 epoch 1 batch: 604 avg loss -1.751952 avg loss no lamb -1.751952 time 2020-06-25 17:18:53.981909
Model ind 665 epoch 1 batch: 605 avg loss -1.700696 avg loss no lamb -1.700696 time 2020-06-25 17:18:54.096187
Model ind 665 epoch 1 batch: 606 avg loss -1.621721 avg loss no lamb -1.621721 time 2020-06-25 17:18:54.247487
Model ind 665 epoch 1 batch: 607 avg loss -1.584643 avg loss no lamb -1.584643 time 2020-06-25 17:18:54.394750
Model ind 665 epoch 1 batch: 608 avg loss -1.795548 avg loss no lamb -1.795548 time 2020-06-25 17:18:54.540107
Model ind 665 epoch 1 batch: 609 avg loss -1.621046 avg loss no lamb -1.621046 time 2020-06-25 17:18:54.673952
Model ind 665 epoch 1 batch: 610 avg loss -1.774548 avg loss no lamb -1.774548 time 2020-06-25 17:18:54.801731
Model ind 665 epoch 1 batch: 611 avg loss -1.627562 avg loss no lamb -1.627562 time 2020-06-25 17:18:54.931699
Model ind 665 epoch 1 batch: 612 avg loss -1.641969 avg loss no lamb -1.641969 time 2020-06-25 17:18:55.067365
Model ind 665 epoch 1 batch: 613 avg loss -1.627795 avg loss no lamb -1.627795 time 2020-06-25 17:18:55.185424
Model ind 665 epoch 1 batch: 614 avg loss -1.715066 avg loss no lamb -1.715066 time 2020-06-25 17:18:55.325005
Model ind 665 epoch 1 batch: 615 avg loss -1.697008 avg loss no lamb -1.697008 time 2020-06-25 17:18:55.462467
Model ind 665 epoch 1 batch: 616 avg loss -1.864159 avg loss no lamb -1.864159 time 2020-06-25 17:18:55.601063
Model ind 665 epoch 1 batch: 617 avg loss -1.672958 avg loss no lamb -1.672958 time 2020-06-25 17:18:55.753196
Model ind 665 epoch 1 batch: 618 avg loss -1.856655 avg loss no lamb -1.856655 time 2020-06-25 17:18:55.875408
Model ind 665 epoch 1 batch: 619 avg loss -1.814088 avg loss no lamb -1.814088 time 2020-06-25 17:18:56.017238
Model ind 665 epoch 1 batch: 620 avg loss -1.846757 avg loss no lamb -1.846757 time 2020-06-25 17:18:56.132616
Model ind 665 epoch 1 batch: 621 avg loss -1.792031 avg loss no lamb -1.792031 time 2020-06-25 17:18:56.252882
Model ind 665 epoch 1 batch: 622 avg loss -1.852278 avg loss no lamb -1.852278 time 2020-06-25 17:18:56.407426
Model ind 665 epoch 1 batch: 623 avg loss -1.789805 avg loss no lamb -1.789805 time 2020-06-25 17:18:56.535315
Model ind 665 epoch 1 batch: 624 avg loss -1.879811 avg loss no lamb -1.879811 time 2020-06-25 17:18:56.659103
Model ind 665 epoch 1 batch: 625 avg loss -1.720658 avg loss no lamb -1.720658 time 2020-06-25 17:18:56.813748
Model ind 665 epoch 1 batch: 626 avg loss -1.773573 avg loss no lamb -1.773573 time 2020-06-25 17:18:56.938942
Model ind 665 epoch 1 batch: 627 avg loss -1.739463 avg loss no lamb -1.739463 time 2020-06-25 17:18:57.066609
Model ind 665 epoch 1 batch: 628 avg loss -1.695515 avg loss no lamb -1.695515 time 2020-06-25 17:18:57.180278
Model ind 665 epoch 1 batch: 629 avg loss -1.788948 avg loss no lamb -1.788948 time 2020-06-25 17:18:57.293557
Model ind 665 epoch 1 batch: 630 avg loss -1.650208 avg loss no lamb -1.650208 time 2020-06-25 17:18:57.426255
Model ind 665 epoch 1 batch: 631 avg loss -1.804035 avg loss no lamb -1.804035 time 2020-06-25 17:18:57.576650
Model ind 665 epoch 1 batch: 632 avg loss -1.787621 avg loss no lamb -1.787621 time 2020-06-25 17:18:57.691348
Model ind 665 epoch 1 batch: 633 avg loss -1.784570 avg loss no lamb -1.784570 time 2020-06-25 17:18:57.827164
Model ind 665 epoch 1 batch: 634 avg loss -1.728101 avg loss no lamb -1.728101 time 2020-06-25 17:18:57.971647
Model ind 665 epoch 1 batch: 635 avg loss -1.905343 avg loss no lamb -1.905343 time 2020-06-25 17:18:58.085862
Model ind 665 epoch 1 batch: 636 avg loss -1.850479 avg loss no lamb -1.850479 time 2020-06-25 17:18:58.200668
Model ind 665 epoch 1 batch: 637 avg loss -1.848170 avg loss no lamb -1.848170 time 2020-06-25 17:18:58.315201
Model ind 665 epoch 1 batch: 638 avg loss -1.883887 avg loss no lamb -1.883887 time 2020-06-25 17:18:58.429532
Model ind 665 epoch 1 batch: 639 avg loss -1.658838 avg loss no lamb -1.658838 time 2020-06-25 17:18:58.564485
Model ind 665 epoch 1 batch: 640 avg loss -1.721729 avg loss no lamb -1.721729 time 2020-06-25 17:18:58.695953
Model ind 665 epoch 1 batch: 641 avg loss -1.709514 avg loss no lamb -1.709514 time 2020-06-25 17:18:58.818242
Model ind 665 epoch 1 batch: 642 avg loss -1.757282 avg loss no lamb -1.757282 time 2020-06-25 17:18:58.934017
Model ind 665 epoch 1 batch: 643 avg loss -1.778462 avg loss no lamb -1.778462 time 2020-06-25 17:18:59.077766
Model ind 665 epoch 1 batch: 644 avg loss -1.781736 avg loss no lamb -1.781736 time 2020-06-25 17:18:59.193645
Model ind 665 epoch 1 batch: 645 avg loss -1.826617 avg loss no lamb -1.826617 time 2020-06-25 17:18:59.309141
Model ind 665 epoch 1 batch: 646 avg loss -1.806327 avg loss no lamb -1.806327 time 2020-06-25 17:18:59.423479
Model ind 665 epoch 1 batch: 647 avg loss -1.813903 avg loss no lamb -1.813903 time 2020-06-25 17:18:59.553265
Model ind 665 epoch 1 batch: 648 avg loss -1.787706 avg loss no lamb -1.787706 time 2020-06-25 17:18:59.676438
Model ind 665 epoch 1 batch: 649 avg loss -1.702520 avg loss no lamb -1.702520 time 2020-06-25 17:18:59.791256
Model ind 665 epoch 1 batch: 650 avg loss -1.746788 avg loss no lamb -1.746788 time 2020-06-25 17:18:59.906479
Model ind 665 epoch 1 batch: 651 avg loss -1.795160 avg loss no lamb -1.795160 time 2020-06-25 17:19:00.022341
Model ind 665 epoch 1 batch: 652 avg loss -1.782265 avg loss no lamb -1.782265 time 2020-06-25 17:19:00.137745
Model ind 665 epoch 1 batch: 653 avg loss -1.699757 avg loss no lamb -1.699757 time 2020-06-25 17:19:00.294744
Model ind 665 epoch 1 batch: 654 avg loss -1.732971 avg loss no lamb -1.732971 time 2020-06-25 17:19:00.423129
Model ind 665 epoch 1 batch: 655 avg loss -1.620998 avg loss no lamb -1.620998 time 2020-06-25 17:19:00.553831
Model ind 665 epoch 1 batch: 656 avg loss -1.706977 avg loss no lamb -1.706977 time 2020-06-25 17:19:00.674035
Model ind 665 epoch 1 batch: 657 avg loss -1.673755 avg loss no lamb -1.673755 time 2020-06-25 17:19:00.790284
Model ind 665 epoch 1 batch: 658 avg loss -1.886611 avg loss no lamb -1.886611 time 2020-06-25 17:19:00.905732
Model ind 665 epoch 1 batch: 659 avg loss -1.805514 avg loss no lamb -1.805514 time 2020-06-25 17:19:01.060926
Model ind 665 epoch 1 batch: 660 avg loss -1.705419 avg loss no lamb -1.705419 time 2020-06-25 17:19:01.205604
Model ind 665 epoch 1 batch: 661 avg loss -1.728090 avg loss no lamb -1.728090 time 2020-06-25 17:19:01.359025
Model ind 665 epoch 1 batch: 662 avg loss -1.768027 avg loss no lamb -1.768027 time 2020-06-25 17:19:01.473353
Model ind 665 epoch 1 batch: 663 avg loss -1.787276 avg loss no lamb -1.787276 time 2020-06-25 17:19:01.586819
Model ind 665 epoch 1 batch: 664 avg loss -1.900406 avg loss no lamb -1.900406 time 2020-06-25 17:19:01.701877
Model ind 665 epoch 1 batch: 665 avg loss -1.783601 avg loss no lamb -1.783601 time 2020-06-25 17:19:01.816095
Model ind 665 epoch 1 batch: 666 avg loss -1.791299 avg loss no lamb -1.791299 time 2020-06-25 17:19:01.935059
Model ind 665 epoch 1 batch: 667 avg loss -1.828819 avg loss no lamb -1.828819 time 2020-06-25 17:19:02.052071
Model ind 665 epoch 1 batch: 668 avg loss -1.796246 avg loss no lamb -1.796246 time 2020-06-25 17:19:02.201001
Model ind 665 epoch 1 batch: 669 avg loss -1.801232 avg loss no lamb -1.801232 time 2020-06-25 17:19:02.314402
Model ind 665 epoch 1 batch: 670 avg loss -1.831586 avg loss no lamb -1.831586 time 2020-06-25 17:19:02.427345
Model ind 665 epoch 1 batch: 671 avg loss -1.822314 avg loss no lamb -1.822314 time 2020-06-25 17:19:02.542720
Model ind 665 epoch 1 batch: 672 avg loss -1.801211 avg loss no lamb -1.801211 time 2020-06-25 17:19:02.665635
Model ind 665 epoch 1 batch: 673 avg loss -1.808555 avg loss no lamb -1.808555 time 2020-06-25 17:19:02.780399
Model ind 665 epoch 1 batch: 674 avg loss -1.791622 avg loss no lamb -1.791622 time 2020-06-25 17:19:02.894080
Model ind 665 epoch 1 batch: 675 avg loss -1.734270 avg loss no lamb -1.734270 time 2020-06-25 17:19:03.016186
Model ind 665 epoch 1 batch: 676 avg loss -1.840379 avg loss no lamb -1.840379 time 2020-06-25 17:19:03.132566
Model ind 665 epoch 1 batch: 677 avg loss -1.725818 avg loss no lamb -1.725818 time 2020-06-25 17:19:03.254772
Model ind 665 epoch 1 batch: 678 avg loss -1.639409 avg loss no lamb -1.639409 time 2020-06-25 17:19:03.404326
Model ind 665 epoch 1 batch: 679 avg loss -1.749566 avg loss no lamb -1.749566 time 2020-06-25 17:19:03.523636
Model ind 665 epoch 1 batch: 680 avg loss -1.711005 avg loss no lamb -1.711005 time 2020-06-25 17:19:03.641737
Model ind 665 epoch 1 batch: 681 avg loss -1.959927 avg loss no lamb -1.959927 time 2020-06-25 17:19:03.773614
Model ind 665 epoch 1 batch: 682 avg loss -1.967678 avg loss no lamb -1.967678 time 2020-06-25 17:19:03.893751
Model ind 665 epoch 1 batch: 683 avg loss -1.857941 avg loss no lamb -1.857941 time 2020-06-25 17:19:04.040119
Model ind 665 epoch 1 batch: 684 avg loss -1.861720 avg loss no lamb -1.861720 time 2020-06-25 17:19:04.165405
Model ind 665 epoch 1 batch: 685 avg loss -1.831796 avg loss no lamb -1.831796 time 2020-06-25 17:19:04.294648
Model ind 665 epoch 1 batch: 686 avg loss -1.895803 avg loss no lamb -1.895803 time 2020-06-25 17:19:04.430242
Model ind 665 epoch 1 batch: 687 avg loss -1.831951 avg loss no lamb -1.831951 time 2020-06-25 17:19:04.550858
Model ind 665 epoch 1 batch: 688 avg loss -1.791918 avg loss no lamb -1.791918 time 2020-06-25 17:19:04.685921
Model ind 665 epoch 1 batch: 689 avg loss -1.839654 avg loss no lamb -1.839654 time 2020-06-25 17:19:04.799610
Model ind 665 epoch 1 batch: 690 avg loss -1.974620 avg loss no lamb -1.974620 time 2020-06-25 17:19:04.923561
Model ind 665 epoch 1 batch: 691 avg loss -1.816917 avg loss no lamb -1.816917 time 2020-06-25 17:19:05.037224
Model ind 665 epoch 1 batch: 692 avg loss -1.840244 avg loss no lamb -1.840244 time 2020-06-25 17:19:05.173789
Model ind 665 epoch 1 batch: 693 avg loss -1.673708 avg loss no lamb -1.673708 time 2020-06-25 17:19:05.320812
Model ind 665 epoch 1 batch: 694 avg loss -1.759280 avg loss no lamb -1.759280 time 2020-06-25 17:19:05.451004
Model ind 665 epoch 1 batch: 695 avg loss -1.955950 avg loss no lamb -1.955950 time 2020-06-25 17:19:05.569985
Model ind 665 epoch 1 batch: 696 avg loss -1.894218 avg loss no lamb -1.894218 time 2020-06-25 17:19:05.685208
Model ind 665 epoch 1 batch: 697 avg loss -1.923875 avg loss no lamb -1.923875 time 2020-06-25 17:19:05.800681
Model ind 665 epoch 1 batch: 698 avg loss -1.709682 avg loss no lamb -1.709682 time 2020-06-25 17:19:05.915867
Model ind 665 epoch 1 batch: 699 avg loss -1.753720 avg loss no lamb -1.753720 time 2020-06-25 17:19:06.031004
Model ind 665 epoch 1 batch: 700 avg loss -1.710740 avg loss no lamb -1.710740 time 2020-06-25 17:19:06.148285
Model ind 665 epoch 1 batch: 701 avg loss -1.808661 avg loss no lamb -1.808661 time 2020-06-25 17:19:06.284862
Model ind 665 epoch 1 batch: 702 avg loss -1.762449 avg loss no lamb -1.762449 time 2020-06-25 17:19:06.402417
Model ind 665 epoch 1 batch: 703 avg loss -1.728075 avg loss no lamb -1.728075 time 2020-06-25 17:19:06.522117
Model ind 665 epoch 1 batch: 704 avg loss -1.897389 avg loss no lamb -1.897389 time 2020-06-25 17:19:06.637562
Model ind 665 epoch 1 batch: 705 avg loss -1.891216 avg loss no lamb -1.891216 time 2020-06-25 17:19:06.765031
Model ind 665 epoch 1 batch: 706 avg loss -1.685247 avg loss no lamb -1.685247 time 2020-06-25 17:19:06.879024
Model ind 665 epoch 1 batch: 707 avg loss -1.561696 avg loss no lamb -1.561696 time 2020-06-25 17:19:06.992857
Model ind 665 epoch 1 batch: 708 avg loss -1.802500 avg loss no lamb -1.802500 time 2020-06-25 17:19:07.106544
Model ind 665 epoch 1 batch: 709 avg loss -1.573915 avg loss no lamb -1.573915 time 2020-06-25 17:19:07.220298
Model ind 665 epoch 1 batch: 710 avg loss -1.789383 avg loss no lamb -1.789383 time 2020-06-25 17:19:07.334174
Model ind 665 epoch 1 batch: 711 avg loss -1.631903 avg loss no lamb -1.631903 time 2020-06-25 17:19:07.452847
Model ind 665 epoch 1 batch: 712 avg loss -1.841037 avg loss no lamb -1.841037 time 2020-06-25 17:19:07.586010
Model ind 665 epoch 1 batch: 713 avg loss -1.803407 avg loss no lamb -1.803407 time 2020-06-25 17:19:07.698998
Model ind 665 epoch 1 batch: 714 avg loss -1.823332 avg loss no lamb -1.823332 time 2020-06-25 17:19:07.820922
Model ind 665 epoch 1 batch: 715 avg loss -1.779305 avg loss no lamb -1.779305 time 2020-06-25 17:19:07.936029
Model ind 665 epoch 1 batch: 716 avg loss -1.816001 avg loss no lamb -1.816001 time 2020-06-25 17:19:08.051609
Model ind 665 epoch 1 batch: 717 avg loss -1.887791 avg loss no lamb -1.887791 time 2020-06-25 17:19:08.167158
Model ind 665 epoch 1 batch: 718 avg loss -1.769276 avg loss no lamb -1.769276 time 2020-06-25 17:19:08.281308
Model ind 665 epoch 1 batch: 719 avg loss -1.652602 avg loss no lamb -1.652602 time 2020-06-25 17:19:08.394075
Model ind 665 epoch 1 batch: 720 avg loss -1.701198 avg loss no lamb -1.701198 time 2020-06-25 17:19:08.507679
Model ind 665 epoch 1 batch: 721 avg loss -1.698917 avg loss no lamb -1.698917 time 2020-06-25 17:19:08.622280
Model ind 665 epoch 1 batch: 722 avg loss -1.858324 avg loss no lamb -1.858324 time 2020-06-25 17:19:08.736782
Model ind 665 epoch 1 batch: 723 avg loss -1.776795 avg loss no lamb -1.776795 time 2020-06-25 17:19:08.851879
Model ind 665 epoch 1 batch: 724 avg loss -1.695917 avg loss no lamb -1.695917 time 2020-06-25 17:19:08.970342
Model ind 665 epoch 1 batch: 725 avg loss -1.849650 avg loss no lamb -1.849650 time 2020-06-25 17:19:09.083763
Model ind 665 epoch 1 batch: 726 avg loss -1.736517 avg loss no lamb -1.736517 time 2020-06-25 17:19:09.197156
Model ind 665 epoch 1 batch: 727 avg loss -1.715822 avg loss no lamb -1.715822 time 2020-06-25 17:19:09.310682
Model ind 665 epoch 1 batch: 728 avg loss -1.952083 avg loss no lamb -1.952083 time 2020-06-25 17:19:09.425129
Model ind 665 epoch 1 batch: 729 avg loss -1.869365 avg loss no lamb -1.869365 time 2020-06-25 17:19:09.551575
Model ind 665 epoch 1 batch: 730 avg loss -1.868290 avg loss no lamb -1.868290 time 2020-06-25 17:19:09.679877
Model ind 665 epoch 1 batch: 731 avg loss -1.752727 avg loss no lamb -1.752727 time 2020-06-25 17:19:09.794113
Model ind 665 epoch 1 batch: 732 avg loss -1.749725 avg loss no lamb -1.749725 time 2020-06-25 17:19:09.908392
Model ind 665 epoch 1 batch: 733 avg loss -1.853028 avg loss no lamb -1.853028 time 2020-06-25 17:19:10.022729
Model ind 665 epoch 1 batch: 734 avg loss -1.885416 avg loss no lamb -1.885416 time 2020-06-25 17:19:10.139394
Model ind 665 epoch 1 batch: 735 avg loss -1.806841 avg loss no lamb -1.806841 time 2020-06-25 17:19:10.257205
Model ind 665 epoch 1 batch: 736 avg loss -1.842332 avg loss no lamb -1.842332 time 2020-06-25 17:19:10.400933
Model ind 665 epoch 1 batch: 737 avg loss -1.870069 avg loss no lamb -1.870069 time 2020-06-25 17:19:10.514397
Model ind 665 epoch 1 batch: 738 avg loss -1.827763 avg loss no lamb -1.827763 time 2020-06-25 17:19:10.628159
Model ind 665 epoch 1 batch: 739 avg loss -1.839630 avg loss no lamb -1.839630 time 2020-06-25 17:19:10.742604
Model ind 665 epoch 1 batch: 740 avg loss -1.870165 avg loss no lamb -1.870165 time 2020-06-25 17:19:10.869878
Model ind 665 epoch 1 batch: 741 avg loss -1.899291 avg loss no lamb -1.899291 time 2020-06-25 17:19:10.991718
Model ind 665 epoch 1 batch: 742 avg loss -1.788884 avg loss no lamb -1.788884 time 2020-06-25 17:19:11.115284
Model ind 665 epoch 1 batch: 743 avg loss -1.940086 avg loss no lamb -1.940086 time 2020-06-25 17:19:11.242743
Model ind 665 epoch 1 batch: 744 avg loss -1.607915 avg loss no lamb -1.607915 time 2020-06-25 17:19:11.371215
Model ind 665 epoch 1 batch: 745 avg loss -1.766003 avg loss no lamb -1.766003 time 2020-06-25 17:19:11.485251
Model ind 665 epoch 1 batch: 746 avg loss -1.822273 avg loss no lamb -1.822273 time 2020-06-25 17:19:11.598484
Model ind 665 epoch 1 batch: 747 avg loss -2.019607 avg loss no lamb -2.019607 time 2020-06-25 17:19:11.711678
Model ind 665 epoch 1 batch: 748 avg loss -1.856264 avg loss no lamb -1.856264 time 2020-06-25 17:19:11.825051
Model ind 665 epoch 1 batch: 749 avg loss -1.940316 avg loss no lamb -1.940316 time 2020-06-25 17:19:11.938600
Model ind 665 epoch 1 batch: 750 avg loss -1.891155 avg loss no lamb -1.891155 time 2020-06-25 17:19:12.056302
Model ind 665 epoch 1 batch: 751 avg loss -1.884490 avg loss no lamb -1.884490 time 2020-06-25 17:19:12.185878
Model ind 665 epoch 1 batch: 752 avg loss -1.881733 avg loss no lamb -1.881733 time 2020-06-25 17:19:12.302658
Model ind 665 epoch 1 batch: 753 avg loss -1.787718 avg loss no lamb -1.787718 time 2020-06-25 17:19:12.415940
Model ind 665 epoch 1 batch: 754 avg loss -1.733298 avg loss no lamb -1.733298 time 2020-06-25 17:19:12.529510
Model ind 665 epoch 1 batch: 755 avg loss -1.554538 avg loss no lamb -1.554538 time 2020-06-25 17:19:12.644175
Model ind 665 epoch 1 batch: 756 avg loss -1.821652 avg loss no lamb -1.821652 time 2020-06-25 17:19:12.760070
Model ind 665 epoch 1 batch: 757 avg loss -1.851634 avg loss no lamb -1.851634 time 2020-06-25 17:19:12.873879
Model ind 665 epoch 1 batch: 758 avg loss -1.788398 avg loss no lamb -1.788398 time 2020-06-25 17:19:12.986805
Model ind 665 epoch 1 batch: 759 avg loss -1.832345 avg loss no lamb -1.832345 time 2020-06-25 17:19:13.099820
Model ind 665 epoch 1 batch: 760 avg loss -1.818858 avg loss no lamb -1.818858 time 2020-06-25 17:19:13.222357
Model ind 665 epoch 1 batch: 761 avg loss -1.949918 avg loss no lamb -1.949918 time 2020-06-25 17:19:13.335745
Model ind 665 epoch 1 batch: 762 avg loss -1.757129 avg loss no lamb -1.757129 time 2020-06-25 17:19:13.472098
Model ind 665 epoch 1 batch: 763 avg loss -1.947175 avg loss no lamb -1.947175 time 2020-06-25 17:19:13.587533
Model ind 665 epoch 1 batch: 764 avg loss -2.018415 avg loss no lamb -2.018415 time 2020-06-25 17:19:13.700382
Model ind 665 epoch 1 batch: 765 avg loss -1.862019 avg loss no lamb -1.862019 time 2020-06-25 17:19:13.826817
Model ind 665 epoch 1 batch: 766 avg loss -1.931652 avg loss no lamb -1.931652 time 2020-06-25 17:19:13.941850
Model ind 665 epoch 1 batch: 767 avg loss -1.759167 avg loss no lamb -1.759167 time 2020-06-25 17:19:14.089718
Model ind 665 epoch 1 batch: 768 avg loss -1.871283 avg loss no lamb -1.871283 time 2020-06-25 17:19:14.203033
Model ind 665 epoch 1 batch: 769 avg loss -1.984282 avg loss no lamb -1.984282 time 2020-06-25 17:19:14.323477
Model ind 665 epoch 1 batch: 770 avg loss -1.870687 avg loss no lamb -1.870687 time 2020-06-25 17:19:14.437986
Model ind 665 epoch 1 batch: 771 avg loss -1.617494 avg loss no lamb -1.617494 time 2020-06-25 17:19:14.577455
Model ind 665 epoch 1 batch: 772 avg loss -1.828956 avg loss no lamb -1.828956 time 2020-06-25 17:19:14.694021
Model ind 665 epoch 1 batch: 773 avg loss -2.033535 avg loss no lamb -2.033535 time 2020-06-25 17:19:14.814393
Model ind 665 epoch 1 batch: 774 avg loss -1.884262 avg loss no lamb -1.884262 time 2020-06-25 17:19:14.929924
Model ind 665 epoch 1 batch: 775 avg loss -1.940328 avg loss no lamb -1.940328 time 2020-06-25 17:19:15.047564
Model ind 665 epoch 1 batch: 776 avg loss -1.890838 avg loss no lamb -1.890838 time 2020-06-25 17:19:15.161893
Model ind 665 epoch 1 batch: 777 avg loss -1.936940 avg loss no lamb -1.936940 time 2020-06-25 17:19:15.279691
Model ind 665 epoch 1 batch: 778 avg loss -1.909242 avg loss no lamb -1.909242 time 2020-06-25 17:19:15.431261
Model ind 665 epoch 1 batch: 779 avg loss -1.812606 avg loss no lamb -1.812606 time 2020-06-25 17:19:15.547935
Model ind 665 epoch 1 batch: 780 avg loss -2.046767 avg loss no lamb -2.046767 time 2020-06-25 17:19:15.681951
Model ind 665 epoch 1 batch: 781 avg loss -1.879023 avg loss no lamb -1.879023 time 2020-06-25 17:19:15.794954
Model ind 665 epoch 1 batch: 782 avg loss -1.803640 avg loss no lamb -1.803640 time 2020-06-25 17:19:15.908741
Model ind 665 epoch 1 batch: 783 avg loss -1.810716 avg loss no lamb -1.810716 time 2020-06-25 17:19:16.026995
Model ind 665 epoch 1 batch: 784 avg loss -1.646890 avg loss no lamb -1.646890 time 2020-06-25 17:19:16.154892
Model ind 665 epoch 1 batch: 785 avg loss -1.827375 avg loss no lamb -1.827375 time 2020-06-25 17:19:16.273344
Model ind 665 epoch 1 batch: 786 avg loss -1.952605 avg loss no lamb -1.952605 time 2020-06-25 17:19:16.414188
Model ind 665 epoch 1 batch: 787 avg loss -1.965803 avg loss no lamb -1.965803 time 2020-06-25 17:19:16.542287
Model ind 665 epoch 1 batch: 788 avg loss -1.922188 avg loss no lamb -1.922188 time 2020-06-25 17:19:16.662447
Model ind 665 epoch 1 batch: 789 avg loss -1.913623 avg loss no lamb -1.913623 time 2020-06-25 17:19:16.778829
Model ind 665 epoch 1 batch: 790 avg loss -1.988047 avg loss no lamb -1.988047 time 2020-06-25 17:19:16.894759
Model ind 665 epoch 1 batch: 791 avg loss -1.780990 avg loss no lamb -1.780990 time 2020-06-25 17:19:17.010353
Model ind 665 epoch 1 batch: 792 avg loss -1.913129 avg loss no lamb -1.913129 time 2020-06-25 17:19:17.125979
Model ind 665 epoch 1 batch: 793 avg loss -1.833478 avg loss no lamb -1.833478 time 2020-06-25 17:19:17.269071
Model ind 665 epoch 1 batch: 794 avg loss -1.868149 avg loss no lamb -1.868149 time 2020-06-25 17:19:17.385166
Model ind 665 epoch 1 batch: 795 avg loss -1.922780 avg loss no lamb -1.922780 time 2020-06-25 17:19:17.500875
Model ind 665 epoch 1 batch: 796 avg loss -1.787208 avg loss no lamb -1.787208 time 2020-06-25 17:19:17.617157
Model ind 665 epoch 1 batch: 797 avg loss -1.982831 avg loss no lamb -1.982831 time 2020-06-25 17:19:17.734180
Model ind 665 epoch 1 batch: 798 avg loss -1.626603 avg loss no lamb -1.626603 time 2020-06-25 17:19:17.875994
Model ind 665 epoch 1 batch: 799 avg loss -1.896400 avg loss no lamb -1.896400 time 2020-06-25 17:19:17.991130
Model ind 665 epoch 1 batch: 800 avg loss -1.978021 avg loss no lamb -1.978021 time 2020-06-25 17:19:18.106026
Model ind 665 epoch 1 batch: 801 avg loss -1.880819 avg loss no lamb -1.880819 time 2020-06-25 17:19:18.220681
Model ind 665 epoch 1 batch: 802 avg loss -1.902943 avg loss no lamb -1.902943 time 2020-06-25 17:19:18.337842
Model ind 665 epoch 1 batch: 803 avg loss -1.787469 avg loss no lamb -1.787469 time 2020-06-25 17:19:18.456607
Model ind 665 epoch 1 batch: 804 avg loss -1.807597 avg loss no lamb -1.807597 time 2020-06-25 17:19:18.600962
Model ind 665 epoch 1 batch: 805 avg loss -1.831978 avg loss no lamb -1.831978 time 2020-06-25 17:19:18.714541
Model ind 665 epoch 1 batch: 806 avg loss -1.793186 avg loss no lamb -1.793186 time 2020-06-25 17:19:18.827990
Model ind 665 epoch 1 batch: 807 avg loss -1.796808 avg loss no lamb -1.796808 time 2020-06-25 17:19:18.971365
Model ind 665 epoch 1 batch: 808 avg loss -2.012288 avg loss no lamb -2.012288 time 2020-06-25 17:19:19.084736
Model ind 665 epoch 1 batch: 809 avg loss -1.868836 avg loss no lamb -1.868836 time 2020-06-25 17:19:19.198044
Model ind 665 epoch 1 batch: 810 avg loss -2.036428 avg loss no lamb -2.036428 time 2020-06-25 17:19:19.311313
Model ind 665 epoch 1 batch: 811 avg loss -1.878605 avg loss no lamb -1.878605 time 2020-06-25 17:19:19.426036
Model ind 665 epoch 1 batch: 812 avg loss -1.908445 avg loss no lamb -1.908445 time 2020-06-25 17:19:19.551790
Model ind 665 epoch 1 batch: 813 avg loss -1.935558 avg loss no lamb -1.935558 time 2020-06-25 17:19:19.668814
Model ind 665 epoch 1 batch: 814 avg loss -1.929788 avg loss no lamb -1.929788 time 2020-06-25 17:19:19.784591
Model ind 665 epoch 1 batch: 815 avg loss -1.895906 avg loss no lamb -1.895906 time 2020-06-25 17:19:19.900752
Model ind 665 epoch 1 batch: 816 avg loss -1.963621 avg loss no lamb -1.963621 time 2020-06-25 17:19:20.016919
Model ind 665 epoch 1 batch: 817 avg loss -1.862402 avg loss no lamb -1.862402 time 2020-06-25 17:19:20.132887
Model ind 665 epoch 1 batch: 818 avg loss -1.697453 avg loss no lamb -1.697453 time 2020-06-25 17:19:20.251349
Model ind 665 epoch 1 batch: 819 avg loss -1.796872 avg loss no lamb -1.796872 time 2020-06-25 17:19:20.401155
Model ind 665 epoch 1 batch: 820 avg loss -1.872772 avg loss no lamb -1.872772 time 2020-06-25 17:19:20.517539
Model ind 665 epoch 1 batch: 821 avg loss -1.839895 avg loss no lamb -1.839895 time 2020-06-25 17:19:20.633935
Model ind 665 epoch 1 batch: 822 avg loss -1.924741 avg loss no lamb -1.924741 time 2020-06-25 17:19:20.769917
Model ind 665 epoch 1 batch: 823 avg loss -1.902040 avg loss no lamb -1.902040 time 2020-06-25 17:19:20.885614
Model ind 665 epoch 1 batch: 824 avg loss -1.809093 avg loss no lamb -1.809093 time 2020-06-25 17:19:21.001378
Model ind 665 epoch 1 batch: 825 avg loss -1.856281 avg loss no lamb -1.856281 time 2020-06-25 17:19:21.117134
Model ind 665 epoch 1 batch: 826 avg loss -1.867112 avg loss no lamb -1.867112 time 2020-06-25 17:19:21.233805
Model ind 665 epoch 1 batch: 827 avg loss -1.800346 avg loss no lamb -1.800346 time 2020-06-25 17:19:21.351868
Model ind 665 epoch 1 batch: 828 avg loss -1.910576 avg loss no lamb -1.910576 time 2020-06-25 17:19:21.479302
Model ind 665 epoch 1 batch: 829 avg loss -1.955880 avg loss no lamb -1.955880 time 2020-06-25 17:19:21.593627
Model ind 665 epoch 1 batch: 830 avg loss -1.961490 avg loss no lamb -1.961490 time 2020-06-25 17:19:21.708639
Model ind 665 epoch 1 batch: 831 avg loss -1.985433 avg loss no lamb -1.985433 time 2020-06-25 17:19:21.823550
Model ind 665 epoch 1 batch: 832 avg loss -1.975730 avg loss no lamb -1.975730 time 2020-06-25 17:19:21.953194
Model ind 665 epoch 1 batch: 833 avg loss -1.890513 avg loss no lamb -1.890513 time 2020-06-25 17:19:22.078648
Model ind 665 epoch 1 batch: 834 avg loss -1.859525 avg loss no lamb -1.859525 time 2020-06-25 17:19:22.193513
Model ind 665 epoch 1 batch: 835 avg loss -1.944144 avg loss no lamb -1.944144 time 2020-06-25 17:19:22.307919
Model ind 665 epoch 1 batch: 836 avg loss -1.936134 avg loss no lamb -1.936134 time 2020-06-25 17:19:22.449272
Model ind 665 epoch 1 batch: 837 avg loss -2.012840 avg loss no lamb -2.012840 time 2020-06-25 17:19:22.577534
Model ind 665 epoch 1 batch: 838 avg loss -2.079194 avg loss no lamb -2.079194 time 2020-06-25 17:19:22.692709
Model ind 665 epoch 1 batch: 839 avg loss -2.039218 avg loss no lamb -2.039218 time 2020-06-25 17:19:22.820465
Model ind 665 epoch 1 batch: 840 avg loss -1.786030 avg loss no lamb -1.786030 time 2020-06-25 17:19:22.934609
Model ind 665 epoch 1 batch: 841 avg loss -1.984326 avg loss no lamb -1.984326 time 2020-06-25 17:19:23.050037
Model ind 665 epoch 1 batch: 842 avg loss -2.052966 avg loss no lamb -2.052966 time 2020-06-25 17:19:23.196685
Model ind 665 epoch 1 batch: 843 avg loss -2.050293 avg loss no lamb -2.050293 time 2020-06-25 17:19:23.309977
Model ind 665 epoch 1 batch: 844 avg loss -2.031633 avg loss no lamb -2.031633 time 2020-06-25 17:19:23.423844
Model ind 665 epoch 1 batch: 845 avg loss -2.023856 avg loss no lamb -2.023856 time 2020-06-25 17:19:23.538902
Model ind 665 epoch 1 batch: 846 avg loss -1.918106 avg loss no lamb -1.918106 time 2020-06-25 17:19:23.680895
Model ind 665 epoch 1 batch: 847 avg loss -1.471457 avg loss no lamb -1.471457 time 2020-06-25 17:19:23.794650
Model ind 665 epoch 1 batch: 848 avg loss -1.650954 avg loss no lamb -1.650954 time 2020-06-25 17:19:23.907659
Model ind 665 epoch 1 batch: 849 avg loss -1.828828 avg loss no lamb -1.828828 time 2020-06-25 17:19:24.020999
Model ind 665 epoch 1 batch: 850 avg loss -2.005124 avg loss no lamb -2.005124 time 2020-06-25 17:19:24.150018
Model ind 665 epoch 1 batch: 851 avg loss -1.901846 avg loss no lamb -1.901846 time 2020-06-25 17:19:24.301984
Model ind 665 epoch 1 batch: 852 avg loss -1.953778 avg loss no lamb -1.953778 time 2020-06-25 17:19:24.417072
Model ind 665 epoch 1 batch: 853 avg loss -1.755430 avg loss no lamb -1.755430 time 2020-06-25 17:19:24.534319
Model ind 665 epoch 1 batch: 854 avg loss -1.951508 avg loss no lamb -1.951508 time 2020-06-25 17:19:24.652853
Model ind 665 epoch 1 batch: 855 avg loss -1.941186 avg loss no lamb -1.941186 time 2020-06-25 17:19:24.781036
Model ind 665 epoch 1 batch: 856 avg loss -1.895807 avg loss no lamb -1.895807 time 2020-06-25 17:19:24.894770
last batch sz 10
Model ind 665 epoch 1 batch: 857 avg loss -1.854441 avg loss no lamb -1.854441 time 2020-06-25 17:19:24.965638
Pre: time 2020-06-25 17:19:33.388164: 
 	std: 0.006002861
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 1), (13, 4), (14, 7), (15, 6), (16, 0), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9234, 0.9259, 0.9105, 0.9173, 0.9262]
	train_accs: [0.91541666, 0.92153335, 0.90458333, 0.9109833, 0.9205]
	best_train_sub_head: 1
	worst: 0.9105
	avg: 0.92066
	best: 0.9259

Starting e_i: 2
Model ind 665 epoch 2 batch: 0 avg loss -1.968922 avg loss no lamb -1.968922 time 2020-06-25 17:19:34.231144
Model ind 665 epoch 2 batch: 100 avg loss -1.937040 avg loss no lamb -1.937040 time 2020-06-25 17:19:46.071672
Model ind 665 epoch 2 batch: 200 avg loss -1.959545 avg loss no lamb -1.959545 time 2020-06-25 17:19:58.019484
Model ind 665 epoch 2 batch: 300 avg loss -1.948143 avg loss no lamb -1.948143 time 2020-06-25 17:20:10.150363
Model ind 665 epoch 2 batch: 400 avg loss -2.080060 avg loss no lamb -2.080060 time 2020-06-25 17:20:22.019388
Model ind 665 epoch 2 batch: 500 avg loss -2.121753 avg loss no lamb -2.121753 time 2020-06-25 17:20:33.936785
Model ind 665 epoch 2 batch: 600 avg loss -2.126575 avg loss no lamb -2.126575 time 2020-06-25 17:20:45.835661
Model ind 665 epoch 2 batch: 700 avg loss -2.159685 avg loss no lamb -2.159685 time 2020-06-25 17:20:58.231684
Model ind 665 epoch 2 batch: 800 avg loss -2.125723 avg loss no lamb -2.125723 time 2020-06-25 17:21:10.418781
last batch sz 10
Pre: time 2020-06-25 17:21:25.888399: 
 	std: 0.0048263506
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 9), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 0), (22, 2), (23, 5), (24, 2)]
	test_accs: [0.9689, 0.9567, 0.9598, 0.9568, 0.9652]
	train_accs: [0.9622167, 0.95485, 0.95375, 0.95196664, 0.96056664]
	best_train_sub_head: 0
	worst: 0.9567
	avg: 0.96148
	best: 0.9689

Starting e_i: 3
Model ind 665 epoch 3 batch: 0 avg loss -2.133204 avg loss no lamb -2.133204 time 2020-06-25 17:21:27.792656
Model ind 665 epoch 3 batch: 100 avg loss -2.136066 avg loss no lamb -2.136066 time 2020-06-25 17:21:39.853581
Model ind 665 epoch 3 batch: 200 avg loss -2.179389 avg loss no lamb -2.179389 time 2020-06-25 17:21:51.950815
Model ind 665 epoch 3 batch: 300 avg loss -2.219223 avg loss no lamb -2.219223 time 2020-06-25 17:22:04.023022
Model ind 665 epoch 3 batch: 400 avg loss -2.216083 avg loss no lamb -2.216083 time 2020-06-25 17:22:15.999907
Model ind 665 epoch 3 batch: 500 avg loss -2.209604 avg loss no lamb -2.209604 time 2020-06-25 17:22:28.095777
Model ind 665 epoch 3 batch: 600 avg loss -2.275970 avg loss no lamb -2.275970 time 2020-06-25 17:22:40.136323
Model ind 665 epoch 3 batch: 700 avg loss -2.148932 avg loss no lamb -2.148932 time 2020-06-25 17:22:52.392563
Model ind 665 epoch 3 batch: 800 avg loss -2.244649 avg loss no lamb -2.244649 time 2020-06-25 17:23:04.691447
last batch sz 10
Pre: time 2020-06-25 17:23:20.678279: 
 	std: 0.0036471372
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 6), (3, 9), (4, 4), (5, 8), (6, 1), (7, 9), (8, 2), (9, 3), (10, 7), (11, 2), (12, 6), (13, 3), (14, 7), (15, 0), (16, 3), (17, 0), (18, 4), (19, 0), (20, 5), (21, 5), (22, 7), (23, 0), (24, 8)]
	test_accs: [0.9704, 0.9661, 0.9669, 0.9607, 0.9708]
	train_accs: [0.96431667, 0.9608833, 0.96138334, 0.9550167, 0.9656]
	best_train_sub_head: 4
	worst: 0.9607
	avg: 0.96698
	best: 0.9708

Starting e_i: 4
Model ind 665 epoch 4 batch: 0 avg loss -2.285059 avg loss no lamb -2.285059 time 2020-06-25 17:23:22.573209
Model ind 665 epoch 4 batch: 100 avg loss -2.205630 avg loss no lamb -2.205630 time 2020-06-25 17:23:35.198231
Model ind 665 epoch 4 batch: 200 avg loss -2.250954 avg loss no lamb -2.250954 time 2020-06-25 17:23:47.216424
Model ind 665 epoch 4 batch: 300 avg loss -2.226696 avg loss no lamb -2.226696 time 2020-06-25 17:23:59.456280
Model ind 665 epoch 4 batch: 400 avg loss -2.330084 avg loss no lamb -2.330084 time 2020-06-25 17:24:11.588435
Model ind 665 epoch 4 batch: 500 avg loss -2.284495 avg loss no lamb -2.284495 time 2020-06-25 17:24:23.657180
Model ind 665 epoch 4 batch: 600 avg loss -2.260202 avg loss no lamb -2.260202 time 2020-06-25 17:24:35.831585
Model ind 665 epoch 4 batch: 700 avg loss -2.201490 avg loss no lamb -2.201490 time 2020-06-25 17:24:47.886003
Model ind 665 epoch 4 batch: 800 avg loss -2.249289 avg loss no lamb -2.249289 time 2020-06-25 17:24:59.882690
last batch sz 10
Pre: time 2020-06-25 17:25:15.199731: 
 	std: 0.0043522036
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 2), (22, 2), (23, 5), (24, 2)]
	test_accs: [0.9697, 0.9648, 0.969, 0.9589, 0.9707]
	train_accs: [0.96495, 0.9605833, 0.96388334, 0.95391667, 0.96428335]
	best_train_sub_head: 0
	worst: 0.9589
	avg: 0.9666201
	best: 0.9697

Starting e_i: 5
Model ind 665 epoch 5 batch: 0 avg loss -2.327214 avg loss no lamb -2.327214 time 2020-06-25 17:25:15.921655
Model ind 665 epoch 5 batch: 100 avg loss -2.258594 avg loss no lamb -2.258594 time 2020-06-25 17:25:28.191485
Model ind 665 epoch 5 batch: 200 avg loss -2.342544 avg loss no lamb -2.342544 time 2020-06-25 17:25:40.351097
Model ind 665 epoch 5 batch: 300 avg loss -2.312047 avg loss no lamb -2.312047 time 2020-06-25 17:25:52.415698
Model ind 665 epoch 5 batch: 400 avg loss -2.363978 avg loss no lamb -2.363978 time 2020-06-25 17:26:04.504888
Model ind 665 epoch 5 batch: 500 avg loss -2.365760 avg loss no lamb -2.365760 time 2020-06-25 17:26:16.535744
Model ind 665 epoch 5 batch: 600 avg loss -2.543256 avg loss no lamb -2.543256 time 2020-06-25 17:26:28.796188
Model ind 665 epoch 5 batch: 700 avg loss -2.193608 avg loss no lamb -2.193608 time 2020-06-25 17:26:40.913609
Model ind 665 epoch 5 batch: 800 avg loss -2.405486 avg loss no lamb -2.405486 time 2020-06-25 17:26:52.933278
last batch sz 10
Pre: time 2020-06-25 17:27:08.300477: 
 	std: 0.0036389013
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 2), (23, 5), (24, 2)]
	test_accs: [0.9729, 0.9713, 0.9673, 0.9625, 0.9671]
	train_accs: [0.96756667, 0.9651667, 0.96491665, 0.95923334, 0.9644833]
	best_train_sub_head: 0
	worst: 0.9625
	avg: 0.96822006
	best: 0.9729

Starting e_i: 6
Model ind 665 epoch 6 batch: 0 avg loss -2.372363 avg loss no lamb -2.372363 time 2020-06-25 17:27:10.184505
Model ind 665 epoch 6 batch: 100 avg loss -2.291576 avg loss no lamb -2.291576 time 2020-06-25 17:27:22.879920
Model ind 665 epoch 6 batch: 200 avg loss -2.366874 avg loss no lamb -2.366874 time 2020-06-25 17:27:36.062534
Model ind 665 epoch 6 batch: 300 avg loss -2.382779 avg loss no lamb -2.382779 time 2020-06-25 17:27:49.903037
Model ind 665 epoch 6 batch: 400 avg loss -2.239700 avg loss no lamb -2.239700 time 2020-06-25 17:28:03.229228
Model ind 665 epoch 6 batch: 500 avg loss -2.403227 avg loss no lamb -2.403227 time 2020-06-25 17:28:17.286278
Model ind 665 epoch 6 batch: 600 avg loss -2.434174 avg loss no lamb -2.434174 time 2020-06-25 17:28:31.176783
Model ind 665 epoch 6 batch: 700 avg loss -2.244772 avg loss no lamb -2.244772 time 2020-06-25 17:28:45.347953
Model ind 665 epoch 6 batch: 800 avg loss -2.401237 avg loss no lamb -2.401237 time 2020-06-25 17:28:59.281303
last batch sz 10
Pre: time 2020-06-25 17:29:15.609457: 
 	std: 0.0035227155
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 6), (23, 5), (24, 2)]
	test_accs: [0.9758, 0.9688, 0.9712, 0.9652, 0.9684]
	train_accs: [0.9726667, 0.9644833, 0.9709167, 0.9640667, 0.9683833]
	best_train_sub_head: 0
	worst: 0.9652
	avg: 0.96988
	best: 0.9758

Starting e_i: 7
Model ind 665 epoch 7 batch: 0 avg loss -2.344739 avg loss no lamb -2.344739 time 2020-06-25 17:29:17.510452
Model ind 665 epoch 7 batch: 100 avg loss -2.359824 avg loss no lamb -2.359824 time 2020-06-25 17:29:31.600068
Model ind 665 epoch 7 batch: 200 avg loss -2.406678 avg loss no lamb -2.406678 time 2020-06-25 17:29:45.559467
Model ind 665 epoch 7 batch: 300 avg loss -2.398396 avg loss no lamb -2.398396 time 2020-06-25 17:29:59.314045
Model ind 665 epoch 7 batch: 400 avg loss -2.362570 avg loss no lamb -2.362570 time 2020-06-25 17:30:13.176961
Model ind 665 epoch 7 batch: 500 avg loss -2.420445 avg loss no lamb -2.420445 time 2020-06-25 17:30:27.346669
Model ind 665 epoch 7 batch: 600 avg loss -2.434401 avg loss no lamb -2.434401 time 2020-06-25 17:30:41.296684
Model ind 665 epoch 7 batch: 700 avg loss -2.261995 avg loss no lamb -2.261995 time 2020-06-25 17:30:55.158775
Model ind 665 epoch 7 batch: 800 avg loss -2.471638 avg loss no lamb -2.471638 time 2020-06-25 17:31:09.089730
last batch sz 10
Pre: time 2020-06-25 17:31:25.394592: 
 	std: 0.003853514
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 4), (23, 5), (24, 2)]
	test_accs: [0.9755, 0.97, 0.9686, 0.9634, 0.9696]
	train_accs: [0.97401667, 0.96783334, 0.96706665, 0.96206665, 0.96818334]
	best_train_sub_head: 0
	worst: 0.9634
	avg: 0.9694201
	best: 0.9755

Starting e_i: 8
Model ind 665 epoch 8 batch: 0 avg loss -2.545784 avg loss no lamb -2.545784 time 2020-06-25 17:31:26.172837
Model ind 665 epoch 8 batch: 100 avg loss -2.368726 avg loss no lamb -2.368726 time 2020-06-25 17:31:39.849229
Model ind 665 epoch 8 batch: 200 avg loss -2.421360 avg loss no lamb -2.421360 time 2020-06-25 17:31:53.624045
Model ind 665 epoch 8 batch: 300 avg loss -2.437581 avg loss no lamb -2.437581 time 2020-06-25 17:32:07.522393
Model ind 665 epoch 8 batch: 400 avg loss -2.357101 avg loss no lamb -2.357101 time 2020-06-25 17:32:21.394358
Model ind 665 epoch 8 batch: 500 avg loss -2.386380 avg loss no lamb -2.386380 time 2020-06-25 17:32:35.162420
Model ind 665 epoch 8 batch: 600 avg loss -2.469271 avg loss no lamb -2.469271 time 2020-06-25 17:32:49.159611
Model ind 665 epoch 8 batch: 700 avg loss -2.303996 avg loss no lamb -2.303996 time 2020-06-25 17:33:02.641649
Model ind 665 epoch 8 batch: 800 avg loss -2.432508 avg loss no lamb -2.432508 time 2020-06-25 17:33:15.141622
last batch sz 10
Pre: time 2020-06-25 17:33:30.803914: 
 	std: 0.003630643
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 8), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9762, 0.9725, 0.9784, 0.9718]
	train_accs: [0.9785333, 0.9731, 0.97036666, 0.9755333, 0.97015]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.97608006
	best: 0.9815

Starting e_i: 9
Model ind 665 epoch 9 batch: 0 avg loss -2.504618 avg loss no lamb -2.504618 time 2020-06-25 17:33:32.691058
Model ind 665 epoch 9 batch: 100 avg loss -2.369862 avg loss no lamb -2.369862 time 2020-06-25 17:33:45.076689
Model ind 665 epoch 9 batch: 200 avg loss -2.417816 avg loss no lamb -2.417816 time 2020-06-25 17:33:57.238218
Model ind 665 epoch 9 batch: 300 avg loss -2.407696 avg loss no lamb -2.407696 time 2020-06-25 17:34:09.484534
Model ind 665 epoch 9 batch: 400 avg loss -2.292729 avg loss no lamb -2.292729 time 2020-06-25 17:34:21.534925
Model ind 665 epoch 9 batch: 500 avg loss -2.412731 avg loss no lamb -2.412731 time 2020-06-25 17:34:33.732484
Model ind 665 epoch 9 batch: 600 avg loss -2.431592 avg loss no lamb -2.431592 time 2020-06-25 17:34:46.002285
Model ind 665 epoch 9 batch: 700 avg loss -2.300400 avg loss no lamb -2.300400 time 2020-06-25 17:34:58.193977
Model ind 665 epoch 9 batch: 800 avg loss -2.464750 avg loss no lamb -2.464750 time 2020-06-25 17:35:10.311855
last batch sz 10
Pre: time 2020-06-25 17:35:25.589007: 
 	std: 0.0036368081
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 6), (23, 5), (24, 2)]
	test_accs: [0.9765, 0.972, 0.9668, 0.9731, 0.9674]
	train_accs: [0.9759333, 0.96891665, 0.9666167, 0.9741, 0.9686667]
	best_train_sub_head: 0
	worst: 0.9668
	avg: 0.97115993
	best: 0.9765

Starting e_i: 10
Model ind 665 epoch 10 batch: 0 avg loss -2.552540 avg loss no lamb -2.552540 time 2020-06-25 17:35:26.306704
Model ind 665 epoch 10 batch: 100 avg loss -2.380117 avg loss no lamb -2.380117 time 2020-06-25 17:35:38.396004
Model ind 665 epoch 10 batch: 200 avg loss -2.413695 avg loss no lamb -2.413695 time 2020-06-25 17:35:50.510366
Model ind 665 epoch 10 batch: 300 avg loss -2.449072 avg loss no lamb -2.449072 time 2020-06-25 17:36:02.704953
Model ind 665 epoch 10 batch: 400 avg loss -2.398560 avg loss no lamb -2.398560 time 2020-06-25 17:36:15.071941
Model ind 665 epoch 10 batch: 500 avg loss -2.420011 avg loss no lamb -2.420011 time 2020-06-25 17:36:27.334080
Model ind 665 epoch 10 batch: 600 avg loss -2.552986 avg loss no lamb -2.552986 time 2020-06-25 17:36:39.383870
Model ind 665 epoch 10 batch: 700 avg loss -2.407725 avg loss no lamb -2.407725 time 2020-06-25 17:36:51.556831
Model ind 665 epoch 10 batch: 800 avg loss -2.507244 avg loss no lamb -2.507244 time 2020-06-25 17:37:03.882284
last batch sz 10
Pre: time 2020-06-25 17:37:19.308910: 
 	std: 0.0018874342
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 1), (23, 5), (24, 2)]
	test_accs: [0.9783, 0.9766, 0.9743, 0.9779, 0.9736]
	train_accs: [0.97686666, 0.97385, 0.9733, 0.9763, 0.973]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97614
	best: 0.9783

Starting e_i: 11
Model ind 665 epoch 11 batch: 0 avg loss -2.551695 avg loss no lamb -2.551695 time 2020-06-25 17:37:20.090266
Model ind 665 epoch 11 batch: 100 avg loss -2.410225 avg loss no lamb -2.410225 time 2020-06-25 17:37:32.225895
Model ind 665 epoch 11 batch: 200 avg loss -2.409094 avg loss no lamb -2.409094 time 2020-06-25 17:37:44.486976
Model ind 665 epoch 11 batch: 300 avg loss -2.369849 avg loss no lamb -2.369849 time 2020-06-25 17:37:56.600721
Model ind 665 epoch 11 batch: 400 avg loss -2.477213 avg loss no lamb -2.477213 time 2020-06-25 17:38:09.242974
Model ind 665 epoch 11 batch: 500 avg loss -2.460452 avg loss no lamb -2.460452 time 2020-06-25 17:38:23.295338
Model ind 665 epoch 11 batch: 600 avg loss -2.547718 avg loss no lamb -2.547718 time 2020-06-25 17:38:34.951609
Model ind 665 epoch 11 batch: 700 avg loss -2.358418 avg loss no lamb -2.358418 time 2020-06-25 17:38:45.644154
Model ind 665 epoch 11 batch: 800 avg loss -2.482279 avg loss no lamb -2.482279 time 2020-06-25 17:38:56.449491
last batch sz 10
Pre: time 2020-06-25 17:39:09.952645: 
 	std: 0.0018192464
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9781, 0.9775, 0.9752, 0.9782, 0.9736]
	train_accs: [0.97655, 0.97393334, 0.97466666, 0.9766, 0.97333336]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97651994
	best: 0.9782

Starting e_i: 12
Model ind 665 epoch 12 batch: 0 avg loss -2.545562 avg loss no lamb -2.545562 time 2020-06-25 17:39:10.580720
Model ind 665 epoch 12 batch: 100 avg loss -2.393761 avg loss no lamb -2.393761 time 2020-06-25 17:39:20.970109
Model ind 665 epoch 12 batch: 200 avg loss -2.466348 avg loss no lamb -2.466348 time 2020-06-25 17:39:31.213204
Model ind 665 epoch 12 batch: 300 avg loss -2.511469 avg loss no lamb -2.511469 time 2020-06-25 17:39:41.582115
Model ind 665 epoch 12 batch: 400 avg loss -2.409591 avg loss no lamb -2.409591 time 2020-06-25 17:39:52.154775
Model ind 665 epoch 12 batch: 500 avg loss -2.398681 avg loss no lamb -2.398681 time 2020-06-25 17:40:02.884162
Model ind 665 epoch 12 batch: 600 avg loss -2.495630 avg loss no lamb -2.495630 time 2020-06-25 17:40:13.364908
Model ind 665 epoch 12 batch: 700 avg loss -2.416765 avg loss no lamb -2.416765 time 2020-06-25 17:40:23.937508
Model ind 665 epoch 12 batch: 800 avg loss -2.515647 avg loss no lamb -2.515647 time 2020-06-25 17:40:34.472718
last batch sz 10
Pre: time 2020-06-25 17:40:47.789182: 
 	std: 0.0025693579
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9709, 0.9685, 0.9659, 0.9694, 0.9637]
	train_accs: [0.9709, 0.96786666, 0.9658333, 0.9709, 0.9673667]
	best_train_sub_head: 0
	worst: 0.9637
	avg: 0.96768
	best: 0.9709

Starting e_i: 13
Model ind 665 epoch 13 batch: 0 avg loss -2.510834 avg loss no lamb -2.510834 time 2020-06-25 17:40:48.439052
Model ind 665 epoch 13 batch: 100 avg loss -2.442668 avg loss no lamb -2.442668 time 2020-06-25 17:40:59.062355
Model ind 665 epoch 13 batch: 200 avg loss -2.479327 avg loss no lamb -2.479327 time 2020-06-25 17:41:09.651862
Model ind 665 epoch 13 batch: 300 avg loss -2.541980 avg loss no lamb -2.541980 time 2020-06-25 17:41:20.172046
Model ind 665 epoch 13 batch: 400 avg loss -2.481457 avg loss no lamb -2.481457 time 2020-06-25 17:41:30.699510
Model ind 665 epoch 13 batch: 500 avg loss -2.447070 avg loss no lamb -2.447070 time 2020-06-25 17:41:41.391858
Model ind 665 epoch 13 batch: 600 avg loss -2.519698 avg loss no lamb -2.519698 time 2020-06-25 17:41:52.235496
Model ind 665 epoch 13 batch: 700 avg loss -2.427254 avg loss no lamb -2.427254 time 2020-06-25 17:42:02.680585
Model ind 665 epoch 13 batch: 800 avg loss -2.559429 avg loss no lamb -2.559429 time 2020-06-25 17:42:12.866681
last batch sz 10
Pre: time 2020-06-25 17:42:26.197977: 
 	std: 0.003661482
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9707, 0.9626, 0.9671, 0.9731, 0.9707]
	train_accs: [0.97031665, 0.9626167, 0.9659, 0.97293335, 0.9715667]
	best_train_sub_head: 3
	worst: 0.9626
	avg: 0.96884
	best: 0.9731

Starting e_i: 14
Model ind 665 epoch 14 batch: 0 avg loss -2.532871 avg loss no lamb -2.532871 time 2020-06-25 17:42:26.878484
Model ind 665 epoch 14 batch: 100 avg loss -2.417273 avg loss no lamb -2.417273 time 2020-06-25 17:42:37.596023
Model ind 665 epoch 14 batch: 200 avg loss -2.421212 avg loss no lamb -2.421212 time 2020-06-25 17:42:48.255504
Model ind 665 epoch 14 batch: 300 avg loss -2.536718 avg loss no lamb -2.536718 time 2020-06-25 17:42:58.567626
Model ind 665 epoch 14 batch: 400 avg loss -2.495830 avg loss no lamb -2.495830 time 2020-06-25 17:43:08.980333
Model ind 665 epoch 14 batch: 500 avg loss -2.452627 avg loss no lamb -2.452627 time 2020-06-25 17:43:19.590282
Model ind 665 epoch 14 batch: 600 avg loss -2.518661 avg loss no lamb -2.518661 time 2020-06-25 17:43:30.324952
Model ind 665 epoch 14 batch: 700 avg loss -2.414112 avg loss no lamb -2.414112 time 2020-06-25 17:43:41.077533
Model ind 665 epoch 14 batch: 800 avg loss -2.562915 avg loss no lamb -2.562915 time 2020-06-25 17:43:51.821442
last batch sz 10
Pre: time 2020-06-25 17:44:05.196711: 
 	std: 0.0021525905
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9746, 0.9739, 0.9711, 0.9771, 0.9717]
	train_accs: [0.9741333, 0.97253335, 0.9713333, 0.97611666, 0.97253335]
	best_train_sub_head: 3
	worst: 0.9711
	avg: 0.97368
	best: 0.9771

Starting e_i: 15
Model ind 665 epoch 15 batch: 0 avg loss -2.608361 avg loss no lamb -2.608361 time 2020-06-25 17:44:05.866920
Model ind 665 epoch 15 batch: 100 avg loss -2.440156 avg loss no lamb -2.440156 time 2020-06-25 17:44:16.353724
Model ind 665 epoch 15 batch: 200 avg loss -2.430949 avg loss no lamb -2.430949 time 2020-06-25 17:44:27.042354
Model ind 665 epoch 15 batch: 300 avg loss -2.513414 avg loss no lamb -2.513414 time 2020-06-25 17:44:37.324216
Model ind 665 epoch 15 batch: 400 avg loss -2.469864 avg loss no lamb -2.469864 time 2020-06-25 17:44:47.967857
Model ind 665 epoch 15 batch: 500 avg loss -2.490258 avg loss no lamb -2.490258 time 2020-06-25 17:44:58.582615
Model ind 665 epoch 15 batch: 600 avg loss -2.557520 avg loss no lamb -2.557520 time 2020-06-25 17:45:09.216999
Model ind 665 epoch 15 batch: 700 avg loss -2.501328 avg loss no lamb -2.501328 time 2020-06-25 17:45:19.925350
Model ind 665 epoch 15 batch: 800 avg loss -2.519525 avg loss no lamb -2.519525 time 2020-06-25 17:45:30.486115
last batch sz 10
Pre: time 2020-06-25 17:45:44.038412: 
 	std: 0.002206712
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 6), (3, 9), (4, 4), (5, 8), (6, 1), (7, 9), (8, 2), (9, 3), (10, 7), (11, 2), (12, 6), (13, 3), (14, 7), (15, 0), (16, 3), (17, 0), (18, 4), (19, 8), (20, 5), (21, 5), (22, 7), (23, 0), (24, 9)]
	test_accs: [0.9756, 0.9741, 0.9709, 0.9772, 0.9763]
	train_accs: [0.9739, 0.97218335, 0.9691167, 0.9759833, 0.97646666]
	best_train_sub_head: 4
	worst: 0.9709
	avg: 0.97481996
	best: 0.9763

Starting e_i: 16
Model ind 665 epoch 16 batch: 0 avg loss -2.637918 avg loss no lamb -2.637918 time 2020-06-25 17:45:44.704561
Model ind 665 epoch 16 batch: 100 avg loss -2.409758 avg loss no lamb -2.409758 time 2020-06-25 17:45:55.246785
Model ind 665 epoch 16 batch: 200 avg loss -2.531585 avg loss no lamb -2.531585 time 2020-06-25 17:46:05.759258
Model ind 665 epoch 16 batch: 300 avg loss -2.545683 avg loss no lamb -2.545683 time 2020-06-25 17:46:16.472592
Model ind 665 epoch 16 batch: 400 avg loss -2.455414 avg loss no lamb -2.455414 time 2020-06-25 17:46:27.115329
Model ind 665 epoch 16 batch: 500 avg loss -2.412611 avg loss no lamb -2.412611 time 2020-06-25 17:46:37.799244
Model ind 665 epoch 16 batch: 600 avg loss -2.424167 avg loss no lamb -2.424167 time 2020-06-25 17:46:48.327390
Model ind 665 epoch 16 batch: 700 avg loss -2.319918 avg loss no lamb -2.319918 time 2020-06-25 17:46:58.658402
Model ind 665 epoch 16 batch: 800 avg loss -2.533588 avg loss no lamb -2.533588 time 2020-06-25 17:47:09.347450
last batch sz 10
Pre: time 2020-06-25 17:47:22.940803: 
 	std: 0.0023941656
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9774, 0.973, 0.9794, 0.9775]
	train_accs: [0.97676665, 0.9748333, 0.97366667, 0.97855, 0.97728336]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.9774
	best: 0.9794

Starting e_i: 17
Model ind 665 epoch 17 batch: 0 avg loss -2.638591 avg loss no lamb -2.638591 time 2020-06-25 17:47:23.623945
Model ind 665 epoch 17 batch: 100 avg loss -2.501066 avg loss no lamb -2.501066 time 2020-06-25 17:47:34.182671
Model ind 665 epoch 17 batch: 200 avg loss -2.469271 avg loss no lamb -2.469271 time 2020-06-25 17:47:44.664652
Model ind 665 epoch 17 batch: 300 avg loss -2.505612 avg loss no lamb -2.505612 time 2020-06-25 17:47:55.013683
Model ind 665 epoch 17 batch: 400 avg loss -2.536330 avg loss no lamb -2.536330 time 2020-06-25 17:48:05.661911
Model ind 665 epoch 17 batch: 500 avg loss -2.467825 avg loss no lamb -2.467825 time 2020-06-25 17:48:16.254021
Model ind 665 epoch 17 batch: 600 avg loss -2.546597 avg loss no lamb -2.546597 time 2020-06-25 17:48:26.659726
Model ind 665 epoch 17 batch: 700 avg loss -2.373200 avg loss no lamb -2.373200 time 2020-06-25 17:48:37.168223
Model ind 665 epoch 17 batch: 800 avg loss -2.541040 avg loss no lamb -2.541040 time 2020-06-25 17:48:47.665581
last batch sz 10
Pre: time 2020-06-25 17:49:01.353434: 
 	std: 0.0018658959
	best_train_sub_head_match: [(0, 2), (1, 8), (2, 6), (3, 9), (4, 4), (5, 8), (6, 1), (7, 9), (8, 2), (9, 5), (10, 7), (11, 2), (12, 6), (13, 3), (14, 7), (15, 0), (16, 3), (17, 0), (18, 4), (19, 8), (20, 5), (21, 5), (22, 7), (23, 0), (24, 9)]
	test_accs: [0.976, 0.9758, 0.972, 0.9769, 0.9772]
	train_accs: [0.9737333, 0.97286665, 0.97215, 0.97683334, 0.9769167]
	best_train_sub_head: 4
	worst: 0.972
	avg: 0.97558004
	best: 0.9772

Starting e_i: 18
Model ind 665 epoch 18 batch: 0 avg loss -2.624801 avg loss no lamb -2.624801 time 2020-06-25 17:49:02.024583
Model ind 665 epoch 18 batch: 100 avg loss -2.482141 avg loss no lamb -2.482141 time 2020-06-25 17:49:12.850143
Model ind 665 epoch 18 batch: 200 avg loss -2.498318 avg loss no lamb -2.498318 time 2020-06-25 17:49:23.650318
Model ind 665 epoch 18 batch: 300 avg loss -2.513572 avg loss no lamb -2.513572 time 2020-06-25 17:49:34.182189
Model ind 665 epoch 18 batch: 400 avg loss -2.529093 avg loss no lamb -2.529093 time 2020-06-25 17:49:44.623454
Model ind 665 epoch 18 batch: 500 avg loss -2.468636 avg loss no lamb -2.468636 time 2020-06-25 17:49:55.120301
Model ind 665 epoch 18 batch: 600 avg loss -2.554088 avg loss no lamb -2.554088 time 2020-06-25 17:50:05.955676
Model ind 665 epoch 18 batch: 700 avg loss -2.404808 avg loss no lamb -2.404808 time 2020-06-25 17:50:16.421593
Model ind 665 epoch 18 batch: 800 avg loss -2.494698 avg loss no lamb -2.494698 time 2020-06-25 17:50:26.751408
last batch sz 10
Pre: time 2020-06-25 17:50:40.125080: 
 	std: 0.0018039998
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9721, 0.9701, 0.9677, 0.9728, 0.9715]
	train_accs: [0.9702, 0.96605, 0.9665833, 0.9734333, 0.97118336]
	best_train_sub_head: 3
	worst: 0.9677
	avg: 0.97084
	best: 0.9728

Starting e_i: 19
Model ind 665 epoch 19 batch: 0 avg loss -2.659953 avg loss no lamb -2.659953 time 2020-06-25 17:50:40.838864
Model ind 665 epoch 19 batch: 100 avg loss -2.499686 avg loss no lamb -2.499686 time 2020-06-25 17:50:51.390197
Model ind 665 epoch 19 batch: 200 avg loss -2.503198 avg loss no lamb -2.503198 time 2020-06-25 17:51:02.110658
Model ind 665 epoch 19 batch: 300 avg loss -2.553636 avg loss no lamb -2.553636 time 2020-06-25 17:51:12.697664
Model ind 665 epoch 19 batch: 400 avg loss -2.520563 avg loss no lamb -2.520563 time 2020-06-25 17:51:23.015285
Model ind 665 epoch 19 batch: 500 avg loss -2.498915 avg loss no lamb -2.498915 time 2020-06-25 17:51:33.475618
Model ind 665 epoch 19 batch: 600 avg loss -2.593976 avg loss no lamb -2.593976 time 2020-06-25 17:51:44.255701
Model ind 665 epoch 19 batch: 700 avg loss -2.443336 avg loss no lamb -2.443336 time 2020-06-25 17:51:54.755657
Model ind 665 epoch 19 batch: 800 avg loss -2.518032 avg loss no lamb -2.518032 time 2020-06-25 17:52:05.284169
last batch sz 10
Pre: time 2020-06-25 17:52:18.516626: 
 	std: 0.002274556
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.971, 0.9675, 0.9664, 0.9719, 0.9668]
	train_accs: [0.9691833, 0.96566665, 0.96496665, 0.972, 0.96868336]
	best_train_sub_head: 3
	worst: 0.9664
	avg: 0.9687201
	best: 0.9719

Starting e_i: 20
Model ind 665 epoch 20 batch: 0 avg loss -2.620549 avg loss no lamb -2.620549 time 2020-06-25 17:52:19.128113
Model ind 665 epoch 20 batch: 100 avg loss -2.480180 avg loss no lamb -2.480180 time 2020-06-25 17:52:29.349103
Model ind 665 epoch 20 batch: 200 avg loss -2.572106 avg loss no lamb -2.572106 time 2020-06-25 17:52:40.078633
Model ind 665 epoch 20 batch: 300 avg loss -2.520697 avg loss no lamb -2.520697 time 2020-06-25 17:52:50.792055
Model ind 665 epoch 20 batch: 400 avg loss -2.586897 avg loss no lamb -2.586897 time 2020-06-25 17:53:01.386180
Model ind 665 epoch 20 batch: 500 avg loss -2.472561 avg loss no lamb -2.472561 time 2020-06-25 17:53:11.859429
Model ind 665 epoch 20 batch: 600 avg loss -2.569371 avg loss no lamb -2.569371 time 2020-06-25 17:53:22.312540
Model ind 665 epoch 20 batch: 700 avg loss -2.513363 avg loss no lamb -2.513363 time 2020-06-25 17:53:33.052834
Model ind 665 epoch 20 batch: 800 avg loss -2.566369 avg loss no lamb -2.566369 time 2020-06-25 17:53:43.692549
last batch sz 10
Pre: time 2020-06-25 17:53:57.330933: 
 	std: 0.00181725
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9768, 0.9751, 0.976, 0.9802, 0.9756]
	train_accs: [0.9755, 0.9727167, 0.97395, 0.97795, 0.97566664]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97674006
	best: 0.9802

Starting e_i: 21
Model ind 665 epoch 21 batch: 0 avg loss -2.651248 avg loss no lamb -2.651248 time 2020-06-25 17:53:59.184176
Model ind 665 epoch 21 batch: 100 avg loss -2.538727 avg loss no lamb -2.538727 time 2020-06-25 17:54:09.420904
Model ind 665 epoch 21 batch: 200 avg loss -2.540101 avg loss no lamb -2.540101 time 2020-06-25 17:54:19.962552
Model ind 665 epoch 21 batch: 300 avg loss -2.509302 avg loss no lamb -2.509302 time 2020-06-25 17:54:30.596710
Model ind 665 epoch 21 batch: 400 avg loss -2.506238 avg loss no lamb -2.506238 time 2020-06-25 17:54:41.178539
Model ind 665 epoch 21 batch: 500 avg loss -2.485206 avg loss no lamb -2.485206 time 2020-06-25 17:54:51.702600
Model ind 665 epoch 21 batch: 600 avg loss -2.706704 avg loss no lamb -2.706704 time 2020-06-25 17:55:02.206069
Model ind 665 epoch 21 batch: 700 avg loss -2.480262 avg loss no lamb -2.480262 time 2020-06-25 17:55:12.823885
Model ind 665 epoch 21 batch: 800 avg loss -2.599457 avg loss no lamb -2.599457 time 2020-06-25 17:55:23.562519
last batch sz 10
Pre: time 2020-06-25 17:55:37.073088: 
 	std: 0.0024843507
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9751, 0.9739, 0.9708, 0.9784, 0.9758]
	train_accs: [0.9737833, 0.97221667, 0.9711667, 0.9785, 0.97585]
	best_train_sub_head: 3
	worst: 0.9708
	avg: 0.9748
	best: 0.9784

Starting e_i: 22
Model ind 665 epoch 22 batch: 0 avg loss -2.693655 avg loss no lamb -2.693655 time 2020-06-25 17:55:37.733679
Model ind 665 epoch 22 batch: 100 avg loss -2.538324 avg loss no lamb -2.538324 time 2020-06-25 17:55:48.277643
Model ind 665 epoch 22 batch: 200 avg loss -2.549337 avg loss no lamb -2.549337 time 2020-06-25 17:55:58.868459
Model ind 665 epoch 22 batch: 300 avg loss -2.531299 avg loss no lamb -2.531299 time 2020-06-25 17:56:09.571404
Model ind 665 epoch 22 batch: 400 avg loss -2.479488 avg loss no lamb -2.479488 time 2020-06-25 17:56:20.452391
Model ind 665 epoch 22 batch: 500 avg loss -2.493376 avg loss no lamb -2.493376 time 2020-06-25 17:56:30.975342
Model ind 665 epoch 22 batch: 600 avg loss -2.599751 avg loss no lamb -2.599751 time 2020-06-25 17:56:41.495648
Model ind 665 epoch 22 batch: 700 avg loss -2.415469 avg loss no lamb -2.415469 time 2020-06-25 17:56:52.086463
Model ind 665 epoch 22 batch: 800 avg loss -2.565813 avg loss no lamb -2.565813 time 2020-06-25 17:57:02.831903
last batch sz 10
Pre: time 2020-06-25 17:57:16.449042: 
 	std: 0.0023879705
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9739, 0.9689, 0.9719, 0.976, 0.9715]
	train_accs: [0.97295, 0.9676833, 0.9712333, 0.97431666, 0.97286665]
	best_train_sub_head: 3
	worst: 0.9689
	avg: 0.97244006
	best: 0.976

Starting e_i: 23
Model ind 665 epoch 23 batch: 0 avg loss -2.650378 avg loss no lamb -2.650378 time 2020-06-25 17:57:17.074451
Model ind 665 epoch 23 batch: 100 avg loss -2.461709 avg loss no lamb -2.461709 time 2020-06-25 17:57:27.591624
Model ind 665 epoch 23 batch: 200 avg loss -2.527020 avg loss no lamb -2.527020 time 2020-06-25 17:57:38.164563
Model ind 665 epoch 23 batch: 300 avg loss -2.532393 avg loss no lamb -2.532393 time 2020-06-25 17:57:48.696526
Model ind 665 epoch 23 batch: 400 avg loss -2.559542 avg loss no lamb -2.559542 time 2020-06-25 17:57:59.530065
Model ind 665 epoch 23 batch: 500 avg loss -2.507595 avg loss no lamb -2.507595 time 2020-06-25 17:58:10.433717
Model ind 665 epoch 23 batch: 600 avg loss -2.574275 avg loss no lamb -2.574275 time 2020-06-25 17:58:21.053026
Model ind 665 epoch 23 batch: 700 avg loss -2.518795 avg loss no lamb -2.518795 time 2020-06-25 17:58:31.476102
Model ind 665 epoch 23 batch: 800 avg loss -2.571353 avg loss no lamb -2.571353 time 2020-06-25 17:58:42.122927
last batch sz 10
Pre: time 2020-06-25 17:58:55.632066: 
 	std: 0.0018187852
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9783, 0.9759, 0.9802, 0.9764]
	train_accs: [0.97781664, 0.97541666, 0.9758667, 0.9787833, 0.9762]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97819996
	best: 0.9802

Starting e_i: 24
Model ind 665 epoch 24 batch: 0 avg loss -2.728980 avg loss no lamb -2.728980 time 2020-06-25 17:58:56.328256
Model ind 665 epoch 24 batch: 100 avg loss -2.503847 avg loss no lamb -2.503847 time 2020-06-25 17:59:06.989647
Model ind 665 epoch 24 batch: 200 avg loss -2.579393 avg loss no lamb -2.579393 time 2020-06-25 17:59:17.497547
Model ind 665 epoch 24 batch: 300 avg loss -2.543348 avg loss no lamb -2.543348 time 2020-06-25 17:59:27.988655
Model ind 665 epoch 24 batch: 400 avg loss -2.629941 avg loss no lamb -2.629941 time 2020-06-25 17:59:38.435548
Model ind 665 epoch 24 batch: 500 avg loss -2.483977 avg loss no lamb -2.483977 time 2020-06-25 17:59:49.162088
Model ind 665 epoch 24 batch: 600 avg loss -2.649602 avg loss no lamb -2.649602 time 2020-06-25 17:59:59.755604
Model ind 665 epoch 24 batch: 700 avg loss -2.523374 avg loss no lamb -2.523374 time 2020-06-25 18:00:10.388486
Model ind 665 epoch 24 batch: 800 avg loss -2.653447 avg loss no lamb -2.653447 time 2020-06-25 18:00:20.653947
last batch sz 10
Pre: time 2020-06-25 18:00:33.987344: 
 	std: 0.0016871274
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9757, 0.9763, 0.9797, 0.976]
	train_accs: [0.97816664, 0.974, 0.9770667, 0.97858334, 0.97713333]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97735995
	best: 0.9797

Starting e_i: 25
Model ind 665 epoch 25 batch: 0 avg loss -2.736567 avg loss no lamb -2.736567 time 2020-06-25 18:00:34.642119
Model ind 665 epoch 25 batch: 100 avg loss -2.568547 avg loss no lamb -2.568547 time 2020-06-25 18:00:45.344604
Model ind 665 epoch 25 batch: 200 avg loss -2.584800 avg loss no lamb -2.584800 time 2020-06-25 18:00:55.965568
Model ind 665 epoch 25 batch: 300 avg loss -2.585663 avg loss no lamb -2.585663 time 2020-06-25 18:01:06.658530
Model ind 665 epoch 25 batch: 400 avg loss -2.530973 avg loss no lamb -2.530973 time 2020-06-25 18:01:17.459462
Model ind 665 epoch 25 batch: 500 avg loss -2.516212 avg loss no lamb -2.516212 time 2020-06-25 18:01:28.111870
Model ind 665 epoch 25 batch: 600 avg loss -2.684980 avg loss no lamb -2.684980 time 2020-06-25 18:01:38.686855
Model ind 665 epoch 25 batch: 700 avg loss -2.365107 avg loss no lamb -2.365107 time 2020-06-25 18:01:49.298275
Model ind 665 epoch 25 batch: 800 avg loss -2.531618 avg loss no lamb -2.531618 time 2020-06-25 18:01:59.884501
last batch sz 10
Pre: time 2020-06-25 18:02:13.462385: 
 	std: 0.0033178285
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.976, 0.9728, 0.968, 0.9773, 0.9714]
	train_accs: [0.9762667, 0.9719333, 0.9680833, 0.97793335, 0.9726833]
	best_train_sub_head: 3
	worst: 0.968
	avg: 0.9731
	best: 0.9773

Starting e_i: 26
Model ind 665 epoch 26 batch: 0 avg loss -2.665138 avg loss no lamb -2.665138 time 2020-06-25 18:02:14.116828
Model ind 665 epoch 26 batch: 100 avg loss -2.480480 avg loss no lamb -2.480480 time 2020-06-25 18:02:24.673195
Model ind 665 epoch 26 batch: 200 avg loss -2.567712 avg loss no lamb -2.567712 time 2020-06-25 18:02:35.137675
Model ind 665 epoch 26 batch: 300 avg loss -2.531809 avg loss no lamb -2.531809 time 2020-06-25 18:02:45.954467
Model ind 665 epoch 26 batch: 400 avg loss -2.524561 avg loss no lamb -2.524561 time 2020-06-25 18:02:56.601954
Model ind 665 epoch 26 batch: 500 avg loss -2.543590 avg loss no lamb -2.543590 time 2020-06-25 18:03:07.227745
Model ind 665 epoch 26 batch: 600 avg loss -2.664051 avg loss no lamb -2.664051 time 2020-06-25 18:03:17.480786
Model ind 665 epoch 26 batch: 700 avg loss -2.515260 avg loss no lamb -2.515260 time 2020-06-25 18:03:27.767821
Model ind 665 epoch 26 batch: 800 avg loss -2.626855 avg loss no lamb -2.626855 time 2020-06-25 18:03:38.272510
last batch sz 10
Pre: time 2020-06-25 18:03:51.969940: 
 	std: 0.0025111008
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9787, 0.9759, 0.9835, 0.9788]
	train_accs: [0.97965, 0.9763, 0.9743, 0.9812, 0.97865]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97951996
	best: 0.9835

Starting e_i: 27
Model ind 665 epoch 27 batch: 0 avg loss -2.753999 avg loss no lamb -2.753999 time 2020-06-25 18:03:53.734479
Model ind 665 epoch 27 batch: 100 avg loss -2.585384 avg loss no lamb -2.585384 time 2020-06-25 18:04:04.320726
Model ind 665 epoch 27 batch: 200 avg loss -2.592457 avg loss no lamb -2.592457 time 2020-06-25 18:04:14.954435
Model ind 665 epoch 27 batch: 300 avg loss -2.551692 avg loss no lamb -2.551692 time 2020-06-25 18:04:25.362627
Model ind 665 epoch 27 batch: 400 avg loss -2.541142 avg loss no lamb -2.541142 time 2020-06-25 18:04:36.057037
Model ind 665 epoch 27 batch: 500 avg loss -2.533773 avg loss no lamb -2.533773 time 2020-06-25 18:04:46.734582
Model ind 665 epoch 27 batch: 600 avg loss -2.703072 avg loss no lamb -2.703072 time 2020-06-25 18:04:57.176865
Model ind 665 epoch 27 batch: 700 avg loss -2.515215 avg loss no lamb -2.515215 time 2020-06-25 18:05:07.511685
Model ind 665 epoch 27 batch: 800 avg loss -2.632586 avg loss no lamb -2.632586 time 2020-06-25 18:05:17.951764
last batch sz 10
Pre: time 2020-06-25 18:05:31.473299: 
 	std: 0.0024187618
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9755, 0.9736, 0.9802, 0.9753]
	train_accs: [0.97796667, 0.9745333, 0.97293335, 0.97961664, 0.97475]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97665995
	best: 0.9802

Starting e_i: 28
Model ind 665 epoch 28 batch: 0 avg loss -2.855847 avg loss no lamb -2.855847 time 2020-06-25 18:05:32.108305
Model ind 665 epoch 28 batch: 100 avg loss -2.571748 avg loss no lamb -2.571748 time 2020-06-25 18:05:42.637487
Model ind 665 epoch 28 batch: 200 avg loss -2.566992 avg loss no lamb -2.566992 time 2020-06-25 18:05:53.164576
Model ind 665 epoch 28 batch: 300 avg loss -2.545615 avg loss no lamb -2.545615 time 2020-06-25 18:06:03.614738
Model ind 665 epoch 28 batch: 400 avg loss -2.561583 avg loss no lamb -2.561583 time 2020-06-25 18:06:14.077688
Model ind 665 epoch 28 batch: 500 avg loss -2.524438 avg loss no lamb -2.524438 time 2020-06-25 18:06:24.803103
Model ind 665 epoch 28 batch: 600 avg loss -2.644212 avg loss no lamb -2.644212 time 2020-06-25 18:06:35.401845
Model ind 665 epoch 28 batch: 700 avg loss -2.486096 avg loss no lamb -2.486096 time 2020-06-25 18:06:45.879126
Model ind 665 epoch 28 batch: 800 avg loss -2.628963 avg loss no lamb -2.628963 time 2020-06-25 18:06:56.444452
last batch sz 10
Pre: time 2020-06-25 18:07:10.111138: 
 	std: 0.002895087
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9764, 0.9721, 0.9794, 0.9742]
	train_accs: [0.97863334, 0.97541666, 0.9727333, 0.9792333, 0.9749333]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97631997
	best: 0.9794

Starting e_i: 29
Model ind 665 epoch 29 batch: 0 avg loss -2.631239 avg loss no lamb -2.631239 time 2020-06-25 18:07:10.798622
Model ind 665 epoch 29 batch: 100 avg loss -2.549602 avg loss no lamb -2.549602 time 2020-06-25 18:07:21.082902
Model ind 665 epoch 29 batch: 200 avg loss -2.531855 avg loss no lamb -2.531855 time 2020-06-25 18:07:31.647324
Model ind 665 epoch 29 batch: 300 avg loss -2.595880 avg loss no lamb -2.595880 time 2020-06-25 18:07:42.261737
Model ind 665 epoch 29 batch: 400 avg loss -2.538764 avg loss no lamb -2.538764 time 2020-06-25 18:07:53.035953
Model ind 665 epoch 29 batch: 500 avg loss -2.517090 avg loss no lamb -2.517090 time 2020-06-25 18:08:03.855820
Model ind 665 epoch 29 batch: 600 avg loss -2.648236 avg loss no lamb -2.648236 time 2020-06-25 18:08:14.529294
Model ind 665 epoch 29 batch: 700 avg loss -2.485764 avg loss no lamb -2.485764 time 2020-06-25 18:08:25.097220
Model ind 665 epoch 29 batch: 800 avg loss -2.592531 avg loss no lamb -2.592531 time 2020-06-25 18:08:35.935554
last batch sz 10
Pre: time 2020-06-25 18:08:49.588318: 
 	std: 0.0023601635
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9754, 0.9731, 0.9791, 0.9772]
	train_accs: [0.9787667, 0.97386664, 0.9730333, 0.9792167, 0.97541666]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.9768399
	best: 0.9791

Starting e_i: 30
Model ind 665 epoch 30 batch: 0 avg loss -2.775511 avg loss no lamb -2.775511 time 2020-06-25 18:08:50.236675
Model ind 665 epoch 30 batch: 100 avg loss -2.609118 avg loss no lamb -2.609118 time 2020-06-25 18:09:00.868615
Model ind 665 epoch 30 batch: 200 avg loss -2.571557 avg loss no lamb -2.571557 time 2020-06-25 18:09:11.281063
Model ind 665 epoch 30 batch: 300 avg loss -2.600828 avg loss no lamb -2.600828 time 2020-06-25 18:09:22.215262
Model ind 665 epoch 30 batch: 400 avg loss -2.531510 avg loss no lamb -2.531510 time 2020-06-25 18:09:32.938419
Model ind 665 epoch 30 batch: 500 avg loss -2.549510 avg loss no lamb -2.549510 time 2020-06-25 18:09:43.463166
Model ind 665 epoch 30 batch: 600 avg loss -2.716271 avg loss no lamb -2.716271 time 2020-06-25 18:09:54.035534
Model ind 665 epoch 30 batch: 700 avg loss -2.485818 avg loss no lamb -2.485818 time 2020-06-25 18:10:04.383088
Model ind 665 epoch 30 batch: 800 avg loss -2.601423 avg loss no lamb -2.601423 time 2020-06-25 18:10:14.953753
last batch sz 10
Pre: time 2020-06-25 18:10:28.663697: 
 	std: 0.0028414107
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9772, 0.9726, 0.9802, 0.9742]
	train_accs: [0.97885, 0.9759667, 0.9740667, 0.9797, 0.97693336]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97662
	best: 0.9802

Starting e_i: 31
Model ind 665 epoch 31 batch: 0 avg loss -2.751691 avg loss no lamb -2.751691 time 2020-06-25 18:10:30.604816
Model ind 665 epoch 31 batch: 100 avg loss -2.539068 avg loss no lamb -2.539068 time 2020-06-25 18:10:41.349668
Model ind 665 epoch 31 batch: 200 avg loss -2.593878 avg loss no lamb -2.593878 time 2020-06-25 18:10:51.768769
Model ind 665 epoch 31 batch: 300 avg loss -2.602304 avg loss no lamb -2.602304 time 2020-06-25 18:11:02.285649
Model ind 665 epoch 31 batch: 400 avg loss -2.579280 avg loss no lamb -2.579280 time 2020-06-25 18:11:13.147162
Model ind 665 epoch 31 batch: 500 avg loss -2.551805 avg loss no lamb -2.551805 time 2020-06-25 18:11:23.555597
Model ind 665 epoch 31 batch: 600 avg loss -2.723006 avg loss no lamb -2.723006 time 2020-06-25 18:11:34.123015
Model ind 665 epoch 31 batch: 700 avg loss -2.529204 avg loss no lamb -2.529204 time 2020-06-25 18:11:44.629523
Model ind 665 epoch 31 batch: 800 avg loss -2.581848 avg loss no lamb -2.581848 time 2020-06-25 18:11:55.120420
last batch sz 10
Pre: time 2020-06-25 18:12:08.574318: 
 	std: 0.0021382337
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9777, 0.9758, 0.9813, 0.9762]
	train_accs: [0.9787667, 0.9751, 0.97536665, 0.9791, 0.9767333]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.9782001
	best: 0.9813

Starting e_i: 32
Model ind 665 epoch 32 batch: 0 avg loss -2.801736 avg loss no lamb -2.801736 time 2020-06-25 18:12:09.234534
Model ind 665 epoch 32 batch: 100 avg loss -2.536177 avg loss no lamb -2.536177 time 2020-06-25 18:12:19.893064
Model ind 665 epoch 32 batch: 200 avg loss -2.549349 avg loss no lamb -2.549349 time 2020-06-25 18:12:30.380617
Model ind 665 epoch 32 batch: 300 avg loss -2.523545 avg loss no lamb -2.523545 time 2020-06-25 18:12:40.967471
Model ind 665 epoch 32 batch: 400 avg loss -2.589433 avg loss no lamb -2.589433 time 2020-06-25 18:12:51.281832
Model ind 665 epoch 32 batch: 500 avg loss -2.519051 avg loss no lamb -2.519051 time 2020-06-25 18:13:02.031583
Model ind 665 epoch 32 batch: 600 avg loss -2.663330 avg loss no lamb -2.663330 time 2020-06-25 18:13:12.825863
Model ind 665 epoch 32 batch: 700 avg loss -2.537554 avg loss no lamb -2.537554 time 2020-06-25 18:13:23.286921
Model ind 665 epoch 32 batch: 800 avg loss -2.634926 avg loss no lamb -2.634926 time 2020-06-25 18:13:33.761664
last batch sz 10
Pre: time 2020-06-25 18:13:47.506668: 
 	std: 0.0023298142
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.98, 0.9773, 0.983, 0.9783]
	train_accs: [0.97985, 0.9780667, 0.9766167, 0.9797, 0.97723335]
	best_train_sub_head: 0
	worst: 0.9773
	avg: 0.98030007
	best: 0.9829

Starting e_i: 33
Model ind 665 epoch 33 batch: 0 avg loss -2.842630 avg loss no lamb -2.842630 time 2020-06-25 18:13:48.194892
Model ind 665 epoch 33 batch: 100 avg loss -2.495472 avg loss no lamb -2.495472 time 2020-06-25 18:13:58.649190
Model ind 665 epoch 33 batch: 200 avg loss -2.647353 avg loss no lamb -2.647353 time 2020-06-25 18:14:09.179760
Model ind 665 epoch 33 batch: 300 avg loss -2.613689 avg loss no lamb -2.613689 time 2020-06-25 18:14:19.867669
Model ind 665 epoch 33 batch: 400 avg loss -2.562721 avg loss no lamb -2.562721 time 2020-06-25 18:14:30.242612
Model ind 665 epoch 33 batch: 500 avg loss -2.573203 avg loss no lamb -2.573203 time 2020-06-25 18:14:40.826411
Model ind 665 epoch 33 batch: 600 avg loss -2.645629 avg loss no lamb -2.645629 time 2020-06-25 18:14:51.458986
Model ind 665 epoch 33 batch: 700 avg loss -2.497912 avg loss no lamb -2.497912 time 2020-06-25 18:15:02.364819
Model ind 665 epoch 33 batch: 800 avg loss -2.663620 avg loss no lamb -2.663620 time 2020-06-25 18:15:12.671930
last batch sz 10
Pre: time 2020-06-25 18:15:25.894685: 
 	std: 0.0026125798
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9784, 0.9727, 0.9783, 0.9745]
	train_accs: [0.97831666, 0.97595, 0.97318333, 0.97765, 0.9748833]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.9766801
	best: 0.9795

Starting e_i: 34
Model ind 665 epoch 34 batch: 0 avg loss -2.753235 avg loss no lamb -2.753235 time 2020-06-25 18:15:26.557267
Model ind 665 epoch 34 batch: 100 avg loss -2.561176 avg loss no lamb -2.561176 time 2020-06-25 18:15:37.249213
Model ind 665 epoch 34 batch: 200 avg loss -2.590386 avg loss no lamb -2.590386 time 2020-06-25 18:15:47.948951
Model ind 665 epoch 34 batch: 300 avg loss -2.646589 avg loss no lamb -2.646589 time 2020-06-25 18:15:58.482128
Model ind 665 epoch 34 batch: 400 avg loss -2.580864 avg loss no lamb -2.580864 time 2020-06-25 18:16:09.167733
Model ind 665 epoch 34 batch: 500 avg loss -2.582347 avg loss no lamb -2.582347 time 2020-06-25 18:16:19.616906
Model ind 665 epoch 34 batch: 600 avg loss -2.672141 avg loss no lamb -2.672141 time 2020-06-25 18:16:30.160432
Model ind 665 epoch 34 batch: 700 avg loss -2.434087 avg loss no lamb -2.434087 time 2020-06-25 18:16:40.734668
Model ind 665 epoch 34 batch: 800 avg loss -2.607798 avg loss no lamb -2.607798 time 2020-06-25 18:16:51.438774
last batch sz 10
Pre: time 2020-06-25 18:17:04.944001: 
 	std: 0.0022703344
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9782, 0.9734, 0.9797, 0.9781]
	train_accs: [0.97903335, 0.97805, 0.97403336, 0.9791667, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97776
	best: 0.9797

Starting e_i: 35
Model ind 665 epoch 35 batch: 0 avg loss -2.804642 avg loss no lamb -2.804642 time 2020-06-25 18:17:05.626895
Model ind 665 epoch 35 batch: 100 avg loss -2.621526 avg loss no lamb -2.621526 time 2020-06-25 18:17:15.936215
Model ind 665 epoch 35 batch: 200 avg loss -2.633231 avg loss no lamb -2.633231 time 2020-06-25 18:17:26.647747
Model ind 665 epoch 35 batch: 300 avg loss -2.690708 avg loss no lamb -2.690708 time 2020-06-25 18:17:37.076320
Model ind 665 epoch 35 batch: 400 avg loss -2.585708 avg loss no lamb -2.585708 time 2020-06-25 18:17:47.852813
Model ind 665 epoch 35 batch: 500 avg loss -2.481381 avg loss no lamb -2.481381 time 2020-06-25 18:17:58.202499
Model ind 665 epoch 35 batch: 600 avg loss -2.723834 avg loss no lamb -2.723834 time 2020-06-25 18:18:08.899140
Model ind 665 epoch 35 batch: 700 avg loss -2.467999 avg loss no lamb -2.467999 time 2020-06-25 18:18:19.267115
Model ind 665 epoch 35 batch: 800 avg loss -2.634766 avg loss no lamb -2.634766 time 2020-06-25 18:18:29.909718
last batch sz 10
Pre: time 2020-06-25 18:18:43.349594: 
 	std: 0.0030318375
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9787, 0.9729, 0.981, 0.9759]
	train_accs: [0.9802167, 0.97893333, 0.9731167, 0.98048335, 0.9766667]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.9778
	best: 0.981

Starting e_i: 36
Model ind 665 epoch 36 batch: 0 avg loss -2.804491 avg loss no lamb -2.804491 time 2020-06-25 18:18:44.009631
Model ind 665 epoch 36 batch: 100 avg loss -2.678161 avg loss no lamb -2.678161 time 2020-06-25 18:18:54.653978
Model ind 665 epoch 36 batch: 200 avg loss -2.581526 avg loss no lamb -2.581526 time 2020-06-25 18:19:05.690920
Model ind 665 epoch 36 batch: 300 avg loss -2.592174 avg loss no lamb -2.592174 time 2020-06-25 18:19:16.294992
Model ind 665 epoch 36 batch: 400 avg loss -2.622775 avg loss no lamb -2.622775 time 2020-06-25 18:19:26.729112
Model ind 665 epoch 36 batch: 500 avg loss -2.544381 avg loss no lamb -2.544381 time 2020-06-25 18:19:37.320645
Model ind 665 epoch 36 batch: 600 avg loss -2.777508 avg loss no lamb -2.777508 time 2020-06-25 18:19:47.697754
Model ind 665 epoch 36 batch: 700 avg loss -2.443681 avg loss no lamb -2.443681 time 2020-06-25 18:19:58.367608
Model ind 665 epoch 36 batch: 800 avg loss -2.613168 avg loss no lamb -2.613168 time 2020-06-25 18:20:09.058548
last batch sz 10
Pre: time 2020-06-25 18:20:22.732958: 
 	std: 0.0026127428
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9794, 0.9757, 0.9807, 0.974]
	train_accs: [0.9794667, 0.97938335, 0.9756167, 0.9798167, 0.9758]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97793996
	best: 0.9807

Starting e_i: 37
Model ind 665 epoch 37 batch: 0 avg loss -2.851296 avg loss no lamb -2.851296 time 2020-06-25 18:20:23.397540
Model ind 665 epoch 37 batch: 100 avg loss -2.671797 avg loss no lamb -2.671797 time 2020-06-25 18:20:33.725488
Model ind 665 epoch 37 batch: 200 avg loss -2.547659 avg loss no lamb -2.547659 time 2020-06-25 18:20:44.045387
Model ind 665 epoch 37 batch: 300 avg loss -2.559381 avg loss no lamb -2.559381 time 2020-06-25 18:20:54.551371
Model ind 665 epoch 37 batch: 400 avg loss -2.540924 avg loss no lamb -2.540924 time 2020-06-25 18:21:05.062765
Model ind 665 epoch 37 batch: 500 avg loss -2.603854 avg loss no lamb -2.603854 time 2020-06-25 18:21:15.568388
Model ind 665 epoch 37 batch: 600 avg loss -2.685118 avg loss no lamb -2.685118 time 2020-06-25 18:21:26.099351
Model ind 665 epoch 37 batch: 700 avg loss -2.421945 avg loss no lamb -2.421945 time 2020-06-25 18:21:36.726993
Model ind 665 epoch 37 batch: 800 avg loss -2.582397 avg loss no lamb -2.582397 time 2020-06-25 18:21:47.420517
last batch sz 10
Pre: time 2020-06-25 18:22:01.015094: 
 	std: 0.0022485438
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9784, 0.9739, 0.9802, 0.9766]
	train_accs: [0.9792, 0.97748333, 0.9738167, 0.9788667, 0.9765667]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97770005
	best: 0.9794

Starting e_i: 38
Model ind 665 epoch 38 batch: 0 avg loss -2.826816 avg loss no lamb -2.826816 time 2020-06-25 18:22:01.642491
Model ind 665 epoch 38 batch: 100 avg loss -2.679644 avg loss no lamb -2.679644 time 2020-06-25 18:22:12.165876
Model ind 665 epoch 38 batch: 200 avg loss -2.601711 avg loss no lamb -2.601711 time 2020-06-25 18:22:22.710630
Model ind 665 epoch 38 batch: 300 avg loss -2.656826 avg loss no lamb -2.656826 time 2020-06-25 18:22:33.337110
Model ind 665 epoch 38 batch: 400 avg loss -2.614250 avg loss no lamb -2.614250 time 2020-06-25 18:22:43.818889
Model ind 665 epoch 38 batch: 500 avg loss -2.590713 avg loss no lamb -2.590713 time 2020-06-25 18:22:54.256165
Model ind 665 epoch 38 batch: 600 avg loss -2.671890 avg loss no lamb -2.671890 time 2020-06-25 18:23:04.796502
Model ind 665 epoch 38 batch: 700 avg loss -2.543153 avg loss no lamb -2.543153 time 2020-06-25 18:23:15.469486
Model ind 665 epoch 38 batch: 800 avg loss -2.487741 avg loss no lamb -2.487741 time 2020-06-25 18:23:26.062264
last batch sz 10
Pre: time 2020-06-25 18:23:39.613934: 
 	std: 0.0022675178
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9799, 0.9759, 0.9822, 0.9782]
	train_accs: [0.9799833, 0.97931665, 0.9766833, 0.98075, 0.97793335]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97952
	best: 0.9822

Starting e_i: 39
Model ind 665 epoch 39 batch: 0 avg loss -2.815872 avg loss no lamb -2.815872 time 2020-06-25 18:23:40.307152
Model ind 665 epoch 39 batch: 100 avg loss -2.649762 avg loss no lamb -2.649762 time 2020-06-25 18:23:51.319083
Model ind 665 epoch 39 batch: 200 avg loss -2.703767 avg loss no lamb -2.703767 time 2020-06-25 18:24:01.961589
Model ind 665 epoch 39 batch: 300 avg loss -2.539266 avg loss no lamb -2.539266 time 2020-06-25 18:24:12.699351
Model ind 665 epoch 39 batch: 400 avg loss -2.650979 avg loss no lamb -2.650979 time 2020-06-25 18:24:23.366139
Model ind 665 epoch 39 batch: 500 avg loss -2.564770 avg loss no lamb -2.564770 time 2020-06-25 18:24:33.767522
Model ind 665 epoch 39 batch: 600 avg loss -2.710022 avg loss no lamb -2.710022 time 2020-06-25 18:24:44.052234
Model ind 665 epoch 39 batch: 700 avg loss -2.512344 avg loss no lamb -2.512344 time 2020-06-25 18:24:54.563523
Model ind 665 epoch 39 batch: 800 avg loss -2.592522 avg loss no lamb -2.592522 time 2020-06-25 18:25:05.289372
last batch sz 10
Pre: time 2020-06-25 18:25:18.789423: 
 	std: 0.0021067583
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.9802, 0.9758, 0.9801, 0.9754]
	train_accs: [0.97996664, 0.97925, 0.9756, 0.9799167, 0.97763336]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97814
	best: 0.9792

Starting e_i: 40
Model ind 665 epoch 40 batch: 0 avg loss -2.818315 avg loss no lamb -2.818315 time 2020-06-25 18:25:19.461117
Model ind 665 epoch 40 batch: 100 avg loss -2.638155 avg loss no lamb -2.638155 time 2020-06-25 18:25:29.958242
Model ind 665 epoch 40 batch: 200 avg loss -2.523366 avg loss no lamb -2.523366 time 2020-06-25 18:25:40.639542
Model ind 665 epoch 40 batch: 300 avg loss -2.562870 avg loss no lamb -2.562870 time 2020-06-25 18:25:51.025934
Model ind 665 epoch 40 batch: 400 avg loss -2.580759 avg loss no lamb -2.580759 time 2020-06-25 18:26:01.643904
Model ind 665 epoch 40 batch: 500 avg loss -2.547693 avg loss no lamb -2.547693 time 2020-06-25 18:26:12.318576
Model ind 665 epoch 40 batch: 600 avg loss -2.698526 avg loss no lamb -2.698526 time 2020-06-25 18:26:22.776399
Model ind 665 epoch 40 batch: 700 avg loss -2.485458 avg loss no lamb -2.485458 time 2020-06-25 18:26:33.188992
Model ind 665 epoch 40 batch: 800 avg loss -2.613157 avg loss no lamb -2.613157 time 2020-06-25 18:26:43.344658
last batch sz 10
Pre: time 2020-06-25 18:26:57.135635: 
 	std: 0.0016709319
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9799, 0.9775, 0.9815, 0.9774]
	train_accs: [0.9799833, 0.97855, 0.9770833, 0.9807, 0.97821665]
	best_train_sub_head: 3
	worst: 0.9774
	avg: 0.9794
	best: 0.9815

Starting e_i: 41
Model ind 665 epoch 41 batch: 0 avg loss -2.799153 avg loss no lamb -2.799153 time 2020-06-25 18:26:58.919373
Model ind 665 epoch 41 batch: 100 avg loss -2.621144 avg loss no lamb -2.621144 time 2020-06-25 18:27:09.635122
Model ind 665 epoch 41 batch: 200 avg loss -2.608549 avg loss no lamb -2.608549 time 2020-06-25 18:27:20.035114
Model ind 665 epoch 41 batch: 300 avg loss -2.679923 avg loss no lamb -2.679923 time 2020-06-25 18:27:30.392818
Model ind 665 epoch 41 batch: 400 avg loss -2.599040 avg loss no lamb -2.599040 time 2020-06-25 18:27:41.365816
Model ind 665 epoch 41 batch: 500 avg loss -2.586974 avg loss no lamb -2.586974 time 2020-06-25 18:27:51.967309
Model ind 665 epoch 41 batch: 600 avg loss -2.754347 avg loss no lamb -2.754347 time 2020-06-25 18:28:02.762213
Model ind 665 epoch 41 batch: 700 avg loss -2.470141 avg loss no lamb -2.470141 time 2020-06-25 18:28:12.998685
Model ind 665 epoch 41 batch: 800 avg loss -2.679191 avg loss no lamb -2.679191 time 2020-06-25 18:28:23.302116
last batch sz 10
Pre: time 2020-06-25 18:28:36.665453: 
 	std: 0.0037648876
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.98, 0.972, 0.9824, 0.978]
	train_accs: [0.9802, 0.9788, 0.97025, 0.98076665, 0.9769167]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97886
	best: 0.9824

Starting e_i: 42
Model ind 665 epoch 42 batch: 0 avg loss -2.763190 avg loss no lamb -2.763190 time 2020-06-25 18:28:37.324852
Model ind 665 epoch 42 batch: 100 avg loss -2.639714 avg loss no lamb -2.639714 time 2020-06-25 18:28:47.661313
Model ind 665 epoch 42 batch: 200 avg loss -2.617167 avg loss no lamb -2.617167 time 2020-06-25 18:28:58.200971
Model ind 665 epoch 42 batch: 300 avg loss -2.631088 avg loss no lamb -2.631088 time 2020-06-25 18:29:08.662674
Model ind 665 epoch 42 batch: 400 avg loss -2.624428 avg loss no lamb -2.624428 time 2020-06-25 18:29:19.228081
Model ind 665 epoch 42 batch: 500 avg loss -2.621125 avg loss no lamb -2.621125 time 2020-06-25 18:29:29.829173
Model ind 665 epoch 42 batch: 600 avg loss -2.671210 avg loss no lamb -2.671210 time 2020-06-25 18:29:40.367614
Model ind 665 epoch 42 batch: 700 avg loss -2.509003 avg loss no lamb -2.509003 time 2020-06-25 18:29:50.886310
Model ind 665 epoch 42 batch: 800 avg loss -2.638718 avg loss no lamb -2.638718 time 2020-06-25 18:30:01.374033
last batch sz 10
Pre: time 2020-06-25 18:30:14.975198: 
 	std: 0.0013392521
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9775, 0.9758, 0.975, 0.9784, 0.9752]
	train_accs: [0.9788333, 0.97678334, 0.9758, 0.97973335, 0.97715]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97638
	best: 0.9784

Starting e_i: 43
Model ind 665 epoch 43 batch: 0 avg loss -2.818961 avg loss no lamb -2.818961 time 2020-06-25 18:30:15.646785
Model ind 665 epoch 43 batch: 100 avg loss -2.659771 avg loss no lamb -2.659771 time 2020-06-25 18:30:26.073409
Model ind 665 epoch 43 batch: 200 avg loss -2.620107 avg loss no lamb -2.620107 time 2020-06-25 18:30:36.512582
Model ind 665 epoch 43 batch: 300 avg loss -2.690770 avg loss no lamb -2.690770 time 2020-06-25 18:30:47.235925
Model ind 665 epoch 43 batch: 400 avg loss -2.634983 avg loss no lamb -2.634983 time 2020-06-25 18:30:57.944161
Model ind 665 epoch 43 batch: 500 avg loss -2.592352 avg loss no lamb -2.592352 time 2020-06-25 18:31:08.669650
Model ind 665 epoch 43 batch: 600 avg loss -2.730944 avg loss no lamb -2.730944 time 2020-06-25 18:31:19.274436
Model ind 665 epoch 43 batch: 700 avg loss -2.515746 avg loss no lamb -2.515746 time 2020-06-25 18:31:29.634246
Model ind 665 epoch 43 batch: 800 avg loss -2.621716 avg loss no lamb -2.621716 time 2020-06-25 18:31:40.333928
last batch sz 10
Pre: time 2020-06-25 18:31:53.839596: 
 	std: 0.002200906
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9792, 0.9763, 0.9819, 0.9778]
	train_accs: [0.9809333, 0.97965, 0.9763, 0.9809333, 0.9788]
	best_train_sub_head: 0
	worst: 0.9763
	avg: 0.9794
	best: 0.9818

Starting e_i: 44
Model ind 665 epoch 44 batch: 0 avg loss -2.801976 avg loss no lamb -2.801976 time 2020-06-25 18:31:54.478682
Model ind 665 epoch 44 batch: 100 avg loss -2.521138 avg loss no lamb -2.521138 time 2020-06-25 18:32:04.986978
Model ind 665 epoch 44 batch: 200 avg loss -2.593485 avg loss no lamb -2.593485 time 2020-06-25 18:32:15.359569
Model ind 665 epoch 44 batch: 300 avg loss -2.594428 avg loss no lamb -2.594428 time 2020-06-25 18:32:25.669268
Model ind 665 epoch 44 batch: 400 avg loss -2.606458 avg loss no lamb -2.606458 time 2020-06-25 18:32:36.225993
Model ind 665 epoch 44 batch: 500 avg loss -2.581078 avg loss no lamb -2.581078 time 2020-06-25 18:32:46.912416
Model ind 665 epoch 44 batch: 600 avg loss -2.750286 avg loss no lamb -2.750286 time 2020-06-25 18:32:57.673964
Model ind 665 epoch 44 batch: 700 avg loss -2.540602 avg loss no lamb -2.540602 time 2020-06-25 18:33:08.471419
Model ind 665 epoch 44 batch: 800 avg loss -2.605180 avg loss no lamb -2.605180 time 2020-06-25 18:33:18.686512
last batch sz 10
Pre: time 2020-06-25 18:33:31.801918: 
 	std: 0.001982315
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9796, 0.9761, 0.9817, 0.9783]
	train_accs: [0.9802833, 0.97996664, 0.97603333, 0.9813667, 0.9795833]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.97931993
	best: 0.9817

Starting e_i: 45
Model ind 665 epoch 45 batch: 0 avg loss -2.835685 avg loss no lamb -2.835685 time 2020-06-25 18:33:32.458757
Model ind 665 epoch 45 batch: 100 avg loss -2.594860 avg loss no lamb -2.594860 time 2020-06-25 18:33:42.949826
Model ind 665 epoch 45 batch: 200 avg loss -2.603519 avg loss no lamb -2.603519 time 2020-06-25 18:33:53.473052
Model ind 665 epoch 45 batch: 300 avg loss -2.530400 avg loss no lamb -2.530400 time 2020-06-25 18:34:04.003123
Model ind 665 epoch 45 batch: 400 avg loss -2.575165 avg loss no lamb -2.575165 time 2020-06-25 18:34:14.654760
Model ind 665 epoch 45 batch: 500 avg loss -2.606773 avg loss no lamb -2.606773 time 2020-06-25 18:34:24.758966
Model ind 665 epoch 45 batch: 600 avg loss -2.830832 avg loss no lamb -2.830832 time 2020-06-25 18:34:35.229440
Model ind 665 epoch 45 batch: 700 avg loss -2.488280 avg loss no lamb -2.488280 time 2020-06-25 18:34:45.727971
Model ind 665 epoch 45 batch: 800 avg loss -2.595725 avg loss no lamb -2.595725 time 2020-06-25 18:34:56.599664
last batch sz 10
Pre: time 2020-06-25 18:35:10.225958: 
 	std: 0.0025064661
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9777, 0.9732, 0.9792, 0.9752]
	train_accs: [0.98006666, 0.97835, 0.97466666, 0.9802667, 0.97675]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97704
	best: 0.9792

Starting e_i: 46
Model ind 665 epoch 46 batch: 0 avg loss -2.870671 avg loss no lamb -2.870671 time 2020-06-25 18:35:10.890473
Model ind 665 epoch 46 batch: 100 avg loss -2.553683 avg loss no lamb -2.553683 time 2020-06-25 18:35:21.418840
Model ind 665 epoch 46 batch: 200 avg loss -2.603032 avg loss no lamb -2.603032 time 2020-06-25 18:35:32.004480
Model ind 665 epoch 46 batch: 300 avg loss -2.618754 avg loss no lamb -2.618754 time 2020-06-25 18:35:42.782098
Model ind 665 epoch 46 batch: 400 avg loss -2.496160 avg loss no lamb -2.496160 time 2020-06-25 18:35:53.364475
Model ind 665 epoch 46 batch: 500 avg loss -2.590518 avg loss no lamb -2.590518 time 2020-06-25 18:36:03.973914
Model ind 665 epoch 46 batch: 600 avg loss -2.679431 avg loss no lamb -2.679431 time 2020-06-25 18:36:14.329617
Model ind 665 epoch 46 batch: 700 avg loss -2.449575 avg loss no lamb -2.449575 time 2020-06-25 18:36:24.716830
Model ind 665 epoch 46 batch: 800 avg loss -2.614391 avg loss no lamb -2.614391 time 2020-06-25 18:36:35.363854
last batch sz 10
Pre: time 2020-06-25 18:36:49.195584: 
 	std: 0.0028010104
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9797, 0.974, 0.9812, 0.9765]
	train_accs: [0.9794667, 0.97833335, 0.9737333, 0.9798667, 0.97765]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97848
	best: 0.9812

Starting e_i: 47
Model ind 665 epoch 47 batch: 0 avg loss -2.801968 avg loss no lamb -2.801968 time 2020-06-25 18:36:49.889320
Model ind 665 epoch 47 batch: 100 avg loss -2.653134 avg loss no lamb -2.653134 time 2020-06-25 18:37:00.693318
Model ind 665 epoch 47 batch: 200 avg loss -2.671942 avg loss no lamb -2.671942 time 2020-06-25 18:37:11.233922
Model ind 665 epoch 47 batch: 300 avg loss -2.494628 avg loss no lamb -2.494628 time 2020-06-25 18:37:22.170422
Model ind 665 epoch 47 batch: 400 avg loss -2.657845 avg loss no lamb -2.657845 time 2020-06-25 18:37:32.775068
Model ind 665 epoch 47 batch: 500 avg loss -2.516175 avg loss no lamb -2.516175 time 2020-06-25 18:37:43.262975
Model ind 665 epoch 47 batch: 600 avg loss -2.707313 avg loss no lamb -2.707313 time 2020-06-25 18:37:54.024517
Model ind 665 epoch 47 batch: 700 avg loss -2.513768 avg loss no lamb -2.513768 time 2020-06-25 18:38:04.615213
Model ind 665 epoch 47 batch: 800 avg loss -2.616313 avg loss no lamb -2.616313 time 2020-06-25 18:38:15.270355
last batch sz 10
Pre: time 2020-06-25 18:38:28.918680: 
 	std: 0.0018399977
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9787, 0.9755, 0.9808, 0.9777]
	train_accs: [0.97926664, 0.9779, 0.9752167, 0.97936666, 0.97753334]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97852004
	best: 0.9808

Starting e_i: 48
Model ind 665 epoch 48 batch: 0 avg loss -2.827769 avg loss no lamb -2.827769 time 2020-06-25 18:38:29.537763
Model ind 665 epoch 48 batch: 100 avg loss -2.625716 avg loss no lamb -2.625716 time 2020-06-25 18:38:40.042428
Model ind 665 epoch 48 batch: 200 avg loss -2.611437 avg loss no lamb -2.611437 time 2020-06-25 18:38:50.516305
Model ind 665 epoch 48 batch: 300 avg loss -2.621902 avg loss no lamb -2.621902 time 2020-06-25 18:39:01.108384
Model ind 665 epoch 48 batch: 400 avg loss -2.607242 avg loss no lamb -2.607242 time 2020-06-25 18:39:11.622525
Model ind 665 epoch 48 batch: 500 avg loss -2.577880 avg loss no lamb -2.577880 time 2020-06-25 18:39:22.099231
Model ind 665 epoch 48 batch: 600 avg loss -2.718127 avg loss no lamb -2.718127 time 2020-06-25 18:39:32.650726
Model ind 665 epoch 48 batch: 700 avg loss -2.450419 avg loss no lamb -2.450419 time 2020-06-25 18:39:43.176749
Model ind 665 epoch 48 batch: 800 avg loss -2.696777 avg loss no lamb -2.696777 time 2020-06-25 18:39:53.243725
last batch sz 10
Pre: time 2020-06-25 18:40:06.867774: 
 	std: 0.0013991595
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9778, 0.9774, 0.9759, 0.9786, 0.9747]
	train_accs: [0.97726667, 0.97566664, 0.9756167, 0.9780333, 0.97606665]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97687995
	best: 0.9786

Starting e_i: 49
Model ind 665 epoch 49 batch: 0 avg loss -2.823018 avg loss no lamb -2.823018 time 2020-06-25 18:40:07.517779
Model ind 665 epoch 49 batch: 100 avg loss -2.601454 avg loss no lamb -2.601454 time 2020-06-25 18:40:18.226042
Model ind 665 epoch 49 batch: 200 avg loss -2.673123 avg loss no lamb -2.673123 time 2020-06-25 18:40:28.651570
Model ind 665 epoch 49 batch: 300 avg loss -2.649483 avg loss no lamb -2.649483 time 2020-06-25 18:40:38.829522
Model ind 665 epoch 49 batch: 400 avg loss -2.576292 avg loss no lamb -2.576292 time 2020-06-25 18:40:49.451963
Model ind 665 epoch 49 batch: 500 avg loss -2.516865 avg loss no lamb -2.516865 time 2020-06-25 18:41:00.016673
Model ind 665 epoch 49 batch: 600 avg loss -2.774483 avg loss no lamb -2.774483 time 2020-06-25 18:41:10.575902
Model ind 665 epoch 49 batch: 700 avg loss -2.494409 avg loss no lamb -2.494409 time 2020-06-25 18:41:21.247272
Model ind 665 epoch 49 batch: 800 avg loss -2.621953 avg loss no lamb -2.621953 time 2020-06-25 18:41:31.955354
last batch sz 10
Pre: time 2020-06-25 18:41:45.525636: 
 	std: 0.0019339108
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9785, 0.9772, 0.9749, 0.9783, 0.9736]
	train_accs: [0.9780333, 0.97716665, 0.9744167, 0.97718334, 0.9743]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.9764999
	best: 0.9785

Starting e_i: 50
Model ind 665 epoch 50 batch: 0 avg loss -2.846051 avg loss no lamb -2.846051 time 2020-06-25 18:41:46.193588
Model ind 665 epoch 50 batch: 100 avg loss -2.649981 avg loss no lamb -2.649981 time 2020-06-25 18:41:56.957113
Model ind 665 epoch 50 batch: 200 avg loss -2.633112 avg loss no lamb -2.633112 time 2020-06-25 18:42:07.572390
Model ind 665 epoch 50 batch: 300 avg loss -2.709796 avg loss no lamb -2.709796 time 2020-06-25 18:42:18.375598
Model ind 665 epoch 50 batch: 400 avg loss -2.607489 avg loss no lamb -2.607489 time 2020-06-25 18:42:28.985439
Model ind 665 epoch 50 batch: 500 avg loss -2.589899 avg loss no lamb -2.589899 time 2020-06-25 18:42:39.530337
Model ind 665 epoch 50 batch: 600 avg loss -2.713780 avg loss no lamb -2.713780 time 2020-06-25 18:42:50.199433
Model ind 665 epoch 50 batch: 700 avg loss -2.563925 avg loss no lamb -2.563925 time 2020-06-25 18:43:00.767313
Model ind 665 epoch 50 batch: 800 avg loss -2.654285 avg loss no lamb -2.654285 time 2020-06-25 18:43:11.372626
last batch sz 10
Pre: time 2020-06-25 18:43:24.624205: 
 	std: 0.0026806004
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9783, 0.9775, 0.9736, 0.9795, 0.9727]
	train_accs: [0.9799, 0.9777333, 0.9756333, 0.97975, 0.97643334]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97631997
	best: 0.9783

Starting e_i: 51
Model ind 665 epoch 51 batch: 0 avg loss -2.803959 avg loss no lamb -2.803959 time 2020-06-25 18:43:26.472364
Model ind 665 epoch 51 batch: 100 avg loss -2.695839 avg loss no lamb -2.695839 time 2020-06-25 18:43:37.064180
Model ind 665 epoch 51 batch: 200 avg loss -2.641532 avg loss no lamb -2.641532 time 2020-06-25 18:43:47.878831
Model ind 665 epoch 51 batch: 300 avg loss -2.592991 avg loss no lamb -2.592991 time 2020-06-25 18:43:58.534435
Model ind 665 epoch 51 batch: 400 avg loss -2.579927 avg loss no lamb -2.579927 time 2020-06-25 18:44:09.059508
Model ind 665 epoch 51 batch: 500 avg loss -2.620394 avg loss no lamb -2.620394 time 2020-06-25 18:44:19.404662
Model ind 665 epoch 51 batch: 600 avg loss -2.740505 avg loss no lamb -2.740505 time 2020-06-25 18:44:30.192510
Model ind 665 epoch 51 batch: 700 avg loss -2.498492 avg loss no lamb -2.498492 time 2020-06-25 18:44:40.877713
Model ind 665 epoch 51 batch: 800 avg loss -2.575763 avg loss no lamb -2.575763 time 2020-06-25 18:44:51.555668
last batch sz 10
Pre: time 2020-06-25 18:45:05.209634: 
 	std: 0.0021905252
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9798, 0.9766, 0.982, 0.9773]
	train_accs: [0.9811, 0.97935, 0.9769167, 0.9815, 0.9784333]
	best_train_sub_head: 3
	worst: 0.9766
	avg: 0.97946006
	best: 0.982

Starting e_i: 52
Model ind 665 epoch 52 batch: 0 avg loss -2.753523 avg loss no lamb -2.753523 time 2020-06-25 18:45:05.887447
Model ind 665 epoch 52 batch: 100 avg loss -2.660235 avg loss no lamb -2.660235 time 2020-06-25 18:45:16.566600
Model ind 665 epoch 52 batch: 200 avg loss -2.602011 avg loss no lamb -2.602011 time 2020-06-25 18:45:27.202256
Model ind 665 epoch 52 batch: 300 avg loss -2.691570 avg loss no lamb -2.691570 time 2020-06-25 18:45:37.887550
Model ind 665 epoch 52 batch: 400 avg loss -2.578586 avg loss no lamb -2.578586 time 2020-06-25 18:45:48.492751
Model ind 665 epoch 52 batch: 500 avg loss -2.568736 avg loss no lamb -2.568736 time 2020-06-25 18:45:58.907901
Model ind 665 epoch 52 batch: 600 avg loss -2.730709 avg loss no lamb -2.730709 time 2020-06-25 18:46:09.883143
Model ind 665 epoch 52 batch: 700 avg loss -2.474384 avg loss no lamb -2.474384 time 2020-06-25 18:46:20.455204
Model ind 665 epoch 52 batch: 800 avg loss -2.677694 avg loss no lamb -2.677694 time 2020-06-25 18:46:31.082534
last batch sz 10
Pre: time 2020-06-25 18:46:44.576850: 
 	std: 0.0026209827
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9792, 0.9746, 0.9798, 0.9745]
	train_accs: [0.98116666, 0.9799333, 0.97658336, 0.98081666, 0.97746664]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97771996
	best: 0.9805

Starting e_i: 53
Model ind 665 epoch 53 batch: 0 avg loss -2.788296 avg loss no lamb -2.788296 time 2020-06-25 18:46:45.299149
Model ind 665 epoch 53 batch: 100 avg loss -2.658498 avg loss no lamb -2.658498 time 2020-06-25 18:46:55.956725
Model ind 665 epoch 53 batch: 200 avg loss -2.624081 avg loss no lamb -2.624081 time 2020-06-25 18:47:06.426705
Model ind 665 epoch 53 batch: 300 avg loss -2.725983 avg loss no lamb -2.725983 time 2020-06-25 18:47:17.109572
Model ind 665 epoch 53 batch: 400 avg loss -2.636437 avg loss no lamb -2.636437 time 2020-06-25 18:47:27.522716
Model ind 665 epoch 53 batch: 500 avg loss -2.564517 avg loss no lamb -2.564517 time 2020-06-25 18:47:38.564463
Model ind 665 epoch 53 batch: 600 avg loss -2.711260 avg loss no lamb -2.711260 time 2020-06-25 18:47:49.166307
Model ind 665 epoch 53 batch: 700 avg loss -2.604661 avg loss no lamb -2.604661 time 2020-06-25 18:47:59.877939
Model ind 665 epoch 53 batch: 800 avg loss -2.602973 avg loss no lamb -2.602973 time 2020-06-25 18:48:10.666450
last batch sz 10
Pre: time 2020-06-25 18:48:24.021694: 
 	std: 0.0032999436
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9822, 0.9754, 0.982, 0.9764]
	train_accs: [0.9823833, 0.98156667, 0.97671664, 0.9819667, 0.9781]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97988
	best: 0.9834

Starting e_i: 54
Model ind 665 epoch 54 batch: 0 avg loss -2.745327 avg loss no lamb -2.745327 time 2020-06-25 18:48:24.695493
Model ind 665 epoch 54 batch: 100 avg loss -2.652665 avg loss no lamb -2.652665 time 2020-06-25 18:48:35.301050
Model ind 665 epoch 54 batch: 200 avg loss -2.613515 avg loss no lamb -2.613515 time 2020-06-25 18:48:45.947277
Model ind 665 epoch 54 batch: 300 avg loss -2.720113 avg loss no lamb -2.720113 time 2020-06-25 18:48:56.469413
Model ind 665 epoch 54 batch: 400 avg loss -2.610456 avg loss no lamb -2.610456 time 2020-06-25 18:49:07.082347
Model ind 665 epoch 54 batch: 500 avg loss -2.528596 avg loss no lamb -2.528596 time 2020-06-25 18:49:17.467972
Model ind 665 epoch 54 batch: 600 avg loss -2.727471 avg loss no lamb -2.727471 time 2020-06-25 18:49:28.150266
Model ind 665 epoch 54 batch: 700 avg loss -2.589772 avg loss no lamb -2.589772 time 2020-06-25 18:49:38.963109
Model ind 665 epoch 54 batch: 800 avg loss -2.635049 avg loss no lamb -2.635049 time 2020-06-25 18:49:49.768172
last batch sz 10
Pre: time 2020-06-25 18:50:03.354420: 
 	std: 0.002602772
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9832, 0.9802, 0.9778, 0.9836, 0.9774]
	train_accs: [0.9813333, 0.97891665, 0.97655, 0.98115, 0.97723335]
	best_train_sub_head: 0
	worst: 0.9774
	avg: 0.98043996
	best: 0.9832

Starting e_i: 55
Model ind 665 epoch 55 batch: 0 avg loss -2.789693 avg loss no lamb -2.789693 time 2020-06-25 18:50:04.051088
Model ind 665 epoch 55 batch: 100 avg loss -2.643927 avg loss no lamb -2.643927 time 2020-06-25 18:50:14.717288
Model ind 665 epoch 55 batch: 200 avg loss -2.651899 avg loss no lamb -2.651899 time 2020-06-25 18:50:25.507344
Model ind 665 epoch 55 batch: 300 avg loss -2.713286 avg loss no lamb -2.713286 time 2020-06-25 18:50:36.178055
Model ind 665 epoch 55 batch: 400 avg loss -2.696688 avg loss no lamb -2.696688 time 2020-06-25 18:50:46.946462
Model ind 665 epoch 55 batch: 500 avg loss -2.636800 avg loss no lamb -2.636800 time 2020-06-25 18:50:57.545715
Model ind 665 epoch 55 batch: 600 avg loss -2.684486 avg loss no lamb -2.684486 time 2020-06-25 18:51:08.170786
Model ind 665 epoch 55 batch: 700 avg loss -2.509225 avg loss no lamb -2.509225 time 2020-06-25 18:51:18.565664
Model ind 665 epoch 55 batch: 800 avg loss -2.650051 avg loss no lamb -2.650051 time 2020-06-25 18:51:29.277576
last batch sz 10
Pre: time 2020-06-25 18:51:42.687449: 
 	std: 0.0026363535
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.984, 0.9822, 0.9796, 0.9844, 0.9775]
	train_accs: [0.98363334, 0.98153335, 0.9798167, 0.98373336, 0.97978336]
	best_train_sub_head: 3
	worst: 0.9775
	avg: 0.98154
	best: 0.9844

Starting e_i: 56
Model ind 665 epoch 56 batch: 0 avg loss -2.869526 avg loss no lamb -2.869526 time 2020-06-25 18:51:44.549381
Model ind 665 epoch 56 batch: 100 avg loss -2.662731 avg loss no lamb -2.662731 time 2020-06-25 18:51:55.079845
Model ind 665 epoch 56 batch: 200 avg loss -2.665726 avg loss no lamb -2.665726 time 2020-06-25 18:52:05.660078
Model ind 665 epoch 56 batch: 300 avg loss -2.707168 avg loss no lamb -2.707168 time 2020-06-25 18:52:16.315402
Model ind 665 epoch 56 batch: 400 avg loss -2.588846 avg loss no lamb -2.588846 time 2020-06-25 18:52:26.946398
Model ind 665 epoch 56 batch: 500 avg loss -2.669245 avg loss no lamb -2.669245 time 2020-06-25 18:52:37.080245
Model ind 665 epoch 56 batch: 600 avg loss -2.673626 avg loss no lamb -2.673626 time 2020-06-25 18:52:47.561424
Model ind 665 epoch 56 batch: 700 avg loss -2.543687 avg loss no lamb -2.543687 time 2020-06-25 18:52:58.446557
Model ind 665 epoch 56 batch: 800 avg loss -2.665042 avg loss no lamb -2.665042 time 2020-06-25 18:53:09.125304
last batch sz 10
Pre: time 2020-06-25 18:53:22.740895: 
 	std: 0.0031651873
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9787, 0.9734, 0.9798, 0.9738]
	train_accs: [0.98008335, 0.97825, 0.97535, 0.97968334, 0.9751]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97735995
	best: 0.9811

Starting e_i: 57
Model ind 665 epoch 57 batch: 0 avg loss -2.804060 avg loss no lamb -2.804060 time 2020-06-25 18:53:23.415748
Model ind 665 epoch 57 batch: 100 avg loss -2.580184 avg loss no lamb -2.580184 time 2020-06-25 18:53:33.798367
Model ind 665 epoch 57 batch: 200 avg loss -2.599169 avg loss no lamb -2.599169 time 2020-06-25 18:53:44.394083
Model ind 665 epoch 57 batch: 300 avg loss -2.608320 avg loss no lamb -2.608320 time 2020-06-25 18:53:55.010588
Model ind 665 epoch 57 batch: 400 avg loss -2.648268 avg loss no lamb -2.648268 time 2020-06-25 18:54:05.391980
Model ind 665 epoch 57 batch: 500 avg loss -2.583376 avg loss no lamb -2.583376 time 2020-06-25 18:54:15.801796
Model ind 665 epoch 57 batch: 600 avg loss -2.777639 avg loss no lamb -2.777639 time 2020-06-25 18:54:26.247612
Model ind 665 epoch 57 batch: 700 avg loss -2.535336 avg loss no lamb -2.535336 time 2020-06-25 18:54:36.947115
Model ind 665 epoch 57 batch: 800 avg loss -2.658599 avg loss no lamb -2.658599 time 2020-06-25 18:54:47.695257
last batch sz 10
Pre: time 2020-06-25 18:55:01.232312: 
 	std: 0.0028237659
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9801, 0.9752, 0.9821, 0.9772]
	train_accs: [0.98146665, 0.97938335, 0.9766, 0.98118335, 0.97861665]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97942
	best: 0.9825

Starting e_i: 58
Model ind 665 epoch 58 batch: 0 avg loss -2.800489 avg loss no lamb -2.800489 time 2020-06-25 18:55:01.895438
Model ind 665 epoch 58 batch: 100 avg loss -2.645802 avg loss no lamb -2.645802 time 2020-06-25 18:55:12.396339
Model ind 665 epoch 58 batch: 200 avg loss -2.682545 avg loss no lamb -2.682545 time 2020-06-25 18:55:23.301208
Model ind 665 epoch 58 batch: 300 avg loss -2.600072 avg loss no lamb -2.600072 time 2020-06-25 18:55:34.159866
Model ind 665 epoch 58 batch: 400 avg loss -2.656475 avg loss no lamb -2.656475 time 2020-06-25 18:55:44.591672
Model ind 665 epoch 58 batch: 500 avg loss -2.523226 avg loss no lamb -2.523226 time 2020-06-25 18:55:54.987035
Model ind 665 epoch 58 batch: 600 avg loss -2.714543 avg loss no lamb -2.714543 time 2020-06-25 18:56:05.727910
Model ind 665 epoch 58 batch: 700 avg loss -2.532183 avg loss no lamb -2.532183 time 2020-06-25 18:56:16.540182
Model ind 665 epoch 58 batch: 800 avg loss -2.665535 avg loss no lamb -2.665535 time 2020-06-25 18:56:27.278557
last batch sz 10
Pre: time 2020-06-25 18:56:40.780572: 
 	std: 0.0019001126
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9797, 0.9769, 0.9812, 0.9769]
	train_accs: [0.98121667, 0.9794667, 0.97786665, 0.9813167, 0.97891665]
	best_train_sub_head: 3
	worst: 0.9769
	avg: 0.97914
	best: 0.9812

Starting e_i: 59
Model ind 665 epoch 59 batch: 0 avg loss -2.868995 avg loss no lamb -2.868995 time 2020-06-25 18:56:41.479362
Model ind 665 epoch 59 batch: 100 avg loss -2.652115 avg loss no lamb -2.652115 time 2020-06-25 18:56:51.795781
Model ind 665 epoch 59 batch: 200 avg loss -2.661251 avg loss no lamb -2.661251 time 2020-06-25 18:57:02.327672
Model ind 665 epoch 59 batch: 300 avg loss -2.761807 avg loss no lamb -2.761807 time 2020-06-25 18:57:13.152735
Model ind 665 epoch 59 batch: 400 avg loss -2.614300 avg loss no lamb -2.614300 time 2020-06-25 18:57:23.786784
Model ind 665 epoch 59 batch: 500 avg loss -2.592726 avg loss no lamb -2.592726 time 2020-06-25 18:57:34.261004
Model ind 665 epoch 59 batch: 600 avg loss -2.782773 avg loss no lamb -2.782773 time 2020-06-25 18:57:44.587272
Model ind 665 epoch 59 batch: 700 avg loss -2.540261 avg loss no lamb -2.540261 time 2020-06-25 18:57:55.248731
Model ind 665 epoch 59 batch: 800 avg loss -2.613745 avg loss no lamb -2.613745 time 2020-06-25 18:58:05.712957
last batch sz 10
Pre: time 2020-06-25 18:58:19.354530: 
 	std: 0.0028890257
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9807, 0.9752, 0.9829, 0.9777]
	train_accs: [0.9824, 0.98078334, 0.9765, 0.98261666, 0.97915]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97973996
	best: 0.9829

Starting e_i: 60
Model ind 665 epoch 60 batch: 0 avg loss -2.799651 avg loss no lamb -2.799651 time 2020-06-25 18:58:20.058679
Model ind 665 epoch 60 batch: 100 avg loss -2.613606 avg loss no lamb -2.613606 time 2020-06-25 18:58:30.781426
Model ind 665 epoch 60 batch: 200 avg loss -2.596241 avg loss no lamb -2.596241 time 2020-06-25 18:58:41.487543
Model ind 665 epoch 60 batch: 300 avg loss -2.651670 avg loss no lamb -2.651670 time 2020-06-25 18:58:52.030527
Model ind 665 epoch 60 batch: 400 avg loss -2.600063 avg loss no lamb -2.600063 time 2020-06-25 18:59:02.976304
Model ind 665 epoch 60 batch: 500 avg loss -2.555095 avg loss no lamb -2.555095 time 2020-06-25 18:59:13.656317
Model ind 665 epoch 60 batch: 600 avg loss -2.759503 avg loss no lamb -2.759503 time 2020-06-25 18:59:24.217924
Model ind 665 epoch 60 batch: 700 avg loss -2.398681 avg loss no lamb -2.398681 time 2020-06-25 18:59:34.901825
Model ind 665 epoch 60 batch: 800 avg loss -2.631490 avg loss no lamb -2.631490 time 2020-06-25 18:59:45.562772
last batch sz 10
Pre: time 2020-06-25 18:59:58.957778: 
 	std: 0.0030676334
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9798, 0.9735, 0.981, 0.9755]
	train_accs: [0.98085, 0.97915, 0.9748333, 0.9803333, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97814
	best: 0.9809

Starting e_i: 61
Model ind 665 epoch 61 batch: 0 avg loss -2.810799 avg loss no lamb -2.810799 time 2020-06-25 19:00:00.801259
Model ind 665 epoch 61 batch: 100 avg loss -2.708483 avg loss no lamb -2.708483 time 2020-06-25 19:00:11.437547
Model ind 665 epoch 61 batch: 200 avg loss -2.656240 avg loss no lamb -2.656240 time 2020-06-25 19:00:21.874371
Model ind 665 epoch 61 batch: 300 avg loss -2.602008 avg loss no lamb -2.602008 time 2020-06-25 19:00:32.198864
Model ind 665 epoch 61 batch: 400 avg loss -2.629981 avg loss no lamb -2.629981 time 2020-06-25 19:00:42.684300
Model ind 665 epoch 61 batch: 500 avg loss -2.442293 avg loss no lamb -2.442293 time 2020-06-25 19:00:53.433875
Model ind 665 epoch 61 batch: 600 avg loss -2.697851 avg loss no lamb -2.697851 time 2020-06-25 19:01:04.159290
Model ind 665 epoch 61 batch: 700 avg loss -2.629599 avg loss no lamb -2.629599 time 2020-06-25 19:01:14.665179
Model ind 665 epoch 61 batch: 800 avg loss -2.599352 avg loss no lamb -2.599352 time 2020-06-25 19:01:24.970118
last batch sz 10
Pre: time 2020-06-25 19:01:38.584963: 
 	std: 0.0037679784
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9788, 0.9704, 0.9799, 0.9743]
	train_accs: [0.98118335, 0.9802667, 0.97071666, 0.98073334, 0.9754]
	best_train_sub_head: 0
	worst: 0.9704
	avg: 0.97668
	best: 0.98

Starting e_i: 62
Model ind 665 epoch 62 batch: 0 avg loss -2.935494 avg loss no lamb -2.935494 time 2020-06-25 19:01:39.264521
Model ind 665 epoch 62 batch: 100 avg loss -2.702886 avg loss no lamb -2.702886 time 2020-06-25 19:01:49.760037
Model ind 665 epoch 62 batch: 200 avg loss -2.640219 avg loss no lamb -2.640219 time 2020-06-25 19:02:00.452944
Model ind 665 epoch 62 batch: 300 avg loss -2.699268 avg loss no lamb -2.699268 time 2020-06-25 19:02:10.871150
Model ind 665 epoch 62 batch: 400 avg loss -2.531822 avg loss no lamb -2.531822 time 2020-06-25 19:02:21.402584
Model ind 665 epoch 62 batch: 500 avg loss -2.555966 avg loss no lamb -2.555966 time 2020-06-25 19:02:32.062806
Model ind 665 epoch 62 batch: 600 avg loss -2.738554 avg loss no lamb -2.738554 time 2020-06-25 19:02:42.852719
Model ind 665 epoch 62 batch: 700 avg loss -2.614989 avg loss no lamb -2.614989 time 2020-06-25 19:02:53.530071
Model ind 665 epoch 62 batch: 800 avg loss -2.637968 avg loss no lamb -2.637968 time 2020-06-25 19:03:03.975167
last batch sz 10
Pre: time 2020-06-25 19:03:17.290558: 
 	std: 0.0020035917
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.979, 0.9777, 0.9744, 0.9792, 0.975]
	train_accs: [0.9799833, 0.9787, 0.977, 0.9795333, 0.9777167]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97705996
	best: 0.979

Starting e_i: 63
Model ind 665 epoch 63 batch: 0 avg loss -2.781836 avg loss no lamb -2.781836 time 2020-06-25 19:03:17.994071
Model ind 665 epoch 63 batch: 100 avg loss -2.620247 avg loss no lamb -2.620247 time 2020-06-25 19:03:28.771929
Model ind 665 epoch 63 batch: 200 avg loss -2.625238 avg loss no lamb -2.625238 time 2020-06-25 19:03:39.363726
Model ind 665 epoch 63 batch: 300 avg loss -2.729320 avg loss no lamb -2.729320 time 2020-06-25 19:03:49.868529
Model ind 665 epoch 63 batch: 400 avg loss -2.609370 avg loss no lamb -2.609370 time 2020-06-25 19:04:00.450527
Model ind 665 epoch 63 batch: 500 avg loss -2.692097 avg loss no lamb -2.692097 time 2020-06-25 19:04:10.854516
Model ind 665 epoch 63 batch: 600 avg loss -2.789141 avg loss no lamb -2.789141 time 2020-06-25 19:04:21.570239
Model ind 665 epoch 63 batch: 700 avg loss -2.449354 avg loss no lamb -2.449354 time 2020-06-25 19:04:32.253953
Model ind 665 epoch 63 batch: 800 avg loss -2.655671 avg loss no lamb -2.655671 time 2020-06-25 19:04:42.684205
last batch sz 10
Pre: time 2020-06-25 19:04:56.279950: 
 	std: 0.0031283225
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9777, 0.9716, 0.9792, 0.9738]
	train_accs: [0.98025, 0.9780667, 0.9733833, 0.9799333, 0.97578335]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.97636
	best: 0.9795

Starting e_i: 64
Model ind 665 epoch 64 batch: 0 avg loss -2.799512 avg loss no lamb -2.799512 time 2020-06-25 19:04:56.967397
Model ind 665 epoch 64 batch: 100 avg loss -2.618262 avg loss no lamb -2.618262 time 2020-06-25 19:05:07.721687
Model ind 665 epoch 64 batch: 200 avg loss -2.639485 avg loss no lamb -2.639485 time 2020-06-25 19:05:18.263005
Model ind 665 epoch 64 batch: 300 avg loss -2.712312 avg loss no lamb -2.712312 time 2020-06-25 19:05:28.761810
Model ind 665 epoch 64 batch: 400 avg loss -2.645455 avg loss no lamb -2.645455 time 2020-06-25 19:05:39.525046
Model ind 665 epoch 64 batch: 500 avg loss -2.618013 avg loss no lamb -2.618013 time 2020-06-25 19:05:50.033184
Model ind 665 epoch 64 batch: 600 avg loss -2.794171 avg loss no lamb -2.794171 time 2020-06-25 19:06:00.685504
Model ind 665 epoch 64 batch: 700 avg loss -2.512919 avg loss no lamb -2.512919 time 2020-06-25 19:06:11.453745
Model ind 665 epoch 64 batch: 800 avg loss -2.671571 avg loss no lamb -2.671571 time 2020-06-25 19:06:21.996350
last batch sz 10
Pre: time 2020-06-25 19:06:35.653797: 
 	std: 0.0027389126
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9793, 0.9736, 0.9809, 0.9766]
	train_accs: [0.98121667, 0.98038334, 0.97655, 0.9813333, 0.97866666]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97818005
	best: 0.9809

Starting e_i: 65
Model ind 665 epoch 65 batch: 0 avg loss -2.854049 avg loss no lamb -2.854049 time 2020-06-25 19:06:36.374388
Model ind 665 epoch 65 batch: 100 avg loss -2.589696 avg loss no lamb -2.589696 time 2020-06-25 19:06:46.680213
Model ind 665 epoch 65 batch: 200 avg loss -2.606628 avg loss no lamb -2.606628 time 2020-06-25 19:06:57.484313
Model ind 665 epoch 65 batch: 300 avg loss -2.722160 avg loss no lamb -2.722160 time 2020-06-25 19:07:08.214612
Model ind 665 epoch 65 batch: 400 avg loss -2.658731 avg loss no lamb -2.658731 time 2020-06-25 19:07:18.991471
Model ind 665 epoch 65 batch: 500 avg loss -2.620332 avg loss no lamb -2.620332 time 2020-06-25 19:07:29.487817
Model ind 665 epoch 65 batch: 600 avg loss -2.650676 avg loss no lamb -2.650676 time 2020-06-25 19:07:39.971910
Model ind 665 epoch 65 batch: 700 avg loss -2.572909 avg loss no lamb -2.572909 time 2020-06-25 19:07:50.635415
Model ind 665 epoch 65 batch: 800 avg loss -2.631977 avg loss no lamb -2.631977 time 2020-06-25 19:08:01.557167
last batch sz 10
Pre: time 2020-06-25 19:08:15.006314: 
 	std: 0.0027224917
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9789, 0.9739, 0.9806, 0.9758]
	train_accs: [0.98083335, 0.9806, 0.97501665, 0.98185, 0.9777167]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.978
	best: 0.9806

Starting e_i: 66
Model ind 665 epoch 66 batch: 0 avg loss -2.901077 avg loss no lamb -2.901077 time 2020-06-25 19:08:15.659739
Model ind 665 epoch 66 batch: 100 avg loss -2.741880 avg loss no lamb -2.741880 time 2020-06-25 19:08:26.067667
Model ind 665 epoch 66 batch: 200 avg loss -2.535085 avg loss no lamb -2.535085 time 2020-06-25 19:08:36.829703
Model ind 665 epoch 66 batch: 300 avg loss -2.592191 avg loss no lamb -2.592191 time 2020-06-25 19:08:47.656696
Model ind 665 epoch 66 batch: 400 avg loss -2.622408 avg loss no lamb -2.622408 time 2020-06-25 19:08:59.092393
Model ind 665 epoch 66 batch: 500 avg loss -2.552295 avg loss no lamb -2.552295 time 2020-06-25 19:09:09.829150
Model ind 665 epoch 66 batch: 600 avg loss -2.780957 avg loss no lamb -2.780957 time 2020-06-25 19:09:20.677003
Model ind 665 epoch 66 batch: 700 avg loss -2.506623 avg loss no lamb -2.506623 time 2020-06-25 19:09:31.344757
Model ind 665 epoch 66 batch: 800 avg loss -2.630838 avg loss no lamb -2.630838 time 2020-06-25 19:09:42.003105
last batch sz 10
Pre: time 2020-06-25 19:09:55.857727: 
 	std: 0.0033145773
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.979, 0.9727, 0.9815, 0.9769]
	train_accs: [0.981, 0.97793335, 0.9731167, 0.98055, 0.97763336]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97834
	best: 0.9816

Starting e_i: 67
Model ind 665 epoch 67 batch: 0 avg loss -2.793626 avg loss no lamb -2.793626 time 2020-06-25 19:09:56.565905
Model ind 665 epoch 67 batch: 100 avg loss -2.610976 avg loss no lamb -2.610976 time 2020-06-25 19:10:07.623645
Model ind 665 epoch 67 batch: 200 avg loss -2.635041 avg loss no lamb -2.635041 time 2020-06-25 19:10:18.067256
Model ind 665 epoch 67 batch: 300 avg loss -2.697622 avg loss no lamb -2.697622 time 2020-06-25 19:10:28.774498
Model ind 665 epoch 67 batch: 400 avg loss -2.554099 avg loss no lamb -2.554099 time 2020-06-25 19:10:39.320521
Model ind 665 epoch 67 batch: 500 avg loss -2.610280 avg loss no lamb -2.610280 time 2020-06-25 19:10:49.843937
Model ind 665 epoch 67 batch: 600 avg loss -2.757117 avg loss no lamb -2.757117 time 2020-06-25 19:11:00.511123
Model ind 665 epoch 67 batch: 700 avg loss -2.564569 avg loss no lamb -2.564569 time 2020-06-25 19:11:11.107969
Model ind 665 epoch 67 batch: 800 avg loss -2.657523 avg loss no lamb -2.657523 time 2020-06-25 19:11:21.623980
last batch sz 10
Pre: time 2020-06-25 19:11:35.001735: 
 	std: 0.0017951005
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9812, 0.9776, 0.9814, 0.9779]
	train_accs: [0.9809167, 0.9794667, 0.97725, 0.98083335, 0.9785]
	best_train_sub_head: 0
	worst: 0.9776
	avg: 0.97994006
	best: 0.9816

Starting e_i: 68
Model ind 665 epoch 68 batch: 0 avg loss -2.828407 avg loss no lamb -2.828407 time 2020-06-25 19:11:35.661680
Model ind 665 epoch 68 batch: 100 avg loss -2.673665 avg loss no lamb -2.673665 time 2020-06-25 19:11:46.513936
Model ind 665 epoch 68 batch: 200 avg loss -2.717869 avg loss no lamb -2.717869 time 2020-06-25 19:11:56.982912
Model ind 665 epoch 68 batch: 300 avg loss -2.599256 avg loss no lamb -2.599256 time 2020-06-25 19:12:07.616033
Model ind 665 epoch 68 batch: 400 avg loss -2.617092 avg loss no lamb -2.617092 time 2020-06-25 19:12:17.968224
Model ind 665 epoch 68 batch: 500 avg loss -2.646083 avg loss no lamb -2.646083 time 2020-06-25 19:12:28.600538
Model ind 665 epoch 68 batch: 600 avg loss -2.749611 avg loss no lamb -2.749611 time 2020-06-25 19:12:39.258440
Model ind 665 epoch 68 batch: 700 avg loss -2.557186 avg loss no lamb -2.557186 time 2020-06-25 19:12:49.866077
Model ind 665 epoch 68 batch: 800 avg loss -2.606014 avg loss no lamb -2.606014 time 2020-06-25 19:13:00.320104
last batch sz 10
Pre: time 2020-06-25 19:13:13.600461: 
 	std: 0.0035453548
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.981, 0.9732, 0.9812, 0.9755]
	train_accs: [0.9806167, 0.9795833, 0.9741, 0.98071665, 0.97505]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97858
	best: 0.9812

Starting e_i: 69
Model ind 665 epoch 69 batch: 0 avg loss -2.862999 avg loss no lamb -2.862999 time 2020-06-25 19:13:14.300414
Model ind 665 epoch 69 batch: 100 avg loss -2.678910 avg loss no lamb -2.678910 time 2020-06-25 19:13:25.020193
Model ind 665 epoch 69 batch: 200 avg loss -2.673470 avg loss no lamb -2.673470 time 2020-06-25 19:13:35.718005
Model ind 665 epoch 69 batch: 300 avg loss -2.701715 avg loss no lamb -2.701715 time 2020-06-25 19:13:46.380081
Model ind 665 epoch 69 batch: 400 avg loss -2.593889 avg loss no lamb -2.593889 time 2020-06-25 19:13:56.657872
Model ind 665 epoch 69 batch: 500 avg loss -2.574403 avg loss no lamb -2.574403 time 2020-06-25 19:14:07.283479
Model ind 665 epoch 69 batch: 600 avg loss -2.713166 avg loss no lamb -2.713166 time 2020-06-25 19:14:17.847600
Model ind 665 epoch 69 batch: 700 avg loss -2.572890 avg loss no lamb -2.572890 time 2020-06-25 19:14:28.256261
Model ind 665 epoch 69 batch: 800 avg loss -2.695195 avg loss no lamb -2.695195 time 2020-06-25 19:14:38.818470
last batch sz 10
Pre: time 2020-06-25 19:14:52.301755: 
 	std: 0.0034706753
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9792, 0.9718, 0.9804, 0.9741]
	train_accs: [0.9805667, 0.9793, 0.97215, 0.9804, 0.9737333]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.97708005
	best: 0.9799

Starting e_i: 70
Model ind 665 epoch 70 batch: 0 avg loss -2.872444 avg loss no lamb -2.872444 time 2020-06-25 19:14:53.019374
Model ind 665 epoch 70 batch: 100 avg loss -2.679271 avg loss no lamb -2.679271 time 2020-06-25 19:15:03.838469
Model ind 665 epoch 70 batch: 200 avg loss -2.669626 avg loss no lamb -2.669626 time 2020-06-25 19:15:14.663014
Model ind 665 epoch 70 batch: 300 avg loss -2.683866 avg loss no lamb -2.683866 time 2020-06-25 19:15:25.181637
Model ind 665 epoch 70 batch: 400 avg loss -2.675238 avg loss no lamb -2.675238 time 2020-06-25 19:15:35.775183
Model ind 665 epoch 70 batch: 500 avg loss -2.641938 avg loss no lamb -2.641938 time 2020-06-25 19:15:46.241253
Model ind 665 epoch 70 batch: 600 avg loss -2.802565 avg loss no lamb -2.802565 time 2020-06-25 19:15:56.759965
Model ind 665 epoch 70 batch: 700 avg loss -2.519965 avg loss no lamb -2.519965 time 2020-06-25 19:16:07.887134
Model ind 665 epoch 70 batch: 800 avg loss -2.638073 avg loss no lamb -2.638073 time 2020-06-25 19:16:18.364036
last batch sz 10
Pre: time 2020-06-25 19:16:31.904550: 
 	std: 0.0031308683
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9835, 0.9827, 0.9756, 0.9833, 0.9786]
	train_accs: [0.9810333, 0.98088336, 0.9755333, 0.98153335, 0.97735]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.98073995
	best: 0.9833

Starting e_i: 71
Model ind 665 epoch 71 batch: 0 avg loss -2.873986 avg loss no lamb -2.873986 time 2020-06-25 19:16:33.718682
Model ind 665 epoch 71 batch: 100 avg loss -2.748492 avg loss no lamb -2.748492 time 2020-06-25 19:16:44.178998
Model ind 665 epoch 71 batch: 200 avg loss -2.649754 avg loss no lamb -2.649754 time 2020-06-25 19:16:54.844090
Model ind 665 epoch 71 batch: 300 avg loss -2.680335 avg loss no lamb -2.680335 time 2020-06-25 19:17:05.557167
Model ind 665 epoch 71 batch: 400 avg loss -2.704093 avg loss no lamb -2.704093 time 2020-06-25 19:17:15.978664
Model ind 665 epoch 71 batch: 500 avg loss -2.651656 avg loss no lamb -2.651656 time 2020-06-25 19:17:26.691074
Model ind 665 epoch 71 batch: 600 avg loss -2.720690 avg loss no lamb -2.720690 time 2020-06-25 19:17:37.253059
Model ind 665 epoch 71 batch: 700 avg loss -2.587899 avg loss no lamb -2.587899 time 2020-06-25 19:17:47.771533
Model ind 665 epoch 71 batch: 800 avg loss -2.672955 avg loss no lamb -2.672955 time 2020-06-25 19:17:58.424483
last batch sz 10
Pre: time 2020-06-25 19:18:11.994555: 
 	std: 0.0034683673
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9818, 0.9735, 0.982, 0.9773]
	train_accs: [0.9813, 0.98041666, 0.97276664, 0.9816167, 0.9763333]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97938
	best: 0.982

Starting e_i: 72
Model ind 665 epoch 72 batch: 0 avg loss -2.844867 avg loss no lamb -2.844867 time 2020-06-25 19:18:12.677938
Model ind 665 epoch 72 batch: 100 avg loss -2.678259 avg loss no lamb -2.678259 time 2020-06-25 19:18:23.246209
Model ind 665 epoch 72 batch: 200 avg loss -2.722144 avg loss no lamb -2.722144 time 2020-06-25 19:18:33.837916
Model ind 665 epoch 72 batch: 300 avg loss -2.709580 avg loss no lamb -2.709580 time 2020-06-25 19:18:44.378127
Model ind 665 epoch 72 batch: 400 avg loss -2.619785 avg loss no lamb -2.619785 time 2020-06-25 19:18:54.883922
Model ind 665 epoch 72 batch: 500 avg loss -2.664351 avg loss no lamb -2.664351 time 2020-06-25 19:19:05.447853
Model ind 665 epoch 72 batch: 600 avg loss -2.753871 avg loss no lamb -2.753871 time 2020-06-25 19:19:16.145187
Model ind 665 epoch 72 batch: 700 avg loss -2.472473 avg loss no lamb -2.472473 time 2020-06-25 19:19:26.717448
Model ind 665 epoch 72 batch: 800 avg loss -2.621507 avg loss no lamb -2.621507 time 2020-06-25 19:19:37.252704
last batch sz 10
Pre: time 2020-06-25 19:19:50.613467: 
 	std: 0.003359422
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9786, 0.9722, 0.9799, 0.9736]
	train_accs: [0.98108333, 0.9791833, 0.97328335, 0.98071665, 0.9755833]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97691995
	best: 0.9803

Starting e_i: 73
Model ind 665 epoch 73 batch: 0 avg loss -2.865759 avg loss no lamb -2.865759 time 2020-06-25 19:19:51.400110
Model ind 665 epoch 73 batch: 100 avg loss -2.702713 avg loss no lamb -2.702713 time 2020-06-25 19:20:02.066530
Model ind 665 epoch 73 batch: 200 avg loss -2.709961 avg loss no lamb -2.709961 time 2020-06-25 19:20:12.797157
Model ind 665 epoch 73 batch: 300 avg loss -2.694321 avg loss no lamb -2.694321 time 2020-06-25 19:20:23.387230
Model ind 665 epoch 73 batch: 400 avg loss -2.564048 avg loss no lamb -2.564048 time 2020-06-25 19:20:34.023983
Model ind 665 epoch 73 batch: 500 avg loss -2.636097 avg loss no lamb -2.636097 time 2020-06-25 19:20:45.028968
Model ind 665 epoch 73 batch: 600 avg loss -2.688108 avg loss no lamb -2.688108 time 2020-06-25 19:20:55.493257
Model ind 665 epoch 73 batch: 700 avg loss -2.520241 avg loss no lamb -2.520241 time 2020-06-25 19:21:06.115961
Model ind 665 epoch 73 batch: 800 avg loss -2.653646 avg loss no lamb -2.653646 time 2020-06-25 19:21:16.771458
last batch sz 10
Pre: time 2020-06-25 19:21:29.950553: 
 	std: 0.0020890192
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9806, 0.9762, 0.9807, 0.9767]
	train_accs: [0.9813667, 0.9809667, 0.97798336, 0.98155, 0.9787]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.979
	best: 0.9807

Starting e_i: 74
Model ind 665 epoch 74 batch: 0 avg loss -2.798359 avg loss no lamb -2.798359 time 2020-06-25 19:21:30.689057
Model ind 665 epoch 74 batch: 100 avg loss -2.782446 avg loss no lamb -2.782446 time 2020-06-25 19:21:41.156172
Model ind 665 epoch 74 batch: 200 avg loss -2.653702 avg loss no lamb -2.653702 time 2020-06-25 19:21:51.709572
Model ind 665 epoch 74 batch: 300 avg loss -2.625922 avg loss no lamb -2.625922 time 2020-06-25 19:22:02.382749
Model ind 665 epoch 74 batch: 400 avg loss -2.644346 avg loss no lamb -2.644346 time 2020-06-25 19:22:12.907912
Model ind 665 epoch 74 batch: 500 avg loss -2.591874 avg loss no lamb -2.591874 time 2020-06-25 19:22:23.585221
Model ind 665 epoch 74 batch: 600 avg loss -2.762478 avg loss no lamb -2.762478 time 2020-06-25 19:22:34.214573
Model ind 665 epoch 74 batch: 700 avg loss -2.504789 avg loss no lamb -2.504789 time 2020-06-25 19:22:44.854988
Model ind 665 epoch 74 batch: 800 avg loss -2.722739 avg loss no lamb -2.722739 time 2020-06-25 19:22:55.267436
last batch sz 10
Pre: time 2020-06-25 19:23:08.911853: 
 	std: 0.003747752
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9815, 0.973, 0.9818, 0.975]
	train_accs: [0.98143333, 0.9809, 0.97496665, 0.98156667, 0.9773333]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97852004
	best: 0.9818

Starting e_i: 75
Model ind 665 epoch 75 batch: 0 avg loss -2.848140 avg loss no lamb -2.848140 time 2020-06-25 19:23:09.633588
Model ind 665 epoch 75 batch: 100 avg loss -2.727953 avg loss no lamb -2.727953 time 2020-06-25 19:23:20.267334
Model ind 665 epoch 75 batch: 200 avg loss -2.724734 avg loss no lamb -2.724734 time 2020-06-25 19:23:30.554994
Model ind 665 epoch 75 batch: 300 avg loss -2.768363 avg loss no lamb -2.768363 time 2020-06-25 19:23:40.860931
Model ind 665 epoch 75 batch: 400 avg loss -2.561061 avg loss no lamb -2.561061 time 2020-06-25 19:23:51.637536
Model ind 665 epoch 75 batch: 500 avg loss -2.643124 avg loss no lamb -2.643124 time 2020-06-25 19:24:02.457585
Model ind 665 epoch 75 batch: 600 avg loss -2.764279 avg loss no lamb -2.764279 time 2020-06-25 19:24:13.118284
Model ind 665 epoch 75 batch: 700 avg loss -2.537060 avg loss no lamb -2.537060 time 2020-06-25 19:24:23.458046
Model ind 665 epoch 75 batch: 800 avg loss -2.681107 avg loss no lamb -2.681107 time 2020-06-25 19:24:34.043246
last batch sz 10
Pre: time 2020-06-25 19:24:47.557152: 
 	std: 0.003070253
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.981, 0.9751, 0.9825, 0.9762]
	train_accs: [0.98151666, 0.98083335, 0.97466666, 0.98185, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97933996
	best: 0.9825

Starting e_i: 76
Model ind 665 epoch 76 batch: 0 avg loss -2.819262 avg loss no lamb -2.819262 time 2020-06-25 19:24:48.256177
Model ind 665 epoch 76 batch: 100 avg loss -2.736790 avg loss no lamb -2.736790 time 2020-06-25 19:24:58.878155
Model ind 665 epoch 76 batch: 200 avg loss -2.687724 avg loss no lamb -2.687724 time 2020-06-25 19:25:09.697077
Model ind 665 epoch 76 batch: 300 avg loss -2.733860 avg loss no lamb -2.733860 time 2020-06-25 19:25:20.286803
Model ind 665 epoch 76 batch: 400 avg loss -2.593894 avg loss no lamb -2.593894 time 2020-06-25 19:25:30.891135
Model ind 665 epoch 76 batch: 500 avg loss -2.624709 avg loss no lamb -2.624709 time 2020-06-25 19:25:41.709280
Model ind 665 epoch 76 batch: 600 avg loss -2.811062 avg loss no lamb -2.811062 time 2020-06-25 19:25:52.523904
Model ind 665 epoch 76 batch: 700 avg loss -2.507269 avg loss no lamb -2.507269 time 2020-06-25 19:26:02.952132
Model ind 665 epoch 76 batch: 800 avg loss -2.625893 avg loss no lamb -2.625893 time 2020-06-25 19:26:13.294587
last batch sz 10
Pre: time 2020-06-25 19:26:26.737995: 
 	std: 0.003816073
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9806, 0.9727, 0.9814, 0.9739]
	train_accs: [0.982, 0.9809667, 0.97613335, 0.98225, 0.97751665]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97793996
	best: 0.9814

Starting e_i: 77
Model ind 665 epoch 77 batch: 0 avg loss -2.886245 avg loss no lamb -2.886245 time 2020-06-25 19:26:27.441816
Model ind 665 epoch 77 batch: 100 avg loss -2.707414 avg loss no lamb -2.707414 time 2020-06-25 19:26:37.946930
Model ind 665 epoch 77 batch: 200 avg loss -2.668350 avg loss no lamb -2.668350 time 2020-06-25 19:26:48.560765
Model ind 665 epoch 77 batch: 300 avg loss -2.712358 avg loss no lamb -2.712358 time 2020-06-25 19:26:59.194922
Model ind 665 epoch 77 batch: 400 avg loss -2.585648 avg loss no lamb -2.585648 time 2020-06-25 19:27:09.878013
Model ind 665 epoch 77 batch: 500 avg loss -2.601638 avg loss no lamb -2.601638 time 2020-06-25 19:27:20.834018
Model ind 665 epoch 77 batch: 600 avg loss -2.747135 avg loss no lamb -2.747135 time 2020-06-25 19:27:31.708225
Model ind 665 epoch 77 batch: 700 avg loss -2.547270 avg loss no lamb -2.547270 time 2020-06-25 19:27:42.464506
Model ind 665 epoch 77 batch: 800 avg loss -2.637072 avg loss no lamb -2.637072 time 2020-06-25 19:27:53.189727
last batch sz 10
Pre: time 2020-06-25 19:28:06.761537: 
 	std: 0.0023224107
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9807, 0.9764, 0.9822, 0.9778]
	train_accs: [0.98223335, 0.98118335, 0.9773667, 0.98225, 0.9791]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.97981995
	best: 0.9822

Starting e_i: 78
Model ind 665 epoch 78 batch: 0 avg loss -2.871956 avg loss no lamb -2.871956 time 2020-06-25 19:28:07.516289
Model ind 665 epoch 78 batch: 100 avg loss -2.719277 avg loss no lamb -2.719277 time 2020-06-25 19:28:18.274576
Model ind 665 epoch 78 batch: 200 avg loss -2.660662 avg loss no lamb -2.660662 time 2020-06-25 19:28:28.625776
Model ind 665 epoch 78 batch: 300 avg loss -2.708231 avg loss no lamb -2.708231 time 2020-06-25 19:28:39.160569
Model ind 665 epoch 78 batch: 400 avg loss -2.644085 avg loss no lamb -2.644085 time 2020-06-25 19:28:49.727513
Model ind 665 epoch 78 batch: 500 avg loss -2.690603 avg loss no lamb -2.690603 time 2020-06-25 19:29:00.336404
Model ind 665 epoch 78 batch: 600 avg loss -2.730946 avg loss no lamb -2.730946 time 2020-06-25 19:29:10.764879
Model ind 665 epoch 78 batch: 700 avg loss -2.477288 avg loss no lamb -2.477288 time 2020-06-25 19:29:21.255831
Model ind 665 epoch 78 batch: 800 avg loss -2.611323 avg loss no lamb -2.611323 time 2020-06-25 19:29:31.857042
last batch sz 10
Pre: time 2020-06-25 19:29:45.222931: 
 	std: 0.0039967024
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9797, 0.9715, 0.9808, 0.9733]
	train_accs: [0.98115, 0.97966665, 0.97398335, 0.9812833, 0.97533333]
	best_train_sub_head: 3
	worst: 0.9715
	avg: 0.97721994
	best: 0.9808

Starting e_i: 79
Model ind 665 epoch 79 batch: 0 avg loss -2.884623 avg loss no lamb -2.884623 time 2020-06-25 19:29:45.940931
Model ind 665 epoch 79 batch: 100 avg loss -2.719188 avg loss no lamb -2.719188 time 2020-06-25 19:29:56.558535
Model ind 665 epoch 79 batch: 200 avg loss -2.712452 avg loss no lamb -2.712452 time 2020-06-25 19:30:07.179130
Model ind 665 epoch 79 batch: 300 avg loss -2.718371 avg loss no lamb -2.718371 time 2020-06-25 19:30:17.620267
Model ind 665 epoch 79 batch: 400 avg loss -2.638412 avg loss no lamb -2.638412 time 2020-06-25 19:30:28.418554
Model ind 665 epoch 79 batch: 500 avg loss -2.590110 avg loss no lamb -2.590110 time 2020-06-25 19:30:39.081022
Model ind 665 epoch 79 batch: 600 avg loss -2.776826 avg loss no lamb -2.776826 time 2020-06-25 19:30:49.378735
Model ind 665 epoch 79 batch: 700 avg loss -2.498111 avg loss no lamb -2.498111 time 2020-06-25 19:30:59.752175
Model ind 665 epoch 79 batch: 800 avg loss -2.666607 avg loss no lamb -2.666607 time 2020-06-25 19:31:10.382045
last batch sz 10
Pre: time 2020-06-25 19:31:23.901540: 
 	std: 0.0022419714
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9812, 0.9769, 0.9822, 0.9781]
	train_accs: [0.98143333, 0.98046666, 0.97705, 0.9819, 0.9786]
	best_train_sub_head: 3
	worst: 0.9769
	avg: 0.98015994
	best: 0.9822

Starting e_i: 80
Model ind 665 epoch 80 batch: 0 avg loss -2.848398 avg loss no lamb -2.848398 time 2020-06-25 19:31:24.670314
Model ind 665 epoch 80 batch: 100 avg loss -2.775301 avg loss no lamb -2.775301 time 2020-06-25 19:31:35.681828
Model ind 665 epoch 80 batch: 200 avg loss -2.621045 avg loss no lamb -2.621045 time 2020-06-25 19:31:46.332446
Model ind 665 epoch 80 batch: 300 avg loss -2.687188 avg loss no lamb -2.687188 time 2020-06-25 19:31:56.730680
Model ind 665 epoch 80 batch: 400 avg loss -2.597286 avg loss no lamb -2.597286 time 2020-06-25 19:32:07.265321
Model ind 665 epoch 80 batch: 500 avg loss -2.580787 avg loss no lamb -2.580787 time 2020-06-25 19:32:17.838019
Model ind 665 epoch 80 batch: 600 avg loss -2.857784 avg loss no lamb -2.857784 time 2020-06-25 19:32:28.660203
Model ind 665 epoch 80 batch: 700 avg loss -2.600434 avg loss no lamb -2.600434 time 2020-06-25 19:32:39.392172
Model ind 665 epoch 80 batch: 800 avg loss -2.667226 avg loss no lamb -2.667226 time 2020-06-25 19:32:50.037168
last batch sz 10
Pre: time 2020-06-25 19:33:03.476208: 
 	std: 0.002332038
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9832, 0.9815, 0.9775, 0.9835, 0.9791]
	train_accs: [0.98211664, 0.9805667, 0.978, 0.9823667, 0.9788667]
	best_train_sub_head: 3
	worst: 0.9775
	avg: 0.98096
	best: 0.9835

Starting e_i: 81
Model ind 665 epoch 81 batch: 0 avg loss -2.778525 avg loss no lamb -2.778525 time 2020-06-25 19:33:05.305453
Model ind 665 epoch 81 batch: 100 avg loss -2.731731 avg loss no lamb -2.731731 time 2020-06-25 19:33:15.993269
Model ind 665 epoch 81 batch: 200 avg loss -2.686073 avg loss no lamb -2.686073 time 2020-06-25 19:33:26.521857
Model ind 665 epoch 81 batch: 300 avg loss -2.725891 avg loss no lamb -2.725891 time 2020-06-25 19:33:37.484534
Model ind 665 epoch 81 batch: 400 avg loss -2.644417 avg loss no lamb -2.644417 time 2020-06-25 19:33:48.208562
Model ind 665 epoch 81 batch: 500 avg loss -2.616686 avg loss no lamb -2.616686 time 2020-06-25 19:33:59.097906
Model ind 665 epoch 81 batch: 600 avg loss -2.794350 avg loss no lamb -2.794350 time 2020-06-25 19:34:09.666061
Model ind 665 epoch 81 batch: 700 avg loss -2.465315 avg loss no lamb -2.465315 time 2020-06-25 19:34:20.155949
Model ind 665 epoch 81 batch: 800 avg loss -2.682471 avg loss no lamb -2.682471 time 2020-06-25 19:34:30.501102
last batch sz 10
Pre: time 2020-06-25 19:34:43.871779: 
 	std: 0.003246913
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9825, 0.9757, 0.9832, 0.9769]
	train_accs: [0.98228335, 0.9813833, 0.9773667, 0.9827, 0.9784167]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.98024005
	best: 0.9832

Starting e_i: 82
Model ind 665 epoch 82 batch: 0 avg loss -2.877914 avg loss no lamb -2.877914 time 2020-06-25 19:34:44.560728
Model ind 665 epoch 82 batch: 100 avg loss -2.728772 avg loss no lamb -2.728772 time 2020-06-25 19:34:55.352413
Model ind 665 epoch 82 batch: 200 avg loss -2.640214 avg loss no lamb -2.640214 time 2020-06-25 19:35:05.934390
Model ind 665 epoch 82 batch: 300 avg loss -2.638342 avg loss no lamb -2.638342 time 2020-06-25 19:35:16.347067
Model ind 665 epoch 82 batch: 400 avg loss -2.594774 avg loss no lamb -2.594774 time 2020-06-25 19:35:26.651702
Model ind 665 epoch 82 batch: 500 avg loss -2.592165 avg loss no lamb -2.592165 time 2020-06-25 19:35:37.245661
Model ind 665 epoch 82 batch: 600 avg loss -2.667977 avg loss no lamb -2.667977 time 2020-06-25 19:35:47.821481
Model ind 665 epoch 82 batch: 700 avg loss -2.551403 avg loss no lamb -2.551403 time 2020-06-25 19:35:58.433610
Model ind 665 epoch 82 batch: 800 avg loss -2.644357 avg loss no lamb -2.644357 time 2020-06-25 19:36:08.738017
last batch sz 10
Pre: time 2020-06-25 19:36:22.088221: 
 	std: 0.0041474863
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9803, 0.9727, 0.9813, 0.973]
	train_accs: [0.9817, 0.98078334, 0.97546667, 0.98186666, 0.9767333]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97788
	best: 0.9813

Starting e_i: 83
Model ind 665 epoch 83 batch: 0 avg loss -2.872978 avg loss no lamb -2.872978 time 2020-06-25 19:36:22.792027
Model ind 665 epoch 83 batch: 100 avg loss -2.643165 avg loss no lamb -2.643165 time 2020-06-25 19:36:33.619410
Model ind 665 epoch 83 batch: 200 avg loss -2.686602 avg loss no lamb -2.686602 time 2020-06-25 19:36:44.139874
Model ind 665 epoch 83 batch: 300 avg loss -2.680642 avg loss no lamb -2.680642 time 2020-06-25 19:36:54.582474
Model ind 665 epoch 83 batch: 400 avg loss -2.642674 avg loss no lamb -2.642674 time 2020-06-25 19:37:05.349323
Model ind 665 epoch 83 batch: 500 avg loss -2.658861 avg loss no lamb -2.658861 time 2020-06-25 19:37:16.341920
Model ind 665 epoch 83 batch: 600 avg loss -2.769357 avg loss no lamb -2.769357 time 2020-06-25 19:37:26.847371
Model ind 665 epoch 83 batch: 700 avg loss -2.596304 avg loss no lamb -2.596304 time 2020-06-25 19:37:37.422184
Model ind 665 epoch 83 batch: 800 avg loss -2.679451 avg loss no lamb -2.679451 time 2020-06-25 19:37:47.953003
last batch sz 10
Pre: time 2020-06-25 19:38:01.346479: 
 	std: 0.0031511292
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9815, 0.976, 0.9821, 0.9753]
	train_accs: [0.9822, 0.9816, 0.9777833, 0.9823833, 0.97815]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97947997
	best: 0.9821

Starting e_i: 84
Model ind 665 epoch 84 batch: 0 avg loss -2.849164 avg loss no lamb -2.849164 time 2020-06-25 19:38:02.031455
Model ind 665 epoch 84 batch: 100 avg loss -2.714755 avg loss no lamb -2.714755 time 2020-06-25 19:38:12.653012
Model ind 665 epoch 84 batch: 200 avg loss -2.688275 avg loss no lamb -2.688275 time 2020-06-25 19:38:23.389355
Model ind 665 epoch 84 batch: 300 avg loss -2.775391 avg loss no lamb -2.775391 time 2020-06-25 19:38:34.082980
Model ind 665 epoch 84 batch: 400 avg loss -2.669962 avg loss no lamb -2.669962 time 2020-06-25 19:38:44.508952
Model ind 665 epoch 84 batch: 500 avg loss -2.600364 avg loss no lamb -2.600364 time 2020-06-25 19:38:55.132791
Model ind 665 epoch 84 batch: 600 avg loss -2.726263 avg loss no lamb -2.726263 time 2020-06-25 19:39:05.963039
Model ind 665 epoch 84 batch: 700 avg loss -2.610643 avg loss no lamb -2.610643 time 2020-06-25 19:39:16.671809
Model ind 665 epoch 84 batch: 800 avg loss -2.708401 avg loss no lamb -2.708401 time 2020-06-25 19:39:27.228124
last batch sz 10
Pre: time 2020-06-25 19:39:40.788285: 
 	std: 0.0027830866
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9794, 0.9745, 0.981, 0.9752]
	train_accs: [0.98041666, 0.97965, 0.97636664, 0.98071665, 0.97726667]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97818005
	best: 0.981

Starting e_i: 85
Model ind 665 epoch 85 batch: 0 avg loss -2.874746 avg loss no lamb -2.874746 time 2020-06-25 19:39:41.440593
Model ind 665 epoch 85 batch: 100 avg loss -2.605379 avg loss no lamb -2.605379 time 2020-06-25 19:39:52.179040
Model ind 665 epoch 85 batch: 200 avg loss -2.731525 avg loss no lamb -2.731525 time 2020-06-25 19:40:02.668992
Model ind 665 epoch 85 batch: 300 avg loss -2.769244 avg loss no lamb -2.769244 time 2020-06-25 19:40:13.176853
Model ind 665 epoch 85 batch: 400 avg loss -2.654514 avg loss no lamb -2.654514 time 2020-06-25 19:40:23.768999
Model ind 665 epoch 85 batch: 500 avg loss -2.612484 avg loss no lamb -2.612484 time 2020-06-25 19:40:34.284468
Model ind 665 epoch 85 batch: 600 avg loss -2.723284 avg loss no lamb -2.723284 time 2020-06-25 19:40:44.910346
Model ind 665 epoch 85 batch: 700 avg loss -2.613828 avg loss no lamb -2.613828 time 2020-06-25 19:40:55.504633
Model ind 665 epoch 85 batch: 800 avg loss -2.706224 avg loss no lamb -2.706224 time 2020-06-25 19:41:06.192641
last batch sz 10
Pre: time 2020-06-25 19:41:19.741455: 
 	std: 0.00301091
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9815, 0.9753, 0.9815, 0.9757]
	train_accs: [0.98146665, 0.97978336, 0.97625, 0.98135, 0.97745]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97918
	best: 0.9819

Starting e_i: 86
Model ind 665 epoch 86 batch: 0 avg loss -2.885147 avg loss no lamb -2.885147 time 2020-06-25 19:41:20.401753
Model ind 665 epoch 86 batch: 100 avg loss -2.645313 avg loss no lamb -2.645313 time 2020-06-25 19:41:30.941249
Model ind 665 epoch 86 batch: 200 avg loss -2.716885 avg loss no lamb -2.716885 time 2020-06-25 19:41:41.440723
Model ind 665 epoch 86 batch: 300 avg loss -2.660531 avg loss no lamb -2.660531 time 2020-06-25 19:41:52.090065
Model ind 665 epoch 86 batch: 400 avg loss -2.589380 avg loss no lamb -2.589380 time 2020-06-25 19:42:02.834992
Model ind 665 epoch 86 batch: 500 avg loss -2.574707 avg loss no lamb -2.574707 time 2020-06-25 19:42:13.278643
Model ind 665 epoch 86 batch: 600 avg loss -2.762863 avg loss no lamb -2.762863 time 2020-06-25 19:42:23.820145
Model ind 665 epoch 86 batch: 700 avg loss -2.582124 avg loss no lamb -2.582124 time 2020-06-25 19:42:34.424050
Model ind 665 epoch 86 batch: 800 avg loss -2.603478 avg loss no lamb -2.603478 time 2020-06-25 19:42:44.969613
last batch sz 10
Pre: time 2020-06-25 19:42:58.613080: 
 	std: 0.003124738
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9806, 0.9739, 0.981, 0.9745]
	train_accs: [0.98081666, 0.98071665, 0.97536665, 0.98153335, 0.97678334]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.978
	best: 0.981

Starting e_i: 87
Model ind 665 epoch 87 batch: 0 avg loss -2.840780 avg loss no lamb -2.840780 time 2020-06-25 19:42:59.291369
Model ind 665 epoch 87 batch: 100 avg loss -2.666220 avg loss no lamb -2.666220 time 2020-06-25 19:43:09.984932
Model ind 665 epoch 87 batch: 200 avg loss -2.703329 avg loss no lamb -2.703329 time 2020-06-25 19:43:20.646957
Model ind 665 epoch 87 batch: 300 avg loss -2.679165 avg loss no lamb -2.679165 time 2020-06-25 19:43:31.349833
Model ind 665 epoch 87 batch: 400 avg loss -2.604655 avg loss no lamb -2.604655 time 2020-06-25 19:43:41.922500
Model ind 665 epoch 87 batch: 500 avg loss -2.640806 avg loss no lamb -2.640806 time 2020-06-25 19:43:52.577943
Model ind 665 epoch 87 batch: 600 avg loss -2.808752 avg loss no lamb -2.808752 time 2020-06-25 19:44:03.345832
Model ind 665 epoch 87 batch: 700 avg loss -2.541393 avg loss no lamb -2.541393 time 2020-06-25 19:44:14.060483
Model ind 665 epoch 87 batch: 800 avg loss -2.619996 avg loss no lamb -2.619996 time 2020-06-25 19:44:24.569720
last batch sz 10
Pre: time 2020-06-25 19:44:38.074322: 
 	std: 0.003615745
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9795, 0.9716, 0.9806, 0.9738]
	train_accs: [0.97985, 0.97901666, 0.97358334, 0.98031664, 0.97565]
	best_train_sub_head: 3
	worst: 0.9716
	avg: 0.9770201
	best: 0.9806

Starting e_i: 88
Model ind 665 epoch 88 batch: 0 avg loss -2.870153 avg loss no lamb -2.870153 time 2020-06-25 19:44:38.793516
Model ind 665 epoch 88 batch: 100 avg loss -2.737585 avg loss no lamb -2.737585 time 2020-06-25 19:44:49.333051
Model ind 665 epoch 88 batch: 200 avg loss -2.730762 avg loss no lamb -2.730762 time 2020-06-25 19:44:59.872982
Model ind 665 epoch 88 batch: 300 avg loss -2.673061 avg loss no lamb -2.673061 time 2020-06-25 19:45:10.554869
Model ind 665 epoch 88 batch: 400 avg loss -2.656151 avg loss no lamb -2.656151 time 2020-06-25 19:45:21.057199
Model ind 665 epoch 88 batch: 500 avg loss -2.656047 avg loss no lamb -2.656047 time 2020-06-25 19:45:31.385374
Model ind 665 epoch 88 batch: 600 avg loss -2.760699 avg loss no lamb -2.760699 time 2020-06-25 19:45:42.177104
Model ind 665 epoch 88 batch: 700 avg loss -2.575446 avg loss no lamb -2.575446 time 2020-06-25 19:45:52.660705
Model ind 665 epoch 88 batch: 800 avg loss -2.729717 avg loss no lamb -2.729717 time 2020-06-25 19:46:03.160542
last batch sz 10
Pre: time 2020-06-25 19:46:16.635454: 
 	std: 0.004187128
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9805, 0.9723, 0.9822, 0.9738]
	train_accs: [0.98155, 0.98078334, 0.97276664, 0.98226666, 0.97571665]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.9781
	best: 0.9822

Starting e_i: 89
Model ind 665 epoch 89 batch: 0 avg loss -2.835379 avg loss no lamb -2.835379 time 2020-06-25 19:46:17.310060
Model ind 665 epoch 89 batch: 100 avg loss -2.726203 avg loss no lamb -2.726203 time 2020-06-25 19:46:27.831087
Model ind 665 epoch 89 batch: 200 avg loss -2.693116 avg loss no lamb -2.693116 time 2020-06-25 19:46:38.431419
Model ind 665 epoch 89 batch: 300 avg loss -2.705358 avg loss no lamb -2.705358 time 2020-06-25 19:46:49.170780
Model ind 665 epoch 89 batch: 400 avg loss -2.637169 avg loss no lamb -2.637169 time 2020-06-25 19:46:59.788698
Model ind 665 epoch 89 batch: 500 avg loss -2.620209 avg loss no lamb -2.620209 time 2020-06-25 19:47:10.343520
Model ind 665 epoch 89 batch: 600 avg loss -2.793787 avg loss no lamb -2.793787 time 2020-06-25 19:47:21.055310
Model ind 665 epoch 89 batch: 700 avg loss -2.465733 avg loss no lamb -2.465733 time 2020-06-25 19:47:31.559298
Model ind 665 epoch 89 batch: 800 avg loss -2.717211 avg loss no lamb -2.717211 time 2020-06-25 19:47:42.533068
last batch sz 10
Pre: time 2020-06-25 19:47:56.507286: 
 	std: 0.0038747413
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9828, 0.9741, 0.9825, 0.9753]
	train_accs: [0.98261666, 0.98221666, 0.97606665, 0.98303336, 0.9780833]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97942
	best: 0.9825

Starting e_i: 90
Model ind 665 epoch 90 batch: 0 avg loss -2.883390 avg loss no lamb -2.883390 time 2020-06-25 19:47:57.234128
Model ind 665 epoch 90 batch: 100 avg loss -2.687544 avg loss no lamb -2.687544 time 2020-06-25 19:48:08.029615
Model ind 665 epoch 90 batch: 200 avg loss -2.675209 avg loss no lamb -2.675209 time 2020-06-25 19:48:18.546602
Model ind 665 epoch 90 batch: 300 avg loss -2.682645 avg loss no lamb -2.682645 time 2020-06-25 19:48:29.238695
Model ind 665 epoch 90 batch: 400 avg loss -2.668590 avg loss no lamb -2.668590 time 2020-06-25 19:48:39.901387
Model ind 665 epoch 90 batch: 500 avg loss -2.684195 avg loss no lamb -2.684195 time 2020-06-25 19:48:50.711729
Model ind 665 epoch 90 batch: 600 avg loss -2.775559 avg loss no lamb -2.775559 time 2020-06-25 19:49:01.338394
Model ind 665 epoch 90 batch: 700 avg loss -2.605381 avg loss no lamb -2.605381 time 2020-06-25 19:49:12.001592
Model ind 665 epoch 90 batch: 800 avg loss -2.624701 avg loss no lamb -2.624701 time 2020-06-25 19:49:22.670794
last batch sz 10
Pre: time 2020-06-25 19:49:36.345468: 
 	std: 0.004841486
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9805, 0.9709, 0.9823, 0.9726]
	train_accs: [0.98216665, 0.98108333, 0.9737833, 0.98226666, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9709
	avg: 0.9776
	best: 0.9823

Starting e_i: 91
Model ind 665 epoch 91 batch: 0 avg loss -2.861146 avg loss no lamb -2.861146 time 2020-06-25 19:49:38.205664
Model ind 665 epoch 91 batch: 100 avg loss -2.730886 avg loss no lamb -2.730886 time 2020-06-25 19:49:48.742616
Model ind 665 epoch 91 batch: 200 avg loss -2.666669 avg loss no lamb -2.666669 time 2020-06-25 19:49:59.389948
Model ind 665 epoch 91 batch: 300 avg loss -2.661316 avg loss no lamb -2.661316 time 2020-06-25 19:50:10.251995
Model ind 665 epoch 91 batch: 400 avg loss -2.658601 avg loss no lamb -2.658601 time 2020-06-25 19:50:20.777272
Model ind 665 epoch 91 batch: 500 avg loss -2.592005 avg loss no lamb -2.592005 time 2020-06-25 19:50:31.323558
Model ind 665 epoch 91 batch: 600 avg loss -2.787737 avg loss no lamb -2.787737 time 2020-06-25 19:50:41.761183
Model ind 665 epoch 91 batch: 700 avg loss -2.576823 avg loss no lamb -2.576823 time 2020-06-25 19:50:52.169319
Model ind 665 epoch 91 batch: 800 avg loss -2.660174 avg loss no lamb -2.660174 time 2020-06-25 19:51:03.040993
last batch sz 10
Pre: time 2020-06-25 19:51:16.789574: 
 	std: 0.003825177
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9815, 0.9741, 0.9833, 0.9755]
	train_accs: [0.98246664, 0.9817167, 0.97455, 0.98301667, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.9794
	best: 0.9833

Starting e_i: 92
Model ind 665 epoch 92 batch: 0 avg loss -2.817113 avg loss no lamb -2.817113 time 2020-06-25 19:51:17.475776
Model ind 665 epoch 92 batch: 100 avg loss -2.709617 avg loss no lamb -2.709617 time 2020-06-25 19:51:28.189551
Model ind 665 epoch 92 batch: 200 avg loss -2.629820 avg loss no lamb -2.629820 time 2020-06-25 19:51:38.699683
Model ind 665 epoch 92 batch: 300 avg loss -2.694131 avg loss no lamb -2.694131 time 2020-06-25 19:51:49.422644
Model ind 665 epoch 92 batch: 400 avg loss -2.620242 avg loss no lamb -2.620242 time 2020-06-25 19:52:00.028621
Model ind 665 epoch 92 batch: 500 avg loss -2.670251 avg loss no lamb -2.670251 time 2020-06-25 19:52:10.941547
Model ind 665 epoch 92 batch: 600 avg loss -2.735027 avg loss no lamb -2.735027 time 2020-06-25 19:52:21.633836
Model ind 665 epoch 92 batch: 700 avg loss -2.514599 avg loss no lamb -2.514599 time 2020-06-25 19:52:32.160648
Model ind 665 epoch 92 batch: 800 avg loss -2.665920 avg loss no lamb -2.665920 time 2020-06-25 19:52:42.827599
last batch sz 10
Pre: time 2020-06-25 19:52:56.553177: 
 	std: 0.003599463
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9824, 0.9747, 0.9836, 0.9769]
	train_accs: [0.98245, 0.9810167, 0.97541666, 0.98255, 0.9773167]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.98010004
	best: 0.9836

Starting e_i: 93
Model ind 665 epoch 93 batch: 0 avg loss -2.793474 avg loss no lamb -2.793474 time 2020-06-25 19:52:57.297510
Model ind 665 epoch 93 batch: 100 avg loss -2.684039 avg loss no lamb -2.684039 time 2020-06-25 19:53:07.967734
Model ind 665 epoch 93 batch: 200 avg loss -2.697106 avg loss no lamb -2.697106 time 2020-06-25 19:53:18.515219
Model ind 665 epoch 93 batch: 300 avg loss -2.694677 avg loss no lamb -2.694677 time 2020-06-25 19:53:29.153385
Model ind 665 epoch 93 batch: 400 avg loss -2.644766 avg loss no lamb -2.644766 time 2020-06-25 19:53:39.788094
Model ind 665 epoch 93 batch: 500 avg loss -2.693037 avg loss no lamb -2.693037 time 2020-06-25 19:53:50.390612
Model ind 665 epoch 93 batch: 600 avg loss -2.678823 avg loss no lamb -2.678823 time 2020-06-25 19:54:01.405062
Model ind 665 epoch 93 batch: 700 avg loss -2.606622 avg loss no lamb -2.606622 time 2020-06-25 19:54:12.251594
Model ind 665 epoch 93 batch: 800 avg loss -2.691959 avg loss no lamb -2.691959 time 2020-06-25 19:54:23.054245
last batch sz 10
Pre: time 2020-06-25 19:54:36.599658: 
 	std: 0.00456042
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9816, 0.9721, 0.9837, 0.9746]
	train_accs: [0.98286664, 0.9819667, 0.97535, 0.98368335, 0.9772]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97878
	best: 0.9837

Starting e_i: 94
Model ind 665 epoch 94 batch: 0 avg loss -2.845648 avg loss no lamb -2.845648 time 2020-06-25 19:54:37.290106
Model ind 665 epoch 94 batch: 100 avg loss -2.664953 avg loss no lamb -2.664953 time 2020-06-25 19:54:47.915029
Model ind 665 epoch 94 batch: 200 avg loss -2.694256 avg loss no lamb -2.694256 time 2020-06-25 19:54:58.623734
Model ind 665 epoch 94 batch: 300 avg loss -2.742435 avg loss no lamb -2.742435 time 2020-06-25 19:55:09.433846
Model ind 665 epoch 94 batch: 400 avg loss -2.676865 avg loss no lamb -2.676865 time 2020-06-25 19:55:20.202139
Model ind 665 epoch 94 batch: 500 avg loss -2.621267 avg loss no lamb -2.621267 time 2020-06-25 19:55:30.871201
Model ind 665 epoch 94 batch: 600 avg loss -2.781855 avg loss no lamb -2.781855 time 2020-06-25 19:55:41.579408
Model ind 665 epoch 94 batch: 700 avg loss -2.514383 avg loss no lamb -2.514383 time 2020-06-25 19:55:52.273834
Model ind 665 epoch 94 batch: 800 avg loss -2.684439 avg loss no lamb -2.684439 time 2020-06-25 19:56:03.177928
last batch sz 10
Pre: time 2020-06-25 19:56:16.779401: 
 	std: 0.0025364615
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.981, 0.9765, 0.9824, 0.9762]
	train_accs: [0.9817167, 0.9812, 0.97613335, 0.9824167, 0.9770167]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.97937995
	best: 0.9824

Starting e_i: 95
Model ind 665 epoch 95 batch: 0 avg loss -2.841008 avg loss no lamb -2.841008 time 2020-06-25 19:56:17.470034
Model ind 665 epoch 95 batch: 100 avg loss -2.678082 avg loss no lamb -2.678082 time 2020-06-25 19:56:27.875412
Model ind 665 epoch 95 batch: 200 avg loss -2.627794 avg loss no lamb -2.627794 time 2020-06-25 19:56:38.479657
Model ind 665 epoch 95 batch: 300 avg loss -2.668363 avg loss no lamb -2.668363 time 2020-06-25 19:56:49.197927
Model ind 665 epoch 95 batch: 400 avg loss -2.692231 avg loss no lamb -2.692231 time 2020-06-25 19:56:59.859340
Model ind 665 epoch 95 batch: 500 avg loss -2.603221 avg loss no lamb -2.603221 time 2020-06-25 19:57:10.560499
Model ind 665 epoch 95 batch: 600 avg loss -2.787328 avg loss no lamb -2.787328 time 2020-06-25 19:57:21.236978
Model ind 665 epoch 95 batch: 700 avg loss -2.500892 avg loss no lamb -2.500892 time 2020-06-25 19:57:32.135049
Model ind 665 epoch 95 batch: 800 avg loss -2.671846 avg loss no lamb -2.671846 time 2020-06-25 19:57:42.953422
last batch sz 10
Pre: time 2020-06-25 19:57:56.855679: 
 	std: 0.003654049
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9828, 0.9733, 0.9815, 0.9757]
	train_accs: [0.98123336, 0.98046666, 0.9758667, 0.98165, 0.97585]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9788
	best: 0.9815

Starting e_i: 96
Model ind 665 epoch 96 batch: 0 avg loss -2.887055 avg loss no lamb -2.887055 time 2020-06-25 19:57:57.546061
Model ind 665 epoch 96 batch: 100 avg loss -2.719457 avg loss no lamb -2.719457 time 2020-06-25 19:58:08.246847
Model ind 665 epoch 96 batch: 200 avg loss -2.680697 avg loss no lamb -2.680697 time 2020-06-25 19:58:18.952027
Model ind 665 epoch 96 batch: 300 avg loss -2.681888 avg loss no lamb -2.681888 time 2020-06-25 19:58:29.739641
Model ind 665 epoch 96 batch: 400 avg loss -2.664347 avg loss no lamb -2.664347 time 2020-06-25 19:58:40.465105
Model ind 665 epoch 96 batch: 500 avg loss -2.681648 avg loss no lamb -2.681648 time 2020-06-25 19:58:51.275117
Model ind 665 epoch 96 batch: 600 avg loss -2.789373 avg loss no lamb -2.789373 time 2020-06-25 19:59:01.946493
Model ind 665 epoch 96 batch: 700 avg loss -2.582750 avg loss no lamb -2.582750 time 2020-06-25 19:59:12.662629
Model ind 665 epoch 96 batch: 800 avg loss -2.665335 avg loss no lamb -2.665335 time 2020-06-25 19:59:23.340094
last batch sz 10
Pre: time 2020-06-25 19:59:36.972409: 
 	std: 0.0029691828
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9836, 0.9824, 0.9762, 0.9834, 0.9784]
	train_accs: [0.98303336, 0.9820833, 0.9763833, 0.9831833, 0.97833335]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.98080003
	best: 0.9834

Starting e_i: 97
Model ind 665 epoch 97 batch: 0 avg loss -2.850647 avg loss no lamb -2.850647 time 2020-06-25 19:59:37.695089
Model ind 665 epoch 97 batch: 100 avg loss -2.744915 avg loss no lamb -2.744915 time 2020-06-25 19:59:48.634911
Model ind 665 epoch 97 batch: 200 avg loss -2.718552 avg loss no lamb -2.718552 time 2020-06-25 19:59:59.386688
Model ind 665 epoch 97 batch: 300 avg loss -2.691471 avg loss no lamb -2.691471 time 2020-06-25 20:00:10.229894
Model ind 665 epoch 97 batch: 400 avg loss -2.653263 avg loss no lamb -2.653263 time 2020-06-25 20:00:20.872368
Model ind 665 epoch 97 batch: 500 avg loss -2.636545 avg loss no lamb -2.636545 time 2020-06-25 20:00:31.422865
Model ind 665 epoch 97 batch: 600 avg loss -2.789616 avg loss no lamb -2.789616 time 2020-06-25 20:00:42.062472
Model ind 665 epoch 97 batch: 700 avg loss -2.552315 avg loss no lamb -2.552315 time 2020-06-25 20:00:52.716897
Model ind 665 epoch 97 batch: 800 avg loss -2.717219 avg loss no lamb -2.717219 time 2020-06-25 20:01:03.366038
last batch sz 10
Pre: time 2020-06-25 20:01:17.067765: 
 	std: 0.0037209813
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9811, 0.9729, 0.9831, 0.9778]
	train_accs: [0.9823, 0.9813167, 0.97505, 0.9826667, 0.9776667]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97942
	best: 0.9831

Starting e_i: 98
Model ind 665 epoch 98 batch: 0 avg loss -2.881308 avg loss no lamb -2.881308 time 2020-06-25 20:01:17.790778
Model ind 665 epoch 98 batch: 100 avg loss -2.709578 avg loss no lamb -2.709578 time 2020-06-25 20:01:28.420571
Model ind 665 epoch 98 batch: 200 avg loss -2.674792 avg loss no lamb -2.674792 time 2020-06-25 20:01:38.937322
Model ind 665 epoch 98 batch: 300 avg loss -2.727038 avg loss no lamb -2.727038 time 2020-06-25 20:01:49.780972
Model ind 665 epoch 98 batch: 400 avg loss -2.648359 avg loss no lamb -2.648359 time 2020-06-25 20:02:00.439655
Model ind 665 epoch 98 batch: 500 avg loss -2.738011 avg loss no lamb -2.738011 time 2020-06-25 20:02:11.184596
Model ind 665 epoch 98 batch: 600 avg loss -2.757223 avg loss no lamb -2.757223 time 2020-06-25 20:02:21.781672
Model ind 665 epoch 98 batch: 700 avg loss -2.666122 avg loss no lamb -2.666122 time 2020-06-25 20:02:32.524612
Model ind 665 epoch 98 batch: 800 avg loss -2.677149 avg loss no lamb -2.677149 time 2020-06-25 20:02:43.333440
last batch sz 10
Pre: time 2020-06-25 20:02:57.054084: 
 	std: 0.0036979013
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9813, 0.9747, 0.9828, 0.9744]
	train_accs: [0.98251665, 0.98178333, 0.9766333, 0.98265, 0.97793335]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97903997
	best: 0.9828

Starting e_i: 99
Model ind 665 epoch 99 batch: 0 avg loss -2.941190 avg loss no lamb -2.941190 time 2020-06-25 20:02:57.783638
Model ind 665 epoch 99 batch: 100 avg loss -2.756403 avg loss no lamb -2.756403 time 2020-06-25 20:03:08.581078
Model ind 665 epoch 99 batch: 200 avg loss -2.640152 avg loss no lamb -2.640152 time 2020-06-25 20:03:19.256893
Model ind 665 epoch 99 batch: 300 avg loss -2.666735 avg loss no lamb -2.666735 time 2020-06-25 20:03:29.831885
Model ind 665 epoch 99 batch: 400 avg loss -2.650907 avg loss no lamb -2.650907 time 2020-06-25 20:03:40.626728
Model ind 665 epoch 99 batch: 500 avg loss -2.581739 avg loss no lamb -2.581739 time 2020-06-25 20:03:51.320456
Model ind 665 epoch 99 batch: 600 avg loss -2.820225 avg loss no lamb -2.820225 time 2020-06-25 20:04:02.223678
Model ind 665 epoch 99 batch: 700 avg loss -2.565326 avg loss no lamb -2.565326 time 2020-06-25 20:04:12.873208
Model ind 665 epoch 99 batch: 800 avg loss -2.620808 avg loss no lamb -2.620808 time 2020-06-25 20:04:23.322211
last batch sz 10
Pre: time 2020-06-25 20:04:36.905295: 
 	std: 0.0040850895
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9818, 0.9728, 0.9831, 0.9762]
	train_accs: [0.9824833, 0.9819833, 0.97328335, 0.98291665, 0.9770333]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.9793
	best: 0.9831

Starting e_i: 100
Model ind 665 epoch 100 batch: 0 avg loss -2.921379 avg loss no lamb -2.921379 time 2020-06-25 20:04:37.598147
Model ind 665 epoch 100 batch: 100 avg loss -2.673530 avg loss no lamb -2.673530 time 2020-06-25 20:04:48.229583
Model ind 665 epoch 100 batch: 200 avg loss -2.655089 avg loss no lamb -2.655089 time 2020-06-25 20:04:58.864396
Model ind 665 epoch 100 batch: 300 avg loss -2.678724 avg loss no lamb -2.678724 time 2020-06-25 20:05:09.588264
Model ind 665 epoch 100 batch: 400 avg loss -2.621817 avg loss no lamb -2.621817 time 2020-06-25 20:05:19.938767
Model ind 665 epoch 100 batch: 500 avg loss -2.672526 avg loss no lamb -2.672526 time 2020-06-25 20:05:30.661971
Model ind 665 epoch 100 batch: 600 avg loss -2.807449 avg loss no lamb -2.807449 time 2020-06-25 20:05:41.348360
Model ind 665 epoch 100 batch: 700 avg loss -2.564460 avg loss no lamb -2.564460 time 2020-06-25 20:05:52.093847
Model ind 665 epoch 100 batch: 800 avg loss -2.675056 avg loss no lamb -2.675056 time 2020-06-25 20:06:02.675643
last batch sz 10
Pre: time 2020-06-25 20:06:16.084896: 
 	std: 0.0035238403
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.982, 0.9743, 0.9826, 0.9764]
	train_accs: [0.98315, 0.98265, 0.97705, 0.98326665, 0.9775]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97958004
	best: 0.9826

Starting e_i: 101
Model ind 665 epoch 101 batch: 0 avg loss -2.884596 avg loss no lamb -2.884596 time 2020-06-25 20:06:17.929640
Model ind 665 epoch 101 batch: 100 avg loss -2.603674 avg loss no lamb -2.603674 time 2020-06-25 20:06:28.570645
Model ind 665 epoch 101 batch: 200 avg loss -2.615695 avg loss no lamb -2.615695 time 2020-06-25 20:06:39.235546
Model ind 665 epoch 101 batch: 300 avg loss -2.735812 avg loss no lamb -2.735812 time 2020-06-25 20:06:49.837043
Model ind 665 epoch 101 batch: 400 avg loss -2.616086 avg loss no lamb -2.616086 time 2020-06-25 20:07:00.542676
Model ind 665 epoch 101 batch: 500 avg loss -2.729107 avg loss no lamb -2.729107 time 2020-06-25 20:07:11.330066
Model ind 665 epoch 101 batch: 600 avg loss -2.788764 avg loss no lamb -2.788764 time 2020-06-25 20:07:22.075954
Model ind 665 epoch 101 batch: 700 avg loss -2.569047 avg loss no lamb -2.569047 time 2020-06-25 20:07:32.991936
Model ind 665 epoch 101 batch: 800 avg loss -2.647361 avg loss no lamb -2.647361 time 2020-06-25 20:07:43.946681
last batch sz 10
Pre: time 2020-06-25 20:07:57.424003: 
 	std: 0.0040134867
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9808, 0.9733, 0.9827, 0.9736]
	train_accs: [0.98153335, 0.98083335, 0.9748, 0.98255, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9783
	best: 0.9827

Starting e_i: 102
Model ind 665 epoch 102 batch: 0 avg loss -2.791496 avg loss no lamb -2.791496 time 2020-06-25 20:07:58.147713
Model ind 665 epoch 102 batch: 100 avg loss -2.697683 avg loss no lamb -2.697683 time 2020-06-25 20:08:08.939014
Model ind 665 epoch 102 batch: 200 avg loss -2.642125 avg loss no lamb -2.642125 time 2020-06-25 20:08:19.592954
Model ind 665 epoch 102 batch: 300 avg loss -2.671267 avg loss no lamb -2.671267 time 2020-06-25 20:08:30.269178
Model ind 665 epoch 102 batch: 400 avg loss -2.670541 avg loss no lamb -2.670541 time 2020-06-25 20:08:40.731099
Model ind 665 epoch 102 batch: 500 avg loss -2.600333 avg loss no lamb -2.600333 time 2020-06-25 20:08:51.261690
Model ind 665 epoch 102 batch: 600 avg loss -2.749813 avg loss no lamb -2.749813 time 2020-06-25 20:09:01.777985
Model ind 665 epoch 102 batch: 700 avg loss -2.627804 avg loss no lamb -2.627804 time 2020-06-25 20:09:12.460485
Model ind 665 epoch 102 batch: 800 avg loss -2.664885 avg loss no lamb -2.664885 time 2020-06-25 20:09:22.937201
last batch sz 10
Pre: time 2020-06-25 20:09:36.544258: 
 	std: 0.0033482022
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9817, 0.9748, 0.9827, 0.9758]
	train_accs: [0.9816167, 0.9809333, 0.97555, 0.9821, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97936
	best: 0.9827

Starting e_i: 103
Model ind 665 epoch 103 batch: 0 avg loss -2.860476 avg loss no lamb -2.860476 time 2020-06-25 20:09:37.244059
Model ind 665 epoch 103 batch: 100 avg loss -2.711299 avg loss no lamb -2.711299 time 2020-06-25 20:09:47.743999
Model ind 665 epoch 103 batch: 200 avg loss -2.696080 avg loss no lamb -2.696080 time 2020-06-25 20:09:58.382462
Model ind 665 epoch 103 batch: 300 avg loss -2.673789 avg loss no lamb -2.673789 time 2020-06-25 20:10:09.131141
Model ind 665 epoch 103 batch: 400 avg loss -2.657939 avg loss no lamb -2.657939 time 2020-06-25 20:10:19.801330
Model ind 665 epoch 103 batch: 500 avg loss -2.628293 avg loss no lamb -2.628293 time 2020-06-25 20:10:30.404995
Model ind 665 epoch 103 batch: 600 avg loss -2.803248 avg loss no lamb -2.803248 time 2020-06-25 20:10:41.130746
Model ind 665 epoch 103 batch: 700 avg loss -2.576016 avg loss no lamb -2.576016 time 2020-06-25 20:10:51.939071
Model ind 665 epoch 103 batch: 800 avg loss -2.676077 avg loss no lamb -2.676077 time 2020-06-25 20:11:02.623981
last batch sz 10
Pre: time 2020-06-25 20:11:16.168130: 
 	std: 0.0037051286
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9808, 0.9741, 0.9823, 0.9741]
	train_accs: [0.98228335, 0.9806167, 0.97545, 0.9827667, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.9786
	best: 0.9823

Starting e_i: 104
Model ind 665 epoch 104 batch: 0 avg loss -2.807930 avg loss no lamb -2.807930 time 2020-06-25 20:11:16.874628
Model ind 665 epoch 104 batch: 100 avg loss -2.719571 avg loss no lamb -2.719571 time 2020-06-25 20:11:27.481021
Model ind 665 epoch 104 batch: 200 avg loss -2.685281 avg loss no lamb -2.685281 time 2020-06-25 20:11:38.088753
Model ind 665 epoch 104 batch: 300 avg loss -2.709534 avg loss no lamb -2.709534 time 2020-06-25 20:11:48.640444
Model ind 665 epoch 104 batch: 400 avg loss -2.622964 avg loss no lamb -2.622964 time 2020-06-25 20:11:59.249762
Model ind 665 epoch 104 batch: 500 avg loss -2.596984 avg loss no lamb -2.596984 time 2020-06-25 20:12:09.807206
Model ind 665 epoch 104 batch: 600 avg loss -2.752342 avg loss no lamb -2.752342 time 2020-06-25 20:12:20.135077
Model ind 665 epoch 104 batch: 700 avg loss -2.505464 avg loss no lamb -2.505464 time 2020-06-25 20:12:30.836418
Model ind 665 epoch 104 batch: 800 avg loss -2.731786 avg loss no lamb -2.731786 time 2020-06-25 20:12:41.374510
last batch sz 10
Pre: time 2020-06-25 20:12:54.882978: 
 	std: 0.003194622
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9838, 0.9852, 0.9777, 0.9853, 0.9791]
	train_accs: [0.9834667, 0.98275, 0.97665, 0.98356664, 0.9781]
	best_train_sub_head: 3
	worst: 0.9777
	avg: 0.98222005
	best: 0.9853

Starting e_i: 105
Model ind 665 epoch 105 batch: 0 avg loss -2.886657 avg loss no lamb -2.886657 time 2020-06-25 20:12:56.770004
Model ind 665 epoch 105 batch: 100 avg loss -2.686331 avg loss no lamb -2.686331 time 2020-06-25 20:13:07.357435
Model ind 665 epoch 105 batch: 200 avg loss -2.670954 avg loss no lamb -2.670954 time 2020-06-25 20:13:18.065367
Model ind 665 epoch 105 batch: 300 avg loss -2.771372 avg loss no lamb -2.771372 time 2020-06-25 20:13:28.705730
Model ind 665 epoch 105 batch: 400 avg loss -2.593464 avg loss no lamb -2.593464 time 2020-06-25 20:13:39.435434
Model ind 665 epoch 105 batch: 500 avg loss -2.634981 avg loss no lamb -2.634981 time 2020-06-25 20:13:50.189953
Model ind 665 epoch 105 batch: 600 avg loss -2.793331 avg loss no lamb -2.793331 time 2020-06-25 20:14:00.814963
Model ind 665 epoch 105 batch: 700 avg loss -2.602526 avg loss no lamb -2.602526 time 2020-06-25 20:14:11.268761
Model ind 665 epoch 105 batch: 800 avg loss -2.642020 avg loss no lamb -2.642020 time 2020-06-25 20:14:21.847124
last batch sz 10
Pre: time 2020-06-25 20:14:35.488729: 
 	std: 0.0033534481
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9828, 0.9759, 0.9834, 0.9769]
	train_accs: [0.98328334, 0.9822, 0.9757, 0.9831, 0.9776833]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.98048
	best: 0.9834

Starting e_i: 106
Model ind 665 epoch 106 batch: 0 avg loss -2.861664 avg loss no lamb -2.861664 time 2020-06-25 20:14:36.155060
Model ind 665 epoch 106 batch: 100 avg loss -2.735325 avg loss no lamb -2.735325 time 2020-06-25 20:14:46.854621
Model ind 665 epoch 106 batch: 200 avg loss -2.712364 avg loss no lamb -2.712364 time 2020-06-25 20:14:57.512010
Model ind 665 epoch 106 batch: 300 avg loss -2.701993 avg loss no lamb -2.701993 time 2020-06-25 20:15:08.276645
Model ind 665 epoch 106 batch: 400 avg loss -2.574135 avg loss no lamb -2.574135 time 2020-06-25 20:15:19.117249
Model ind 665 epoch 106 batch: 500 avg loss -2.606020 avg loss no lamb -2.606020 time 2020-06-25 20:15:29.797265
Model ind 665 epoch 106 batch: 600 avg loss -2.733145 avg loss no lamb -2.733145 time 2020-06-25 20:15:40.687030
Model ind 665 epoch 106 batch: 700 avg loss -2.588258 avg loss no lamb -2.588258 time 2020-06-25 20:15:51.504093
Model ind 665 epoch 106 batch: 800 avg loss -2.717021 avg loss no lamb -2.717021 time 2020-06-25 20:16:02.189365
last batch sz 10
Pre: time 2020-06-25 20:16:15.599525: 
 	std: 0.0032369224
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9835, 0.9821, 0.9766, 0.9832, 0.9762]
	train_accs: [0.98263335, 0.98145, 0.9755167, 0.9831167, 0.9767]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.98032
	best: 0.9832

Starting e_i: 107
Model ind 665 epoch 107 batch: 0 avg loss -2.817717 avg loss no lamb -2.817717 time 2020-06-25 20:16:16.319587
Model ind 665 epoch 107 batch: 100 avg loss -2.760847 avg loss no lamb -2.760847 time 2020-06-25 20:16:26.939365
Model ind 665 epoch 107 batch: 200 avg loss -2.696107 avg loss no lamb -2.696107 time 2020-06-25 20:16:37.838050
Model ind 665 epoch 107 batch: 300 avg loss -2.757104 avg loss no lamb -2.757104 time 2020-06-25 20:16:48.496624
Model ind 665 epoch 107 batch: 400 avg loss -2.596992 avg loss no lamb -2.596992 time 2020-06-25 20:16:59.093447
Model ind 665 epoch 107 batch: 500 avg loss -2.619681 avg loss no lamb -2.619681 time 2020-06-25 20:17:09.915052
Model ind 665 epoch 107 batch: 600 avg loss -2.812135 avg loss no lamb -2.812135 time 2020-06-25 20:17:20.829844
Model ind 665 epoch 107 batch: 700 avg loss -2.506213 avg loss no lamb -2.506213 time 2020-06-25 20:17:31.636774
Model ind 665 epoch 107 batch: 800 avg loss -2.601674 avg loss no lamb -2.601674 time 2020-06-25 20:17:42.372503
last batch sz 10
Pre: time 2020-06-25 20:17:55.938453: 
 	std: 0.0036224965
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9807, 0.973, 0.9811, 0.9745]
	train_accs: [0.98188335, 0.9816, 0.9747, 0.9817333, 0.97533333]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97814006
	best: 0.9814

Starting e_i: 108
Model ind 665 epoch 108 batch: 0 avg loss -2.883685 avg loss no lamb -2.883685 time 2020-06-25 20:17:56.683950
Model ind 665 epoch 108 batch: 100 avg loss -2.681164 avg loss no lamb -2.681164 time 2020-06-25 20:18:07.293324
Model ind 665 epoch 108 batch: 200 avg loss -2.657895 avg loss no lamb -2.657895 time 2020-06-25 20:18:17.876412
Model ind 665 epoch 108 batch: 300 avg loss -2.687318 avg loss no lamb -2.687318 time 2020-06-25 20:18:28.535728
Model ind 665 epoch 108 batch: 400 avg loss -2.579510 avg loss no lamb -2.579510 time 2020-06-25 20:18:39.031283
Model ind 665 epoch 108 batch: 500 avg loss -2.700518 avg loss no lamb -2.700518 time 2020-06-25 20:18:49.382212
Model ind 665 epoch 108 batch: 600 avg loss -2.747803 avg loss no lamb -2.747803 time 2020-06-25 20:18:59.830448
Model ind 665 epoch 108 batch: 700 avg loss -2.513562 avg loss no lamb -2.513562 time 2020-06-25 20:19:10.287167
Model ind 665 epoch 108 batch: 800 avg loss -2.683288 avg loss no lamb -2.683288 time 2020-06-25 20:19:20.754439
last batch sz 10
Pre: time 2020-06-25 20:19:34.198891: 
 	std: 0.0042976327
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9817, 0.974, 0.9826, 0.9726]
	train_accs: [0.98256665, 0.98193336, 0.97683334, 0.98288333, 0.97711664]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97852004
	best: 0.9826

Starting e_i: 109
Model ind 665 epoch 109 batch: 0 avg loss -2.818184 avg loss no lamb -2.818184 time 2020-06-25 20:19:34.921842
Model ind 665 epoch 109 batch: 100 avg loss -2.676665 avg loss no lamb -2.676665 time 2020-06-25 20:19:45.512084
Model ind 665 epoch 109 batch: 200 avg loss -2.768430 avg loss no lamb -2.768430 time 2020-06-25 20:19:56.100157
Model ind 665 epoch 109 batch: 300 avg loss -2.771487 avg loss no lamb -2.771487 time 2020-06-25 20:20:06.847084
Model ind 665 epoch 109 batch: 400 avg loss -2.628047 avg loss no lamb -2.628047 time 2020-06-25 20:20:17.662985
Model ind 665 epoch 109 batch: 500 avg loss -2.793914 avg loss no lamb -2.793914 time 2020-06-25 20:20:28.230899
Model ind 665 epoch 109 batch: 600 avg loss -2.749865 avg loss no lamb -2.749865 time 2020-06-25 20:20:39.070960
Model ind 665 epoch 109 batch: 700 avg loss -2.605233 avg loss no lamb -2.605233 time 2020-06-25 20:20:49.872151
Model ind 665 epoch 109 batch: 800 avg loss -2.637360 avg loss no lamb -2.637360 time 2020-06-25 20:21:00.455270
last batch sz 10
Pre: time 2020-06-25 20:21:14.147984: 
 	std: 0.0033926973
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9796, 0.9739, 0.981, 0.9738]
	train_accs: [0.98191667, 0.98046666, 0.97426665, 0.9817, 0.97543335]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97793996
	best: 0.9814

Starting e_i: 110
Model ind 665 epoch 110 batch: 0 avg loss -2.846956 avg loss no lamb -2.846956 time 2020-06-25 20:21:14.883525
Model ind 665 epoch 110 batch: 100 avg loss -2.733664 avg loss no lamb -2.733664 time 2020-06-25 20:21:25.275578
Model ind 665 epoch 110 batch: 200 avg loss -2.708953 avg loss no lamb -2.708953 time 2020-06-25 20:21:35.828790
Model ind 665 epoch 110 batch: 300 avg loss -2.675657 avg loss no lamb -2.675657 time 2020-06-25 20:21:46.406146
Model ind 665 epoch 110 batch: 400 avg loss -2.664089 avg loss no lamb -2.664089 time 2020-06-25 20:21:56.973087
Model ind 665 epoch 110 batch: 500 avg loss -2.695488 avg loss no lamb -2.695488 time 2020-06-25 20:22:07.666962
Model ind 665 epoch 110 batch: 600 avg loss -2.737844 avg loss no lamb -2.737844 time 2020-06-25 20:22:18.174640
Model ind 665 epoch 110 batch: 700 avg loss -2.649383 avg loss no lamb -2.649383 time 2020-06-25 20:22:28.875870
Model ind 665 epoch 110 batch: 800 avg loss -2.703964 avg loss no lamb -2.703964 time 2020-06-25 20:22:39.463513
last batch sz 10
Pre: time 2020-06-25 20:22:53.086129: 
 	std: 0.004546044
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9811, 0.9712, 0.9822, 0.9753]
	train_accs: [0.9820167, 0.9802833, 0.97385, 0.9816333, 0.9762]
	best_train_sub_head: 0
	worst: 0.9712
	avg: 0.97853994
	best: 0.9829

Starting e_i: 111
Model ind 665 epoch 111 batch: 0 avg loss -2.773379 avg loss no lamb -2.773379 time 2020-06-25 20:22:55.025189
Model ind 665 epoch 111 batch: 100 avg loss -2.671077 avg loss no lamb -2.671077 time 2020-06-25 20:23:05.677351
Model ind 665 epoch 111 batch: 200 avg loss -2.738648 avg loss no lamb -2.738648 time 2020-06-25 20:23:16.466237
Model ind 665 epoch 111 batch: 300 avg loss -2.763921 avg loss no lamb -2.763921 time 2020-06-25 20:23:27.199439
Model ind 665 epoch 111 batch: 400 avg loss -2.603813 avg loss no lamb -2.603813 time 2020-06-25 20:23:37.945858
Model ind 665 epoch 111 batch: 500 avg loss -2.704756 avg loss no lamb -2.704756 time 2020-06-25 20:23:48.642278
Model ind 665 epoch 111 batch: 600 avg loss -2.751669 avg loss no lamb -2.751669 time 2020-06-25 20:23:59.173433
Model ind 665 epoch 111 batch: 700 avg loss -2.573365 avg loss no lamb -2.573365 time 2020-06-25 20:24:10.021507
Model ind 665 epoch 111 batch: 800 avg loss -2.633262 avg loss no lamb -2.633262 time 2020-06-25 20:24:20.755999
last batch sz 10
Pre: time 2020-06-25 20:24:34.313424: 
 	std: 0.0023181064
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9814, 0.9769, 0.9812, 0.9764]
	train_accs: [0.98258334, 0.98176664, 0.97641665, 0.98233336, 0.97765]
	best_train_sub_head: 0
	worst: 0.9764
	avg: 0.97947997
	best: 0.9815

Starting e_i: 112
Model ind 665 epoch 112 batch: 0 avg loss -2.868424 avg loss no lamb -2.868424 time 2020-06-25 20:24:35.005179
Model ind 665 epoch 112 batch: 100 avg loss -2.631079 avg loss no lamb -2.631079 time 2020-06-25 20:24:45.470457
Model ind 665 epoch 112 batch: 200 avg loss -2.754102 avg loss no lamb -2.754102 time 2020-06-25 20:24:56.146004
Model ind 665 epoch 112 batch: 300 avg loss -2.662197 avg loss no lamb -2.662197 time 2020-06-25 20:25:06.941373
Model ind 665 epoch 112 batch: 400 avg loss -2.622493 avg loss no lamb -2.622493 time 2020-06-25 20:25:17.673289
Model ind 665 epoch 112 batch: 500 avg loss -2.648860 avg loss no lamb -2.648860 time 2020-06-25 20:25:28.345947
Model ind 665 epoch 112 batch: 600 avg loss -2.794227 avg loss no lamb -2.794227 time 2020-06-25 20:25:38.841695
Model ind 665 epoch 112 batch: 700 avg loss -2.540193 avg loss no lamb -2.540193 time 2020-06-25 20:25:49.639941
Model ind 665 epoch 112 batch: 800 avg loss -2.773717 avg loss no lamb -2.773717 time 2020-06-25 20:26:00.482908
last batch sz 10
Pre: time 2020-06-25 20:26:14.018188: 
 	std: 0.0030443294
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9812, 0.9749, 0.9827, 0.9774]
	train_accs: [0.98305, 0.98195, 0.9769167, 0.98333335, 0.97816664]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97969997
	best: 0.9827

Starting e_i: 113
Model ind 665 epoch 113 batch: 0 avg loss -2.900444 avg loss no lamb -2.900444 time 2020-06-25 20:26:14.687872
Model ind 665 epoch 113 batch: 100 avg loss -2.606848 avg loss no lamb -2.606848 time 2020-06-25 20:26:25.454212
Model ind 665 epoch 113 batch: 200 avg loss -2.730609 avg loss no lamb -2.730609 time 2020-06-25 20:26:35.942397
Model ind 665 epoch 113 batch: 300 avg loss -2.759598 avg loss no lamb -2.759598 time 2020-06-25 20:26:46.447410
Model ind 665 epoch 113 batch: 400 avg loss -2.648373 avg loss no lamb -2.648373 time 2020-06-25 20:26:57.019474
Model ind 665 epoch 113 batch: 500 avg loss -2.715464 avg loss no lamb -2.715464 time 2020-06-25 20:27:07.888396
Model ind 665 epoch 113 batch: 600 avg loss -2.766252 avg loss no lamb -2.766252 time 2020-06-25 20:27:18.506028
Model ind 665 epoch 113 batch: 700 avg loss -2.658920 avg loss no lamb -2.658920 time 2020-06-25 20:27:29.225243
Model ind 665 epoch 113 batch: 800 avg loss -2.698204 avg loss no lamb -2.698204 time 2020-06-25 20:27:39.912279
last batch sz 10
Pre: time 2020-06-25 20:27:53.724952: 
 	std: 0.0034353402
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9818, 0.9735, 0.9813, 0.9761]
	train_accs: [0.9822, 0.98125, 0.97613335, 0.9816167, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97888005
	best: 0.9817

Starting e_i: 114
Model ind 665 epoch 114 batch: 0 avg loss -2.809695 avg loss no lamb -2.809695 time 2020-06-25 20:27:54.441062
Model ind 665 epoch 114 batch: 100 avg loss -2.718795 avg loss no lamb -2.718795 time 2020-06-25 20:28:05.060791
Model ind 665 epoch 114 batch: 200 avg loss -2.706173 avg loss no lamb -2.706173 time 2020-06-25 20:28:15.461819
Model ind 665 epoch 114 batch: 300 avg loss -2.698034 avg loss no lamb -2.698034 time 2020-06-25 20:28:25.739758
Model ind 665 epoch 114 batch: 400 avg loss -2.635685 avg loss no lamb -2.635685 time 2020-06-25 20:28:36.323812
Model ind 665 epoch 114 batch: 500 avg loss -2.631630 avg loss no lamb -2.631630 time 2020-06-25 20:28:47.114998
Model ind 665 epoch 114 batch: 600 avg loss -2.719113 avg loss no lamb -2.719113 time 2020-06-25 20:28:57.657334
Model ind 665 epoch 114 batch: 700 avg loss -2.651062 avg loss no lamb -2.651062 time 2020-06-25 20:29:08.467624
Model ind 665 epoch 114 batch: 800 avg loss -2.706647 avg loss no lamb -2.706647 time 2020-06-25 20:29:19.064118
last batch sz 10
Pre: time 2020-06-25 20:29:32.604465: 
 	std: 0.003915287
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9833, 0.9819, 0.9739, 0.9832, 0.9761]
	train_accs: [0.98268336, 0.98205, 0.97426665, 0.9831833, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97968006
	best: 0.9832

Starting e_i: 115
Model ind 665 epoch 115 batch: 0 avg loss -2.859964 avg loss no lamb -2.859964 time 2020-06-25 20:29:33.315806
Model ind 665 epoch 115 batch: 100 avg loss -2.709658 avg loss no lamb -2.709658 time 2020-06-25 20:29:43.912436
Model ind 665 epoch 115 batch: 200 avg loss -2.737775 avg loss no lamb -2.737775 time 2020-06-25 20:29:54.617177
Model ind 665 epoch 115 batch: 300 avg loss -2.729319 avg loss no lamb -2.729319 time 2020-06-25 20:30:05.303773
Model ind 665 epoch 115 batch: 400 avg loss -2.688606 avg loss no lamb -2.688606 time 2020-06-25 20:30:15.773009
Model ind 665 epoch 115 batch: 500 avg loss -2.674319 avg loss no lamb -2.674319 time 2020-06-25 20:30:26.181036
Model ind 665 epoch 115 batch: 600 avg loss -2.710311 avg loss no lamb -2.710311 time 2020-06-25 20:30:36.836357
Model ind 665 epoch 115 batch: 700 avg loss -2.509422 avg loss no lamb -2.509422 time 2020-06-25 20:30:47.402584
Model ind 665 epoch 115 batch: 800 avg loss -2.740782 avg loss no lamb -2.740782 time 2020-06-25 20:30:58.203240
last batch sz 10
Pre: time 2020-06-25 20:31:11.561144: 
 	std: 0.00404446
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9818, 0.9751, 0.9822, 0.9731]
	train_accs: [0.9834333, 0.98245, 0.9766333, 0.98336667, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97898006
	best: 0.9827

Starting e_i: 116
Model ind 665 epoch 116 batch: 0 avg loss -2.877959 avg loss no lamb -2.877959 time 2020-06-25 20:31:12.264661
Model ind 665 epoch 116 batch: 100 avg loss -2.705965 avg loss no lamb -2.705965 time 2020-06-25 20:31:22.662267
Model ind 665 epoch 116 batch: 200 avg loss -2.706227 avg loss no lamb -2.706227 time 2020-06-25 20:31:33.522343
Model ind 665 epoch 116 batch: 300 avg loss -2.743974 avg loss no lamb -2.743974 time 2020-06-25 20:31:44.142929
Model ind 665 epoch 116 batch: 400 avg loss -2.676334 avg loss no lamb -2.676334 time 2020-06-25 20:31:54.745055
Model ind 665 epoch 116 batch: 500 avg loss -2.622035 avg loss no lamb -2.622035 time 2020-06-25 20:32:05.351147
Model ind 665 epoch 116 batch: 600 avg loss -2.754563 avg loss no lamb -2.754563 time 2020-06-25 20:32:15.960485
Model ind 665 epoch 116 batch: 700 avg loss -2.561000 avg loss no lamb -2.561000 time 2020-06-25 20:32:26.699232
Model ind 665 epoch 116 batch: 800 avg loss -2.666512 avg loss no lamb -2.666512 time 2020-06-25 20:32:37.340346
last batch sz 10
Pre: time 2020-06-25 20:32:50.916673: 
 	std: 0.004318976
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9812, 0.9719, 0.9808, 0.9721]
	train_accs: [0.9817, 0.9820333, 0.97501665, 0.9823167, 0.9763]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97728
	best: 0.9808

Starting e_i: 117
Model ind 665 epoch 117 batch: 0 avg loss -2.901768 avg loss no lamb -2.901768 time 2020-06-25 20:32:51.609411
Model ind 665 epoch 117 batch: 100 avg loss -2.690145 avg loss no lamb -2.690145 time 2020-06-25 20:33:02.454688
Model ind 665 epoch 117 batch: 200 avg loss -2.684289 avg loss no lamb -2.684289 time 2020-06-25 20:33:13.226016
Model ind 665 epoch 117 batch: 300 avg loss -2.705350 avg loss no lamb -2.705350 time 2020-06-25 20:33:23.787347
Model ind 665 epoch 117 batch: 400 avg loss -2.638886 avg loss no lamb -2.638886 time 2020-06-25 20:33:34.187116
Model ind 665 epoch 117 batch: 500 avg loss -2.682330 avg loss no lamb -2.682330 time 2020-06-25 20:33:44.741237
Model ind 665 epoch 117 batch: 600 avg loss -2.769989 avg loss no lamb -2.769989 time 2020-06-25 20:33:55.358464
Model ind 665 epoch 117 batch: 700 avg loss -2.588611 avg loss no lamb -2.588611 time 2020-06-25 20:34:06.079140
Model ind 665 epoch 117 batch: 800 avg loss -2.751017 avg loss no lamb -2.751017 time 2020-06-25 20:34:16.887922
last batch sz 10
Pre: time 2020-06-25 20:34:30.731515: 
 	std: 0.0047376747
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9803, 0.9703, 0.9805, 0.9722]
	train_accs: [0.98175, 0.9808667, 0.9738167, 0.98135, 0.97356665]
	best_train_sub_head: 0
	worst: 0.9703
	avg: 0.97698003
	best: 0.9816

Starting e_i: 118
Model ind 665 epoch 118 batch: 0 avg loss -2.842719 avg loss no lamb -2.842719 time 2020-06-25 20:34:31.426076
Model ind 665 epoch 118 batch: 100 avg loss -2.670671 avg loss no lamb -2.670671 time 2020-06-25 20:34:42.206060
Model ind 665 epoch 118 batch: 200 avg loss -2.692105 avg loss no lamb -2.692105 time 2020-06-25 20:34:53.145485
Model ind 665 epoch 118 batch: 300 avg loss -2.749302 avg loss no lamb -2.749302 time 2020-06-25 20:35:03.832802
Model ind 665 epoch 118 batch: 400 avg loss -2.692969 avg loss no lamb -2.692969 time 2020-06-25 20:35:14.533430
Model ind 665 epoch 118 batch: 500 avg loss -2.572887 avg loss no lamb -2.572887 time 2020-06-25 20:35:25.338892
Model ind 665 epoch 118 batch: 600 avg loss -2.800405 avg loss no lamb -2.800405 time 2020-06-25 20:35:36.078911
Model ind 665 epoch 118 batch: 700 avg loss -2.558270 avg loss no lamb -2.558270 time 2020-06-25 20:35:46.920571
Model ind 665 epoch 118 batch: 800 avg loss -2.731953 avg loss no lamb -2.731953 time 2020-06-25 20:35:57.843554
last batch sz 10
Pre: time 2020-06-25 20:36:11.236248: 
 	std: 0.004873842
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9812, 0.9706, 0.9828, 0.9749]
	train_accs: [0.98275, 0.9819833, 0.97141665, 0.9824833, 0.9752833]
	best_train_sub_head: 0
	worst: 0.9706
	avg: 0.97844
	best: 0.9827

Starting e_i: 119
Model ind 665 epoch 119 batch: 0 avg loss -2.852857 avg loss no lamb -2.852857 time 2020-06-25 20:36:12.028118
Model ind 665 epoch 119 batch: 100 avg loss -2.723638 avg loss no lamb -2.723638 time 2020-06-25 20:36:22.699100
Model ind 665 epoch 119 batch: 200 avg loss -2.719520 avg loss no lamb -2.719520 time 2020-06-25 20:36:33.491785
Model ind 665 epoch 119 batch: 300 avg loss -2.764125 avg loss no lamb -2.764125 time 2020-06-25 20:36:44.179849
Model ind 665 epoch 119 batch: 400 avg loss -2.697565 avg loss no lamb -2.697565 time 2020-06-25 20:36:54.865854
Model ind 665 epoch 119 batch: 500 avg loss -2.690000 avg loss no lamb -2.690000 time 2020-06-25 20:37:05.436215
Model ind 665 epoch 119 batch: 600 avg loss -2.753493 avg loss no lamb -2.753493 time 2020-06-25 20:37:16.024282
Model ind 665 epoch 119 batch: 700 avg loss -2.583553 avg loss no lamb -2.583553 time 2020-06-25 20:37:26.856402
Model ind 665 epoch 119 batch: 800 avg loss -2.657981 avg loss no lamb -2.657981 time 2020-06-25 20:37:37.395531
last batch sz 10
Pre: time 2020-06-25 20:37:50.946468: 
 	std: 0.0031031717
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9832, 0.9821, 0.9754, 0.9827, 0.9777]
	train_accs: [0.98385, 0.9823833, 0.9752667, 0.98338336, 0.97798336]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.98022
	best: 0.9832

Starting e_i: 120
Model ind 665 epoch 120 batch: 0 avg loss -2.862075 avg loss no lamb -2.862075 time 2020-06-25 20:37:51.649016
Model ind 665 epoch 120 batch: 100 avg loss -2.728935 avg loss no lamb -2.728935 time 2020-06-25 20:38:02.189026
Model ind 665 epoch 120 batch: 200 avg loss -2.679528 avg loss no lamb -2.679528 time 2020-06-25 20:38:12.593652
Model ind 665 epoch 120 batch: 300 avg loss -2.775528 avg loss no lamb -2.775528 time 2020-06-25 20:38:23.162031
Model ind 665 epoch 120 batch: 400 avg loss -2.668633 avg loss no lamb -2.668633 time 2020-06-25 20:38:33.951045
Model ind 665 epoch 120 batch: 500 avg loss -2.636120 avg loss no lamb -2.636120 time 2020-06-25 20:38:44.574203
Model ind 665 epoch 120 batch: 600 avg loss -2.793327 avg loss no lamb -2.793327 time 2020-06-25 20:38:54.954426
Model ind 665 epoch 120 batch: 700 avg loss -2.665444 avg loss no lamb -2.665444 time 2020-06-25 20:39:05.742200
Model ind 665 epoch 120 batch: 800 avg loss -2.663627 avg loss no lamb -2.663627 time 2020-06-25 20:39:16.462428
last batch sz 10
Pre: time 2020-06-25 20:39:30.277916: 
 	std: 0.003593565
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9803, 0.9751, 0.9817, 0.9729]
	train_accs: [0.98258334, 0.9820333, 0.97541666, 0.98245, 0.9745167]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97827995
	best: 0.9814

Starting e_i: 121
Model ind 665 epoch 121 batch: 0 avg loss -2.814800 avg loss no lamb -2.814800 time 2020-06-25 20:39:32.101042
Model ind 665 epoch 121 batch: 100 avg loss -2.759977 avg loss no lamb -2.759977 time 2020-06-25 20:39:42.994050
Model ind 665 epoch 121 batch: 200 avg loss -2.712683 avg loss no lamb -2.712683 time 2020-06-25 20:39:53.851927
Model ind 665 epoch 121 batch: 300 avg loss -2.731912 avg loss no lamb -2.731912 time 2020-06-25 20:40:04.685150
Model ind 665 epoch 121 batch: 400 avg loss -2.724799 avg loss no lamb -2.724799 time 2020-06-25 20:40:15.438952
Model ind 665 epoch 121 batch: 500 avg loss -2.656902 avg loss no lamb -2.656902 time 2020-06-25 20:40:26.054546
Model ind 665 epoch 121 batch: 600 avg loss -2.805332 avg loss no lamb -2.805332 time 2020-06-25 20:40:36.696840
Model ind 665 epoch 121 batch: 700 avg loss -2.583929 avg loss no lamb -2.583929 time 2020-06-25 20:40:47.248253
Model ind 665 epoch 121 batch: 800 avg loss -2.743469 avg loss no lamb -2.743469 time 2020-06-25 20:40:57.883729
last batch sz 10
Pre: time 2020-06-25 20:41:11.756261: 
 	std: 0.0025253957
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9806, 0.9754, 0.981, 0.976]
	train_accs: [0.98176664, 0.98081666, 0.97536665, 0.98178333, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97878
	best: 0.981

Starting e_i: 122
Model ind 665 epoch 122 batch: 0 avg loss -2.881292 avg loss no lamb -2.881292 time 2020-06-25 20:41:12.458933
Model ind 665 epoch 122 batch: 100 avg loss -2.738768 avg loss no lamb -2.738768 time 2020-06-25 20:41:22.868926
Model ind 665 epoch 122 batch: 200 avg loss -2.683871 avg loss no lamb -2.683871 time 2020-06-25 20:41:33.754372
Model ind 665 epoch 122 batch: 300 avg loss -2.729026 avg loss no lamb -2.729026 time 2020-06-25 20:41:44.358329
Model ind 665 epoch 122 batch: 400 avg loss -2.679133 avg loss no lamb -2.679133 time 2020-06-25 20:41:54.967138
Model ind 665 epoch 122 batch: 500 avg loss -2.670914 avg loss no lamb -2.670914 time 2020-06-25 20:42:05.668929
Model ind 665 epoch 122 batch: 600 avg loss -2.782734 avg loss no lamb -2.782734 time 2020-06-25 20:42:16.068000
Model ind 665 epoch 122 batch: 700 avg loss -2.555626 avg loss no lamb -2.555626 time 2020-06-25 20:42:26.576613
Model ind 665 epoch 122 batch: 800 avg loss -2.671767 avg loss no lamb -2.671767 time 2020-06-25 20:42:37.405455
last batch sz 10
Pre: time 2020-06-25 20:42:51.101026: 
 	std: 0.004301801
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.979, 0.9703, 0.9808, 0.9736]
	train_accs: [0.9813167, 0.98013335, 0.97195, 0.98078334, 0.9749]
	best_train_sub_head: 0
	worst: 0.9703
	avg: 0.97698003
	best: 0.9812

Starting e_i: 123
Model ind 665 epoch 123 batch: 0 avg loss -2.887501 avg loss no lamb -2.887501 time 2020-06-25 20:42:51.845819
Model ind 665 epoch 123 batch: 100 avg loss -2.713567 avg loss no lamb -2.713567 time 2020-06-25 20:43:02.511158
Model ind 665 epoch 123 batch: 200 avg loss -2.735901 avg loss no lamb -2.735901 time 2020-06-25 20:43:13.300727
Model ind 665 epoch 123 batch: 300 avg loss -2.705724 avg loss no lamb -2.705724 time 2020-06-25 20:43:23.872896
Model ind 665 epoch 123 batch: 400 avg loss -2.701106 avg loss no lamb -2.701106 time 2020-06-25 20:43:34.439388
Model ind 665 epoch 123 batch: 500 avg loss -2.676206 avg loss no lamb -2.676206 time 2020-06-25 20:43:45.320623
Model ind 665 epoch 123 batch: 600 avg loss -2.720031 avg loss no lamb -2.720031 time 2020-06-25 20:43:56.155489
Model ind 665 epoch 123 batch: 700 avg loss -2.548226 avg loss no lamb -2.548226 time 2020-06-25 20:44:06.887988
Model ind 665 epoch 123 batch: 800 avg loss -2.655624 avg loss no lamb -2.655624 time 2020-06-25 20:44:17.580188
last batch sz 10
Pre: time 2020-06-25 20:44:31.155690: 
 	std: 0.0035307813
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9813, 0.9741, 0.982, 0.9752]
	train_accs: [0.98321664, 0.9821, 0.9751833, 0.9832, 0.97761667]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97894
	best: 0.9821

Starting e_i: 124
Model ind 665 epoch 124 batch: 0 avg loss -2.861358 avg loss no lamb -2.861358 time 2020-06-25 20:44:31.868949
Model ind 665 epoch 124 batch: 100 avg loss -2.751612 avg loss no lamb -2.751612 time 2020-06-25 20:44:42.547569
Model ind 665 epoch 124 batch: 200 avg loss -2.739366 avg loss no lamb -2.739366 time 2020-06-25 20:44:53.051673
Model ind 665 epoch 124 batch: 300 avg loss -2.696048 avg loss no lamb -2.696048 time 2020-06-25 20:45:03.620264
Model ind 665 epoch 124 batch: 400 avg loss -2.649173 avg loss no lamb -2.649173 time 2020-06-25 20:45:14.110468
Model ind 665 epoch 124 batch: 500 avg loss -2.660767 avg loss no lamb -2.660767 time 2020-06-25 20:45:24.643325
Model ind 665 epoch 124 batch: 600 avg loss -2.764738 avg loss no lamb -2.764738 time 2020-06-25 20:45:35.564089
Model ind 665 epoch 124 batch: 700 avg loss -2.601100 avg loss no lamb -2.601100 time 2020-06-25 20:45:46.253466
Model ind 665 epoch 124 batch: 800 avg loss -2.739095 avg loss no lamb -2.739095 time 2020-06-25 20:45:56.760278
last batch sz 10
Pre: time 2020-06-25 20:46:10.414811: 
 	std: 0.002521586
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9829, 0.9775, 0.9827, 0.9783]
	train_accs: [0.9834, 0.9827667, 0.9774333, 0.98335, 0.9788167]
	best_train_sub_head: 0
	worst: 0.9775
	avg: 0.98096
	best: 0.9834

Starting e_i: 125
Model ind 665 epoch 125 batch: 0 avg loss -2.899764 avg loss no lamb -2.899764 time 2020-06-25 20:46:11.120168
Model ind 665 epoch 125 batch: 100 avg loss -2.747593 avg loss no lamb -2.747593 time 2020-06-25 20:46:21.836458
Model ind 665 epoch 125 batch: 200 avg loss -2.737067 avg loss no lamb -2.737067 time 2020-06-25 20:46:32.464353
Model ind 665 epoch 125 batch: 300 avg loss -2.791265 avg loss no lamb -2.791265 time 2020-06-25 20:46:42.985483
Model ind 665 epoch 125 batch: 400 avg loss -2.704858 avg loss no lamb -2.704858 time 2020-06-25 20:46:53.774157
Model ind 665 epoch 125 batch: 500 avg loss -2.688855 avg loss no lamb -2.688855 time 2020-06-25 20:47:04.459989
Model ind 665 epoch 125 batch: 600 avg loss -2.762238 avg loss no lamb -2.762238 time 2020-06-25 20:47:15.047141
Model ind 665 epoch 125 batch: 700 avg loss -2.543425 avg loss no lamb -2.543425 time 2020-06-25 20:47:25.690028
Model ind 665 epoch 125 batch: 800 avg loss -2.714598 avg loss no lamb -2.714598 time 2020-06-25 20:47:36.191560
last batch sz 10
Pre: time 2020-06-25 20:47:50.013306: 
 	std: 0.0027666474
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9838, 0.9834, 0.9778, 0.9842, 0.9786]
	train_accs: [0.98361665, 0.98296666, 0.97711664, 0.9838, 0.97823334]
	best_train_sub_head: 3
	worst: 0.9778
	avg: 0.98156005
	best: 0.9842

Starting e_i: 126
Model ind 665 epoch 126 batch: 0 avg loss -2.842420 avg loss no lamb -2.842420 time 2020-06-25 20:47:50.748811
Model ind 665 epoch 126 batch: 100 avg loss -2.721485 avg loss no lamb -2.721485 time 2020-06-25 20:48:01.321936
Model ind 665 epoch 126 batch: 200 avg loss -2.712826 avg loss no lamb -2.712826 time 2020-06-25 20:48:11.867316
Model ind 665 epoch 126 batch: 300 avg loss -2.754396 avg loss no lamb -2.754396 time 2020-06-25 20:48:22.347539
Model ind 665 epoch 126 batch: 400 avg loss -2.651370 avg loss no lamb -2.651370 time 2020-06-25 20:48:32.921135
Model ind 665 epoch 126 batch: 500 avg loss -2.663109 avg loss no lamb -2.663109 time 2020-06-25 20:48:43.647195
Model ind 665 epoch 126 batch: 600 avg loss -2.796481 avg loss no lamb -2.796481 time 2020-06-25 20:48:54.150855
Model ind 665 epoch 126 batch: 700 avg loss -2.595722 avg loss no lamb -2.595722 time 2020-06-25 20:49:04.645915
Model ind 665 epoch 126 batch: 800 avg loss -2.681293 avg loss no lamb -2.681293 time 2020-06-25 20:49:15.470958
last batch sz 10
Pre: time 2020-06-25 20:49:29.357124: 
 	std: 0.003010381
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9809, 0.9754, 0.9827, 0.9761]
	train_accs: [0.9820833, 0.9810167, 0.9748833, 0.98246664, 0.9777333]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97936
	best: 0.9827

Starting e_i: 127
Model ind 665 epoch 127 batch: 0 avg loss -2.775413 avg loss no lamb -2.775413 time 2020-06-25 20:49:30.106455
Model ind 665 epoch 127 batch: 100 avg loss -2.693568 avg loss no lamb -2.693568 time 2020-06-25 20:49:40.659039
Model ind 665 epoch 127 batch: 200 avg loss -2.697039 avg loss no lamb -2.697039 time 2020-06-25 20:49:51.229040
Model ind 665 epoch 127 batch: 300 avg loss -2.707707 avg loss no lamb -2.707707 time 2020-06-25 20:50:02.140570
Model ind 665 epoch 127 batch: 400 avg loss -2.620028 avg loss no lamb -2.620028 time 2020-06-25 20:50:12.849515
Model ind 665 epoch 127 batch: 500 avg loss -2.693259 avg loss no lamb -2.693259 time 2020-06-25 20:50:23.798883
Model ind 665 epoch 127 batch: 600 avg loss -2.746971 avg loss no lamb -2.746971 time 2020-06-25 20:50:34.636550
Model ind 665 epoch 127 batch: 700 avg loss -2.531582 avg loss no lamb -2.531582 time 2020-06-25 20:50:45.077272
Model ind 665 epoch 127 batch: 800 avg loss -2.637494 avg loss no lamb -2.637494 time 2020-06-25 20:50:55.634536
last batch sz 10
Pre: time 2020-06-25 20:51:09.546629: 
 	std: 0.0028681676
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9826, 0.9772, 0.9835, 0.9775]
	train_accs: [0.98301667, 0.98158336, 0.97583336, 0.9827833, 0.97726667]
	best_train_sub_head: 0
	worst: 0.9772
	avg: 0.98084
	best: 0.9834

Starting e_i: 128
Model ind 665 epoch 128 batch: 0 avg loss -2.820367 avg loss no lamb -2.820367 time 2020-06-25 20:51:10.300942
Model ind 665 epoch 128 batch: 100 avg loss -2.680379 avg loss no lamb -2.680379 time 2020-06-25 20:51:21.218075
Model ind 665 epoch 128 batch: 200 avg loss -2.714570 avg loss no lamb -2.714570 time 2020-06-25 20:51:31.696837
Model ind 665 epoch 128 batch: 300 avg loss -2.722796 avg loss no lamb -2.722796 time 2020-06-25 20:51:42.272206
Model ind 665 epoch 128 batch: 400 avg loss -2.648513 avg loss no lamb -2.648513 time 2020-06-25 20:51:52.866514
Model ind 665 epoch 128 batch: 500 avg loss -2.639489 avg loss no lamb -2.639489 time 2020-06-25 20:52:03.565065
Model ind 665 epoch 128 batch: 600 avg loss -2.821881 avg loss no lamb -2.821881 time 2020-06-25 20:52:14.079403
Model ind 665 epoch 128 batch: 700 avg loss -2.557251 avg loss no lamb -2.557251 time 2020-06-25 20:52:24.464796
Model ind 665 epoch 128 batch: 800 avg loss -2.692006 avg loss no lamb -2.692006 time 2020-06-25 20:52:35.233851
last batch sz 10
Pre: time 2020-06-25 20:52:49.374468: 
 	std: 0.0034351258
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9817, 0.9736, 0.9814, 0.976]
	train_accs: [0.9821333, 0.9813167, 0.97415, 0.98183334, 0.97651666]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97889996
	best: 0.9818

Starting e_i: 129
Model ind 665 epoch 129 batch: 0 avg loss -2.905932 avg loss no lamb -2.905932 time 2020-06-25 20:52:50.091474
Model ind 665 epoch 129 batch: 100 avg loss -2.683405 avg loss no lamb -2.683405 time 2020-06-25 20:53:00.973059
Model ind 665 epoch 129 batch: 200 avg loss -2.678036 avg loss no lamb -2.678036 time 2020-06-25 20:53:11.643855
Model ind 665 epoch 129 batch: 300 avg loss -2.696734 avg loss no lamb -2.696734 time 2020-06-25 20:53:22.405111
Model ind 665 epoch 129 batch: 400 avg loss -2.701242 avg loss no lamb -2.701242 time 2020-06-25 20:53:33.230884
Model ind 665 epoch 129 batch: 500 avg loss -2.607173 avg loss no lamb -2.607173 time 2020-06-25 20:53:43.842666
Model ind 665 epoch 129 batch: 600 avg loss -2.729111 avg loss no lamb -2.729111 time 2020-06-25 20:53:54.586715
Model ind 665 epoch 129 batch: 700 avg loss -2.582019 avg loss no lamb -2.582019 time 2020-06-25 20:54:05.006659
Model ind 665 epoch 129 batch: 800 avg loss -2.692976 avg loss no lamb -2.692976 time 2020-06-25 20:54:15.444741
last batch sz 10
Pre: time 2020-06-25 20:54:29.191981: 
 	std: 0.004138888
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.983, 0.9825, 0.974, 0.9827, 0.9746]
	train_accs: [0.98328334, 0.98275, 0.9745333, 0.98333335, 0.9758]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.9793599
	best: 0.9827

Starting e_i: 130
Model ind 665 epoch 130 batch: 0 avg loss -2.844714 avg loss no lamb -2.844714 time 2020-06-25 20:54:29.959557
Model ind 665 epoch 130 batch: 100 avg loss -2.695145 avg loss no lamb -2.695145 time 2020-06-25 20:54:40.741501
Model ind 665 epoch 130 batch: 200 avg loss -2.662022 avg loss no lamb -2.662022 time 2020-06-25 20:54:51.523168
Model ind 665 epoch 130 batch: 300 avg loss -2.798286 avg loss no lamb -2.798286 time 2020-06-25 20:55:02.400217
Model ind 665 epoch 130 batch: 400 avg loss -2.644860 avg loss no lamb -2.644860 time 2020-06-25 20:55:12.942989
Model ind 665 epoch 130 batch: 500 avg loss -2.664703 avg loss no lamb -2.664703 time 2020-06-25 20:55:23.155404
Model ind 665 epoch 130 batch: 600 avg loss -2.781428 avg loss no lamb -2.781428 time 2020-06-25 20:55:33.823813
Model ind 665 epoch 130 batch: 700 avg loss -2.499117 avg loss no lamb -2.499117 time 2020-06-25 20:55:44.572964
Model ind 665 epoch 130 batch: 800 avg loss -2.734518 avg loss no lamb -2.734518 time 2020-06-25 20:55:55.399807
last batch sz 10
Pre: time 2020-06-25 20:56:09.116075: 
 	std: 0.003804541
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9822, 0.9736, 0.9823, 0.9762]
	train_accs: [0.98368335, 0.9827667, 0.9754, 0.9835333, 0.97798336]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97944003
	best: 0.9829

Starting e_i: 131
Model ind 665 epoch 131 batch: 0 avg loss -2.875970 avg loss no lamb -2.875970 time 2020-06-25 20:56:11.074645
Model ind 665 epoch 131 batch: 100 avg loss -2.787084 avg loss no lamb -2.787084 time 2020-06-25 20:56:21.830881
Model ind 665 epoch 131 batch: 200 avg loss -2.701856 avg loss no lamb -2.701856 time 2020-06-25 20:56:32.401907
Model ind 665 epoch 131 batch: 300 avg loss -2.740775 avg loss no lamb -2.740775 time 2020-06-25 20:56:43.079654
Model ind 665 epoch 131 batch: 400 avg loss -2.725180 avg loss no lamb -2.725180 time 2020-06-25 20:56:53.735495
Model ind 665 epoch 131 batch: 500 avg loss -2.688717 avg loss no lamb -2.688717 time 2020-06-25 20:57:04.179833
Model ind 665 epoch 131 batch: 600 avg loss -2.856930 avg loss no lamb -2.856930 time 2020-06-25 20:57:14.979931
Model ind 665 epoch 131 batch: 700 avg loss -2.544499 avg loss no lamb -2.544499 time 2020-06-25 20:57:25.570616
Model ind 665 epoch 131 batch: 800 avg loss -2.723064 avg loss no lamb -2.723064 time 2020-06-25 20:57:36.159300
last batch sz 10
Pre: time 2020-06-25 20:57:49.659489: 
 	std: 0.004230553
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9806, 0.9715, 0.9823, 0.9753]
	train_accs: [0.98261666, 0.9812833, 0.9729667, 0.9827167, 0.97636664]
	best_train_sub_head: 3
	worst: 0.9715
	avg: 0.97832
	best: 0.9823

Starting e_i: 132
Model ind 665 epoch 132 batch: 0 avg loss -2.878367 avg loss no lamb -2.878367 time 2020-06-25 20:57:50.371457
Model ind 665 epoch 132 batch: 100 avg loss -2.707727 avg loss no lamb -2.707727 time 2020-06-25 20:58:00.919811
Model ind 665 epoch 132 batch: 200 avg loss -2.733952 avg loss no lamb -2.733952 time 2020-06-25 20:58:11.535688
Model ind 665 epoch 132 batch: 300 avg loss -2.748840 avg loss no lamb -2.748840 time 2020-06-25 20:58:22.182747
Model ind 665 epoch 132 batch: 400 avg loss -2.673293 avg loss no lamb -2.673293 time 2020-06-25 20:58:32.646562
Model ind 665 epoch 132 batch: 500 avg loss -2.657111 avg loss no lamb -2.657111 time 2020-06-25 20:58:42.980561
Model ind 665 epoch 132 batch: 600 avg loss -2.728627 avg loss no lamb -2.728627 time 2020-06-25 20:58:53.538658
Model ind 665 epoch 132 batch: 700 avg loss -2.619164 avg loss no lamb -2.619164 time 2020-06-25 20:59:04.153660
Model ind 665 epoch 132 batch: 800 avg loss -2.653635 avg loss no lamb -2.653635 time 2020-06-25 20:59:14.805663
last batch sz 10
Pre: time 2020-06-25 20:59:28.541083: 
 	std: 0.0036423125
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9795, 0.9708, 0.9789, 0.974]
	train_accs: [0.98088336, 0.97963333, 0.97173333, 0.98055, 0.97471666]
	best_train_sub_head: 0
	worst: 0.9708
	avg: 0.97665995
	best: 0.9801

Starting e_i: 133
Model ind 665 epoch 133 batch: 0 avg loss -2.810311 avg loss no lamb -2.810311 time 2020-06-25 20:59:29.240849
Model ind 665 epoch 133 batch: 100 avg loss -2.737430 avg loss no lamb -2.737430 time 2020-06-25 20:59:39.777705
Model ind 665 epoch 133 batch: 200 avg loss -2.722407 avg loss no lamb -2.722407 time 2020-06-25 20:59:50.422287
Model ind 665 epoch 133 batch: 300 avg loss -2.696255 avg loss no lamb -2.696255 time 2020-06-25 21:00:01.151487
Model ind 665 epoch 133 batch: 400 avg loss -2.655385 avg loss no lamb -2.655385 time 2020-06-25 21:00:11.837310
Model ind 665 epoch 133 batch: 500 avg loss -2.654387 avg loss no lamb -2.654387 time 2020-06-25 21:00:22.563107
Model ind 665 epoch 133 batch: 600 avg loss -2.802540 avg loss no lamb -2.802540 time 2020-06-25 21:00:33.141350
Model ind 665 epoch 133 batch: 700 avg loss -2.620764 avg loss no lamb -2.620764 time 2020-06-25 21:00:43.677839
Model ind 665 epoch 133 batch: 800 avg loss -2.729797 avg loss no lamb -2.729797 time 2020-06-25 21:00:54.596503
last batch sz 10
Pre: time 2020-06-25 21:01:08.166607: 
 	std: 0.003030774
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9821, 0.9762, 0.9832, 0.9768]
	train_accs: [0.98328334, 0.9824833, 0.97721666, 0.98333335, 0.9777833]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.98017997
	best: 0.9832

Starting e_i: 134
Model ind 665 epoch 134 batch: 0 avg loss -2.899925 avg loss no lamb -2.899925 time 2020-06-25 21:01:08.869489
Model ind 665 epoch 134 batch: 100 avg loss -2.767519 avg loss no lamb -2.767519 time 2020-06-25 21:01:19.368534
Model ind 665 epoch 134 batch: 200 avg loss -2.674324 avg loss no lamb -2.674324 time 2020-06-25 21:01:29.850822
Model ind 665 epoch 134 batch: 300 avg loss -2.729610 avg loss no lamb -2.729610 time 2020-06-25 21:01:40.596179
Model ind 665 epoch 134 batch: 400 avg loss -2.665457 avg loss no lamb -2.665457 time 2020-06-25 21:01:51.132662
Model ind 665 epoch 134 batch: 500 avg loss -2.710950 avg loss no lamb -2.710950 time 2020-06-25 21:02:01.745669
Model ind 665 epoch 134 batch: 600 avg loss -2.708918 avg loss no lamb -2.708918 time 2020-06-25 21:02:12.469596
Model ind 665 epoch 134 batch: 700 avg loss -2.590550 avg loss no lamb -2.590550 time 2020-06-25 21:02:22.950165
Model ind 665 epoch 134 batch: 800 avg loss -2.606362 avg loss no lamb -2.606362 time 2020-06-25 21:02:33.529342
last batch sz 10
Pre: time 2020-06-25 21:02:46.899053: 
 	std: 0.002769546
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9821, 0.9753, 0.9819, 0.9777]
	train_accs: [0.9823, 0.98185, 0.9756167, 0.98211664, 0.97823334]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97976
	best: 0.9818

Starting e_i: 135
Model ind 665 epoch 135 batch: 0 avg loss -2.911348 avg loss no lamb -2.911348 time 2020-06-25 21:02:47.632492
Model ind 665 epoch 135 batch: 100 avg loss -2.664071 avg loss no lamb -2.664071 time 2020-06-25 21:02:58.129658
Model ind 665 epoch 135 batch: 200 avg loss -2.769221 avg loss no lamb -2.769221 time 2020-06-25 21:03:08.812779
Model ind 665 epoch 135 batch: 300 avg loss -2.640495 avg loss no lamb -2.640495 time 2020-06-25 21:03:19.541433
Model ind 665 epoch 135 batch: 400 avg loss -2.646147 avg loss no lamb -2.646147 time 2020-06-25 21:03:30.019672
Model ind 665 epoch 135 batch: 500 avg loss -2.692519 avg loss no lamb -2.692519 time 2020-06-25 21:03:40.463332
Model ind 665 epoch 135 batch: 600 avg loss -2.789533 avg loss no lamb -2.789533 time 2020-06-25 21:03:51.439486
Model ind 665 epoch 135 batch: 700 avg loss -2.533724 avg loss no lamb -2.533724 time 2020-06-25 21:04:02.203066
Model ind 665 epoch 135 batch: 800 avg loss -2.726530 avg loss no lamb -2.726530 time 2020-06-25 21:04:12.757775
last batch sz 10
Pre: time 2020-06-25 21:04:26.376919: 
 	std: 0.0026505962
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.983, 0.9822, 0.9772, 0.9831, 0.9776]
	train_accs: [0.9824833, 0.98153335, 0.9769, 0.98261666, 0.9777833]
	best_train_sub_head: 3
	worst: 0.9772
	avg: 0.98062
	best: 0.9831

Starting e_i: 136
Model ind 665 epoch 136 batch: 0 avg loss -2.878890 avg loss no lamb -2.878890 time 2020-06-25 21:04:27.174610
Model ind 665 epoch 136 batch: 100 avg loss -2.714909 avg loss no lamb -2.714909 time 2020-06-25 21:04:37.883061
Model ind 665 epoch 136 batch: 200 avg loss -2.733407 avg loss no lamb -2.733407 time 2020-06-25 21:04:48.770637
Model ind 665 epoch 136 batch: 300 avg loss -2.771491 avg loss no lamb -2.771491 time 2020-06-25 21:04:59.559345
Model ind 665 epoch 136 batch: 400 avg loss -2.711431 avg loss no lamb -2.711431 time 2020-06-25 21:05:10.041290
Model ind 665 epoch 136 batch: 500 avg loss -2.633998 avg loss no lamb -2.633998 time 2020-06-25 21:05:20.525231
Model ind 665 epoch 136 batch: 600 avg loss -2.785525 avg loss no lamb -2.785525 time 2020-06-25 21:05:31.148796
Model ind 665 epoch 136 batch: 700 avg loss -2.498014 avg loss no lamb -2.498014 time 2020-06-25 21:05:41.876699
Model ind 665 epoch 136 batch: 800 avg loss -2.554038 avg loss no lamb -2.554038 time 2020-06-25 21:05:52.446730
last batch sz 10
Pre: time 2020-06-25 21:06:06.337439: 
 	std: 0.0035718966
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9813, 0.9741, 0.982, 0.9747]
	train_accs: [0.98191667, 0.9812833, 0.9739, 0.9822, 0.97576666]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97875994
	best: 0.982

Starting e_i: 137
Model ind 665 epoch 137 batch: 0 avg loss -2.952208 avg loss no lamb -2.952208 time 2020-06-25 21:06:07.066183
Model ind 665 epoch 137 batch: 100 avg loss -2.760432 avg loss no lamb -2.760432 time 2020-06-25 21:06:17.447971
Model ind 665 epoch 137 batch: 200 avg loss -2.769245 avg loss no lamb -2.769245 time 2020-06-25 21:06:27.974411
Model ind 665 epoch 137 batch: 300 avg loss -2.701647 avg loss no lamb -2.701647 time 2020-06-25 21:06:38.660666
Model ind 665 epoch 137 batch: 400 avg loss -2.676915 avg loss no lamb -2.676915 time 2020-06-25 21:06:49.153542
Model ind 665 epoch 137 batch: 500 avg loss -2.636239 avg loss no lamb -2.636239 time 2020-06-25 21:06:59.753766
Model ind 665 epoch 137 batch: 600 avg loss -2.769499 avg loss no lamb -2.769499 time 2020-06-25 21:07:10.427839
Model ind 665 epoch 137 batch: 700 avg loss -2.489174 avg loss no lamb -2.489174 time 2020-06-25 21:07:21.140213
Model ind 665 epoch 137 batch: 800 avg loss -2.644530 avg loss no lamb -2.644530 time 2020-06-25 21:07:31.749966
last batch sz 10
Pre: time 2020-06-25 21:07:45.354895: 
 	std: 0.003451027
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9832, 0.9822, 0.9753, 0.9833, 0.9766]
	train_accs: [0.98331666, 0.98291665, 0.97505, 0.98358333, 0.97735]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.98012
	best: 0.9833

Starting e_i: 138
Model ind 665 epoch 138 batch: 0 avg loss -2.899189 avg loss no lamb -2.899189 time 2020-06-25 21:07:46.109016
Model ind 665 epoch 138 batch: 100 avg loss -2.779465 avg loss no lamb -2.779465 time 2020-06-25 21:07:56.722650
Model ind 665 epoch 138 batch: 200 avg loss -2.777484 avg loss no lamb -2.777484 time 2020-06-25 21:08:07.475143
Model ind 665 epoch 138 batch: 300 avg loss -2.777421 avg loss no lamb -2.777421 time 2020-06-25 21:08:18.021730
Model ind 665 epoch 138 batch: 400 avg loss -2.638790 avg loss no lamb -2.638790 time 2020-06-25 21:08:28.583743
Model ind 665 epoch 138 batch: 500 avg loss -2.723880 avg loss no lamb -2.723880 time 2020-06-25 21:08:39.239744
Model ind 665 epoch 138 batch: 600 avg loss -2.804862 avg loss no lamb -2.804862 time 2020-06-25 21:08:50.036350
Model ind 665 epoch 138 batch: 700 avg loss -2.647806 avg loss no lamb -2.647806 time 2020-06-25 21:09:00.976686
Model ind 665 epoch 138 batch: 800 avg loss -2.626289 avg loss no lamb -2.626289 time 2020-06-25 21:09:11.669289
last batch sz 10
Pre: time 2020-06-25 21:09:25.195413: 
 	std: 0.003860876
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.983, 0.9821, 0.9747, 0.983, 0.975]
	train_accs: [0.98268336, 0.98188335, 0.97508335, 0.9831167, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97956
	best: 0.983

Starting e_i: 139
Model ind 665 epoch 139 batch: 0 avg loss -2.903189 avg loss no lamb -2.903189 time 2020-06-25 21:09:25.918320
Model ind 665 epoch 139 batch: 100 avg loss -2.823156 avg loss no lamb -2.823156 time 2020-06-25 21:09:36.734331
Model ind 665 epoch 139 batch: 200 avg loss -2.680400 avg loss no lamb -2.680400 time 2020-06-25 21:09:47.256671
Model ind 665 epoch 139 batch: 300 avg loss -2.746859 avg loss no lamb -2.746859 time 2020-06-25 21:09:57.843759
Model ind 665 epoch 139 batch: 400 avg loss -2.668683 avg loss no lamb -2.668683 time 2020-06-25 21:10:08.296888
Model ind 665 epoch 139 batch: 500 avg loss -2.685981 avg loss no lamb -2.685981 time 2020-06-25 21:10:18.933593
Model ind 665 epoch 139 batch: 600 avg loss -2.767921 avg loss no lamb -2.767921 time 2020-06-25 21:10:29.713706
Model ind 665 epoch 139 batch: 700 avg loss -2.566473 avg loss no lamb -2.566473 time 2020-06-25 21:10:40.373508
Model ind 665 epoch 139 batch: 800 avg loss -2.695990 avg loss no lamb -2.695990 time 2020-06-25 21:10:50.900353
last batch sz 10
Pre: time 2020-06-25 21:11:04.342079: 
 	std: 0.0028394305
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9833, 0.9825, 0.9766, 0.9833, 0.9781]
	train_accs: [0.9819833, 0.9816667, 0.9752167, 0.9827667, 0.97755]
	best_train_sub_head: 3
	worst: 0.9766
	avg: 0.98076
	best: 0.9833

Starting e_i: 140
Model ind 665 epoch 140 batch: 0 avg loss -2.804414 avg loss no lamb -2.804414 time 2020-06-25 21:11:05.096932
Model ind 665 epoch 140 batch: 100 avg loss -2.799289 avg loss no lamb -2.799289 time 2020-06-25 21:11:15.737249
Model ind 665 epoch 140 batch: 200 avg loss -2.692990 avg loss no lamb -2.692990 time 2020-06-25 21:11:26.358588
Model ind 665 epoch 140 batch: 300 avg loss -2.783242 avg loss no lamb -2.783242 time 2020-06-25 21:11:36.916599
Model ind 665 epoch 140 batch: 400 avg loss -2.682451 avg loss no lamb -2.682451 time 2020-06-25 21:11:47.695286
Model ind 665 epoch 140 batch: 500 avg loss -2.651883 avg loss no lamb -2.651883 time 2020-06-25 21:11:58.188663
Model ind 665 epoch 140 batch: 600 avg loss -2.781701 avg loss no lamb -2.781701 time 2020-06-25 21:12:08.989824
Model ind 665 epoch 140 batch: 700 avg loss -2.615727 avg loss no lamb -2.615727 time 2020-06-25 21:12:19.756266
Model ind 665 epoch 140 batch: 800 avg loss -2.666299 avg loss no lamb -2.666299 time 2020-06-25 21:12:30.362450
last batch sz 10
Pre: time 2020-06-25 21:12:44.150715: 
 	std: 0.0029899818
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9822, 0.9756, 0.9819, 0.9763]
	train_accs: [0.98246664, 0.98216665, 0.9753, 0.98215, 0.97678334]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97959995
	best: 0.982

Starting e_i: 141
Model ind 665 epoch 141 batch: 0 avg loss -2.888181 avg loss no lamb -2.888181 time 2020-06-25 21:12:45.986971
Model ind 665 epoch 141 batch: 100 avg loss -2.794544 avg loss no lamb -2.794544 time 2020-06-25 21:12:56.648783
Model ind 665 epoch 141 batch: 200 avg loss -2.688143 avg loss no lamb -2.688143 time 2020-06-25 21:13:07.237383
Model ind 665 epoch 141 batch: 300 avg loss -2.748020 avg loss no lamb -2.748020 time 2020-06-25 21:13:17.896470
Model ind 665 epoch 141 batch: 400 avg loss -2.591007 avg loss no lamb -2.591007 time 2020-06-25 21:13:28.473216
Model ind 665 epoch 141 batch: 500 avg loss -2.718221 avg loss no lamb -2.718221 time 2020-06-25 21:13:38.997414
Model ind 665 epoch 141 batch: 600 avg loss -2.847038 avg loss no lamb -2.847038 time 2020-06-25 21:13:49.838768
Model ind 665 epoch 141 batch: 700 avg loss -2.537055 avg loss no lamb -2.537055 time 2020-06-25 21:14:00.744164
Model ind 665 epoch 141 batch: 800 avg loss -2.733517 avg loss no lamb -2.733517 time 2020-06-25 21:14:11.282998
last batch sz 10
Pre: time 2020-06-25 21:14:24.813006: 
 	std: 0.0034949123
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9799, 0.9817, 0.9734, 0.9801, 0.9737]
	train_accs: [0.9813333, 0.98145, 0.97525, 0.98108333, 0.9755833]
	best_train_sub_head: 1
	worst: 0.9734
	avg: 0.97775996
	best: 0.9817

Starting e_i: 142
Model ind 665 epoch 142 batch: 0 avg loss -2.852811 avg loss no lamb -2.852811 time 2020-06-25 21:14:25.525975
Model ind 665 epoch 142 batch: 100 avg loss -2.693268 avg loss no lamb -2.693268 time 2020-06-25 21:14:36.364986
Model ind 665 epoch 142 batch: 200 avg loss -2.752504 avg loss no lamb -2.752504 time 2020-06-25 21:14:47.448448
Model ind 665 epoch 142 batch: 300 avg loss -2.726989 avg loss no lamb -2.726989 time 2020-06-25 21:14:58.398028
Model ind 665 epoch 142 batch: 400 avg loss -2.680224 avg loss no lamb -2.680224 time 2020-06-25 21:15:09.146160
Model ind 665 epoch 142 batch: 500 avg loss -2.684746 avg loss no lamb -2.684746 time 2020-06-25 21:15:19.782518
Model ind 665 epoch 142 batch: 600 avg loss -2.755363 avg loss no lamb -2.755363 time 2020-06-25 21:15:30.842577
Model ind 665 epoch 142 batch: 700 avg loss -2.581756 avg loss no lamb -2.581756 time 2020-06-25 21:15:41.501218
Model ind 665 epoch 142 batch: 800 avg loss -2.696901 avg loss no lamb -2.696901 time 2020-06-25 21:15:52.124659
last batch sz 10
Pre: time 2020-06-25 21:16:05.896170: 
 	std: 0.0025744888
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9814, 0.976, 0.9811, 0.9761]
	train_accs: [0.98245, 0.9816667, 0.9765, 0.98245, 0.97726667]
	best_train_sub_head: 0
	worst: 0.976
	avg: 0.97920007
	best: 0.9814

Starting e_i: 143
Model ind 665 epoch 143 batch: 0 avg loss -2.810605 avg loss no lamb -2.810605 time 2020-06-25 21:16:06.593330
Model ind 665 epoch 143 batch: 100 avg loss -2.724730 avg loss no lamb -2.724730 time 2020-06-25 21:16:17.204844
Model ind 665 epoch 143 batch: 200 avg loss -2.726127 avg loss no lamb -2.726127 time 2020-06-25 21:16:27.852674
Model ind 665 epoch 143 batch: 300 avg loss -2.727893 avg loss no lamb -2.727893 time 2020-06-25 21:16:38.696949
Model ind 665 epoch 143 batch: 400 avg loss -2.629915 avg loss no lamb -2.629915 time 2020-06-25 21:16:49.312306
Model ind 665 epoch 143 batch: 500 avg loss -2.622822 avg loss no lamb -2.622822 time 2020-06-25 21:16:59.902441
Model ind 665 epoch 143 batch: 600 avg loss -2.717775 avg loss no lamb -2.717775 time 2020-06-25 21:17:10.501855
Model ind 665 epoch 143 batch: 700 avg loss -2.584852 avg loss no lamb -2.584852 time 2020-06-25 21:17:21.206305
Model ind 665 epoch 143 batch: 800 avg loss -2.774771 avg loss no lamb -2.774771 time 2020-06-25 21:17:31.868559
last batch sz 10
Pre: time 2020-06-25 21:17:45.554146: 
 	std: 0.0036829493
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9824, 0.9748, 0.9825, 0.9754]
	train_accs: [0.98286664, 0.9824, 0.97545, 0.9827833, 0.9765]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.9796001
	best: 0.9829

Starting e_i: 144
Model ind 665 epoch 144 batch: 0 avg loss -2.905223 avg loss no lamb -2.905223 time 2020-06-25 21:17:46.291907
Model ind 665 epoch 144 batch: 100 avg loss -2.730852 avg loss no lamb -2.730852 time 2020-06-25 21:17:57.149955
Model ind 665 epoch 144 batch: 200 avg loss -2.715906 avg loss no lamb -2.715906 time 2020-06-25 21:18:08.081373
Model ind 665 epoch 144 batch: 300 avg loss -2.807509 avg loss no lamb -2.807509 time 2020-06-25 21:18:18.889833
Model ind 665 epoch 144 batch: 400 avg loss -2.635682 avg loss no lamb -2.635682 time 2020-06-25 21:18:29.552362
Model ind 665 epoch 144 batch: 500 avg loss -2.685259 avg loss no lamb -2.685259 time 2020-06-25 21:18:40.430510
Model ind 665 epoch 144 batch: 600 avg loss -2.831248 avg loss no lamb -2.831248 time 2020-06-25 21:18:51.073945
Model ind 665 epoch 144 batch: 700 avg loss -2.574011 avg loss no lamb -2.574011 time 2020-06-25 21:19:02.002538
Model ind 665 epoch 144 batch: 800 avg loss -2.680175 avg loss no lamb -2.680175 time 2020-06-25 21:19:12.826084
last batch sz 10
Pre: time 2020-06-25 21:19:26.408590: 
 	std: 0.0026095286
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.981, 0.9758, 0.9817, 0.9762]
	train_accs: [0.98228335, 0.98155, 0.9759333, 0.9824833, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.97918
	best: 0.9817

Starting e_i: 145
Model ind 665 epoch 145 batch: 0 avg loss -2.864676 avg loss no lamb -2.864676 time 2020-06-25 21:19:27.135549
Model ind 665 epoch 145 batch: 100 avg loss -2.672127 avg loss no lamb -2.672127 time 2020-06-25 21:19:37.781004
Model ind 665 epoch 145 batch: 200 avg loss -2.766916 avg loss no lamb -2.766916 time 2020-06-25 21:19:48.401210
Model ind 665 epoch 145 batch: 300 avg loss -2.686240 avg loss no lamb -2.686240 time 2020-06-25 21:19:59.211396
Model ind 665 epoch 145 batch: 400 avg loss -2.646422 avg loss no lamb -2.646422 time 2020-06-25 21:20:10.030753
Model ind 665 epoch 145 batch: 500 avg loss -2.706918 avg loss no lamb -2.706918 time 2020-06-25 21:20:20.680844
Model ind 665 epoch 145 batch: 600 avg loss -2.857928 avg loss no lamb -2.857928 time 2020-06-25 21:20:31.678342
Model ind 665 epoch 145 batch: 700 avg loss -2.609974 avg loss no lamb -2.609974 time 2020-06-25 21:20:42.491790
Model ind 665 epoch 145 batch: 800 avg loss -2.788538 avg loss no lamb -2.788538 time 2020-06-25 21:20:53.497688
last batch sz 10
Pre: time 2020-06-25 21:21:07.383206: 
 	std: 0.002557659
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9828, 0.9773, 0.9828, 0.9775]
	train_accs: [0.9829, 0.9827, 0.97783333, 0.98323333, 0.97833335]
	best_train_sub_head: 3
	worst: 0.9773
	avg: 0.98052007
	best: 0.9828

Starting e_i: 146
Model ind 665 epoch 146 batch: 0 avg loss -2.834007 avg loss no lamb -2.834007 time 2020-06-25 21:21:08.146897
Model ind 665 epoch 146 batch: 100 avg loss -2.688049 avg loss no lamb -2.688049 time 2020-06-25 21:21:18.684374
Model ind 665 epoch 146 batch: 200 avg loss -2.708493 avg loss no lamb -2.708493 time 2020-06-25 21:21:29.655475
Model ind 665 epoch 146 batch: 300 avg loss -2.722292 avg loss no lamb -2.722292 time 2020-06-25 21:21:40.387116
Model ind 665 epoch 146 batch: 400 avg loss -2.682068 avg loss no lamb -2.682068 time 2020-06-25 21:21:51.026731
Model ind 665 epoch 146 batch: 500 avg loss -2.684373 avg loss no lamb -2.684373 time 2020-06-25 21:22:01.898583
Model ind 665 epoch 146 batch: 600 avg loss -2.809284 avg loss no lamb -2.809284 time 2020-06-25 21:22:12.678226
Model ind 665 epoch 146 batch: 700 avg loss -2.496540 avg loss no lamb -2.496540 time 2020-06-25 21:22:23.379845
Model ind 665 epoch 146 batch: 800 avg loss -2.693383 avg loss no lamb -2.693383 time 2020-06-25 21:22:34.233581
last batch sz 10
Pre: time 2020-06-25 21:22:47.954655: 
 	std: 0.0031167914
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9809, 0.9755, 0.9827, 0.9764]
	train_accs: [0.98288333, 0.98153335, 0.97665, 0.98295, 0.9776833]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.9796599
	best: 0.9827

Starting e_i: 147
Model ind 665 epoch 147 batch: 0 avg loss -2.871151 avg loss no lamb -2.871151 time 2020-06-25 21:22:48.698705
Model ind 665 epoch 147 batch: 100 avg loss -2.768350 avg loss no lamb -2.768350 time 2020-06-25 21:22:59.391722
Model ind 665 epoch 147 batch: 200 avg loss -2.706733 avg loss no lamb -2.706733 time 2020-06-25 21:23:10.244874
Model ind 665 epoch 147 batch: 300 avg loss -2.699805 avg loss no lamb -2.699805 time 2020-06-25 21:23:20.981182
Model ind 665 epoch 147 batch: 400 avg loss -2.697192 avg loss no lamb -2.697192 time 2020-06-25 21:23:31.876156
Model ind 665 epoch 147 batch: 500 avg loss -2.645313 avg loss no lamb -2.645313 time 2020-06-25 21:23:42.450350
Model ind 665 epoch 147 batch: 600 avg loss -2.761957 avg loss no lamb -2.761957 time 2020-06-25 21:23:52.896905
Model ind 665 epoch 147 batch: 700 avg loss -2.599303 avg loss no lamb -2.599303 time 2020-06-25 21:24:03.817739
Model ind 665 epoch 147 batch: 800 avg loss -2.624440 avg loss no lamb -2.624440 time 2020-06-25 21:24:14.560279
last batch sz 10
Pre: time 2020-06-25 21:24:28.670802: 
 	std: 0.0032603096
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9813, 0.9755, 0.9821, 0.975]
	train_accs: [0.98268336, 0.9819, 0.9759833, 0.9828167, 0.9766167]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97922003
	best: 0.9821

Starting e_i: 148
Model ind 665 epoch 148 batch: 0 avg loss -2.921619 avg loss no lamb -2.921619 time 2020-06-25 21:24:29.454782
Model ind 665 epoch 148 batch: 100 avg loss -2.804433 avg loss no lamb -2.804433 time 2020-06-25 21:24:40.228084
Model ind 665 epoch 148 batch: 200 avg loss -2.782250 avg loss no lamb -2.782250 time 2020-06-25 21:24:50.902794
Model ind 665 epoch 148 batch: 300 avg loss -2.740054 avg loss no lamb -2.740054 time 2020-06-25 21:25:01.739338
Model ind 665 epoch 148 batch: 400 avg loss -2.605013 avg loss no lamb -2.605013 time 2020-06-25 21:25:12.280091
Model ind 665 epoch 148 batch: 500 avg loss -2.680744 avg loss no lamb -2.680744 time 2020-06-25 21:25:23.273432
Model ind 665 epoch 148 batch: 600 avg loss -2.770223 avg loss no lamb -2.770223 time 2020-06-25 21:25:34.134892
Model ind 665 epoch 148 batch: 700 avg loss -2.588337 avg loss no lamb -2.588337 time 2020-06-25 21:25:44.934068
Model ind 665 epoch 148 batch: 800 avg loss -2.701339 avg loss no lamb -2.701339 time 2020-06-25 21:25:55.578203
last batch sz 10
Pre: time 2020-06-25 21:26:09.616968: 
 	std: 0.00268522
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9799, 0.9748, 0.9809, 0.9756]
	train_accs: [0.9825, 0.9813667, 0.97641665, 0.98258334, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97844
	best: 0.9809

Starting e_i: 149
Model ind 665 epoch 149 batch: 0 avg loss -2.919898 avg loss no lamb -2.919898 time 2020-06-25 21:26:10.353382
Model ind 665 epoch 149 batch: 100 avg loss -2.763812 avg loss no lamb -2.763812 time 2020-06-25 21:26:21.070867
Model ind 665 epoch 149 batch: 200 avg loss -2.795777 avg loss no lamb -2.795777 time 2020-06-25 21:26:31.791200
Model ind 665 epoch 149 batch: 300 avg loss -2.693359 avg loss no lamb -2.693359 time 2020-06-25 21:26:42.394169
Model ind 665 epoch 149 batch: 400 avg loss -2.685739 avg loss no lamb -2.685739 time 2020-06-25 21:26:53.189181
Model ind 665 epoch 149 batch: 500 avg loss -2.674339 avg loss no lamb -2.674339 time 2020-06-25 21:27:03.804819
Model ind 665 epoch 149 batch: 600 avg loss -2.754589 avg loss no lamb -2.754589 time 2020-06-25 21:27:14.466359
Model ind 665 epoch 149 batch: 700 avg loss -2.590432 avg loss no lamb -2.590432 time 2020-06-25 21:27:25.232472
Model ind 665 epoch 149 batch: 800 avg loss -2.716596 avg loss no lamb -2.716596 time 2020-06-25 21:27:35.851783
last batch sz 10
Pre: time 2020-06-25 21:27:49.592188: 
 	std: 0.0024532354
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9835, 0.983, 0.9781, 0.9835, 0.9786]
	train_accs: [0.98345, 0.9827, 0.97788334, 0.98331666, 0.97898334]
	best_train_sub_head: 0
	worst: 0.9781
	avg: 0.98134005
	best: 0.9835

Starting e_i: 150
Model ind 665 epoch 150 batch: 0 avg loss -2.839720 avg loss no lamb -2.839720 time 2020-06-25 21:27:50.311864
Model ind 665 epoch 150 batch: 100 avg loss -2.724590 avg loss no lamb -2.724590 time 2020-06-25 21:28:00.925755
Model ind 665 epoch 150 batch: 200 avg loss -2.651818 avg loss no lamb -2.651818 time 2020-06-25 21:28:11.467017
Model ind 665 epoch 150 batch: 300 avg loss -2.825334 avg loss no lamb -2.825334 time 2020-06-25 21:28:22.166672
Model ind 665 epoch 150 batch: 400 avg loss -2.660863 avg loss no lamb -2.660863 time 2020-06-25 21:28:32.770384
Model ind 665 epoch 150 batch: 500 avg loss -2.718553 avg loss no lamb -2.718553 time 2020-06-25 21:28:43.325373
Model ind 665 epoch 150 batch: 600 avg loss -2.776934 avg loss no lamb -2.776934 time 2020-06-25 21:28:53.907150
Model ind 665 epoch 150 batch: 700 avg loss -2.609877 avg loss no lamb -2.609877 time 2020-06-25 21:29:04.852079
Model ind 665 epoch 150 batch: 800 avg loss -2.811388 avg loss no lamb -2.811388 time 2020-06-25 21:29:15.277581
last batch sz 10
Pre: time 2020-06-25 21:29:28.837255: 
 	std: 0.0027925626
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9813, 0.976, 0.9823, 0.9765]
	train_accs: [0.98228335, 0.9813667, 0.9759333, 0.9825, 0.97681665]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.97964
	best: 0.9823

Starting e_i: 151
Model ind 665 epoch 151 batch: 0 avg loss -2.820122 avg loss no lamb -2.820122 time 2020-06-25 21:29:30.709343
Model ind 665 epoch 151 batch: 100 avg loss -2.783763 avg loss no lamb -2.783763 time 2020-06-25 21:29:41.419912
Model ind 665 epoch 151 batch: 200 avg loss -2.681653 avg loss no lamb -2.681653 time 2020-06-25 21:29:52.131002
Model ind 665 epoch 151 batch: 300 avg loss -2.689360 avg loss no lamb -2.689360 time 2020-06-25 21:30:02.672016
Model ind 665 epoch 151 batch: 400 avg loss -2.718203 avg loss no lamb -2.718203 time 2020-06-25 21:30:13.134572
Model ind 665 epoch 151 batch: 500 avg loss -2.718803 avg loss no lamb -2.718803 time 2020-06-25 21:30:23.866029
Model ind 665 epoch 151 batch: 600 avg loss -2.790664 avg loss no lamb -2.790664 time 2020-06-25 21:30:34.698003
Model ind 665 epoch 151 batch: 700 avg loss -2.704413 avg loss no lamb -2.704413 time 2020-06-25 21:30:45.358386
Model ind 665 epoch 151 batch: 800 avg loss -2.654819 avg loss no lamb -2.654819 time 2020-06-25 21:30:55.951249
last batch sz 10
Pre: time 2020-06-25 21:31:09.792184: 
 	std: 0.003263495
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9823, 0.9752, 0.9818, 0.9755]
	train_accs: [0.9825, 0.9817333, 0.9756, 0.9823, 0.9763]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97933996
	best: 0.9819

Starting e_i: 152
Model ind 665 epoch 152 batch: 0 avg loss -2.872143 avg loss no lamb -2.872143 time 2020-06-25 21:31:10.562183
Model ind 665 epoch 152 batch: 100 avg loss -2.717699 avg loss no lamb -2.717699 time 2020-06-25 21:31:21.273741
Model ind 665 epoch 152 batch: 200 avg loss -2.748787 avg loss no lamb -2.748787 time 2020-06-25 21:31:32.041383
Model ind 665 epoch 152 batch: 300 avg loss -2.756863 avg loss no lamb -2.756863 time 2020-06-25 21:31:42.577064
Model ind 665 epoch 152 batch: 400 avg loss -2.598880 avg loss no lamb -2.598880 time 2020-06-25 21:31:53.186283
Model ind 665 epoch 152 batch: 500 avg loss -2.645087 avg loss no lamb -2.645087 time 2020-06-25 21:32:03.943606
Model ind 665 epoch 152 batch: 600 avg loss -2.839552 avg loss no lamb -2.839552 time 2020-06-25 21:32:14.437844
Model ind 665 epoch 152 batch: 700 avg loss -2.563746 avg loss no lamb -2.563746 time 2020-06-25 21:32:25.105754
Model ind 665 epoch 152 batch: 800 avg loss -2.658891 avg loss no lamb -2.658891 time 2020-06-25 21:32:35.893606
last batch sz 10
Pre: time 2020-06-25 21:32:49.536208: 
 	std: 0.002838021
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.983, 0.9763, 0.9823, 0.9773]
	train_accs: [0.98225, 0.9820167, 0.9766167, 0.98256665, 0.9777]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.98024
	best: 0.9823

Starting e_i: 153
Model ind 665 epoch 153 batch: 0 avg loss -2.957684 avg loss no lamb -2.957684 time 2020-06-25 21:32:50.307489
Model ind 665 epoch 153 batch: 100 avg loss -2.698794 avg loss no lamb -2.698794 time 2020-06-25 21:33:00.981440
Model ind 665 epoch 153 batch: 200 avg loss -2.694407 avg loss no lamb -2.694407 time 2020-06-25 21:33:11.671353
Model ind 665 epoch 153 batch: 300 avg loss -2.707827 avg loss no lamb -2.707827 time 2020-06-25 21:33:22.278545
Model ind 665 epoch 153 batch: 400 avg loss -2.659944 avg loss no lamb -2.659944 time 2020-06-25 21:33:32.959896
Model ind 665 epoch 153 batch: 500 avg loss -2.711990 avg loss no lamb -2.711990 time 2020-06-25 21:33:43.548024
Model ind 665 epoch 153 batch: 600 avg loss -2.829324 avg loss no lamb -2.829324 time 2020-06-25 21:33:54.453780
Model ind 665 epoch 153 batch: 700 avg loss -2.590038 avg loss no lamb -2.590038 time 2020-06-25 21:34:05.191925
Model ind 665 epoch 153 batch: 800 avg loss -2.770048 avg loss no lamb -2.770048 time 2020-06-25 21:34:15.872720
last batch sz 10
Pre: time 2020-06-25 21:34:29.597654: 
 	std: 0.0022754269
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9819, 0.9836, 0.9779, 0.9819, 0.9781]
	train_accs: [0.9827333, 0.98303336, 0.97786665, 0.98275, 0.9783]
	best_train_sub_head: 1
	worst: 0.9779
	avg: 0.98068
	best: 0.9836

Starting e_i: 154
Model ind 665 epoch 154 batch: 0 avg loss -2.906500 avg loss no lamb -2.906500 time 2020-06-25 21:34:30.316566
Model ind 665 epoch 154 batch: 100 avg loss -2.736353 avg loss no lamb -2.736353 time 2020-06-25 21:34:41.132524
Model ind 665 epoch 154 batch: 200 avg loss -2.725018 avg loss no lamb -2.725018 time 2020-06-25 21:34:51.959091
Model ind 665 epoch 154 batch: 300 avg loss -2.705538 avg loss no lamb -2.705538 time 2020-06-25 21:35:02.590735
Model ind 665 epoch 154 batch: 400 avg loss -2.671052 avg loss no lamb -2.671052 time 2020-06-25 21:35:13.365445
Model ind 665 epoch 154 batch: 500 avg loss -2.645753 avg loss no lamb -2.645753 time 2020-06-25 21:35:23.991544
Model ind 665 epoch 154 batch: 600 avg loss -2.826770 avg loss no lamb -2.826770 time 2020-06-25 21:35:34.643155
Model ind 665 epoch 154 batch: 700 avg loss -2.566694 avg loss no lamb -2.566694 time 2020-06-25 21:35:45.235931
Model ind 665 epoch 154 batch: 800 avg loss -2.746318 avg loss no lamb -2.746318 time 2020-06-25 21:35:55.760292
last batch sz 10
Pre: time 2020-06-25 21:36:09.714323: 
 	std: 0.0030740239
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9804, 0.9748, 0.9819, 0.9758]
	train_accs: [0.9819833, 0.98078334, 0.975, 0.982, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97897995
	best: 0.9819

Starting e_i: 155
Model ind 665 epoch 155 batch: 0 avg loss -2.852550 avg loss no lamb -2.852550 time 2020-06-25 21:36:10.450641
Model ind 665 epoch 155 batch: 100 avg loss -2.763942 avg loss no lamb -2.763942 time 2020-06-25 21:36:20.989855
Model ind 665 epoch 155 batch: 200 avg loss -2.841661 avg loss no lamb -2.841661 time 2020-06-25 21:36:31.576745
Model ind 665 epoch 155 batch: 300 avg loss -2.787618 avg loss no lamb -2.787618 time 2020-06-25 21:36:42.313659
Model ind 665 epoch 155 batch: 400 avg loss -2.692624 avg loss no lamb -2.692624 time 2020-06-25 21:36:53.147537
Model ind 665 epoch 155 batch: 500 avg loss -2.741627 avg loss no lamb -2.741627 time 2020-06-25 21:37:03.930303
Model ind 665 epoch 155 batch: 600 avg loss -2.821106 avg loss no lamb -2.821106 time 2020-06-25 21:37:14.783367
Model ind 665 epoch 155 batch: 700 avg loss -2.537267 avg loss no lamb -2.537267 time 2020-06-25 21:37:25.512802
Model ind 665 epoch 155 batch: 800 avg loss -2.714156 avg loss no lamb -2.714156 time 2020-06-25 21:37:36.204780
last batch sz 10
Pre: time 2020-06-25 21:37:49.777194: 
 	std: 0.002491102
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.981, 0.9763, 0.9823, 0.9773]
	train_accs: [0.9834167, 0.98235, 0.9773333, 0.9831833, 0.9789]
	best_train_sub_head: 0
	worst: 0.9763
	avg: 0.97978
	best: 0.982

Starting e_i: 156
Model ind 665 epoch 156 batch: 0 avg loss -2.923159 avg loss no lamb -2.923159 time 2020-06-25 21:37:50.546689
Model ind 665 epoch 156 batch: 100 avg loss -2.726921 avg loss no lamb -2.726921 time 2020-06-25 21:38:01.346445
Model ind 665 epoch 156 batch: 200 avg loss -2.808548 avg loss no lamb -2.808548 time 2020-06-25 21:38:11.771882
Model ind 665 epoch 156 batch: 300 avg loss -2.704334 avg loss no lamb -2.704334 time 2020-06-25 21:38:22.299049
Model ind 665 epoch 156 batch: 400 avg loss -2.664943 avg loss no lamb -2.664943 time 2020-06-25 21:38:33.066719
Model ind 665 epoch 156 batch: 500 avg loss -2.714092 avg loss no lamb -2.714092 time 2020-06-25 21:38:43.950499
Model ind 665 epoch 156 batch: 600 avg loss -2.783484 avg loss no lamb -2.783484 time 2020-06-25 21:38:54.720464
Model ind 665 epoch 156 batch: 700 avg loss -2.624703 avg loss no lamb -2.624703 time 2020-06-25 21:39:05.366686
Model ind 665 epoch 156 batch: 800 avg loss -2.688702 avg loss no lamb -2.688702 time 2020-06-25 21:39:15.864822
last batch sz 10
Pre: time 2020-06-25 21:39:29.482813: 
 	std: 0.003581952
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9807, 0.9751, 0.9823, 0.9738]
	train_accs: [0.9827, 0.9810333, 0.9759167, 0.9824, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97876006
	best: 0.9819

Starting e_i: 157
Model ind 665 epoch 157 batch: 0 avg loss -2.862013 avg loss no lamb -2.862013 time 2020-06-25 21:39:30.232221
Model ind 665 epoch 157 batch: 100 avg loss -2.766477 avg loss no lamb -2.766477 time 2020-06-25 21:39:40.915135
Model ind 665 epoch 157 batch: 200 avg loss -2.762825 avg loss no lamb -2.762825 time 2020-06-25 21:39:51.680405
Model ind 665 epoch 157 batch: 300 avg loss -2.804127 avg loss no lamb -2.804127 time 2020-06-25 21:40:02.443597
Model ind 665 epoch 157 batch: 400 avg loss -2.657974 avg loss no lamb -2.657974 time 2020-06-25 21:40:13.100919
Model ind 665 epoch 157 batch: 500 avg loss -2.717566 avg loss no lamb -2.717566 time 2020-06-25 21:40:23.840121
Model ind 665 epoch 157 batch: 600 avg loss -2.762631 avg loss no lamb -2.762631 time 2020-06-25 21:40:34.631953
Model ind 665 epoch 157 batch: 700 avg loss -2.538821 avg loss no lamb -2.538821 time 2020-06-25 21:40:45.282556
Model ind 665 epoch 157 batch: 800 avg loss -2.665330 avg loss no lamb -2.665330 time 2020-06-25 21:40:55.924169
last batch sz 10
Pre: time 2020-06-25 21:41:09.873973: 
 	std: 0.0030056115
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9796, 0.9736, 0.9796, 0.9734]
	train_accs: [0.9816, 0.9809333, 0.9762, 0.9816, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97718
	best: 0.9797

Starting e_i: 158
Model ind 665 epoch 158 batch: 0 avg loss -2.898113 avg loss no lamb -2.898113 time 2020-06-25 21:41:10.583354
Model ind 665 epoch 158 batch: 100 avg loss -2.700516 avg loss no lamb -2.700516 time 2020-06-25 21:41:21.432522
Model ind 665 epoch 158 batch: 200 avg loss -2.682604 avg loss no lamb -2.682604 time 2020-06-25 21:41:32.280291
Model ind 665 epoch 158 batch: 300 avg loss -2.758518 avg loss no lamb -2.758518 time 2020-06-25 21:41:43.037915
Model ind 665 epoch 158 batch: 400 avg loss -2.689861 avg loss no lamb -2.689861 time 2020-06-25 21:41:53.684481
Model ind 665 epoch 158 batch: 500 avg loss -2.739952 avg loss no lamb -2.739952 time 2020-06-25 21:42:04.096517
Model ind 665 epoch 158 batch: 600 avg loss -2.746262 avg loss no lamb -2.746262 time 2020-06-25 21:42:15.101405
Model ind 665 epoch 158 batch: 700 avg loss -2.641287 avg loss no lamb -2.641287 time 2020-06-25 21:42:25.851355
Model ind 665 epoch 158 batch: 800 avg loss -2.718783 avg loss no lamb -2.718783 time 2020-06-25 21:42:36.644084
last batch sz 10
Pre: time 2020-06-25 21:42:50.352588: 
 	std: 0.0027151469
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9833, 0.9825, 0.9778, 0.9832, 0.9772]
	train_accs: [0.9830833, 0.98225, 0.97763336, 0.9831833, 0.97781664]
	best_train_sub_head: 3
	worst: 0.9772
	avg: 0.98080003
	best: 0.9832

Starting e_i: 159
Model ind 665 epoch 159 batch: 0 avg loss -2.938574 avg loss no lamb -2.938574 time 2020-06-25 21:42:51.085134
Model ind 665 epoch 159 batch: 100 avg loss -2.748748 avg loss no lamb -2.748748 time 2020-06-25 21:43:01.894151
Model ind 665 epoch 159 batch: 200 avg loss -2.744468 avg loss no lamb -2.744468 time 2020-06-25 21:43:12.629712
Model ind 665 epoch 159 batch: 300 avg loss -2.732790 avg loss no lamb -2.732790 time 2020-06-25 21:43:23.434173
Model ind 665 epoch 159 batch: 400 avg loss -2.586158 avg loss no lamb -2.586158 time 2020-06-25 21:43:34.108363
Model ind 665 epoch 159 batch: 500 avg loss -2.707748 avg loss no lamb -2.707748 time 2020-06-25 21:43:45.167799
Model ind 665 epoch 159 batch: 600 avg loss -2.758264 avg loss no lamb -2.758264 time 2020-06-25 21:43:55.762350
Model ind 665 epoch 159 batch: 700 avg loss -2.565768 avg loss no lamb -2.565768 time 2020-06-25 21:44:06.564804
Model ind 665 epoch 159 batch: 800 avg loss -2.678144 avg loss no lamb -2.678144 time 2020-06-25 21:44:17.158046
last batch sz 10
Pre: time 2020-06-25 21:44:30.989166: 
 	std: 0.003354046
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9808, 0.9738, 0.981, 0.9748]
	train_accs: [0.9830833, 0.98191667, 0.97615, 0.98268336, 0.97746664]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97838
	best: 0.9815

Starting e_i: 160
Model ind 665 epoch 160 batch: 0 avg loss -2.900178 avg loss no lamb -2.900178 time 2020-06-25 21:44:31.805125
Model ind 665 epoch 160 batch: 100 avg loss -2.712542 avg loss no lamb -2.712542 time 2020-06-25 21:44:42.436328
Model ind 665 epoch 160 batch: 200 avg loss -2.770561 avg loss no lamb -2.770561 time 2020-06-25 21:44:53.138658
Model ind 665 epoch 160 batch: 300 avg loss -2.776584 avg loss no lamb -2.776584 time 2020-06-25 21:45:03.892854
Model ind 665 epoch 160 batch: 400 avg loss -2.696289 avg loss no lamb -2.696289 time 2020-06-25 21:45:14.669003
Model ind 665 epoch 160 batch: 500 avg loss -2.724441 avg loss no lamb -2.724441 time 2020-06-25 21:45:25.373616
Model ind 665 epoch 160 batch: 600 avg loss -2.822261 avg loss no lamb -2.822261 time 2020-06-25 21:45:36.027731
Model ind 665 epoch 160 batch: 700 avg loss -2.580977 avg loss no lamb -2.580977 time 2020-06-25 21:45:46.794927
Model ind 665 epoch 160 batch: 800 avg loss -2.749310 avg loss no lamb -2.749310 time 2020-06-25 21:45:57.754236
last batch sz 10
Pre: time 2020-06-25 21:46:11.518177: 
 	std: 0.0028167989
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9817, 0.9774, 0.9826, 0.9756]
	train_accs: [0.98225, 0.9816667, 0.9777167, 0.9827167, 0.9777167]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97986
	best: 0.9826

Starting e_i: 161
Model ind 665 epoch 161 batch: 0 avg loss -2.853370 avg loss no lamb -2.853370 time 2020-06-25 21:46:13.455390
Model ind 665 epoch 161 batch: 100 avg loss -2.707368 avg loss no lamb -2.707368 time 2020-06-25 21:46:23.833916
Model ind 665 epoch 161 batch: 200 avg loss -2.707913 avg loss no lamb -2.707913 time 2020-06-25 21:46:34.552358
Model ind 665 epoch 161 batch: 300 avg loss -2.826118 avg loss no lamb -2.826118 time 2020-06-25 21:46:45.482944
Model ind 665 epoch 161 batch: 400 avg loss -2.693579 avg loss no lamb -2.693579 time 2020-06-25 21:46:56.188773
Model ind 665 epoch 161 batch: 500 avg loss -2.653743 avg loss no lamb -2.653743 time 2020-06-25 21:47:06.934670
Model ind 665 epoch 161 batch: 600 avg loss -2.767506 avg loss no lamb -2.767506 time 2020-06-25 21:47:17.535914
Model ind 665 epoch 161 batch: 700 avg loss -2.541522 avg loss no lamb -2.541522 time 2020-06-25 21:47:28.067456
Model ind 665 epoch 161 batch: 800 avg loss -2.609899 avg loss no lamb -2.609899 time 2020-06-25 21:47:38.903500
last batch sz 10
Pre: time 2020-06-25 21:47:52.662201: 
 	std: 0.0034496472
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9799, 0.9737, 0.9806, 0.9733]
	train_accs: [0.98218334, 0.9809667, 0.97508335, 0.98156667, 0.976]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97770005
	best: 0.981

Starting e_i: 162
Model ind 665 epoch 162 batch: 0 avg loss -2.832601 avg loss no lamb -2.832601 time 2020-06-25 21:47:53.437248
Model ind 665 epoch 162 batch: 100 avg loss -2.688765 avg loss no lamb -2.688765 time 2020-06-25 21:48:04.278497
Model ind 665 epoch 162 batch: 200 avg loss -2.763292 avg loss no lamb -2.763292 time 2020-06-25 21:48:15.035790
Model ind 665 epoch 162 batch: 300 avg loss -2.828756 avg loss no lamb -2.828756 time 2020-06-25 21:48:26.037855
Model ind 665 epoch 162 batch: 400 avg loss -2.707087 avg loss no lamb -2.707087 time 2020-06-25 21:48:36.844848
Model ind 665 epoch 162 batch: 500 avg loss -2.649745 avg loss no lamb -2.649745 time 2020-06-25 21:48:47.546388
Model ind 665 epoch 162 batch: 600 avg loss -2.825596 avg loss no lamb -2.825596 time 2020-06-25 21:48:58.341866
Model ind 665 epoch 162 batch: 700 avg loss -2.604477 avg loss no lamb -2.604477 time 2020-06-25 21:49:09.038467
Model ind 665 epoch 162 batch: 800 avg loss -2.747940 avg loss no lamb -2.747940 time 2020-06-25 21:49:19.687795
last batch sz 10
Pre: time 2020-06-25 21:49:33.710271: 
 	std: 0.0040429644
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9817, 0.9743, 0.9817, 0.9733]
	train_accs: [0.98288333, 0.9823, 0.9755667, 0.98291665, 0.9749]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97872
	best: 0.9817

Starting e_i: 163
Model ind 665 epoch 163 batch: 0 avg loss -2.871844 avg loss no lamb -2.871844 time 2020-06-25 21:49:34.444446
Model ind 665 epoch 163 batch: 100 avg loss -2.734807 avg loss no lamb -2.734807 time 2020-06-25 21:49:45.332169
Model ind 665 epoch 163 batch: 200 avg loss -2.749925 avg loss no lamb -2.749925 time 2020-06-25 21:49:55.939274
Model ind 665 epoch 163 batch: 300 avg loss -2.767103 avg loss no lamb -2.767103 time 2020-06-25 21:50:06.681299
Model ind 665 epoch 163 batch: 400 avg loss -2.684572 avg loss no lamb -2.684572 time 2020-06-25 21:50:17.607796
Model ind 665 epoch 163 batch: 500 avg loss -2.637015 avg loss no lamb -2.637015 time 2020-06-25 21:50:28.363144
Model ind 665 epoch 163 batch: 600 avg loss -2.758601 avg loss no lamb -2.758601 time 2020-06-25 21:50:39.141392
Model ind 665 epoch 163 batch: 700 avg loss -2.623279 avg loss no lamb -2.623279 time 2020-06-25 21:50:49.664237
Model ind 665 epoch 163 batch: 800 avg loss -2.756437 avg loss no lamb -2.756437 time 2020-06-25 21:51:00.214575
last batch sz 10
Pre: time 2020-06-25 21:51:14.255641: 
 	std: 0.0024795213
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9815, 0.9772, 0.9816, 0.976]
	train_accs: [0.9824833, 0.9819833, 0.977, 0.98263335, 0.97753334]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.9796001
	best: 0.9816

Starting e_i: 164
Model ind 665 epoch 164 batch: 0 avg loss -2.863952 avg loss no lamb -2.863952 time 2020-06-25 21:51:15.081704
Model ind 665 epoch 164 batch: 100 avg loss -2.759147 avg loss no lamb -2.759147 time 2020-06-25 21:51:25.648215
Model ind 665 epoch 164 batch: 200 avg loss -2.762277 avg loss no lamb -2.762277 time 2020-06-25 21:51:36.560003
Model ind 665 epoch 164 batch: 300 avg loss -2.841563 avg loss no lamb -2.841563 time 2020-06-25 21:51:47.353967
Model ind 665 epoch 164 batch: 400 avg loss -2.593344 avg loss no lamb -2.593344 time 2020-06-25 21:51:57.852765
Model ind 665 epoch 164 batch: 500 avg loss -2.702208 avg loss no lamb -2.702208 time 2020-06-25 21:52:08.622965
Model ind 665 epoch 164 batch: 600 avg loss -2.761317 avg loss no lamb -2.761317 time 2020-06-25 21:52:19.361417
Model ind 665 epoch 164 batch: 700 avg loss -2.499885 avg loss no lamb -2.499885 time 2020-06-25 21:52:30.237002
Model ind 665 epoch 164 batch: 800 avg loss -2.697135 avg loss no lamb -2.697135 time 2020-06-25 21:52:41.041537
last batch sz 10
Pre: time 2020-06-25 21:52:54.714850: 
 	std: 0.0034982376
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.982, 0.9747, 0.9826, 0.9762]
	train_accs: [0.9821333, 0.98076665, 0.9744167, 0.9819, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97967994
	best: 0.9829

Starting e_i: 165
Model ind 665 epoch 165 batch: 0 avg loss -2.856119 avg loss no lamb -2.856119 time 2020-06-25 21:52:55.435470
Model ind 665 epoch 165 batch: 100 avg loss -2.660227 avg loss no lamb -2.660227 time 2020-06-25 21:53:06.152013
Model ind 665 epoch 165 batch: 200 avg loss -2.731503 avg loss no lamb -2.731503 time 2020-06-25 21:53:16.777089
Model ind 665 epoch 165 batch: 300 avg loss -2.777873 avg loss no lamb -2.777873 time 2020-06-25 21:53:27.765121
Model ind 665 epoch 165 batch: 400 avg loss -2.642647 avg loss no lamb -2.642647 time 2020-06-25 21:53:38.404327
Model ind 665 epoch 165 batch: 500 avg loss -2.673259 avg loss no lamb -2.673259 time 2020-06-25 21:53:49.444869
Model ind 665 epoch 165 batch: 600 avg loss -2.742517 avg loss no lamb -2.742517 time 2020-06-25 21:54:00.059285
Model ind 665 epoch 165 batch: 700 avg loss -2.643724 avg loss no lamb -2.643724 time 2020-06-25 21:54:10.901343
Model ind 665 epoch 165 batch: 800 avg loss -2.747434 avg loss no lamb -2.747434 time 2020-06-25 21:54:21.620579
last batch sz 10
Pre: time 2020-06-25 21:54:34.979375: 
 	std: 0.0040366375
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9838, 0.9834, 0.9748, 0.9842, 0.9765]
	train_accs: [0.98326665, 0.98228335, 0.97478336, 0.98338336, 0.97675]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.98054
	best: 0.9842

Starting e_i: 166
Model ind 665 epoch 166 batch: 0 avg loss -2.896381 avg loss no lamb -2.896381 time 2020-06-25 21:54:35.705958
Model ind 665 epoch 166 batch: 100 avg loss -2.842141 avg loss no lamb -2.842141 time 2020-06-25 21:54:46.399091
Model ind 665 epoch 166 batch: 200 avg loss -2.815721 avg loss no lamb -2.815721 time 2020-06-25 21:54:57.019122
Model ind 665 epoch 166 batch: 300 avg loss -2.793021 avg loss no lamb -2.793021 time 2020-06-25 21:55:07.950174
Model ind 665 epoch 166 batch: 400 avg loss -2.620679 avg loss no lamb -2.620679 time 2020-06-25 21:55:18.639134
Model ind 665 epoch 166 batch: 500 avg loss -2.738438 avg loss no lamb -2.738438 time 2020-06-25 21:55:29.556365
Model ind 665 epoch 166 batch: 600 avg loss -2.826043 avg loss no lamb -2.826043 time 2020-06-25 21:55:40.337084
Model ind 665 epoch 166 batch: 700 avg loss -2.645427 avg loss no lamb -2.645427 time 2020-06-25 21:55:51.131985
Model ind 665 epoch 166 batch: 800 avg loss -2.715278 avg loss no lamb -2.715278 time 2020-06-25 21:56:01.867940
last batch sz 10
Pre: time 2020-06-25 21:56:15.560413: 
 	std: 0.0037504158
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9811, 0.9739, 0.9811, 0.9735]
	train_accs: [0.9824833, 0.9812, 0.97566664, 0.98223335, 0.9759333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97828007
	best: 0.9818

Starting e_i: 167
Model ind 665 epoch 167 batch: 0 avg loss -2.901391 avg loss no lamb -2.901391 time 2020-06-25 21:56:16.265973
Model ind 665 epoch 167 batch: 100 avg loss -2.774395 avg loss no lamb -2.774395 time 2020-06-25 21:56:26.980417
Model ind 665 epoch 167 batch: 200 avg loss -2.738980 avg loss no lamb -2.738980 time 2020-06-25 21:56:37.695150
Model ind 665 epoch 167 batch: 300 avg loss -2.786349 avg loss no lamb -2.786349 time 2020-06-25 21:56:48.529227
Model ind 665 epoch 167 batch: 400 avg loss -2.661041 avg loss no lamb -2.661041 time 2020-06-25 21:56:58.967963
Model ind 665 epoch 167 batch: 500 avg loss -2.667115 avg loss no lamb -2.667115 time 2020-06-25 21:57:09.680983
Model ind 665 epoch 167 batch: 600 avg loss -2.794498 avg loss no lamb -2.794498 time 2020-06-25 21:57:20.160568
Model ind 665 epoch 167 batch: 700 avg loss -2.619669 avg loss no lamb -2.619669 time 2020-06-25 21:57:31.070222
Model ind 665 epoch 167 batch: 800 avg loss -2.729430 avg loss no lamb -2.729430 time 2020-06-25 21:57:41.795236
last batch sz 10
Pre: time 2020-06-25 21:57:55.817847: 
 	std: 0.002940329
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9817, 0.9756, 0.9823, 0.9764]
	train_accs: [0.98195, 0.98115, 0.97581667, 0.98221666, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97958004
	best: 0.9823

Starting e_i: 168
Model ind 665 epoch 168 batch: 0 avg loss -2.903039 avg loss no lamb -2.903039 time 2020-06-25 21:57:56.638929
Model ind 665 epoch 168 batch: 100 avg loss -2.722583 avg loss no lamb -2.722583 time 2020-06-25 21:58:07.274763
Model ind 665 epoch 168 batch: 200 avg loss -2.773446 avg loss no lamb -2.773446 time 2020-06-25 21:58:17.901468
Model ind 665 epoch 168 batch: 300 avg loss -2.732630 avg loss no lamb -2.732630 time 2020-06-25 21:58:28.621464
Model ind 665 epoch 168 batch: 400 avg loss -2.659789 avg loss no lamb -2.659789 time 2020-06-25 21:58:39.434847
Model ind 665 epoch 168 batch: 500 avg loss -2.619457 avg loss no lamb -2.619457 time 2020-06-25 21:58:50.232774
Model ind 665 epoch 168 batch: 600 avg loss -2.767322 avg loss no lamb -2.767322 time 2020-06-25 21:59:00.845307
Model ind 665 epoch 168 batch: 700 avg loss -2.495047 avg loss no lamb -2.495047 time 2020-06-25 21:59:11.504395
Model ind 665 epoch 168 batch: 800 avg loss -2.606167 avg loss no lamb -2.606167 time 2020-06-25 21:59:22.334794
last batch sz 10
Pre: time 2020-06-25 21:59:36.146144: 
 	std: 0.0026582743
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9814, 0.9771, 0.9829, 0.977]
	train_accs: [0.98296666, 0.9819667, 0.9773333, 0.9827667, 0.97728336]
	best_train_sub_head: 0
	worst: 0.977
	avg: 0.98024005
	best: 0.9828

Starting e_i: 169
Model ind 665 epoch 169 batch: 0 avg loss -2.880946 avg loss no lamb -2.880946 time 2020-06-25 21:59:36.875633
Model ind 665 epoch 169 batch: 100 avg loss -2.779189 avg loss no lamb -2.779189 time 2020-06-25 21:59:47.642321
Model ind 665 epoch 169 batch: 200 avg loss -2.763738 avg loss no lamb -2.763738 time 2020-06-25 21:59:58.077069
Model ind 665 epoch 169 batch: 300 avg loss -2.741322 avg loss no lamb -2.741322 time 2020-06-25 22:00:08.763428
Model ind 665 epoch 169 batch: 400 avg loss -2.701545 avg loss no lamb -2.701545 time 2020-06-25 22:00:19.313710
Model ind 665 epoch 169 batch: 500 avg loss -2.698944 avg loss no lamb -2.698944 time 2020-06-25 22:00:30.038815
Model ind 665 epoch 169 batch: 600 avg loss -2.786107 avg loss no lamb -2.786107 time 2020-06-25 22:00:40.664095
Model ind 665 epoch 169 batch: 700 avg loss -2.622252 avg loss no lamb -2.622252 time 2020-06-25 22:00:51.577215
Model ind 665 epoch 169 batch: 800 avg loss -2.819885 avg loss no lamb -2.819885 time 2020-06-25 22:01:02.394589
last batch sz 10
Pre: time 2020-06-25 22:01:16.405813: 
 	std: 0.0037961132
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9821, 0.9744, 0.9822, 0.9748]
	train_accs: [0.98286664, 0.98226666, 0.9749333, 0.9824167, 0.9762833]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97924006
	best: 0.9827

Starting e_i: 170
Model ind 665 epoch 170 batch: 0 avg loss -2.830265 avg loss no lamb -2.830265 time 2020-06-25 22:01:17.102456
Model ind 665 epoch 170 batch: 100 avg loss -2.774465 avg loss no lamb -2.774465 time 2020-06-25 22:01:27.757960
Model ind 665 epoch 170 batch: 200 avg loss -2.786498 avg loss no lamb -2.786498 time 2020-06-25 22:01:38.635569
Model ind 665 epoch 170 batch: 300 avg loss -2.763788 avg loss no lamb -2.763788 time 2020-06-25 22:01:49.649637
Model ind 665 epoch 170 batch: 400 avg loss -2.684729 avg loss no lamb -2.684729 time 2020-06-25 22:02:00.445571
Model ind 665 epoch 170 batch: 500 avg loss -2.696161 avg loss no lamb -2.696161 time 2020-06-25 22:02:11.098983
Model ind 665 epoch 170 batch: 600 avg loss -2.779898 avg loss no lamb -2.779898 time 2020-06-25 22:02:21.933231
Model ind 665 epoch 170 batch: 700 avg loss -2.593240 avg loss no lamb -2.593240 time 2020-06-25 22:02:32.817963
Model ind 665 epoch 170 batch: 800 avg loss -2.744071 avg loss no lamb -2.744071 time 2020-06-25 22:02:43.522460
last batch sz 10
Pre: time 2020-06-25 22:02:57.158564: 
 	std: 0.0026731286
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9817, 0.9773, 0.9831, 0.9771]
	train_accs: [0.9824333, 0.98146665, 0.97675, 0.9827667, 0.97788334]
	best_train_sub_head: 3
	worst: 0.9771
	avg: 0.98042
	best: 0.9831

Starting e_i: 171
Model ind 665 epoch 171 batch: 0 avg loss -2.904007 avg loss no lamb -2.904007 time 2020-06-25 22:02:59.043517
Model ind 665 epoch 171 batch: 100 avg loss -2.777100 avg loss no lamb -2.777100 time 2020-06-25 22:03:09.796558
Model ind 665 epoch 171 batch: 200 avg loss -2.736297 avg loss no lamb -2.736297 time 2020-06-25 22:03:20.320792
Model ind 665 epoch 171 batch: 300 avg loss -2.798328 avg loss no lamb -2.798328 time 2020-06-25 22:03:30.963282
Model ind 665 epoch 171 batch: 400 avg loss -2.603397 avg loss no lamb -2.603397 time 2020-06-25 22:03:41.336361
Model ind 665 epoch 171 batch: 500 avg loss -2.669250 avg loss no lamb -2.669250 time 2020-06-25 22:03:52.150779
Model ind 665 epoch 171 batch: 600 avg loss -2.713135 avg loss no lamb -2.713135 time 2020-06-25 22:04:03.063443
Model ind 665 epoch 171 batch: 700 avg loss -2.590195 avg loss no lamb -2.590195 time 2020-06-25 22:04:13.825942
Model ind 665 epoch 171 batch: 800 avg loss -2.675521 avg loss no lamb -2.675521 time 2020-06-25 22:04:24.381516
last batch sz 10
Pre: time 2020-06-25 22:04:38.057004: 
 	std: 0.0026114963
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9792, 0.975, 0.9798, 0.9739]
	train_accs: [0.9808, 0.9801667, 0.9755667, 0.98043334, 0.97603333]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.9776
	best: 0.9801

Starting e_i: 172
Model ind 665 epoch 172 batch: 0 avg loss -2.888820 avg loss no lamb -2.888820 time 2020-06-25 22:04:38.841512
Model ind 665 epoch 172 batch: 100 avg loss -2.619517 avg loss no lamb -2.619517 time 2020-06-25 22:04:49.641160
Model ind 665 epoch 172 batch: 200 avg loss -2.732036 avg loss no lamb -2.732036 time 2020-06-25 22:05:00.441615
Model ind 665 epoch 172 batch: 300 avg loss -2.744481 avg loss no lamb -2.744481 time 2020-06-25 22:05:11.088897
Model ind 665 epoch 172 batch: 400 avg loss -2.646035 avg loss no lamb -2.646035 time 2020-06-25 22:05:21.863541
Model ind 665 epoch 172 batch: 500 avg loss -2.756300 avg loss no lamb -2.756300 time 2020-06-25 22:05:32.637780
Model ind 665 epoch 172 batch: 600 avg loss -2.775207 avg loss no lamb -2.775207 time 2020-06-25 22:05:43.453041
Model ind 665 epoch 172 batch: 700 avg loss -2.641285 avg loss no lamb -2.641285 time 2020-06-25 22:05:54.097152
Model ind 665 epoch 172 batch: 800 avg loss -2.658530 avg loss no lamb -2.658530 time 2020-06-25 22:06:04.956701
last batch sz 10
Pre: time 2020-06-25 22:06:18.567378: 
 	std: 0.002324981
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9807, 0.9764, 0.9819, 0.9778]
	train_accs: [0.98286664, 0.98158336, 0.97686666, 0.9827667, 0.97815]
	best_train_sub_head: 0
	worst: 0.9764
	avg: 0.9798201
	best: 0.9823

Starting e_i: 173
Model ind 665 epoch 173 batch: 0 avg loss -2.846633 avg loss no lamb -2.846633 time 2020-06-25 22:06:19.339185
Model ind 665 epoch 173 batch: 100 avg loss -2.748010 avg loss no lamb -2.748010 time 2020-06-25 22:06:29.916859
Model ind 665 epoch 173 batch: 200 avg loss -2.777744 avg loss no lamb -2.777744 time 2020-06-25 22:06:40.630782
Model ind 665 epoch 173 batch: 300 avg loss -2.726222 avg loss no lamb -2.726222 time 2020-06-25 22:06:51.351681
Model ind 665 epoch 173 batch: 400 avg loss -2.691195 avg loss no lamb -2.691195 time 2020-06-25 22:07:01.959210
Model ind 665 epoch 173 batch: 500 avg loss -2.722267 avg loss no lamb -2.722267 time 2020-06-25 22:07:12.928307
Model ind 665 epoch 173 batch: 600 avg loss -2.812215 avg loss no lamb -2.812215 time 2020-06-25 22:07:23.583316
Model ind 665 epoch 173 batch: 700 avg loss -2.568443 avg loss no lamb -2.568443 time 2020-06-25 22:07:34.072418
Model ind 665 epoch 173 batch: 800 avg loss -2.737481 avg loss no lamb -2.737481 time 2020-06-25 22:07:44.679463
last batch sz 10
Pre: time 2020-06-25 22:07:58.921875: 
 	std: 0.0026237296
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9844, 0.983, 0.9781, 0.9845, 0.9795]
	train_accs: [0.98375, 0.98303336, 0.97901666, 0.9835167, 0.97961664]
	best_train_sub_head: 0
	worst: 0.9781
	avg: 0.9818999
	best: 0.9844

Starting e_i: 174
Model ind 665 epoch 174 batch: 0 avg loss -2.830192 avg loss no lamb -2.830192 time 2020-06-25 22:07:59.647449
Model ind 665 epoch 174 batch: 100 avg loss -2.753488 avg loss no lamb -2.753488 time 2020-06-25 22:08:10.297671
Model ind 665 epoch 174 batch: 200 avg loss -2.671275 avg loss no lamb -2.671275 time 2020-06-25 22:08:21.117899
Model ind 665 epoch 174 batch: 300 avg loss -2.742120 avg loss no lamb -2.742120 time 2020-06-25 22:08:31.573908
Model ind 665 epoch 174 batch: 400 avg loss -2.728937 avg loss no lamb -2.728937 time 2020-06-25 22:08:42.137799
Model ind 665 epoch 174 batch: 500 avg loss -2.824620 avg loss no lamb -2.824620 time 2020-06-25 22:08:52.771996
Model ind 665 epoch 174 batch: 600 avg loss -2.816928 avg loss no lamb -2.816928 time 2020-06-25 22:09:03.451041
Model ind 665 epoch 174 batch: 700 avg loss -2.548047 avg loss no lamb -2.548047 time 2020-06-25 22:09:14.191705
Model ind 665 epoch 174 batch: 800 avg loss -2.649792 avg loss no lamb -2.649792 time 2020-06-25 22:09:24.833195
last batch sz 10
Pre: time 2020-06-25 22:09:38.643321: 
 	std: 0.0036809645
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9833, 0.9827, 0.9746, 0.9832, 0.9768]
	train_accs: [0.9830667, 0.98233336, 0.9755167, 0.9831333, 0.97735]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.98012
	best: 0.9832

Starting e_i: 175
Model ind 665 epoch 175 batch: 0 avg loss -2.842518 avg loss no lamb -2.842518 time 2020-06-25 22:09:39.352712
Model ind 665 epoch 175 batch: 100 avg loss -2.817080 avg loss no lamb -2.817080 time 2020-06-25 22:09:49.940638
Model ind 665 epoch 175 batch: 200 avg loss -2.718568 avg loss no lamb -2.718568 time 2020-06-25 22:10:00.630929
Model ind 665 epoch 175 batch: 300 avg loss -2.784764 avg loss no lamb -2.784764 time 2020-06-25 22:10:11.446763
Model ind 665 epoch 175 batch: 400 avg loss -2.666367 avg loss no lamb -2.666367 time 2020-06-25 22:10:22.165671
Model ind 665 epoch 175 batch: 500 avg loss -2.682075 avg loss no lamb -2.682075 time 2020-06-25 22:10:32.562833
Model ind 665 epoch 175 batch: 600 avg loss -2.768641 avg loss no lamb -2.768641 time 2020-06-25 22:10:43.023517
Model ind 665 epoch 175 batch: 700 avg loss -2.604178 avg loss no lamb -2.604178 time 2020-06-25 22:10:53.864826
Model ind 665 epoch 175 batch: 800 avg loss -2.733776 avg loss no lamb -2.733776 time 2020-06-25 22:11:04.453748
last batch sz 10
Pre: time 2020-06-25 22:11:18.131406: 
 	std: 0.002813241
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9832, 0.981, 0.9761, 0.983, 0.9779]
	train_accs: [0.98285, 0.98148334, 0.97675, 0.98293334, 0.97783333]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.98024005
	best: 0.983

Starting e_i: 176
Model ind 665 epoch 176 batch: 0 avg loss -2.927826 avg loss no lamb -2.927826 time 2020-06-25 22:11:18.936364
Model ind 665 epoch 176 batch: 100 avg loss -2.767288 avg loss no lamb -2.767288 time 2020-06-25 22:11:29.700093
Model ind 665 epoch 176 batch: 200 avg loss -2.768864 avg loss no lamb -2.768864 time 2020-06-25 22:11:40.494065
Model ind 665 epoch 176 batch: 300 avg loss -2.755585 avg loss no lamb -2.755585 time 2020-06-25 22:11:51.309279
Model ind 665 epoch 176 batch: 400 avg loss -2.667464 avg loss no lamb -2.667464 time 2020-06-25 22:12:01.972820
Model ind 665 epoch 176 batch: 500 avg loss -2.614084 avg loss no lamb -2.614084 time 2020-06-25 22:12:12.641822
Model ind 665 epoch 176 batch: 600 avg loss -2.825869 avg loss no lamb -2.825869 time 2020-06-25 22:12:23.362814
Model ind 665 epoch 176 batch: 700 avg loss -2.635064 avg loss no lamb -2.635064 time 2020-06-25 22:12:34.199217
Model ind 665 epoch 176 batch: 800 avg loss -2.744125 avg loss no lamb -2.744125 time 2020-06-25 22:12:44.846666
last batch sz 10
Pre: time 2020-06-25 22:12:58.643020: 
 	std: 0.0039560464
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9838, 0.983, 0.9749, 0.9841, 0.9764]
	train_accs: [0.9834667, 0.9819, 0.9755833, 0.98338336, 0.977]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.98043996
	best: 0.9838

Starting e_i: 177
Model ind 665 epoch 177 batch: 0 avg loss -2.853491 avg loss no lamb -2.853491 time 2020-06-25 22:12:59.448383
Model ind 665 epoch 177 batch: 100 avg loss -2.704260 avg loss no lamb -2.704260 time 2020-06-25 22:13:10.029533
Model ind 665 epoch 177 batch: 200 avg loss -2.777428 avg loss no lamb -2.777428 time 2020-06-25 22:13:20.839968
Model ind 665 epoch 177 batch: 300 avg loss -2.798440 avg loss no lamb -2.798440 time 2020-06-25 22:13:31.580624
Model ind 665 epoch 177 batch: 400 avg loss -2.624630 avg loss no lamb -2.624630 time 2020-06-25 22:13:42.117354
Model ind 665 epoch 177 batch: 500 avg loss -2.706191 avg loss no lamb -2.706191 time 2020-06-25 22:13:52.906083
Model ind 665 epoch 177 batch: 600 avg loss -2.756686 avg loss no lamb -2.756686 time 2020-06-25 22:14:03.629025
Model ind 665 epoch 177 batch: 700 avg loss -2.694452 avg loss no lamb -2.694452 time 2020-06-25 22:14:14.270468
Model ind 665 epoch 177 batch: 800 avg loss -2.757557 avg loss no lamb -2.757557 time 2020-06-25 22:14:24.978276
last batch sz 10
Pre: time 2020-06-25 22:14:38.855859: 
 	std: 0.0030042715
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9796, 0.9744, 0.9818, 0.9757]
	train_accs: [0.98293334, 0.9809167, 0.9755167, 0.98305, 0.97751665]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97858
	best: 0.9818

Starting e_i: 178
Model ind 665 epoch 178 batch: 0 avg loss -2.911443 avg loss no lamb -2.911443 time 2020-06-25 22:14:39.646123
Model ind 665 epoch 178 batch: 100 avg loss -2.652020 avg loss no lamb -2.652020 time 2020-06-25 22:14:50.155418
Model ind 665 epoch 178 batch: 200 avg loss -2.780198 avg loss no lamb -2.780198 time 2020-06-25 22:15:01.140172
Model ind 665 epoch 178 batch: 300 avg loss -2.799825 avg loss no lamb -2.799825 time 2020-06-25 22:15:11.542645
Model ind 665 epoch 178 batch: 400 avg loss -2.561131 avg loss no lamb -2.561131 time 2020-06-25 22:15:22.148107
Model ind 665 epoch 178 batch: 500 avg loss -2.756963 avg loss no lamb -2.756963 time 2020-06-25 22:15:32.678834
Model ind 665 epoch 178 batch: 600 avg loss -2.833248 avg loss no lamb -2.833248 time 2020-06-25 22:15:43.368652
Model ind 665 epoch 178 batch: 700 avg loss -2.609559 avg loss no lamb -2.609559 time 2020-06-25 22:15:54.198096
Model ind 665 epoch 178 batch: 800 avg loss -2.712337 avg loss no lamb -2.712337 time 2020-06-25 22:16:04.945331
last batch sz 10
Pre: time 2020-06-25 22:16:18.768709: 
 	std: 0.003511926
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9814, 0.9748, 0.9818, 0.9749]
	train_accs: [0.9823667, 0.98113334, 0.9762167, 0.9819667, 0.97651666]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.9791201
	best: 0.9827

Starting e_i: 179
Model ind 665 epoch 179 batch: 0 avg loss -2.931908 avg loss no lamb -2.931908 time 2020-06-25 22:16:19.562556
Model ind 665 epoch 179 batch: 100 avg loss -2.783684 avg loss no lamb -2.783684 time 2020-06-25 22:16:29.962964
Model ind 665 epoch 179 batch: 200 avg loss -2.813002 avg loss no lamb -2.813002 time 2020-06-25 22:16:40.599255
Model ind 665 epoch 179 batch: 300 avg loss -2.715214 avg loss no lamb -2.715214 time 2020-06-25 22:16:51.130094
Model ind 665 epoch 179 batch: 400 avg loss -2.691250 avg loss no lamb -2.691250 time 2020-06-25 22:17:01.992004
Model ind 665 epoch 179 batch: 500 avg loss -2.708833 avg loss no lamb -2.708833 time 2020-06-25 22:17:12.713309
Model ind 665 epoch 179 batch: 600 avg loss -2.766611 avg loss no lamb -2.766611 time 2020-06-25 22:17:23.735745
Model ind 665 epoch 179 batch: 700 avg loss -2.599545 avg loss no lamb -2.599545 time 2020-06-25 22:17:34.400310
Model ind 665 epoch 179 batch: 800 avg loss -2.777183 avg loss no lamb -2.777183 time 2020-06-25 22:17:45.031770
last batch sz 10
Pre: time 2020-06-25 22:17:58.661328: 
 	std: 0.0028287156
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9812, 0.9751, 0.9818, 0.977]
	train_accs: [0.98223335, 0.98111665, 0.9757, 0.98215, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97942
	best: 0.982

Starting e_i: 180
Model ind 665 epoch 180 batch: 0 avg loss -2.912966 avg loss no lamb -2.912966 time 2020-06-25 22:17:59.432332
Model ind 665 epoch 180 batch: 100 avg loss -2.782852 avg loss no lamb -2.782852 time 2020-06-25 22:18:10.086308
Model ind 665 epoch 180 batch: 200 avg loss -2.762094 avg loss no lamb -2.762094 time 2020-06-25 22:18:20.663937
Model ind 665 epoch 180 batch: 300 avg loss -2.758504 avg loss no lamb -2.758504 time 2020-06-25 22:18:31.467661
Model ind 665 epoch 180 batch: 400 avg loss -2.668009 avg loss no lamb -2.668009 time 2020-06-25 22:18:42.261328
Model ind 665 epoch 180 batch: 500 avg loss -2.710109 avg loss no lamb -2.710109 time 2020-06-25 22:18:53.088635
Model ind 665 epoch 180 batch: 600 avg loss -2.862005 avg loss no lamb -2.862005 time 2020-06-25 22:19:04.032593
Model ind 665 epoch 180 batch: 700 avg loss -2.637498 avg loss no lamb -2.637498 time 2020-06-25 22:19:14.111643
Model ind 665 epoch 180 batch: 800 avg loss -2.652725 avg loss no lamb -2.652725 time 2020-06-25 22:19:24.888939
last batch sz 10
Pre: time 2020-06-25 22:19:38.760394: 
 	std: 0.0036362603
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9845, 0.9838, 0.9767, 0.9837, 0.9765]
	train_accs: [0.9830833, 0.9828167, 0.97713333, 0.9829, 0.97783333]
	best_train_sub_head: 0
	worst: 0.9765
	avg: 0.98104
	best: 0.9845

Starting e_i: 181
Model ind 665 epoch 181 batch: 0 avg loss -2.837342 avg loss no lamb -2.837342 time 2020-06-25 22:19:40.608579
Model ind 665 epoch 181 batch: 100 avg loss -2.724419 avg loss no lamb -2.724419 time 2020-06-25 22:19:51.218005
Model ind 665 epoch 181 batch: 200 avg loss -2.703408 avg loss no lamb -2.703408 time 2020-06-25 22:20:01.878366
Model ind 665 epoch 181 batch: 300 avg loss -2.796679 avg loss no lamb -2.796679 time 2020-06-25 22:20:12.456182
Model ind 665 epoch 181 batch: 400 avg loss -2.646281 avg loss no lamb -2.646281 time 2020-06-25 22:20:23.150430
Model ind 665 epoch 181 batch: 500 avg loss -2.772583 avg loss no lamb -2.772583 time 2020-06-25 22:20:33.756870
Model ind 665 epoch 181 batch: 600 avg loss -2.803765 avg loss no lamb -2.803765 time 2020-06-25 22:20:44.284049
Model ind 665 epoch 181 batch: 700 avg loss -2.675745 avg loss no lamb -2.675745 time 2020-06-25 22:20:54.736459
Model ind 665 epoch 181 batch: 800 avg loss -2.724611 avg loss no lamb -2.724611 time 2020-06-25 22:21:05.469700
last batch sz 10
Pre: time 2020-06-25 22:21:19.321648: 
 	std: 0.0020023934
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9815, 0.9775, 0.9817, 0.9781]
	train_accs: [0.98221666, 0.9813667, 0.97748333, 0.9821, 0.97826666]
	best_train_sub_head: 0
	worst: 0.9775
	avg: 0.98021996
	best: 0.9823

Starting e_i: 182
Model ind 665 epoch 182 batch: 0 avg loss -2.885751 avg loss no lamb -2.885751 time 2020-06-25 22:21:20.047401
Model ind 665 epoch 182 batch: 100 avg loss -2.806847 avg loss no lamb -2.806847 time 2020-06-25 22:21:31.195262
Model ind 665 epoch 182 batch: 200 avg loss -2.764839 avg loss no lamb -2.764839 time 2020-06-25 22:21:41.897815
Model ind 665 epoch 182 batch: 300 avg loss -2.794273 avg loss no lamb -2.794273 time 2020-06-25 22:21:52.379341
Model ind 665 epoch 182 batch: 400 avg loss -2.652431 avg loss no lamb -2.652431 time 2020-06-25 22:22:03.298156
Model ind 665 epoch 182 batch: 500 avg loss -2.682214 avg loss no lamb -2.682214 time 2020-06-25 22:22:13.967192
Model ind 665 epoch 182 batch: 600 avg loss -2.819692 avg loss no lamb -2.819692 time 2020-06-25 22:22:24.655546
Model ind 665 epoch 182 batch: 700 avg loss -2.587709 avg loss no lamb -2.587709 time 2020-06-25 22:22:35.354877
Model ind 665 epoch 182 batch: 800 avg loss -2.720589 avg loss no lamb -2.720589 time 2020-06-25 22:22:45.969094
last batch sz 10
Pre: time 2020-06-25 22:22:59.892058: 
 	std: 0.0030155522
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9834, 0.9834, 0.977, 0.9839, 0.9779]
	train_accs: [0.98321664, 0.98235, 0.9768, 0.98336667, 0.97753334]
	best_train_sub_head: 3
	worst: 0.977
	avg: 0.98112
	best: 0.9839

Starting e_i: 183
Model ind 665 epoch 183 batch: 0 avg loss -2.857263 avg loss no lamb -2.857263 time 2020-06-25 22:23:00.677696
Model ind 665 epoch 183 batch: 100 avg loss -2.786891 avg loss no lamb -2.786891 time 2020-06-25 22:23:11.336847
Model ind 665 epoch 183 batch: 200 avg loss -2.715416 avg loss no lamb -2.715416 time 2020-06-25 22:23:21.919263
Model ind 665 epoch 183 batch: 300 avg loss -2.769043 avg loss no lamb -2.769043 time 2020-06-25 22:23:32.537440
Model ind 665 epoch 183 batch: 400 avg loss -2.730850 avg loss no lamb -2.730850 time 2020-06-25 22:23:43.049251
Model ind 665 epoch 183 batch: 500 avg loss -2.757342 avg loss no lamb -2.757342 time 2020-06-25 22:23:53.660952
Model ind 665 epoch 183 batch: 600 avg loss -2.772511 avg loss no lamb -2.772511 time 2020-06-25 22:24:04.352531
Model ind 665 epoch 183 batch: 700 avg loss -2.611518 avg loss no lamb -2.611518 time 2020-06-25 22:24:14.714534
Model ind 665 epoch 183 batch: 800 avg loss -2.722341 avg loss no lamb -2.722341 time 2020-06-25 22:24:25.179941
last batch sz 10
Pre: time 2020-06-25 22:24:38.853734: 
 	std: 0.0021996368
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9815, 0.9771, 0.9821, 0.9779]
	train_accs: [0.98215, 0.9816667, 0.97726667, 0.98226666, 0.9780167]
	best_train_sub_head: 3
	worst: 0.9771
	avg: 0.98016006
	best: 0.9821

Starting e_i: 184
Model ind 665 epoch 184 batch: 0 avg loss -2.821900 avg loss no lamb -2.821900 time 2020-06-25 22:24:39.623099
Model ind 665 epoch 184 batch: 100 avg loss -2.748730 avg loss no lamb -2.748730 time 2020-06-25 22:24:50.243851
Model ind 665 epoch 184 batch: 200 avg loss -2.719983 avg loss no lamb -2.719983 time 2020-06-25 22:25:00.947382
Model ind 665 epoch 184 batch: 300 avg loss -2.695035 avg loss no lamb -2.695035 time 2020-06-25 22:25:11.628746
Model ind 665 epoch 184 batch: 400 avg loss -2.627306 avg loss no lamb -2.627306 time 2020-06-25 22:25:22.563895
Model ind 665 epoch 184 batch: 500 avg loss -2.757505 avg loss no lamb -2.757505 time 2020-06-25 22:25:33.221656
Model ind 665 epoch 184 batch: 600 avg loss -2.755209 avg loss no lamb -2.755209 time 2020-06-25 22:25:43.965615
Model ind 665 epoch 184 batch: 700 avg loss -2.628805 avg loss no lamb -2.628805 time 2020-06-25 22:25:55.196650
Model ind 665 epoch 184 batch: 800 avg loss -2.763830 avg loss no lamb -2.763830 time 2020-06-25 22:26:06.068623
last batch sz 10
Pre: time 2020-06-25 22:26:19.951232: 
 	std: 0.0020248513
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.98, 0.9764, 0.9813, 0.9772]
	train_accs: [0.9823667, 0.9813, 0.97756666, 0.98253334, 0.9784833]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.9792
	best: 0.9813

Starting e_i: 185
Model ind 665 epoch 185 batch: 0 avg loss -2.890279 avg loss no lamb -2.890279 time 2020-06-25 22:26:20.701533
Model ind 665 epoch 185 batch: 100 avg loss -2.783922 avg loss no lamb -2.783922 time 2020-06-25 22:26:31.142663
Model ind 665 epoch 185 batch: 200 avg loss -2.780815 avg loss no lamb -2.780815 time 2020-06-25 22:26:41.787103
Model ind 665 epoch 185 batch: 300 avg loss -2.768927 avg loss no lamb -2.768927 time 2020-06-25 22:26:52.539640
Model ind 665 epoch 185 batch: 400 avg loss -2.668292 avg loss no lamb -2.668292 time 2020-06-25 22:27:03.530757
Model ind 665 epoch 185 batch: 500 avg loss -2.714460 avg loss no lamb -2.714460 time 2020-06-25 22:27:14.380220
Model ind 665 epoch 185 batch: 600 avg loss -2.772323 avg loss no lamb -2.772323 time 2020-06-25 22:27:25.030781
Model ind 665 epoch 185 batch: 700 avg loss -2.593224 avg loss no lamb -2.593224 time 2020-06-25 22:27:35.634269
Model ind 665 epoch 185 batch: 800 avg loss -2.731075 avg loss no lamb -2.731075 time 2020-06-25 22:27:46.220778
last batch sz 10
Pre: time 2020-06-25 22:27:59.987074: 
 	std: 0.0034742437
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9813, 0.9745, 0.9821, 0.9749]
	train_accs: [0.9823, 0.98151666, 0.9756167, 0.9823833, 0.9759667]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97894
	best: 0.9821

Starting e_i: 186
Model ind 665 epoch 186 batch: 0 avg loss -2.859290 avg loss no lamb -2.859290 time 2020-06-25 22:28:00.785931
Model ind 665 epoch 186 batch: 100 avg loss -2.804567 avg loss no lamb -2.804567 time 2020-06-25 22:28:11.550798
Model ind 665 epoch 186 batch: 200 avg loss -2.735025 avg loss no lamb -2.735025 time 2020-06-25 22:28:22.095372
Model ind 665 epoch 186 batch: 300 avg loss -2.812186 avg loss no lamb -2.812186 time 2020-06-25 22:28:32.758289
Model ind 665 epoch 186 batch: 400 avg loss -2.668143 avg loss no lamb -2.668143 time 2020-06-25 22:28:43.348930
Model ind 665 epoch 186 batch: 500 avg loss -2.704573 avg loss no lamb -2.704573 time 2020-06-25 22:28:54.080452
Model ind 665 epoch 186 batch: 600 avg loss -2.792644 avg loss no lamb -2.792644 time 2020-06-25 22:29:04.937389
Model ind 665 epoch 186 batch: 700 avg loss -2.599967 avg loss no lamb -2.599967 time 2020-06-25 22:29:15.837113
Model ind 665 epoch 186 batch: 800 avg loss -2.716961 avg loss no lamb -2.716961 time 2020-06-25 22:29:26.476290
last batch sz 10
Pre: time 2020-06-25 22:29:40.342598: 
 	std: 0.0024806315
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9832, 0.9778, 0.9833, 0.9782]
	train_accs: [0.98246664, 0.9820667, 0.9773833, 0.9827167, 0.97815]
	best_train_sub_head: 3
	worst: 0.9778
	avg: 0.9810201
	best: 0.9833

Starting e_i: 187
Model ind 665 epoch 187 batch: 0 avg loss -2.798837 avg loss no lamb -2.798837 time 2020-06-25 22:29:41.074643
Model ind 665 epoch 187 batch: 100 avg loss -2.829241 avg loss no lamb -2.829241 time 2020-06-25 22:29:51.533602
Model ind 665 epoch 187 batch: 200 avg loss -2.757192 avg loss no lamb -2.757192 time 2020-06-25 22:30:02.349592
Model ind 665 epoch 187 batch: 300 avg loss -2.783635 avg loss no lamb -2.783635 time 2020-06-25 22:30:13.039399
Model ind 665 epoch 187 batch: 400 avg loss -2.607188 avg loss no lamb -2.607188 time 2020-06-25 22:30:23.603818
Model ind 665 epoch 187 batch: 500 avg loss -2.753518 avg loss no lamb -2.753518 time 2020-06-25 22:30:34.161980
Model ind 665 epoch 187 batch: 600 avg loss -2.750097 avg loss no lamb -2.750097 time 2020-06-25 22:30:44.652574
Model ind 665 epoch 187 batch: 700 avg loss -2.594074 avg loss no lamb -2.594074 time 2020-06-25 22:30:55.317511
Model ind 665 epoch 187 batch: 800 avg loss -2.740069 avg loss no lamb -2.740069 time 2020-06-25 22:31:06.363324
last batch sz 10
Pre: time 2020-06-25 22:31:20.545117: 
 	std: 0.0036558872
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9804, 0.9732, 0.9809, 0.9725]
	train_accs: [0.98123336, 0.9815, 0.97566664, 0.9821, 0.97536665]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97728
	best: 0.9809

Starting e_i: 188
Model ind 665 epoch 188 batch: 0 avg loss -2.888693 avg loss no lamb -2.888693 time 2020-06-25 22:31:21.279664
Model ind 665 epoch 188 batch: 100 avg loss -2.774664 avg loss no lamb -2.774664 time 2020-06-25 22:31:32.055027
Model ind 665 epoch 188 batch: 200 avg loss -2.735838 avg loss no lamb -2.735838 time 2020-06-25 22:31:42.480481
Model ind 665 epoch 188 batch: 300 avg loss -2.769313 avg loss no lamb -2.769313 time 2020-06-25 22:31:53.229368
Model ind 665 epoch 188 batch: 400 avg loss -2.658899 avg loss no lamb -2.658899 time 2020-06-25 22:32:04.122613
Model ind 665 epoch 188 batch: 500 avg loss -2.680252 avg loss no lamb -2.680252 time 2020-06-25 22:32:14.843392
Model ind 665 epoch 188 batch: 600 avg loss -2.807678 avg loss no lamb -2.807678 time 2020-06-25 22:32:25.540037
Model ind 665 epoch 188 batch: 700 avg loss -2.620828 avg loss no lamb -2.620828 time 2020-06-25 22:32:36.149087
Model ind 665 epoch 188 batch: 800 avg loss -2.639037 avg loss no lamb -2.639037 time 2020-06-25 22:32:46.666772
last batch sz 10
Pre: time 2020-06-25 22:33:00.575963: 
 	std: 0.003354157
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9796, 0.9739, 0.9804, 0.9725]
	train_accs: [0.98155, 0.98035, 0.97505, 0.98146665, 0.9755167]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97726
	best: 0.9799

Starting e_i: 189
Model ind 665 epoch 189 batch: 0 avg loss -2.852544 avg loss no lamb -2.852544 time 2020-06-25 22:33:01.402529
Model ind 665 epoch 189 batch: 100 avg loss -2.743864 avg loss no lamb -2.743864 time 2020-06-25 22:33:12.192505
Model ind 665 epoch 189 batch: 200 avg loss -2.623738 avg loss no lamb -2.623738 time 2020-06-25 22:33:22.685690
Model ind 665 epoch 189 batch: 300 avg loss -2.778095 avg loss no lamb -2.778095 time 2020-06-25 22:33:33.544191
Model ind 665 epoch 189 batch: 400 avg loss -2.633618 avg loss no lamb -2.633618 time 2020-06-25 22:33:44.235472
Model ind 665 epoch 189 batch: 500 avg loss -2.687145 avg loss no lamb -2.687145 time 2020-06-25 22:33:54.942560
Model ind 665 epoch 189 batch: 600 avg loss -2.760979 avg loss no lamb -2.760979 time 2020-06-25 22:34:05.617490
Model ind 665 epoch 189 batch: 700 avg loss -2.664108 avg loss no lamb -2.664108 time 2020-06-25 22:34:16.174129
Model ind 665 epoch 189 batch: 800 avg loss -2.746621 avg loss no lamb -2.746621 time 2020-06-25 22:34:26.799546
last batch sz 10
Pre: time 2020-06-25 22:34:40.658170: 
 	std: 0.0033849066
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9835, 0.983, 0.976, 0.9836, 0.977]
	train_accs: [0.98326665, 0.9822, 0.97711664, 0.98338336, 0.9780333]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.98062
	best: 0.9836

Starting e_i: 190
Model ind 665 epoch 190 batch: 0 avg loss -2.846983 avg loss no lamb -2.846983 time 2020-06-25 22:34:41.387521
Model ind 665 epoch 190 batch: 100 avg loss -2.776840 avg loss no lamb -2.776840 time 2020-06-25 22:34:52.241385
Model ind 665 epoch 190 batch: 200 avg loss -2.743879 avg loss no lamb -2.743879 time 2020-06-25 22:35:02.903242
Model ind 665 epoch 190 batch: 300 avg loss -2.711693 avg loss no lamb -2.711693 time 2020-06-25 22:35:13.682316
Model ind 665 epoch 190 batch: 400 avg loss -2.673966 avg loss no lamb -2.673966 time 2020-06-25 22:35:24.283234
Model ind 665 epoch 190 batch: 500 avg loss -2.687614 avg loss no lamb -2.687614 time 2020-06-25 22:35:35.005443
Model ind 665 epoch 190 batch: 600 avg loss -2.821651 avg loss no lamb -2.821651 time 2020-06-25 22:35:45.819176
Model ind 665 epoch 190 batch: 700 avg loss -2.612561 avg loss no lamb -2.612561 time 2020-06-25 22:35:56.564824
Model ind 665 epoch 190 batch: 800 avg loss -2.709830 avg loss no lamb -2.709830 time 2020-06-25 22:36:07.387191
last batch sz 10
Pre: time 2020-06-25 22:36:21.101585: 
 	std: 0.0025499663
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9812, 0.9757, 0.9814, 0.9768]
	train_accs: [0.9822, 0.98185, 0.97645, 0.98205, 0.97761667]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97933996
	best: 0.9816

Starting e_i: 191
Model ind 665 epoch 191 batch: 0 avg loss -2.944149 avg loss no lamb -2.944149 time 2020-06-25 22:36:22.966175
Model ind 665 epoch 191 batch: 100 avg loss -2.784731 avg loss no lamb -2.784731 time 2020-06-25 22:36:33.905356
Model ind 665 epoch 191 batch: 200 avg loss -2.725197 avg loss no lamb -2.725197 time 2020-06-25 22:36:44.767194
Model ind 665 epoch 191 batch: 300 avg loss -2.791241 avg loss no lamb -2.791241 time 2020-06-25 22:36:55.543098
Model ind 665 epoch 191 batch: 400 avg loss -2.702010 avg loss no lamb -2.702010 time 2020-06-25 22:37:06.120279
Model ind 665 epoch 191 batch: 500 avg loss -2.656511 avg loss no lamb -2.656511 time 2020-06-25 22:37:16.682652
Model ind 665 epoch 191 batch: 600 avg loss -2.775414 avg loss no lamb -2.775414 time 2020-06-25 22:37:27.549514
Model ind 665 epoch 191 batch: 700 avg loss -2.559561 avg loss no lamb -2.559561 time 2020-06-25 22:37:38.322139
Model ind 665 epoch 191 batch: 800 avg loss -2.772985 avg loss no lamb -2.772985 time 2020-06-25 22:37:49.064388
last batch sz 10
Pre: time 2020-06-25 22:38:02.864089: 
 	std: 0.0037349218
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9824, 0.9747, 0.9821, 0.9748]
	train_accs: [0.98261666, 0.982, 0.9762667, 0.98225, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97931993
	best: 0.9826

Starting e_i: 192
Model ind 665 epoch 192 batch: 0 avg loss -2.873837 avg loss no lamb -2.873837 time 2020-06-25 22:38:03.571272
Model ind 665 epoch 192 batch: 100 avg loss -2.711962 avg loss no lamb -2.711962 time 2020-06-25 22:38:14.219703
Model ind 665 epoch 192 batch: 200 avg loss -2.714961 avg loss no lamb -2.714961 time 2020-06-25 22:38:25.051587
Model ind 665 epoch 192 batch: 300 avg loss -2.788275 avg loss no lamb -2.788275 time 2020-06-25 22:38:35.701522
Model ind 665 epoch 192 batch: 400 avg loss -2.662786 avg loss no lamb -2.662786 time 2020-06-25 22:38:46.366112
Model ind 665 epoch 192 batch: 500 avg loss -2.800696 avg loss no lamb -2.800696 time 2020-06-25 22:38:57.163673
Model ind 665 epoch 192 batch: 600 avg loss -2.767711 avg loss no lamb -2.767711 time 2020-06-25 22:39:07.886063
Model ind 665 epoch 192 batch: 700 avg loss -2.549114 avg loss no lamb -2.549114 time 2020-06-25 22:39:18.758570
Model ind 665 epoch 192 batch: 800 avg loss -2.662632 avg loss no lamb -2.662632 time 2020-06-25 22:39:29.369286
last batch sz 10
Pre: time 2020-06-25 22:39:42.991194: 
 	std: 0.0039283517
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9827, 0.9833, 0.974, 0.9823, 0.9757]
	train_accs: [0.98195, 0.98221666, 0.9759667, 0.9820333, 0.97683334]
	best_train_sub_head: 1
	worst: 0.974
	avg: 0.9796001
	best: 0.9833

Starting e_i: 193
Model ind 665 epoch 193 batch: 0 avg loss -2.885992 avg loss no lamb -2.885992 time 2020-06-25 22:39:43.800643
Model ind 665 epoch 193 batch: 100 avg loss -2.808521 avg loss no lamb -2.808521 time 2020-06-25 22:39:54.427205
Model ind 665 epoch 193 batch: 200 avg loss -2.752590 avg loss no lamb -2.752590 time 2020-06-25 22:40:05.292020
Model ind 665 epoch 193 batch: 300 avg loss -2.709827 avg loss no lamb -2.709827 time 2020-06-25 22:40:15.939886
Model ind 665 epoch 193 batch: 400 avg loss -2.623500 avg loss no lamb -2.623500 time 2020-06-25 22:40:26.835540
Model ind 665 epoch 193 batch: 500 avg loss -2.719997 avg loss no lamb -2.719997 time 2020-06-25 22:40:37.753766
Model ind 665 epoch 193 batch: 600 avg loss -2.830229 avg loss no lamb -2.830229 time 2020-06-25 22:40:48.460179
Model ind 665 epoch 193 batch: 700 avg loss -2.615555 avg loss no lamb -2.615555 time 2020-06-25 22:40:59.167014
Model ind 665 epoch 193 batch: 800 avg loss -2.728173 avg loss no lamb -2.728173 time 2020-06-25 22:41:09.929823
last batch sz 10
Pre: time 2020-06-25 22:41:23.613248: 
 	std: 0.0027952925
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9829, 0.977, 0.9829, 0.9774]
	train_accs: [0.98331666, 0.98323333, 0.9778, 0.98315, 0.9784333]
	best_train_sub_head: 0
	worst: 0.977
	avg: 0.98062
	best: 0.9829

Starting e_i: 194
Model ind 665 epoch 194 batch: 0 avg loss -2.881251 avg loss no lamb -2.881251 time 2020-06-25 22:41:24.357510
Model ind 665 epoch 194 batch: 100 avg loss -2.741342 avg loss no lamb -2.741342 time 2020-06-25 22:41:34.980660
Model ind 665 epoch 194 batch: 200 avg loss -2.734338 avg loss no lamb -2.734338 time 2020-06-25 22:41:45.742676
Model ind 665 epoch 194 batch: 300 avg loss -2.792137 avg loss no lamb -2.792137 time 2020-06-25 22:41:56.494602
Model ind 665 epoch 194 batch: 400 avg loss -2.665050 avg loss no lamb -2.665050 time 2020-06-25 22:42:07.304213
Model ind 665 epoch 194 batch: 500 avg loss -2.664959 avg loss no lamb -2.664959 time 2020-06-25 22:42:17.966283
Model ind 665 epoch 194 batch: 600 avg loss -2.716626 avg loss no lamb -2.716626 time 2020-06-25 22:42:28.475397
Model ind 665 epoch 194 batch: 700 avg loss -2.602041 avg loss no lamb -2.602041 time 2020-06-25 22:42:39.372879
Model ind 665 epoch 194 batch: 800 avg loss -2.717485 avg loss no lamb -2.717485 time 2020-06-25 22:42:50.179554
last batch sz 10
Pre: time 2020-06-25 22:43:04.073952: 
 	std: 0.003427298
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9817, 0.9745, 0.983, 0.9762]
	train_accs: [0.9827833, 0.98193336, 0.9763333, 0.98288333, 0.9774333]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97945994
	best: 0.983

Starting e_i: 195
Model ind 665 epoch 195 batch: 0 avg loss -2.908187 avg loss no lamb -2.908187 time 2020-06-25 22:43:04.882342
Model ind 665 epoch 195 batch: 100 avg loss -2.727649 avg loss no lamb -2.727649 time 2020-06-25 22:43:15.530697
Model ind 665 epoch 195 batch: 200 avg loss -2.752277 avg loss no lamb -2.752277 time 2020-06-25 22:43:26.135698
Model ind 665 epoch 195 batch: 300 avg loss -2.773326 avg loss no lamb -2.773326 time 2020-06-25 22:43:36.847005
Model ind 665 epoch 195 batch: 400 avg loss -2.696221 avg loss no lamb -2.696221 time 2020-06-25 22:43:47.780445
Model ind 665 epoch 195 batch: 500 avg loss -2.738146 avg loss no lamb -2.738146 time 2020-06-25 22:43:58.368381
Model ind 665 epoch 195 batch: 600 avg loss -2.753592 avg loss no lamb -2.753592 time 2020-06-25 22:44:09.156063
Model ind 665 epoch 195 batch: 700 avg loss -2.572504 avg loss no lamb -2.572504 time 2020-06-25 22:44:19.999400
Model ind 665 epoch 195 batch: 800 avg loss -2.664062 avg loss no lamb -2.664062 time 2020-06-25 22:44:30.696636
last batch sz 10
Pre: time 2020-06-25 22:44:44.504415: 
 	std: 0.0022946892
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9811, 0.9766, 0.9803, 0.9757]
	train_accs: [0.9821333, 0.98151666, 0.97725, 0.98186666, 0.9769833]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97892
	best: 0.9809

Starting e_i: 196
Model ind 665 epoch 196 batch: 0 avg loss -2.801222 avg loss no lamb -2.801222 time 2020-06-25 22:44:45.293978
Model ind 665 epoch 196 batch: 100 avg loss -2.751332 avg loss no lamb -2.751332 time 2020-06-25 22:44:56.136154
Model ind 665 epoch 196 batch: 200 avg loss -2.763992 avg loss no lamb -2.763992 time 2020-06-25 22:45:06.965468
Model ind 665 epoch 196 batch: 300 avg loss -2.679737 avg loss no lamb -2.679737 time 2020-06-25 22:45:17.764830
Model ind 665 epoch 196 batch: 400 avg loss -2.677541 avg loss no lamb -2.677541 time 2020-06-25 22:45:28.463156
Model ind 665 epoch 196 batch: 500 avg loss -2.801156 avg loss no lamb -2.801156 time 2020-06-25 22:45:39.221590
Model ind 665 epoch 196 batch: 600 avg loss -2.833827 avg loss no lamb -2.833827 time 2020-06-25 22:45:49.887013
Model ind 665 epoch 196 batch: 700 avg loss -2.666183 avg loss no lamb -2.666183 time 2020-06-25 22:46:00.769961
Model ind 665 epoch 196 batch: 800 avg loss -2.675694 avg loss no lamb -2.675694 time 2020-06-25 22:46:11.559997
last batch sz 10
Pre: time 2020-06-25 22:46:25.092619: 
 	std: 0.0028677545
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9832, 0.9837, 0.9766, 0.9832, 0.9788]
	train_accs: [0.98333335, 0.98295, 0.97748333, 0.98285, 0.9789]
	best_train_sub_head: 0
	worst: 0.9766
	avg: 0.98109996
	best: 0.9832

Starting e_i: 197
Model ind 665 epoch 197 batch: 0 avg loss -2.890834 avg loss no lamb -2.890834 time 2020-06-25 22:46:25.838873
Model ind 665 epoch 197 batch: 100 avg loss -2.711849 avg loss no lamb -2.711849 time 2020-06-25 22:46:36.434118
Model ind 665 epoch 197 batch: 200 avg loss -2.724843 avg loss no lamb -2.724843 time 2020-06-25 22:46:46.998016
Model ind 665 epoch 197 batch: 300 avg loss -2.785139 avg loss no lamb -2.785139 time 2020-06-25 22:46:57.842340
Model ind 665 epoch 197 batch: 400 avg loss -2.674547 avg loss no lamb -2.674547 time 2020-06-25 22:47:08.738556
Model ind 665 epoch 197 batch: 500 avg loss -2.676026 avg loss no lamb -2.676026 time 2020-06-25 22:47:19.501426
Model ind 665 epoch 197 batch: 600 avg loss -2.778764 avg loss no lamb -2.778764 time 2020-06-25 22:47:30.281850
Model ind 665 epoch 197 batch: 700 avg loss -2.592334 avg loss no lamb -2.592334 time 2020-06-25 22:47:41.034532
Model ind 665 epoch 197 batch: 800 avg loss -2.749612 avg loss no lamb -2.749612 time 2020-06-25 22:47:51.911303
last batch sz 10
Pre: time 2020-06-25 22:48:05.805595: 
 	std: 0.0029092873
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9802, 0.9756, 0.9816, 0.9746]
	train_accs: [0.98233336, 0.98146665, 0.97671664, 0.9825, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.9786
	best: 0.9816

Starting e_i: 198
Model ind 665 epoch 198 batch: 0 avg loss -2.870629 avg loss no lamb -2.870629 time 2020-06-25 22:48:06.564612
Model ind 665 epoch 198 batch: 100 avg loss -2.786469 avg loss no lamb -2.786469 time 2020-06-25 22:48:17.375588
Model ind 665 epoch 198 batch: 200 avg loss -2.784747 avg loss no lamb -2.784747 time 2020-06-25 22:48:28.080727
Model ind 665 epoch 198 batch: 300 avg loss -2.794041 avg loss no lamb -2.794041 time 2020-06-25 22:48:38.755951
Model ind 665 epoch 198 batch: 400 avg loss -2.783885 avg loss no lamb -2.783885 time 2020-06-25 22:48:49.577682
Model ind 665 epoch 198 batch: 500 avg loss -2.654591 avg loss no lamb -2.654591 time 2020-06-25 22:49:00.474241
Model ind 665 epoch 198 batch: 600 avg loss -2.755414 avg loss no lamb -2.755414 time 2020-06-25 22:49:11.170458
Model ind 665 epoch 198 batch: 700 avg loss -2.686167 avg loss no lamb -2.686167 time 2020-06-25 22:49:22.161202
Model ind 665 epoch 198 batch: 800 avg loss -2.662990 avg loss no lamb -2.662990 time 2020-06-25 22:49:32.910544
last batch sz 10
Pre: time 2020-06-25 22:49:46.690766: 
 	std: 0.00408265
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9806, 0.9728, 0.9821, 0.9737]
	train_accs: [0.9819, 0.98105, 0.97431666, 0.98225, 0.97643334]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97819996
	best: 0.9821

Starting e_i: 199
Model ind 665 epoch 199 batch: 0 avg loss -2.907543 avg loss no lamb -2.907543 time 2020-06-25 22:49:47.437009
Model ind 665 epoch 199 batch: 100 avg loss -2.697741 avg loss no lamb -2.697741 time 2020-06-25 22:49:58.073300
Model ind 665 epoch 199 batch: 200 avg loss -2.784027 avg loss no lamb -2.784027 time 2020-06-25 22:50:08.988757
Model ind 665 epoch 199 batch: 300 avg loss -2.811698 avg loss no lamb -2.811698 time 2020-06-25 22:50:19.992062
Model ind 665 epoch 199 batch: 400 avg loss -2.602229 avg loss no lamb -2.602229 time 2020-06-25 22:50:30.781193
Model ind 665 epoch 199 batch: 500 avg loss -2.682628 avg loss no lamb -2.682628 time 2020-06-25 22:50:41.524070
Model ind 665 epoch 199 batch: 600 avg loss -2.803541 avg loss no lamb -2.803541 time 2020-06-25 22:50:52.161606
Model ind 665 epoch 199 batch: 700 avg loss -2.511128 avg loss no lamb -2.511128 time 2020-06-25 22:51:02.981130
Model ind 665 epoch 199 batch: 800 avg loss -2.703763 avg loss no lamb -2.703763 time 2020-06-25 22:51:13.863813
last batch sz 10
Pre: time 2020-06-25 22:51:27.617675: 
 	std: 0.0030637365
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9808, 0.9747, 0.982, 0.9765]
	train_accs: [0.98225, 0.98113334, 0.9758667, 0.98225, 0.9777333]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97924006
	best: 0.9822

Starting e_i: 200
Model ind 665 epoch 200 batch: 0 avg loss -2.907153 avg loss no lamb -2.907153 time 2020-06-25 22:51:28.403133
Model ind 665 epoch 200 batch: 100 avg loss -2.788249 avg loss no lamb -2.788249 time 2020-06-25 22:51:39.005428
Model ind 665 epoch 200 batch: 200 avg loss -2.787312 avg loss no lamb -2.787312 time 2020-06-25 22:51:49.547383
Model ind 665 epoch 200 batch: 300 avg loss -2.756427 avg loss no lamb -2.756427 time 2020-06-25 22:52:00.223443
Model ind 665 epoch 200 batch: 400 avg loss -2.671144 avg loss no lamb -2.671144 time 2020-06-25 22:52:10.720376
Model ind 665 epoch 200 batch: 500 avg loss -2.706114 avg loss no lamb -2.706114 time 2020-06-25 22:52:21.321985
Model ind 665 epoch 200 batch: 600 avg loss -2.800114 avg loss no lamb -2.800114 time 2020-06-25 22:52:32.038832
Model ind 665 epoch 200 batch: 700 avg loss -2.701387 avg loss no lamb -2.701387 time 2020-06-25 22:52:42.879204
Model ind 665 epoch 200 batch: 800 avg loss -2.770740 avg loss no lamb -2.770740 time 2020-06-25 22:52:53.647665
last batch sz 10
Pre: time 2020-06-25 22:53:07.736797: 
 	std: 0.0036080012
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9802, 0.9737, 0.9817, 0.9737]
	train_accs: [0.9812667, 0.9806833, 0.9743, 0.98165, 0.97535]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97808
	best: 0.9817

Starting e_i: 201
Model ind 665 epoch 201 batch: 0 avg loss -2.846264 avg loss no lamb -2.846264 time 2020-06-25 22:53:09.596188
Model ind 665 epoch 201 batch: 100 avg loss -2.830777 avg loss no lamb -2.830777 time 2020-06-25 22:53:20.062063
Model ind 665 epoch 201 batch: 200 avg loss -2.789059 avg loss no lamb -2.789059 time 2020-06-25 22:53:30.833120
Model ind 665 epoch 201 batch: 300 avg loss -2.748659 avg loss no lamb -2.748659 time 2020-06-25 22:53:41.547771
Model ind 665 epoch 201 batch: 400 avg loss -2.636659 avg loss no lamb -2.636659 time 2020-06-25 22:53:52.251392
Model ind 665 epoch 201 batch: 500 avg loss -2.663602 avg loss no lamb -2.663602 time 2020-06-25 22:54:02.886836
Model ind 665 epoch 201 batch: 600 avg loss -2.789927 avg loss no lamb -2.789927 time 2020-06-25 22:54:13.496440
Model ind 665 epoch 201 batch: 700 avg loss -2.640969 avg loss no lamb -2.640969 time 2020-06-25 22:54:24.343302
Model ind 665 epoch 201 batch: 800 avg loss -2.781082 avg loss no lamb -2.781082 time 2020-06-25 22:54:34.951446
last batch sz 10
Pre: time 2020-06-25 22:54:48.731098: 
 	std: 0.0033913865
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9838, 0.983, 0.9767, 0.9841, 0.9768]
	train_accs: [0.98303336, 0.9823, 0.9770667, 0.9831333, 0.9776833]
	best_train_sub_head: 3
	worst: 0.9767
	avg: 0.98087996
	best: 0.9841

Starting e_i: 202
Model ind 665 epoch 202 batch: 0 avg loss -2.879458 avg loss no lamb -2.879458 time 2020-06-25 22:54:49.477310
Model ind 665 epoch 202 batch: 100 avg loss -2.792327 avg loss no lamb -2.792327 time 2020-06-25 22:55:00.360522
Model ind 665 epoch 202 batch: 200 avg loss -2.767858 avg loss no lamb -2.767858 time 2020-06-25 22:55:10.979948
Model ind 665 epoch 202 batch: 300 avg loss -2.776483 avg loss no lamb -2.776483 time 2020-06-25 22:55:21.734778
Model ind 665 epoch 202 batch: 400 avg loss -2.603493 avg loss no lamb -2.603493 time 2020-06-25 22:55:32.334033
Model ind 665 epoch 202 batch: 500 avg loss -2.722388 avg loss no lamb -2.722388 time 2020-06-25 22:55:43.162132
Model ind 665 epoch 202 batch: 600 avg loss -2.727171 avg loss no lamb -2.727171 time 2020-06-25 22:55:54.054547
Model ind 665 epoch 202 batch: 700 avg loss -2.672189 avg loss no lamb -2.672189 time 2020-06-25 22:56:04.702669
Model ind 665 epoch 202 batch: 800 avg loss -2.740906 avg loss no lamb -2.740906 time 2020-06-25 22:56:15.248565
last batch sz 10
Pre: time 2020-06-25 22:56:29.109860: 
 	std: 0.0027528899
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9804, 0.9768, 0.9825, 0.9763]
	train_accs: [0.98263335, 0.98055, 0.9768, 0.9827, 0.9768]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.97976
	best: 0.9825

Starting e_i: 203
Model ind 665 epoch 203 batch: 0 avg loss -2.870037 avg loss no lamb -2.870037 time 2020-06-25 22:56:29.886226
Model ind 665 epoch 203 batch: 100 avg loss -2.835246 avg loss no lamb -2.835246 time 2020-06-25 22:56:40.638129
Model ind 665 epoch 203 batch: 200 avg loss -2.758022 avg loss no lamb -2.758022 time 2020-06-25 22:56:51.186511
Model ind 665 epoch 203 batch: 300 avg loss -2.831837 avg loss no lamb -2.831837 time 2020-06-25 22:57:01.995283
Model ind 665 epoch 203 batch: 400 avg loss -2.683203 avg loss no lamb -2.683203 time 2020-06-25 22:57:12.700095
Model ind 665 epoch 203 batch: 500 avg loss -2.731763 avg loss no lamb -2.731763 time 2020-06-25 22:57:23.438519
Model ind 665 epoch 203 batch: 600 avg loss -2.826024 avg loss no lamb -2.826024 time 2020-06-25 22:57:34.172136
Model ind 665 epoch 203 batch: 700 avg loss -2.642495 avg loss no lamb -2.642495 time 2020-06-25 22:57:44.786004
Model ind 665 epoch 203 batch: 800 avg loss -2.682225 avg loss no lamb -2.682225 time 2020-06-25 22:57:55.334230
last batch sz 10
Pre: time 2020-06-25 22:58:09.400414: 
 	std: 0.0029006298
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9808, 0.9765, 0.9823, 0.9754]
	train_accs: [0.98195, 0.98105, 0.9762833, 0.9817167, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97942
	best: 0.9821

Starting e_i: 204
Model ind 665 epoch 204 batch: 0 avg loss -2.843045 avg loss no lamb -2.843045 time 2020-06-25 22:58:10.142363
Model ind 665 epoch 204 batch: 100 avg loss -2.762285 avg loss no lamb -2.762285 time 2020-06-25 22:58:20.876635
Model ind 665 epoch 204 batch: 200 avg loss -2.737550 avg loss no lamb -2.737550 time 2020-06-25 22:58:31.600661
Model ind 665 epoch 204 batch: 300 avg loss -2.677118 avg loss no lamb -2.677118 time 2020-06-25 22:58:42.253754
Model ind 665 epoch 204 batch: 400 avg loss -2.674573 avg loss no lamb -2.674573 time 2020-06-25 22:58:53.017186
Model ind 665 epoch 204 batch: 500 avg loss -2.735317 avg loss no lamb -2.735317 time 2020-06-25 22:59:03.834080
Model ind 665 epoch 204 batch: 600 avg loss -2.770098 avg loss no lamb -2.770098 time 2020-06-25 22:59:14.632935
Model ind 665 epoch 204 batch: 700 avg loss -2.625000 avg loss no lamb -2.625000 time 2020-06-25 22:59:25.416540
Model ind 665 epoch 204 batch: 800 avg loss -2.756711 avg loss no lamb -2.756711 time 2020-06-25 22:59:36.133315
last batch sz 10
Pre: time 2020-06-25 22:59:49.886309: 
 	std: 0.0032102296
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.983, 0.9824, 0.9762, 0.9831, 0.9764]
	train_accs: [0.9828167, 0.98226666, 0.9764, 0.9828333, 0.9770833]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.98021996
	best: 0.9831

Starting e_i: 205
Model ind 665 epoch 205 batch: 0 avg loss -2.884075 avg loss no lamb -2.884075 time 2020-06-25 22:59:50.608441
Model ind 665 epoch 205 batch: 100 avg loss -2.838289 avg loss no lamb -2.838289 time 2020-06-25 23:00:01.441810
Model ind 665 epoch 205 batch: 200 avg loss -2.817128 avg loss no lamb -2.817128 time 2020-06-25 23:00:12.338610
Model ind 665 epoch 205 batch: 300 avg loss -2.826472 avg loss no lamb -2.826472 time 2020-06-25 23:00:23.052162
Model ind 665 epoch 205 batch: 400 avg loss -2.712133 avg loss no lamb -2.712133 time 2020-06-25 23:00:33.724028
Model ind 665 epoch 205 batch: 500 avg loss -2.718972 avg loss no lamb -2.718972 time 2020-06-25 23:00:44.532214
Model ind 665 epoch 205 batch: 600 avg loss -2.822829 avg loss no lamb -2.822829 time 2020-06-25 23:00:55.097450
Model ind 665 epoch 205 batch: 700 avg loss -2.599495 avg loss no lamb -2.599495 time 2020-06-25 23:01:05.728344
Model ind 665 epoch 205 batch: 800 avg loss -2.763795 avg loss no lamb -2.763795 time 2020-06-25 23:01:16.365247
last batch sz 10
Pre: time 2020-06-25 23:01:29.896968: 
 	std: 0.0027694078
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9829, 0.9764, 0.9829, 0.9777]
	train_accs: [0.9826, 0.9821, 0.97646666, 0.98285, 0.9779]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.98038006
	best: 0.9829

Starting e_i: 206
Model ind 665 epoch 206 batch: 0 avg loss -2.838508 avg loss no lamb -2.838508 time 2020-06-25 23:01:30.700791
Model ind 665 epoch 206 batch: 100 avg loss -2.712405 avg loss no lamb -2.712405 time 2020-06-25 23:01:41.255206
Model ind 665 epoch 206 batch: 200 avg loss -2.703964 avg loss no lamb -2.703964 time 2020-06-25 23:01:51.951612
Model ind 665 epoch 206 batch: 300 avg loss -2.803924 avg loss no lamb -2.803924 time 2020-06-25 23:02:02.712807
Model ind 665 epoch 206 batch: 400 avg loss -2.698859 avg loss no lamb -2.698859 time 2020-06-25 23:02:13.308992
Model ind 665 epoch 206 batch: 500 avg loss -2.691047 avg loss no lamb -2.691047 time 2020-06-25 23:02:24.129061
Model ind 665 epoch 206 batch: 600 avg loss -2.770441 avg loss no lamb -2.770441 time 2020-06-25 23:02:34.705298
Model ind 665 epoch 206 batch: 700 avg loss -2.667084 avg loss no lamb -2.667084 time 2020-06-25 23:02:45.483071
Model ind 665 epoch 206 batch: 800 avg loss -2.782281 avg loss no lamb -2.782281 time 2020-06-25 23:02:56.070295
last batch sz 10
Pre: time 2020-06-25 23:03:10.017355: 
 	std: 0.0029735018
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9822, 0.9762, 0.9828, 0.9765]
	train_accs: [0.98255, 0.98216665, 0.9768, 0.9829, 0.97711664]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.97998
	best: 0.9828

Starting e_i: 207
Model ind 665 epoch 207 batch: 0 avg loss -2.879074 avg loss no lamb -2.879074 time 2020-06-25 23:03:10.843592
Model ind 665 epoch 207 batch: 100 avg loss -2.777011 avg loss no lamb -2.777011 time 2020-06-25 23:03:21.683632
Model ind 665 epoch 207 batch: 200 avg loss -2.699610 avg loss no lamb -2.699610 time 2020-06-25 23:03:32.511297
Model ind 665 epoch 207 batch: 300 avg loss -2.789906 avg loss no lamb -2.789906 time 2020-06-25 23:03:43.088030
Model ind 665 epoch 207 batch: 400 avg loss -2.685603 avg loss no lamb -2.685603 time 2020-06-25 23:03:53.868890
Model ind 665 epoch 207 batch: 500 avg loss -2.726542 avg loss no lamb -2.726542 time 2020-06-25 23:04:04.540481
Model ind 665 epoch 207 batch: 600 avg loss -2.803564 avg loss no lamb -2.803564 time 2020-06-25 23:04:15.318572
Model ind 665 epoch 207 batch: 700 avg loss -2.562567 avg loss no lamb -2.562567 time 2020-06-25 23:04:26.155614
Model ind 665 epoch 207 batch: 800 avg loss -2.693911 avg loss no lamb -2.693911 time 2020-06-25 23:04:37.115497
last batch sz 10
Pre: time 2020-06-25 23:04:51.056175: 
 	std: 0.0021675844
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9807, 0.9769, 0.9821, 0.9774]
	train_accs: [0.98286664, 0.98158336, 0.97788334, 0.9826, 0.9788]
	best_train_sub_head: 0
	worst: 0.9769
	avg: 0.97973996
	best: 0.9816

Starting e_i: 208
Model ind 665 epoch 208 batch: 0 avg loss -2.905069 avg loss no lamb -2.905069 time 2020-06-25 23:04:51.798189
Model ind 665 epoch 208 batch: 100 avg loss -2.753664 avg loss no lamb -2.753664 time 2020-06-25 23:05:02.581201
Model ind 665 epoch 208 batch: 200 avg loss -2.748191 avg loss no lamb -2.748191 time 2020-06-25 23:05:13.182479
Model ind 665 epoch 208 batch: 300 avg loss -2.723592 avg loss no lamb -2.723592 time 2020-06-25 23:05:23.683318
Model ind 665 epoch 208 batch: 400 avg loss -2.690608 avg loss no lamb -2.690608 time 2020-06-25 23:05:34.339697
Model ind 665 epoch 208 batch: 500 avg loss -2.747500 avg loss no lamb -2.747500 time 2020-06-25 23:05:45.083539
Model ind 665 epoch 208 batch: 600 avg loss -2.800992 avg loss no lamb -2.800992 time 2020-06-25 23:05:55.967701
Model ind 665 epoch 208 batch: 700 avg loss -2.624537 avg loss no lamb -2.624537 time 2020-06-25 23:06:06.682636
Model ind 665 epoch 208 batch: 800 avg loss -2.740653 avg loss no lamb -2.740653 time 2020-06-25 23:06:17.255223
last batch sz 10
Pre: time 2020-06-25 23:06:31.213563: 
 	std: 0.003427875
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9817, 0.9757, 0.9821, 0.9743]
	train_accs: [0.9821, 0.98153335, 0.97636664, 0.9821, 0.9761]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97916
	best: 0.982

Starting e_i: 209
Model ind 665 epoch 209 batch: 0 avg loss -2.902206 avg loss no lamb -2.902206 time 2020-06-25 23:06:31.994028
Model ind 665 epoch 209 batch: 100 avg loss -2.756472 avg loss no lamb -2.756472 time 2020-06-25 23:06:42.949253
Model ind 665 epoch 209 batch: 200 avg loss -2.760152 avg loss no lamb -2.760152 time 2020-06-25 23:06:53.518616
Model ind 665 epoch 209 batch: 300 avg loss -2.702477 avg loss no lamb -2.702477 time 2020-06-25 23:07:04.084932
Model ind 665 epoch 209 batch: 400 avg loss -2.680756 avg loss no lamb -2.680756 time 2020-06-25 23:07:14.764110
Model ind 665 epoch 209 batch: 500 avg loss -2.726181 avg loss no lamb -2.726181 time 2020-06-25 23:07:25.518270
Model ind 665 epoch 209 batch: 600 avg loss -2.743352 avg loss no lamb -2.743352 time 2020-06-25 23:07:36.141663
Model ind 665 epoch 209 batch: 700 avg loss -2.563046 avg loss no lamb -2.563046 time 2020-06-25 23:07:46.949726
Model ind 665 epoch 209 batch: 800 avg loss -2.767696 avg loss no lamb -2.767696 time 2020-06-25 23:07:57.495584
last batch sz 10
Pre: time 2020-06-25 23:08:11.072884: 
 	std: 0.0029993374
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9827, 0.982, 0.9767, 0.9829, 0.9762]
	train_accs: [0.98191667, 0.9816667, 0.9766, 0.982, 0.9766833]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.98010004
	best: 0.9829

Starting e_i: 210
Model ind 665 epoch 210 batch: 0 avg loss -2.836976 avg loss no lamb -2.836976 time 2020-06-25 23:08:11.865020
Model ind 665 epoch 210 batch: 100 avg loss -2.815190 avg loss no lamb -2.815190 time 2020-06-25 23:08:22.440822
Model ind 665 epoch 210 batch: 200 avg loss -2.788190 avg loss no lamb -2.788190 time 2020-06-25 23:08:33.240209
Model ind 665 epoch 210 batch: 300 avg loss -2.822002 avg loss no lamb -2.822002 time 2020-06-25 23:08:44.247221
Model ind 665 epoch 210 batch: 400 avg loss -2.680055 avg loss no lamb -2.680055 time 2020-06-25 23:08:54.798426
Model ind 665 epoch 210 batch: 500 avg loss -2.678858 avg loss no lamb -2.678858 time 2020-06-25 23:09:05.488548
Model ind 665 epoch 210 batch: 600 avg loss -2.767791 avg loss no lamb -2.767791 time 2020-06-25 23:09:16.340424
Model ind 665 epoch 210 batch: 700 avg loss -2.717037 avg loss no lamb -2.717037 time 2020-06-25 23:09:27.148408
Model ind 665 epoch 210 batch: 800 avg loss -2.729148 avg loss no lamb -2.729148 time 2020-06-25 23:09:37.780560
last batch sz 10
Pre: time 2020-06-25 23:09:51.664904: 
 	std: 0.002479193
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9818, 0.9777, 0.9831, 0.9774]
	train_accs: [0.98223335, 0.98158336, 0.9765667, 0.9821, 0.9770167]
	best_train_sub_head: 0
	worst: 0.9774
	avg: 0.98054
	best: 0.9827

Starting e_i: 211
Model ind 665 epoch 211 batch: 0 avg loss -2.962198 avg loss no lamb -2.962198 time 2020-06-25 23:09:53.580096
Model ind 665 epoch 211 batch: 100 avg loss -2.758189 avg loss no lamb -2.758189 time 2020-06-25 23:10:04.472060
Model ind 665 epoch 211 batch: 200 avg loss -2.771397 avg loss no lamb -2.771397 time 2020-06-25 23:10:15.205191
Model ind 665 epoch 211 batch: 300 avg loss -2.736494 avg loss no lamb -2.736494 time 2020-06-25 23:10:26.036907
Model ind 665 epoch 211 batch: 400 avg loss -2.699203 avg loss no lamb -2.699203 time 2020-06-25 23:10:36.770861
Model ind 665 epoch 211 batch: 500 avg loss -2.684975 avg loss no lamb -2.684975 time 2020-06-25 23:10:47.687239
Model ind 665 epoch 211 batch: 600 avg loss -2.853038 avg loss no lamb -2.853038 time 2020-06-25 23:10:58.309059
Model ind 665 epoch 211 batch: 700 avg loss -2.681426 avg loss no lamb -2.681426 time 2020-06-25 23:11:08.958218
Model ind 665 epoch 211 batch: 800 avg loss -2.717914 avg loss no lamb -2.717914 time 2020-06-25 23:11:19.559751
last batch sz 10
Pre: time 2020-06-25 23:11:33.196275: 
 	std: 0.003483908
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9829, 0.9746, 0.9831, 0.9773]
	train_accs: [0.98261666, 0.98178333, 0.9752, 0.98246664, 0.97715]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.98008
	best: 0.9825

Starting e_i: 212
Model ind 665 epoch 212 batch: 0 avg loss -2.873701 avg loss no lamb -2.873701 time 2020-06-25 23:11:33.907493
Model ind 665 epoch 212 batch: 100 avg loss -2.807354 avg loss no lamb -2.807354 time 2020-06-25 23:11:44.722868
Model ind 665 epoch 212 batch: 200 avg loss -2.740223 avg loss no lamb -2.740223 time 2020-06-25 23:11:55.388728
Model ind 665 epoch 212 batch: 300 avg loss -2.787804 avg loss no lamb -2.787804 time 2020-06-25 23:12:05.991313
Model ind 665 epoch 212 batch: 400 avg loss -2.695067 avg loss no lamb -2.695067 time 2020-06-25 23:12:16.587737
Model ind 665 epoch 212 batch: 500 avg loss -2.711225 avg loss no lamb -2.711225 time 2020-06-25 23:12:27.184174
Model ind 665 epoch 212 batch: 600 avg loss -2.807443 avg loss no lamb -2.807443 time 2020-06-25 23:12:37.950187
Model ind 665 epoch 212 batch: 700 avg loss -2.649708 avg loss no lamb -2.649708 time 2020-06-25 23:12:48.866478
Model ind 665 epoch 212 batch: 800 avg loss -2.691982 avg loss no lamb -2.691982 time 2020-06-25 23:12:59.477971
last batch sz 10
Pre: time 2020-06-25 23:13:13.098485: 
 	std: 0.0029020049
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9816, 0.9756, 0.9821, 0.9765]
	train_accs: [0.98188335, 0.9810167, 0.9761, 0.98176664, 0.9770167]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97958004
	best: 0.9821

Starting e_i: 213
Model ind 665 epoch 213 batch: 0 avg loss -2.902106 avg loss no lamb -2.902106 time 2020-06-25 23:13:13.874848
Model ind 665 epoch 213 batch: 100 avg loss -2.717986 avg loss no lamb -2.717986 time 2020-06-25 23:13:24.513155
Model ind 665 epoch 213 batch: 200 avg loss -2.745903 avg loss no lamb -2.745903 time 2020-06-25 23:13:35.230537
Model ind 665 epoch 213 batch: 300 avg loss -2.770683 avg loss no lamb -2.770683 time 2020-06-25 23:13:46.097603
Model ind 665 epoch 213 batch: 400 avg loss -2.709228 avg loss no lamb -2.709228 time 2020-06-25 23:13:56.792782
Model ind 665 epoch 213 batch: 500 avg loss -2.712311 avg loss no lamb -2.712311 time 2020-06-25 23:14:07.451917
Model ind 665 epoch 213 batch: 600 avg loss -2.804963 avg loss no lamb -2.804963 time 2020-06-25 23:14:18.423077
Model ind 665 epoch 213 batch: 700 avg loss -2.558304 avg loss no lamb -2.558304 time 2020-06-25 23:14:29.088101
Model ind 665 epoch 213 batch: 800 avg loss -2.728562 avg loss no lamb -2.728562 time 2020-06-25 23:14:39.591167
last batch sz 10
Pre: time 2020-06-25 23:14:53.188574: 
 	std: 0.0023082553
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9802, 0.9762, 0.9828, 0.9784]
	train_accs: [0.98183334, 0.98053336, 0.97655, 0.98235, 0.9780167]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.97980005
	best: 0.9828

Starting e_i: 214
Model ind 665 epoch 214 batch: 0 avg loss -2.907263 avg loss no lamb -2.907263 time 2020-06-25 23:14:53.913394
Model ind 665 epoch 214 batch: 100 avg loss -2.762365 avg loss no lamb -2.762365 time 2020-06-25 23:15:04.917557
Model ind 665 epoch 214 batch: 200 avg loss -2.791356 avg loss no lamb -2.791356 time 2020-06-25 23:15:15.684143
Model ind 665 epoch 214 batch: 300 avg loss -2.765765 avg loss no lamb -2.765765 time 2020-06-25 23:15:26.237289
Model ind 665 epoch 214 batch: 400 avg loss -2.708181 avg loss no lamb -2.708181 time 2020-06-25 23:15:37.064221
Model ind 665 epoch 214 batch: 500 avg loss -2.679226 avg loss no lamb -2.679226 time 2020-06-25 23:15:47.930251
Model ind 665 epoch 214 batch: 600 avg loss -2.776838 avg loss no lamb -2.776838 time 2020-06-25 23:15:58.887126
Model ind 665 epoch 214 batch: 700 avg loss -2.568534 avg loss no lamb -2.568534 time 2020-06-25 23:16:09.548431
Model ind 665 epoch 214 batch: 800 avg loss -2.746079 avg loss no lamb -2.746079 time 2020-06-25 23:16:20.345656
last batch sz 10
Pre: time 2020-06-25 23:16:34.267327: 
 	std: 0.0037828088
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9831, 0.9827, 0.974, 0.9832, 0.9771]
	train_accs: [0.9829, 0.98193336, 0.97541666, 0.98303336, 0.97785]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.98002005
	best: 0.9832

Starting e_i: 215
Model ind 665 epoch 215 batch: 0 avg loss -2.909672 avg loss no lamb -2.909672 time 2020-06-25 23:16:35.003320
Model ind 665 epoch 215 batch: 100 avg loss -2.791078 avg loss no lamb -2.791078 time 2020-06-25 23:16:45.724476
Model ind 665 epoch 215 batch: 200 avg loss -2.725516 avg loss no lamb -2.725516 time 2020-06-25 23:16:56.316120
Model ind 665 epoch 215 batch: 300 avg loss -2.746429 avg loss no lamb -2.746429 time 2020-06-25 23:17:06.895128
Model ind 665 epoch 215 batch: 400 avg loss -2.677093 avg loss no lamb -2.677093 time 2020-06-25 23:17:17.820966
Model ind 665 epoch 215 batch: 500 avg loss -2.735623 avg loss no lamb -2.735623 time 2020-06-25 23:17:28.682168
Model ind 665 epoch 215 batch: 600 avg loss -2.817927 avg loss no lamb -2.817927 time 2020-06-25 23:17:39.560271
Model ind 665 epoch 215 batch: 700 avg loss -2.629763 avg loss no lamb -2.629763 time 2020-06-25 23:17:50.301078
Model ind 665 epoch 215 batch: 800 avg loss -2.735536 avg loss no lamb -2.735536 time 2020-06-25 23:18:01.156822
last batch sz 10
Pre: time 2020-06-25 23:18:15.042251: 
 	std: 0.0033534493
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9822, 0.9748, 0.9823, 0.976]
	train_accs: [0.98176664, 0.9816, 0.9751667, 0.9817333, 0.97665]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97947997
	best: 0.9821

Starting e_i: 216
Model ind 665 epoch 216 batch: 0 avg loss -2.849875 avg loss no lamb -2.849875 time 2020-06-25 23:18:15.790817
Model ind 665 epoch 216 batch: 100 avg loss -2.738050 avg loss no lamb -2.738050 time 2020-06-25 23:18:26.502614
Model ind 665 epoch 216 batch: 200 avg loss -2.773163 avg loss no lamb -2.773163 time 2020-06-25 23:18:37.252986
Model ind 665 epoch 216 batch: 300 avg loss -2.799627 avg loss no lamb -2.799627 time 2020-06-25 23:18:47.985246
Model ind 665 epoch 216 batch: 400 avg loss -2.690845 avg loss no lamb -2.690845 time 2020-06-25 23:18:58.797803
Model ind 665 epoch 216 batch: 500 avg loss -2.735049 avg loss no lamb -2.735049 time 2020-06-25 23:19:09.781614
Model ind 665 epoch 216 batch: 600 avg loss -2.792595 avg loss no lamb -2.792595 time 2020-06-25 23:19:20.564612
Model ind 665 epoch 216 batch: 700 avg loss -2.618945 avg loss no lamb -2.618945 time 2020-06-25 23:19:31.232634
Model ind 665 epoch 216 batch: 800 avg loss -2.777621 avg loss no lamb -2.777621 time 2020-06-25 23:19:42.038416
last batch sz 10
Pre: time 2020-06-25 23:19:55.991371: 
 	std: 0.00357043
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9845, 0.982, 0.9757, 0.9832, 0.9766]
	train_accs: [0.98286664, 0.9816667, 0.97655, 0.98261666, 0.97711664]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.9804001
	best: 0.9845

Starting e_i: 217
Model ind 665 epoch 217 batch: 0 avg loss -2.832939 avg loss no lamb -2.832939 time 2020-06-25 23:19:56.728376
Model ind 665 epoch 217 batch: 100 avg loss -2.767000 avg loss no lamb -2.767000 time 2020-06-25 23:20:07.512049
Model ind 665 epoch 217 batch: 200 avg loss -2.776210 avg loss no lamb -2.776210 time 2020-06-25 23:20:18.339358
Model ind 665 epoch 217 batch: 300 avg loss -2.850493 avg loss no lamb -2.850493 time 2020-06-25 23:20:29.085458
Model ind 665 epoch 217 batch: 400 avg loss -2.607244 avg loss no lamb -2.607244 time 2020-06-25 23:20:40.033532
Model ind 665 epoch 217 batch: 500 avg loss -2.715347 avg loss no lamb -2.715347 time 2020-06-25 23:20:50.951266
Model ind 665 epoch 217 batch: 600 avg loss -2.777248 avg loss no lamb -2.777248 time 2020-06-25 23:21:01.638056
Model ind 665 epoch 217 batch: 700 avg loss -2.640999 avg loss no lamb -2.640999 time 2020-06-25 23:21:12.086438
Model ind 665 epoch 217 batch: 800 avg loss -2.758572 avg loss no lamb -2.758572 time 2020-06-25 23:21:22.830240
last batch sz 10
Pre: time 2020-06-25 23:21:36.806126: 
 	std: 0.0028642486
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9849, 0.9838, 0.9782, 0.9848, 0.9793]
	train_accs: [0.9834167, 0.9824333, 0.97798336, 0.98335, 0.97851664]
	best_train_sub_head: 0
	worst: 0.9782
	avg: 0.98219997
	best: 0.9849

Starting e_i: 218
Model ind 665 epoch 218 batch: 0 avg loss -2.916821 avg loss no lamb -2.916821 time 2020-06-25 23:21:37.510185
Model ind 665 epoch 218 batch: 100 avg loss -2.784009 avg loss no lamb -2.784009 time 2020-06-25 23:21:48.077026
Model ind 665 epoch 218 batch: 200 avg loss -2.756769 avg loss no lamb -2.756769 time 2020-06-25 23:21:58.584295
Model ind 665 epoch 218 batch: 300 avg loss -2.823091 avg loss no lamb -2.823091 time 2020-06-25 23:22:09.479509
Model ind 665 epoch 218 batch: 400 avg loss -2.698283 avg loss no lamb -2.698283 time 2020-06-25 23:22:20.172395
Model ind 665 epoch 218 batch: 500 avg loss -2.758920 avg loss no lamb -2.758920 time 2020-06-25 23:22:30.684714
Model ind 665 epoch 218 batch: 600 avg loss -2.809153 avg loss no lamb -2.809153 time 2020-06-25 23:22:41.376477
Model ind 665 epoch 218 batch: 700 avg loss -2.574685 avg loss no lamb -2.574685 time 2020-06-25 23:22:51.994949
Model ind 665 epoch 218 batch: 800 avg loss -2.715719 avg loss no lamb -2.715719 time 2020-06-25 23:23:02.814843
last batch sz 10
Pre: time 2020-06-25 23:23:16.955287: 
 	std: 0.0031656972
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.983, 0.9827, 0.9754, 0.9827, 0.9776]
	train_accs: [0.9823667, 0.98186666, 0.9767333, 0.98216665, 0.97781664]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.98028004
	best: 0.983

Starting e_i: 219
Model ind 665 epoch 219 batch: 0 avg loss -2.919685 avg loss no lamb -2.919685 time 2020-06-25 23:23:17.705011
Model ind 665 epoch 219 batch: 100 avg loss -2.787205 avg loss no lamb -2.787205 time 2020-06-25 23:23:28.355654
Model ind 665 epoch 219 batch: 200 avg loss -2.829112 avg loss no lamb -2.829112 time 2020-06-25 23:23:39.157221
Model ind 665 epoch 219 batch: 300 avg loss -2.813638 avg loss no lamb -2.813638 time 2020-06-25 23:23:49.950220
Model ind 665 epoch 219 batch: 400 avg loss -2.665704 avg loss no lamb -2.665704 time 2020-06-25 23:24:00.718816
Model ind 665 epoch 219 batch: 500 avg loss -2.715253 avg loss no lamb -2.715253 time 2020-06-25 23:24:11.361890
Model ind 665 epoch 219 batch: 600 avg loss -2.773962 avg loss no lamb -2.773962 time 2020-06-25 23:24:22.005757
Model ind 665 epoch 219 batch: 700 avg loss -2.646874 avg loss no lamb -2.646874 time 2020-06-25 23:24:32.515686
Model ind 665 epoch 219 batch: 800 avg loss -2.743662 avg loss no lamb -2.743662 time 2020-06-25 23:24:43.028425
last batch sz 10
Pre: time 2020-06-25 23:24:56.814356: 
 	std: 0.0029479484
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9835, 0.9823, 0.9766, 0.9838, 0.9781]
	train_accs: [0.9827333, 0.9819833, 0.9766167, 0.98293334, 0.9774167]
	best_train_sub_head: 3
	worst: 0.9766
	avg: 0.98085994
	best: 0.9838

Starting e_i: 220
Model ind 665 epoch 220 batch: 0 avg loss -2.899217 avg loss no lamb -2.899217 time 2020-06-25 23:24:57.575589
Model ind 665 epoch 220 batch: 100 avg loss -2.747464 avg loss no lamb -2.747464 time 2020-06-25 23:25:08.545345
Model ind 665 epoch 220 batch: 200 avg loss -2.756855 avg loss no lamb -2.756855 time 2020-06-25 23:25:18.986767
Model ind 665 epoch 220 batch: 300 avg loss -2.803947 avg loss no lamb -2.803947 time 2020-06-25 23:25:29.443491
Model ind 665 epoch 220 batch: 400 avg loss -2.670031 avg loss no lamb -2.670031 time 2020-06-25 23:25:40.228276
Model ind 665 epoch 220 batch: 500 avg loss -2.704782 avg loss no lamb -2.704782 time 2020-06-25 23:25:51.041353
Model ind 665 epoch 220 batch: 600 avg loss -2.805628 avg loss no lamb -2.805628 time 2020-06-25 23:26:01.730988
Model ind 665 epoch 220 batch: 700 avg loss -2.602272 avg loss no lamb -2.602272 time 2020-06-25 23:26:12.451268
Model ind 665 epoch 220 batch: 800 avg loss -2.681998 avg loss no lamb -2.681998 time 2020-06-25 23:26:23.140630
last batch sz 10
Pre: time 2020-06-25 23:26:36.795603: 
 	std: 0.002651506
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9813, 0.9765, 0.9821, 0.9758]
	train_accs: [0.98183334, 0.98165, 0.97665, 0.98216665, 0.9765667]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.97936
	best: 0.9821

Starting e_i: 221
Model ind 665 epoch 221 batch: 0 avg loss -2.925879 avg loss no lamb -2.925879 time 2020-06-25 23:26:38.733166
Model ind 665 epoch 221 batch: 100 avg loss -2.761941 avg loss no lamb -2.761941 time 2020-06-25 23:26:49.472896
Model ind 665 epoch 221 batch: 200 avg loss -2.754888 avg loss no lamb -2.754888 time 2020-06-25 23:26:59.971199
Model ind 665 epoch 221 batch: 300 avg loss -2.812860 avg loss no lamb -2.812860 time 2020-06-25 23:27:10.664523
Model ind 665 epoch 221 batch: 400 avg loss -2.679009 avg loss no lamb -2.679009 time 2020-06-25 23:27:21.072100
Model ind 665 epoch 221 batch: 500 avg loss -2.711529 avg loss no lamb -2.711529 time 2020-06-25 23:27:31.682995
Model ind 665 epoch 221 batch: 600 avg loss -2.799791 avg loss no lamb -2.799791 time 2020-06-25 23:27:42.501778
Model ind 665 epoch 221 batch: 700 avg loss -2.673105 avg loss no lamb -2.673105 time 2020-06-25 23:27:53.283800
Model ind 665 epoch 221 batch: 800 avg loss -2.774643 avg loss no lamb -2.774643 time 2020-06-25 23:28:03.949068
last batch sz 10
Pre: time 2020-06-25 23:28:17.575406: 
 	std: 0.0030980015
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9824, 0.9755, 0.983, 0.9773]
	train_accs: [0.98255, 0.98158336, 0.9755, 0.9827333, 0.97756666]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.98012
	best: 0.983

Starting e_i: 222
Model ind 665 epoch 222 batch: 0 avg loss -2.939332 avg loss no lamb -2.939332 time 2020-06-25 23:28:18.303047
Model ind 665 epoch 222 batch: 100 avg loss -2.746180 avg loss no lamb -2.746180 time 2020-06-25 23:28:28.841472
Model ind 665 epoch 222 batch: 200 avg loss -2.701762 avg loss no lamb -2.701762 time 2020-06-25 23:28:39.510273
Model ind 665 epoch 222 batch: 300 avg loss -2.782661 avg loss no lamb -2.782661 time 2020-06-25 23:28:50.246485
Model ind 665 epoch 222 batch: 400 avg loss -2.654723 avg loss no lamb -2.654723 time 2020-06-25 23:29:00.816684
Model ind 665 epoch 222 batch: 500 avg loss -2.756495 avg loss no lamb -2.756495 time 2020-06-25 23:29:11.637716
Model ind 665 epoch 222 batch: 600 avg loss -2.772321 avg loss no lamb -2.772321 time 2020-06-25 23:29:22.348926
Model ind 665 epoch 222 batch: 700 avg loss -2.619920 avg loss no lamb -2.619920 time 2020-06-25 23:29:33.284940
Model ind 665 epoch 222 batch: 800 avg loss -2.713583 avg loss no lamb -2.713583 time 2020-06-25 23:29:44.192518
last batch sz 10
Pre: time 2020-06-25 23:29:57.981579: 
 	std: 0.0035952083
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9821, 0.9756, 0.9831, 0.975]
	train_accs: [0.982, 0.98145, 0.97545, 0.9820167, 0.97545]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97967994
	best: 0.9831

Starting e_i: 223
Model ind 665 epoch 223 batch: 0 avg loss -2.935770 avg loss no lamb -2.935770 time 2020-06-25 23:29:58.838293
Model ind 665 epoch 223 batch: 100 avg loss -2.787076 avg loss no lamb -2.787076 time 2020-06-25 23:30:09.544866
Model ind 665 epoch 223 batch: 200 avg loss -2.746660 avg loss no lamb -2.746660 time 2020-06-25 23:30:20.243624
Model ind 665 epoch 223 batch: 300 avg loss -2.777341 avg loss no lamb -2.777341 time 2020-06-25 23:30:30.880681
Model ind 665 epoch 223 batch: 400 avg loss -2.639122 avg loss no lamb -2.639122 time 2020-06-25 23:30:41.634072
Model ind 665 epoch 223 batch: 500 avg loss -2.677056 avg loss no lamb -2.677056 time 2020-06-25 23:30:52.237710
Model ind 665 epoch 223 batch: 600 avg loss -2.787741 avg loss no lamb -2.787741 time 2020-06-25 23:31:02.853722
Model ind 665 epoch 223 batch: 700 avg loss -2.609788 avg loss no lamb -2.609788 time 2020-06-25 23:31:13.539106
Model ind 665 epoch 223 batch: 800 avg loss -2.811903 avg loss no lamb -2.811903 time 2020-06-25 23:31:24.135907
last batch sz 10
Pre: time 2020-06-25 23:31:37.771347: 
 	std: 0.0031601188
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9818, 0.9746, 0.9821, 0.9761]
	train_accs: [0.98215, 0.98108333, 0.97463334, 0.9822, 0.97615]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97916
	best: 0.9821

Starting e_i: 224
Model ind 665 epoch 224 batch: 0 avg loss -2.899314 avg loss no lamb -2.899314 time 2020-06-25 23:31:38.538497
Model ind 665 epoch 224 batch: 100 avg loss -2.821957 avg loss no lamb -2.821957 time 2020-06-25 23:31:48.747626
Model ind 665 epoch 224 batch: 200 avg loss -2.728685 avg loss no lamb -2.728685 time 2020-06-25 23:31:59.165600
Model ind 665 epoch 224 batch: 300 avg loss -2.742379 avg loss no lamb -2.742379 time 2020-06-25 23:32:10.034603
Model ind 665 epoch 224 batch: 400 avg loss -2.680763 avg loss no lamb -2.680763 time 2020-06-25 23:32:20.655763
Model ind 665 epoch 224 batch: 500 avg loss -2.701637 avg loss no lamb -2.701637 time 2020-06-25 23:32:31.442841
Model ind 665 epoch 224 batch: 600 avg loss -2.794658 avg loss no lamb -2.794658 time 2020-06-25 23:32:41.967863
Model ind 665 epoch 224 batch: 700 avg loss -2.592407 avg loss no lamb -2.592407 time 2020-06-25 23:32:52.736072
Model ind 665 epoch 224 batch: 800 avg loss -2.832774 avg loss no lamb -2.832774 time 2020-06-25 23:33:03.560487
last batch sz 10
Pre: time 2020-06-25 23:33:17.313669: 
 	std: 0.0024758033
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9802, 0.9759, 0.9819, 0.9769]
	train_accs: [0.98233336, 0.98076665, 0.97608334, 0.98233336, 0.97723335]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.97931993
	best: 0.9817

Starting e_i: 225
Model ind 665 epoch 225 batch: 0 avg loss -2.868220 avg loss no lamb -2.868220 time 2020-06-25 23:33:18.066313
Model ind 665 epoch 225 batch: 100 avg loss -2.754245 avg loss no lamb -2.754245 time 2020-06-25 23:33:28.629121
Model ind 665 epoch 225 batch: 200 avg loss -2.749603 avg loss no lamb -2.749603 time 2020-06-25 23:33:39.150766
Model ind 665 epoch 225 batch: 300 avg loss -2.828036 avg loss no lamb -2.828036 time 2020-06-25 23:33:50.050220
Model ind 665 epoch 225 batch: 400 avg loss -2.675110 avg loss no lamb -2.675110 time 2020-06-25 23:34:00.735184
Model ind 665 epoch 225 batch: 500 avg loss -2.718558 avg loss no lamb -2.718558 time 2020-06-25 23:34:11.526165
Model ind 665 epoch 225 batch: 600 avg loss -2.828240 avg loss no lamb -2.828240 time 2020-06-25 23:34:21.980870
Model ind 665 epoch 225 batch: 700 avg loss -2.670321 avg loss no lamb -2.670321 time 2020-06-25 23:34:32.387404
Model ind 665 epoch 225 batch: 800 avg loss -2.768896 avg loss no lamb -2.768896 time 2020-06-25 23:34:43.031890
last batch sz 10
Pre: time 2020-06-25 23:34:56.797463: 
 	std: 0.0027141212
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9838, 0.9836, 0.978, 0.9847, 0.9792]
	train_accs: [0.9826, 0.98226666, 0.97725, 0.98305, 0.97828335]
	best_train_sub_head: 3
	worst: 0.978
	avg: 0.98186
	best: 0.9847

Starting e_i: 226
Model ind 665 epoch 226 batch: 0 avg loss -2.942809 avg loss no lamb -2.942809 time 2020-06-25 23:34:57.573910
Model ind 665 epoch 226 batch: 100 avg loss -2.735232 avg loss no lamb -2.735232 time 2020-06-25 23:35:08.348958
Model ind 665 epoch 226 batch: 200 avg loss -2.804923 avg loss no lamb -2.804923 time 2020-06-25 23:35:18.912813
Model ind 665 epoch 226 batch: 300 avg loss -2.726722 avg loss no lamb -2.726722 time 2020-06-25 23:35:29.834863
Model ind 665 epoch 226 batch: 400 avg loss -2.727384 avg loss no lamb -2.727384 time 2020-06-25 23:35:40.519105
Model ind 665 epoch 226 batch: 500 avg loss -2.737285 avg loss no lamb -2.737285 time 2020-06-25 23:35:51.261487
Model ind 665 epoch 226 batch: 600 avg loss -2.743209 avg loss no lamb -2.743209 time 2020-06-25 23:36:01.935184
Model ind 665 epoch 226 batch: 700 avg loss -2.584558 avg loss no lamb -2.584558 time 2020-06-25 23:36:12.455189
Model ind 665 epoch 226 batch: 800 avg loss -2.761752 avg loss no lamb -2.761752 time 2020-06-25 23:36:23.088190
last batch sz 10
Pre: time 2020-06-25 23:36:36.797688: 
 	std: 0.0030863422
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9813, 0.975, 0.9816, 0.9761]
	train_accs: [0.9820667, 0.9810333, 0.9752333, 0.9811, 0.97613335]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.9792801
	best: 0.9824

Starting e_i: 227
Model ind 665 epoch 227 batch: 0 avg loss -2.843231 avg loss no lamb -2.843231 time 2020-06-25 23:36:37.540493
Model ind 665 epoch 227 batch: 100 avg loss -2.793238 avg loss no lamb -2.793238 time 2020-06-25 23:36:48.404637
Model ind 665 epoch 227 batch: 200 avg loss -2.738720 avg loss no lamb -2.738720 time 2020-06-25 23:36:58.836709
Model ind 665 epoch 227 batch: 300 avg loss -2.778605 avg loss no lamb -2.778605 time 2020-06-25 23:37:09.514226
Model ind 665 epoch 227 batch: 400 avg loss -2.637743 avg loss no lamb -2.637743 time 2020-06-25 23:37:20.038879
Model ind 665 epoch 227 batch: 500 avg loss -2.721304 avg loss no lamb -2.721304 time 2020-06-25 23:37:30.754183
Model ind 665 epoch 227 batch: 600 avg loss -2.828805 avg loss no lamb -2.828805 time 2020-06-25 23:37:41.487343
Model ind 665 epoch 227 batch: 700 avg loss -2.680139 avg loss no lamb -2.680139 time 2020-06-25 23:37:52.172776
Model ind 665 epoch 227 batch: 800 avg loss -2.705432 avg loss no lamb -2.705432 time 2020-06-25 23:38:02.708519
last batch sz 10
Pre: time 2020-06-25 23:38:16.683039: 
 	std: 0.0026389416
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9839, 0.984, 0.9781, 0.983, 0.9785]
	train_accs: [0.98328334, 0.9831167, 0.97798336, 0.9831667, 0.97873336]
	best_train_sub_head: 0
	worst: 0.9781
	avg: 0.9815
	best: 0.9839

Starting e_i: 228
Model ind 665 epoch 228 batch: 0 avg loss -2.861489 avg loss no lamb -2.861489 time 2020-06-25 23:38:17.445694
Model ind 665 epoch 228 batch: 100 avg loss -2.779976 avg loss no lamb -2.779976 time 2020-06-25 23:38:28.103353
Model ind 665 epoch 228 batch: 200 avg loss -2.800117 avg loss no lamb -2.800117 time 2020-06-25 23:38:38.771880
Model ind 665 epoch 228 batch: 300 avg loss -2.771997 avg loss no lamb -2.771997 time 2020-06-25 23:38:49.425159
Model ind 665 epoch 228 batch: 400 avg loss -2.668065 avg loss no lamb -2.668065 time 2020-06-25 23:39:00.062223
Model ind 665 epoch 228 batch: 500 avg loss -2.703262 avg loss no lamb -2.703262 time 2020-06-25 23:39:10.687651
Model ind 665 epoch 228 batch: 600 avg loss -2.787196 avg loss no lamb -2.787196 time 2020-06-25 23:39:21.330783
Model ind 665 epoch 228 batch: 700 avg loss -2.642161 avg loss no lamb -2.642161 time 2020-06-25 23:39:32.077444
Model ind 665 epoch 228 batch: 800 avg loss -2.751909 avg loss no lamb -2.751909 time 2020-06-25 23:39:42.923857
last batch sz 10
Pre: time 2020-06-25 23:39:56.788722: 
 	std: 0.0038368816
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9797, 0.9721, 0.9791, 0.9715]
	train_accs: [0.9806833, 0.98, 0.97368336, 0.98035, 0.97316664]
	best_train_sub_head: 0
	worst: 0.9715
	avg: 0.97648
	best: 0.98

Starting e_i: 229
Model ind 665 epoch 229 batch: 0 avg loss -2.848577 avg loss no lamb -2.848577 time 2020-06-25 23:39:57.600963
Model ind 665 epoch 229 batch: 100 avg loss -2.786605 avg loss no lamb -2.786605 time 2020-06-25 23:40:08.079619
Model ind 665 epoch 229 batch: 200 avg loss -2.854304 avg loss no lamb -2.854304 time 2020-06-25 23:40:18.771232
Model ind 665 epoch 229 batch: 300 avg loss -2.778963 avg loss no lamb -2.778963 time 2020-06-25 23:40:29.652120
Model ind 665 epoch 229 batch: 400 avg loss -2.652775 avg loss no lamb -2.652775 time 2020-06-25 23:40:40.369948
Model ind 665 epoch 229 batch: 500 avg loss -2.703283 avg loss no lamb -2.703283 time 2020-06-25 23:40:50.847799
Model ind 665 epoch 229 batch: 600 avg loss -2.742852 avg loss no lamb -2.742852 time 2020-06-25 23:41:01.481087
Model ind 665 epoch 229 batch: 700 avg loss -2.514802 avg loss no lamb -2.514802 time 2020-06-25 23:41:12.187801
Model ind 665 epoch 229 batch: 800 avg loss -2.697664 avg loss no lamb -2.697664 time 2020-06-25 23:41:22.948054
last batch sz 10
Pre: time 2020-06-25 23:41:36.689449: 
 	std: 0.0028917897
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9803, 0.9814, 0.9748, 0.9804, 0.9749]
	train_accs: [0.98121667, 0.9813667, 0.9763, 0.98135, 0.9767]
	best_train_sub_head: 1
	worst: 0.9748
	avg: 0.97836
	best: 0.9814

Starting e_i: 230
Model ind 665 epoch 230 batch: 0 avg loss -2.889794 avg loss no lamb -2.889794 time 2020-06-25 23:41:37.435883
Model ind 665 epoch 230 batch: 100 avg loss -2.791650 avg loss no lamb -2.791650 time 2020-06-25 23:41:48.031333
Model ind 665 epoch 230 batch: 200 avg loss -2.725250 avg loss no lamb -2.725250 time 2020-06-25 23:41:58.899288
Model ind 665 epoch 230 batch: 300 avg loss -2.825845 avg loss no lamb -2.825845 time 2020-06-25 23:42:09.608429
Model ind 665 epoch 230 batch: 400 avg loss -2.647572 avg loss no lamb -2.647572 time 2020-06-25 23:42:20.422417
Model ind 665 epoch 230 batch: 500 avg loss -2.777838 avg loss no lamb -2.777838 time 2020-06-25 23:42:31.081536
Model ind 665 epoch 230 batch: 600 avg loss -2.780450 avg loss no lamb -2.780450 time 2020-06-25 23:42:41.782022
Model ind 665 epoch 230 batch: 700 avg loss -2.628613 avg loss no lamb -2.628613 time 2020-06-25 23:42:52.468067
Model ind 665 epoch 230 batch: 800 avg loss -2.792917 avg loss no lamb -2.792917 time 2020-06-25 23:43:03.167386
last batch sz 10
Pre: time 2020-06-25 23:43:16.981858: 
 	std: 0.003002405
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.981, 0.9738, 0.9804, 0.9755]
	train_accs: [0.98123336, 0.98111665, 0.9751833, 0.98113334, 0.97665]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97826004
	best: 0.9806

Starting e_i: 231
Model ind 665 epoch 231 batch: 0 avg loss -2.914140 avg loss no lamb -2.914140 time 2020-06-25 23:43:18.884735
Model ind 665 epoch 231 batch: 100 avg loss -2.803968 avg loss no lamb -2.803968 time 2020-06-25 23:43:29.715299
Model ind 665 epoch 231 batch: 200 avg loss -2.753175 avg loss no lamb -2.753175 time 2020-06-25 23:43:40.640372
Model ind 665 epoch 231 batch: 300 avg loss -2.751406 avg loss no lamb -2.751406 time 2020-06-25 23:43:51.327327
Model ind 665 epoch 231 batch: 400 avg loss -2.679347 avg loss no lamb -2.679347 time 2020-06-25 23:44:02.152284
Model ind 665 epoch 231 batch: 500 avg loss -2.721235 avg loss no lamb -2.721235 time 2020-06-25 23:44:12.689196
Model ind 665 epoch 231 batch: 600 avg loss -2.749827 avg loss no lamb -2.749827 time 2020-06-25 23:44:23.448811
Model ind 665 epoch 231 batch: 700 avg loss -2.571991 avg loss no lamb -2.571991 time 2020-06-25 23:44:34.299669
Model ind 665 epoch 231 batch: 800 avg loss -2.722836 avg loss no lamb -2.722836 time 2020-06-25 23:44:45.165059
last batch sz 10
Pre: time 2020-06-25 23:44:59.088444: 
 	std: 0.002431131
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.982, 0.9763, 0.9811, 0.9779]
	train_accs: [0.98251665, 0.9813833, 0.97688335, 0.98235, 0.97833335]
	best_train_sub_head: 0
	worst: 0.9763
	avg: 0.9799601
	best: 0.9825

Starting e_i: 232
Model ind 665 epoch 232 batch: 0 avg loss -2.879708 avg loss no lamb -2.879708 time 2020-06-25 23:44:59.913379
Model ind 665 epoch 232 batch: 100 avg loss -2.754305 avg loss no lamb -2.754305 time 2020-06-25 23:45:10.537955
Model ind 665 epoch 232 batch: 200 avg loss -2.717396 avg loss no lamb -2.717396 time 2020-06-25 23:45:21.421565
Model ind 665 epoch 232 batch: 300 avg loss -2.739065 avg loss no lamb -2.739065 time 2020-06-25 23:45:32.169177
Model ind 665 epoch 232 batch: 400 avg loss -2.663025 avg loss no lamb -2.663025 time 2020-06-25 23:45:42.975895
Model ind 665 epoch 232 batch: 500 avg loss -2.718049 avg loss no lamb -2.718049 time 2020-06-25 23:45:53.695437
Model ind 665 epoch 232 batch: 600 avg loss -2.804249 avg loss no lamb -2.804249 time 2020-06-25 23:46:04.531928
Model ind 665 epoch 232 batch: 700 avg loss -2.633752 avg loss no lamb -2.633752 time 2020-06-25 23:46:15.451023
Model ind 665 epoch 232 batch: 800 avg loss -2.803035 avg loss no lamb -2.803035 time 2020-06-25 23:46:26.278736
last batch sz 10
Pre: time 2020-06-25 23:46:40.156604: 
 	std: 0.0019544752
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9842, 0.9833, 0.98, 0.9844, 0.9801]
	train_accs: [0.98375, 0.98246664, 0.97893333, 0.9836, 0.9792333]
	best_train_sub_head: 0
	worst: 0.98
	avg: 0.98240006
	best: 0.9842

Starting e_i: 233
Model ind 665 epoch 233 batch: 0 avg loss -2.887610 avg loss no lamb -2.887610 time 2020-06-25 23:46:40.958324
Model ind 665 epoch 233 batch: 100 avg loss -2.774111 avg loss no lamb -2.774111 time 2020-06-25 23:46:51.757250
Model ind 665 epoch 233 batch: 200 avg loss -2.784466 avg loss no lamb -2.784466 time 2020-06-25 23:47:02.634523
Model ind 665 epoch 233 batch: 300 avg loss -2.683897 avg loss no lamb -2.683897 time 2020-06-25 23:47:13.381828
Model ind 665 epoch 233 batch: 400 avg loss -2.715438 avg loss no lamb -2.715438 time 2020-06-25 23:47:24.142178
Model ind 665 epoch 233 batch: 500 avg loss -2.717051 avg loss no lamb -2.717051 time 2020-06-25 23:47:34.861996
Model ind 665 epoch 233 batch: 600 avg loss -2.812298 avg loss no lamb -2.812298 time 2020-06-25 23:47:45.642056
Model ind 665 epoch 233 batch: 700 avg loss -2.608374 avg loss no lamb -2.608374 time 2020-06-25 23:47:56.312461
Model ind 665 epoch 233 batch: 800 avg loss -2.724213 avg loss no lamb -2.724213 time 2020-06-25 23:48:07.097120
last batch sz 10
Pre: time 2020-06-25 23:48:20.863935: 
 	std: 0.0029674233
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.983, 0.9833, 0.9769, 0.983, 0.9772]
	train_accs: [0.98263335, 0.98218334, 0.97678334, 0.9827333, 0.97735]
	best_train_sub_head: 3
	worst: 0.9769
	avg: 0.98068
	best: 0.983

Starting e_i: 234
Model ind 665 epoch 234 batch: 0 avg loss -2.878588 avg loss no lamb -2.878588 time 2020-06-25 23:48:21.570628
Model ind 665 epoch 234 batch: 100 avg loss -2.809416 avg loss no lamb -2.809416 time 2020-06-25 23:48:32.231873
Model ind 665 epoch 234 batch: 200 avg loss -2.789013 avg loss no lamb -2.789013 time 2020-06-25 23:48:42.862400
Model ind 665 epoch 234 batch: 300 avg loss -2.767452 avg loss no lamb -2.767452 time 2020-06-25 23:48:53.648462
Model ind 665 epoch 234 batch: 400 avg loss -2.688974 avg loss no lamb -2.688974 time 2020-06-25 23:49:04.242651
Model ind 665 epoch 234 batch: 500 avg loss -2.708356 avg loss no lamb -2.708356 time 2020-06-25 23:49:14.930876
Model ind 665 epoch 234 batch: 600 avg loss -2.772121 avg loss no lamb -2.772121 time 2020-06-25 23:49:25.713624
Model ind 665 epoch 234 batch: 700 avg loss -2.664692 avg loss no lamb -2.664692 time 2020-06-25 23:49:36.254537
Model ind 665 epoch 234 batch: 800 avg loss -2.776194 avg loss no lamb -2.776194 time 2020-06-25 23:49:46.928528
last batch sz 10
Pre: time 2020-06-25 23:50:00.448785: 
 	std: 0.0027650632
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9821, 0.9761, 0.9826, 0.9776]
	train_accs: [0.9827333, 0.98186666, 0.97685, 0.98255, 0.97815]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.98017997
	best: 0.9825

Starting e_i: 235
Model ind 665 epoch 235 batch: 0 avg loss -2.908420 avg loss no lamb -2.908420 time 2020-06-25 23:50:01.299027
Model ind 665 epoch 235 batch: 100 avg loss -2.710676 avg loss no lamb -2.710676 time 2020-06-25 23:50:12.099524
Model ind 665 epoch 235 batch: 200 avg loss -2.699785 avg loss no lamb -2.699785 time 2020-06-25 23:50:22.678705
Model ind 665 epoch 235 batch: 300 avg loss -2.792273 avg loss no lamb -2.792273 time 2020-06-25 23:50:33.406006
Model ind 665 epoch 235 batch: 400 avg loss -2.675492 avg loss no lamb -2.675492 time 2020-06-25 23:50:43.765558
Model ind 665 epoch 235 batch: 500 avg loss -2.725043 avg loss no lamb -2.725043 time 2020-06-25 23:50:54.635835
Model ind 665 epoch 235 batch: 600 avg loss -2.806183 avg loss no lamb -2.806183 time 2020-06-25 23:51:05.341438
Model ind 665 epoch 235 batch: 700 avg loss -2.594857 avg loss no lamb -2.594857 time 2020-06-25 23:51:16.040296
Model ind 665 epoch 235 batch: 800 avg loss -2.755716 avg loss no lamb -2.755716 time 2020-06-25 23:51:26.940715
last batch sz 10
Pre: time 2020-06-25 23:51:40.810784: 
 	std: 0.0035022267
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9831, 0.9821, 0.9753, 0.9825, 0.9756]
	train_accs: [0.9828333, 0.98143333, 0.9755667, 0.9823, 0.9767]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97972
	best: 0.9831

Starting e_i: 236
Model ind 665 epoch 236 batch: 0 avg loss -2.856067 avg loss no lamb -2.856067 time 2020-06-25 23:51:41.584395
Model ind 665 epoch 236 batch: 100 avg loss -2.781322 avg loss no lamb -2.781322 time 2020-06-25 23:51:52.422900
Model ind 665 epoch 236 batch: 200 avg loss -2.713084 avg loss no lamb -2.713084 time 2020-06-25 23:52:03.170237
Model ind 665 epoch 236 batch: 300 avg loss -2.695905 avg loss no lamb -2.695905 time 2020-06-25 23:52:14.338067
Model ind 665 epoch 236 batch: 400 avg loss -2.645256 avg loss no lamb -2.645256 time 2020-06-25 23:52:25.072479
Model ind 665 epoch 236 batch: 500 avg loss -2.706796 avg loss no lamb -2.706796 time 2020-06-25 23:52:35.663459
Model ind 665 epoch 236 batch: 600 avg loss -2.741647 avg loss no lamb -2.741647 time 2020-06-25 23:52:46.671325
Model ind 665 epoch 236 batch: 700 avg loss -2.667061 avg loss no lamb -2.667061 time 2020-06-25 23:52:57.483546
Model ind 665 epoch 236 batch: 800 avg loss -2.704143 avg loss no lamb -2.704143 time 2020-06-25 23:53:08.166308
last batch sz 10
Pre: time 2020-06-25 23:53:22.013991: 
 	std: 0.0034562347
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9821, 0.975, 0.9823, 0.9751]
	train_accs: [0.9823667, 0.9819, 0.9762, 0.98256665, 0.97695]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97928
	best: 0.9823

Starting e_i: 237
Model ind 665 epoch 237 batch: 0 avg loss -2.874447 avg loss no lamb -2.874447 time 2020-06-25 23:53:22.766718
Model ind 665 epoch 237 batch: 100 avg loss -2.784703 avg loss no lamb -2.784703 time 2020-06-25 23:53:33.230739
Model ind 665 epoch 237 batch: 200 avg loss -2.834295 avg loss no lamb -2.834295 time 2020-06-25 23:53:44.135880
Model ind 665 epoch 237 batch: 300 avg loss -2.775640 avg loss no lamb -2.775640 time 2020-06-25 23:53:54.762783
Model ind 665 epoch 237 batch: 400 avg loss -2.649491 avg loss no lamb -2.649491 time 2020-06-25 23:54:05.587329
Model ind 665 epoch 237 batch: 500 avg loss -2.783047 avg loss no lamb -2.783047 time 2020-06-25 23:54:16.522657
Model ind 665 epoch 237 batch: 600 avg loss -2.825327 avg loss no lamb -2.825327 time 2020-06-25 23:54:27.436369
Model ind 665 epoch 237 batch: 700 avg loss -2.622999 avg loss no lamb -2.622999 time 2020-06-25 23:54:38.064699
Model ind 665 epoch 237 batch: 800 avg loss -2.756358 avg loss no lamb -2.756358 time 2020-06-25 23:54:48.900820
last batch sz 10
Pre: time 2020-06-25 23:55:02.904088: 
 	std: 0.002221717
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9828, 0.9777, 0.9825, 0.9785]
	train_accs: [0.98291665, 0.9822, 0.97821665, 0.98258334, 0.97915]
	best_train_sub_head: 0
	worst: 0.9777
	avg: 0.98080003
	best: 0.9825

Starting e_i: 238
Model ind 665 epoch 238 batch: 0 avg loss -2.906670 avg loss no lamb -2.906670 time 2020-06-25 23:55:03.696941
Model ind 665 epoch 238 batch: 100 avg loss -2.836240 avg loss no lamb -2.836240 time 2020-06-25 23:55:14.437869
Model ind 665 epoch 238 batch: 200 avg loss -2.754384 avg loss no lamb -2.754384 time 2020-06-25 23:55:25.133119
Model ind 665 epoch 238 batch: 300 avg loss -2.765668 avg loss no lamb -2.765668 time 2020-06-25 23:55:35.939792
Model ind 665 epoch 238 batch: 400 avg loss -2.670831 avg loss no lamb -2.670831 time 2020-06-25 23:55:46.633821
Model ind 665 epoch 238 batch: 500 avg loss -2.723821 avg loss no lamb -2.723821 time 2020-06-25 23:55:57.469605
Model ind 665 epoch 238 batch: 600 avg loss -2.797104 avg loss no lamb -2.797104 time 2020-06-25 23:56:08.331081
Model ind 665 epoch 238 batch: 700 avg loss -2.667628 avg loss no lamb -2.667628 time 2020-06-25 23:56:19.118794
Model ind 665 epoch 238 batch: 800 avg loss -2.750397 avg loss no lamb -2.750397 time 2020-06-25 23:56:29.835051
last batch sz 10
Pre: time 2020-06-25 23:56:43.550557: 
 	std: 0.0041590277
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9838, 0.9824, 0.9743, 0.9838, 0.9756]
	train_accs: [0.982, 0.9814, 0.9748, 0.9820667, 0.97595]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97998
	best: 0.9838

Starting e_i: 239
Model ind 665 epoch 239 batch: 0 avg loss -2.913654 avg loss no lamb -2.913654 time 2020-06-25 23:56:44.280685
Model ind 665 epoch 239 batch: 100 avg loss -2.812919 avg loss no lamb -2.812919 time 2020-06-25 23:56:55.120338
Model ind 665 epoch 239 batch: 200 avg loss -2.774099 avg loss no lamb -2.774099 time 2020-06-25 23:57:05.661210
Model ind 665 epoch 239 batch: 300 avg loss -2.759223 avg loss no lamb -2.759223 time 2020-06-25 23:57:16.544950
Model ind 665 epoch 239 batch: 400 avg loss -2.672846 avg loss no lamb -2.672846 time 2020-06-25 23:57:27.350497
Model ind 665 epoch 239 batch: 500 avg loss -2.731260 avg loss no lamb -2.731260 time 2020-06-25 23:57:37.959668
Model ind 665 epoch 239 batch: 600 avg loss -2.860687 avg loss no lamb -2.860687 time 2020-06-25 23:57:48.587891
Model ind 665 epoch 239 batch: 700 avg loss -2.695674 avg loss no lamb -2.695674 time 2020-06-25 23:57:59.341505
Model ind 665 epoch 239 batch: 800 avg loss -2.708512 avg loss no lamb -2.708512 time 2020-06-25 23:58:10.178031
last batch sz 10
Pre: time 2020-06-25 23:58:23.885969: 
 	std: 0.0027492475
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9812, 0.9757, 0.9817, 0.9759]
	train_accs: [0.9820667, 0.9814, 0.97651666, 0.98221666, 0.9765]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97916
	best: 0.9817

Starting e_i: 240
Model ind 665 epoch 240 batch: 0 avg loss -2.898198 avg loss no lamb -2.898198 time 2020-06-25 23:58:24.651468
Model ind 665 epoch 240 batch: 100 avg loss -2.819653 avg loss no lamb -2.819653 time 2020-06-25 23:58:35.237621
Model ind 665 epoch 240 batch: 200 avg loss -2.794826 avg loss no lamb -2.794826 time 2020-06-25 23:58:45.659708
Model ind 665 epoch 240 batch: 300 avg loss -2.791175 avg loss no lamb -2.791175 time 2020-06-25 23:58:56.345422
Model ind 665 epoch 240 batch: 400 avg loss -2.675465 avg loss no lamb -2.675465 time 2020-06-25 23:59:07.131759
Model ind 665 epoch 240 batch: 500 avg loss -2.688001 avg loss no lamb -2.688001 time 2020-06-25 23:59:17.902770
Model ind 665 epoch 240 batch: 600 avg loss -2.818616 avg loss no lamb -2.818616 time 2020-06-25 23:59:28.481965
Model ind 665 epoch 240 batch: 700 avg loss -2.656596 avg loss no lamb -2.656596 time 2020-06-25 23:59:39.177797
Model ind 665 epoch 240 batch: 800 avg loss -2.746186 avg loss no lamb -2.746186 time 2020-06-25 23:59:50.094214
last batch sz 10
Pre: time 2020-06-26 00:00:04.432825: 
 	std: 0.0028477416
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9817, 0.9759, 0.9828, 0.9778]
	train_accs: [0.982, 0.98106664, 0.9763833, 0.98218334, 0.97765]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.98022
	best: 0.9828

Starting e_i: 241
Model ind 665 epoch 241 batch: 0 avg loss -2.872054 avg loss no lamb -2.872054 time 2020-06-26 00:00:06.509894
Model ind 665 epoch 241 batch: 100 avg loss -2.826299 avg loss no lamb -2.826299 time 2020-06-26 00:00:17.240255
Model ind 665 epoch 241 batch: 200 avg loss -2.808187 avg loss no lamb -2.808187 time 2020-06-26 00:00:27.765343
Model ind 665 epoch 241 batch: 300 avg loss -2.889880 avg loss no lamb -2.889880 time 2020-06-26 00:00:38.540236
Model ind 665 epoch 241 batch: 400 avg loss -2.705334 avg loss no lamb -2.705334 time 2020-06-26 00:00:49.342843
Model ind 665 epoch 241 batch: 500 avg loss -2.674998 avg loss no lamb -2.674998 time 2020-06-26 00:01:00.164897
Model ind 665 epoch 241 batch: 600 avg loss -2.746608 avg loss no lamb -2.746608 time 2020-06-26 00:01:10.787396
Model ind 665 epoch 241 batch: 700 avg loss -2.670733 avg loss no lamb -2.670733 time 2020-06-26 00:01:21.164733
Model ind 665 epoch 241 batch: 800 avg loss -2.750491 avg loss no lamb -2.750491 time 2020-06-26 00:01:31.757853
last batch sz 10
Pre: time 2020-06-26 00:01:45.583368: 
 	std: 0.0026351542
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9828, 0.9769, 0.9825, 0.9779]
	train_accs: [0.98298335, 0.98245, 0.9773167, 0.98286664, 0.9785]
	best_train_sub_head: 0
	worst: 0.9769
	avg: 0.9806
	best: 0.9829

Starting e_i: 242
Model ind 665 epoch 242 batch: 0 avg loss -2.936013 avg loss no lamb -2.936013 time 2020-06-26 00:01:46.317132
Model ind 665 epoch 242 batch: 100 avg loss -2.743456 avg loss no lamb -2.743456 time 2020-06-26 00:01:57.082256
Model ind 665 epoch 242 batch: 200 avg loss -2.700340 avg loss no lamb -2.700340 time 2020-06-26 00:02:07.595926
Model ind 665 epoch 242 batch: 300 avg loss -2.757712 avg loss no lamb -2.757712 time 2020-06-26 00:02:18.315785
Model ind 665 epoch 242 batch: 400 avg loss -2.619264 avg loss no lamb -2.619264 time 2020-06-26 00:02:28.985868
Model ind 665 epoch 242 batch: 500 avg loss -2.684935 avg loss no lamb -2.684935 time 2020-06-26 00:02:39.578772
Model ind 665 epoch 242 batch: 600 avg loss -2.841727 avg loss no lamb -2.841727 time 2020-06-26 00:02:50.273719
Model ind 665 epoch 242 batch: 700 avg loss -2.576838 avg loss no lamb -2.576838 time 2020-06-26 00:03:00.631067
Model ind 665 epoch 242 batch: 800 avg loss -2.690062 avg loss no lamb -2.690062 time 2020-06-26 00:03:11.497954
last batch sz 10
Pre: time 2020-06-26 00:03:25.349145: 
 	std: 0.0027825278
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.981, 0.9758, 0.9828, 0.9765]
	train_accs: [0.98246664, 0.9820667, 0.9768, 0.9832, 0.9777667]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.97945994
	best: 0.9828

Starting e_i: 243
Model ind 665 epoch 243 batch: 0 avg loss -2.924099 avg loss no lamb -2.924099 time 2020-06-26 00:03:26.116427
Model ind 665 epoch 243 batch: 100 avg loss -2.794922 avg loss no lamb -2.794922 time 2020-06-26 00:03:37.023764
Model ind 665 epoch 243 batch: 200 avg loss -2.754553 avg loss no lamb -2.754553 time 2020-06-26 00:03:47.760580
Model ind 665 epoch 243 batch: 300 avg loss -2.704206 avg loss no lamb -2.704206 time 2020-06-26 00:03:58.503488
Model ind 665 epoch 243 batch: 400 avg loss -2.567469 avg loss no lamb -2.567469 time 2020-06-26 00:04:09.232439
Model ind 665 epoch 243 batch: 500 avg loss -2.784298 avg loss no lamb -2.784298 time 2020-06-26 00:04:19.981043
Model ind 665 epoch 243 batch: 600 avg loss -2.851579 avg loss no lamb -2.851579 time 2020-06-26 00:04:30.616541
Model ind 665 epoch 243 batch: 700 avg loss -2.697651 avg loss no lamb -2.697651 time 2020-06-26 00:04:41.498412
Model ind 665 epoch 243 batch: 800 avg loss -2.739997 avg loss no lamb -2.739997 time 2020-06-26 00:04:52.197314
last batch sz 10
Pre: time 2020-06-26 00:05:05.869960: 
 	std: 0.0039922846
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.983, 0.9824, 0.9741, 0.983, 0.9753]
	train_accs: [0.98246664, 0.98178333, 0.97613335, 0.98246664, 0.97725]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97956
	best: 0.983

Starting e_i: 244
Model ind 665 epoch 244 batch: 0 avg loss -2.877760 avg loss no lamb -2.877760 time 2020-06-26 00:05:06.690736
Model ind 665 epoch 244 batch: 100 avg loss -2.783464 avg loss no lamb -2.783464 time 2020-06-26 00:05:17.531674
Model ind 665 epoch 244 batch: 200 avg loss -2.711705 avg loss no lamb -2.711705 time 2020-06-26 00:05:28.102964
Model ind 665 epoch 244 batch: 300 avg loss -2.835593 avg loss no lamb -2.835593 time 2020-06-26 00:05:38.799355
Model ind 665 epoch 244 batch: 400 avg loss -2.652448 avg loss no lamb -2.652448 time 2020-06-26 00:05:49.524044
Model ind 665 epoch 244 batch: 500 avg loss -2.772150 avg loss no lamb -2.772150 time 2020-06-26 00:06:00.431423
Model ind 665 epoch 244 batch: 600 avg loss -2.844385 avg loss no lamb -2.844385 time 2020-06-26 00:06:10.958322
Model ind 665 epoch 244 batch: 700 avg loss -2.512373 avg loss no lamb -2.512373 time 2020-06-26 00:06:21.617197
Model ind 665 epoch 244 batch: 800 avg loss -2.695309 avg loss no lamb -2.695309 time 2020-06-26 00:06:32.249212
last batch sz 10
Pre: time 2020-06-26 00:06:46.027985: 
 	std: 0.0031093492
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9829, 0.976, 0.9818, 0.9755]
	train_accs: [0.9820667, 0.9820333, 0.97705, 0.9824333, 0.9773]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.9795
	best: 0.9818

Starting e_i: 245
Model ind 665 epoch 245 batch: 0 avg loss -2.891906 avg loss no lamb -2.891906 time 2020-06-26 00:06:46.753299
Model ind 665 epoch 245 batch: 100 avg loss -2.826534 avg loss no lamb -2.826534 time 2020-06-26 00:06:57.471422
Model ind 665 epoch 245 batch: 200 avg loss -2.709147 avg loss no lamb -2.709147 time 2020-06-26 00:07:07.947017
Model ind 665 epoch 245 batch: 300 avg loss -2.776562 avg loss no lamb -2.776562 time 2020-06-26 00:07:18.940436
Model ind 665 epoch 245 batch: 400 avg loss -2.646339 avg loss no lamb -2.646339 time 2020-06-26 00:07:29.720831
Model ind 665 epoch 245 batch: 500 avg loss -2.725143 avg loss no lamb -2.725143 time 2020-06-26 00:07:40.492874
Model ind 665 epoch 245 batch: 600 avg loss -2.725871 avg loss no lamb -2.725871 time 2020-06-26 00:07:51.063537
Model ind 665 epoch 245 batch: 700 avg loss -2.643960 avg loss no lamb -2.643960 time 2020-06-26 00:08:01.641029
Model ind 665 epoch 245 batch: 800 avg loss -2.779377 avg loss no lamb -2.779377 time 2020-06-26 00:08:12.478390
last batch sz 10
Pre: time 2020-06-26 00:08:26.473071: 
 	std: 0.00230825
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9815, 0.9775, 0.9823, 0.9769]
	train_accs: [0.9822, 0.98143333, 0.9772, 0.98251665, 0.9776]
	best_train_sub_head: 3
	worst: 0.9769
	avg: 0.98
	best: 0.9823

Starting e_i: 246
Model ind 665 epoch 246 batch: 0 avg loss -2.844276 avg loss no lamb -2.844276 time 2020-06-26 00:08:27.250225
Model ind 665 epoch 246 batch: 100 avg loss -2.851713 avg loss no lamb -2.851713 time 2020-06-26 00:08:38.062992
Model ind 665 epoch 246 batch: 200 avg loss -2.801625 avg loss no lamb -2.801625 time 2020-06-26 00:08:48.834490
Model ind 665 epoch 246 batch: 300 avg loss -2.788691 avg loss no lamb -2.788691 time 2020-06-26 00:08:59.655706
Model ind 665 epoch 246 batch: 400 avg loss -2.703315 avg loss no lamb -2.703315 time 2020-06-26 00:09:10.433583
Model ind 665 epoch 246 batch: 500 avg loss -2.744314 avg loss no lamb -2.744314 time 2020-06-26 00:09:21.102290
Model ind 665 epoch 246 batch: 600 avg loss -2.827315 avg loss no lamb -2.827315 time 2020-06-26 00:09:31.645955
Model ind 665 epoch 246 batch: 700 avg loss -2.631315 avg loss no lamb -2.631315 time 2020-06-26 00:09:42.360183
Model ind 665 epoch 246 batch: 800 avg loss -2.727075 avg loss no lamb -2.727075 time 2020-06-26 00:09:53.216418
last batch sz 10
Pre: time 2020-06-26 00:10:07.042466: 
 	std: 0.0022729656
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9808, 0.9756, 0.9806, 0.9762]
	train_accs: [0.9816333, 0.9814, 0.97603333, 0.98181665, 0.9777333]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97866
	best: 0.9806

Starting e_i: 247
Model ind 665 epoch 247 batch: 0 avg loss -2.893583 avg loss no lamb -2.893583 time 2020-06-26 00:10:07.892372
Model ind 665 epoch 247 batch: 100 avg loss -2.768114 avg loss no lamb -2.768114 time 2020-06-26 00:10:18.153684
Model ind 665 epoch 247 batch: 200 avg loss -2.805844 avg loss no lamb -2.805844 time 2020-06-26 00:10:28.729976
Model ind 665 epoch 247 batch: 300 avg loss -2.783122 avg loss no lamb -2.783122 time 2020-06-26 00:10:39.386753
Model ind 665 epoch 247 batch: 400 avg loss -2.682035 avg loss no lamb -2.682035 time 2020-06-26 00:10:50.101583
Model ind 665 epoch 247 batch: 500 avg loss -2.700479 avg loss no lamb -2.700479 time 2020-06-26 00:11:00.743056
Model ind 665 epoch 247 batch: 600 avg loss -2.771019 avg loss no lamb -2.771019 time 2020-06-26 00:11:11.158890
Model ind 665 epoch 247 batch: 700 avg loss -2.640215 avg loss no lamb -2.640215 time 2020-06-26 00:11:21.699525
Model ind 665 epoch 247 batch: 800 avg loss -2.790668 avg loss no lamb -2.790668 time 2020-06-26 00:11:32.382897
last batch sz 10
Pre: time 2020-06-26 00:11:46.139267: 
 	std: 0.0026158018
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9829, 0.9777, 0.9835, 0.9778]
	train_accs: [0.9826667, 0.98195, 0.97746664, 0.9831333, 0.9781167]
	best_train_sub_head: 3
	worst: 0.9777
	avg: 0.98094
	best: 0.9835

Starting e_i: 248
Model ind 665 epoch 248 batch: 0 avg loss -2.933789 avg loss no lamb -2.933789 time 2020-06-26 00:11:46.884798
Model ind 665 epoch 248 batch: 100 avg loss -2.820327 avg loss no lamb -2.820327 time 2020-06-26 00:11:57.465762
Model ind 665 epoch 248 batch: 200 avg loss -2.751217 avg loss no lamb -2.751217 time 2020-06-26 00:12:07.753970
Model ind 665 epoch 248 batch: 300 avg loss -2.743849 avg loss no lamb -2.743849 time 2020-06-26 00:12:18.218889
Model ind 665 epoch 248 batch: 400 avg loss -2.657592 avg loss no lamb -2.657592 time 2020-06-26 00:12:29.021780
Model ind 665 epoch 248 batch: 500 avg loss -2.681552 avg loss no lamb -2.681552 time 2020-06-26 00:12:39.654578
Model ind 665 epoch 248 batch: 600 avg loss -2.788502 avg loss no lamb -2.788502 time 2020-06-26 00:12:50.274911
Model ind 665 epoch 248 batch: 700 avg loss -2.569995 avg loss no lamb -2.569995 time 2020-06-26 00:13:01.033517
Model ind 665 epoch 248 batch: 800 avg loss -2.725658 avg loss no lamb -2.725658 time 2020-06-26 00:13:11.635000
last batch sz 10
Pre: time 2020-06-26 00:13:25.656739: 
 	std: 0.0025444627
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.983, 0.9833, 0.9779, 0.9834, 0.9782]
	train_accs: [0.98301667, 0.9827333, 0.97785, 0.9831833, 0.97838336]
	best_train_sub_head: 3
	worst: 0.9779
	avg: 0.98116
	best: 0.9834

Starting e_i: 249
Model ind 665 epoch 249 batch: 0 avg loss -2.916882 avg loss no lamb -2.916882 time 2020-06-26 00:13:26.431627
Model ind 665 epoch 249 batch: 100 avg loss -2.813965 avg loss no lamb -2.813965 time 2020-06-26 00:13:37.083043
Model ind 665 epoch 249 batch: 200 avg loss -2.739479 avg loss no lamb -2.739479 time 2020-06-26 00:13:48.040077
Model ind 665 epoch 249 batch: 300 avg loss -2.800102 avg loss no lamb -2.800102 time 2020-06-26 00:13:58.572846
Model ind 665 epoch 249 batch: 400 avg loss -2.703587 avg loss no lamb -2.703587 time 2020-06-26 00:14:09.241213
Model ind 665 epoch 249 batch: 500 avg loss -2.713451 avg loss no lamb -2.713451 time 2020-06-26 00:14:19.789408
Model ind 665 epoch 249 batch: 600 avg loss -2.813879 avg loss no lamb -2.813879 time 2020-06-26 00:14:30.370019
Model ind 665 epoch 249 batch: 700 avg loss -2.676527 avg loss no lamb -2.676527 time 2020-06-26 00:14:41.028299
Model ind 665 epoch 249 batch: 800 avg loss -2.710090 avg loss no lamb -2.710090 time 2020-06-26 00:14:51.503453
last batch sz 10
Pre: time 2020-06-26 00:15:05.348718: 
 	std: 0.0039055794
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9827, 0.9742, 0.9821, 0.9743]
	train_accs: [0.9826, 0.98225, 0.97511667, 0.98263335, 0.9759833]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97902
	best: 0.9821

Starting e_i: 250
Model ind 665 epoch 250 batch: 0 avg loss -2.854499 avg loss no lamb -2.854499 time 2020-06-26 00:15:06.142233
Model ind 665 epoch 250 batch: 100 avg loss -2.810649 avg loss no lamb -2.810649 time 2020-06-26 00:15:16.673535
Model ind 665 epoch 250 batch: 200 avg loss -2.799404 avg loss no lamb -2.799404 time 2020-06-26 00:15:27.538157
Model ind 665 epoch 250 batch: 300 avg loss -2.797673 avg loss no lamb -2.797673 time 2020-06-26 00:15:38.339483
Model ind 665 epoch 250 batch: 400 avg loss -2.763712 avg loss no lamb -2.763712 time 2020-06-26 00:15:49.016157
Model ind 665 epoch 250 batch: 500 avg loss -2.760844 avg loss no lamb -2.760844 time 2020-06-26 00:15:59.983831
Model ind 665 epoch 250 batch: 600 avg loss -2.852154 avg loss no lamb -2.852154 time 2020-06-26 00:16:10.463585
Model ind 665 epoch 250 batch: 700 avg loss -2.649124 avg loss no lamb -2.649124 time 2020-06-26 00:16:21.118586
Model ind 665 epoch 250 batch: 800 avg loss -2.724540 avg loss no lamb -2.724540 time 2020-06-26 00:16:31.866235
last batch sz 10
Pre: time 2020-06-26 00:16:45.601935: 
 	std: 0.0027691072
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9819, 0.9768, 0.9822, 0.9757]
	train_accs: [0.98205, 0.98158336, 0.97665, 0.98253334, 0.97705]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97959995
	best: 0.9822

Starting e_i: 251
Model ind 665 epoch 251 batch: 0 avg loss -2.913492 avg loss no lamb -2.913492 time 2020-06-26 00:16:47.510696
Model ind 665 epoch 251 batch: 100 avg loss -2.837448 avg loss no lamb -2.837448 time 2020-06-26 00:16:58.206738
Model ind 665 epoch 251 batch: 200 avg loss -2.788989 avg loss no lamb -2.788989 time 2020-06-26 00:17:09.026347
Model ind 665 epoch 251 batch: 300 avg loss -2.823518 avg loss no lamb -2.823518 time 2020-06-26 00:17:19.772708
Model ind 665 epoch 251 batch: 400 avg loss -2.671599 avg loss no lamb -2.671599 time 2020-06-26 00:17:30.651572
Model ind 665 epoch 251 batch: 500 avg loss -2.717665 avg loss no lamb -2.717665 time 2020-06-26 00:17:41.335379
Model ind 665 epoch 251 batch: 600 avg loss -2.838592 avg loss no lamb -2.838592 time 2020-06-26 00:17:52.062977
Model ind 665 epoch 251 batch: 700 avg loss -2.653271 avg loss no lamb -2.653271 time 2020-06-26 00:18:02.841486
Model ind 665 epoch 251 batch: 800 avg loss -2.789613 avg loss no lamb -2.789613 time 2020-06-26 00:18:13.805901
last batch sz 10
Pre: time 2020-06-26 00:18:27.658953: 
 	std: 0.0022530046
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9837, 0.979, 0.9832, 0.9787]
	train_accs: [0.98305, 0.98261666, 0.97798336, 0.98293334, 0.978]
	best_train_sub_head: 0
	worst: 0.9787
	avg: 0.9816
	best: 0.9834

Starting e_i: 252
Model ind 665 epoch 252 batch: 0 avg loss -2.901720 avg loss no lamb -2.901720 time 2020-06-26 00:18:28.466835
Model ind 665 epoch 252 batch: 100 avg loss -2.864615 avg loss no lamb -2.864615 time 2020-06-26 00:18:39.069907
Model ind 665 epoch 252 batch: 200 avg loss -2.745765 avg loss no lamb -2.745765 time 2020-06-26 00:18:49.629161
Model ind 665 epoch 252 batch: 300 avg loss -2.800433 avg loss no lamb -2.800433 time 2020-06-26 00:19:00.495932
Model ind 665 epoch 252 batch: 400 avg loss -2.669912 avg loss no lamb -2.669912 time 2020-06-26 00:19:11.355669
Model ind 665 epoch 252 batch: 500 avg loss -2.681686 avg loss no lamb -2.681686 time 2020-06-26 00:19:22.176850
Model ind 665 epoch 252 batch: 600 avg loss -2.752510 avg loss no lamb -2.752510 time 2020-06-26 00:19:32.845827
Model ind 665 epoch 252 batch: 700 avg loss -2.670944 avg loss no lamb -2.670944 time 2020-06-26 00:19:43.667422
Model ind 665 epoch 252 batch: 800 avg loss -2.739442 avg loss no lamb -2.739442 time 2020-06-26 00:19:54.163821
last batch sz 10
Pre: time 2020-06-26 00:20:07.918263: 
 	std: 0.0025384927
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9809, 0.9761, 0.982, 0.977]
	train_accs: [0.98186666, 0.9808667, 0.9766333, 0.98186666, 0.97765]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.97959995
	best: 0.982

Starting e_i: 253
Model ind 665 epoch 253 batch: 0 avg loss -2.890852 avg loss no lamb -2.890852 time 2020-06-26 00:20:08.711214
Model ind 665 epoch 253 batch: 100 avg loss -2.783459 avg loss no lamb -2.783459 time 2020-06-26 00:20:19.697846
Model ind 665 epoch 253 batch: 200 avg loss -2.716468 avg loss no lamb -2.716468 time 2020-06-26 00:20:30.535966
Model ind 665 epoch 253 batch: 300 avg loss -2.801187 avg loss no lamb -2.801187 time 2020-06-26 00:20:41.450308
Model ind 665 epoch 253 batch: 400 avg loss -2.719914 avg loss no lamb -2.719914 time 2020-06-26 00:20:52.237825
Model ind 665 epoch 253 batch: 500 avg loss -2.769164 avg loss no lamb -2.769164 time 2020-06-26 00:21:03.180741
Model ind 665 epoch 253 batch: 600 avg loss -2.767013 avg loss no lamb -2.767013 time 2020-06-26 00:21:14.079944
Model ind 665 epoch 253 batch: 700 avg loss -2.670411 avg loss no lamb -2.670411 time 2020-06-26 00:21:24.984150
Model ind 665 epoch 253 batch: 800 avg loss -2.772582 avg loss no lamb -2.772582 time 2020-06-26 00:21:35.484577
last batch sz 10
Pre: time 2020-06-26 00:21:49.055065: 
 	std: 0.0026202374
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9807, 0.9747, 0.9805, 0.9765]
	train_accs: [0.98183334, 0.98085, 0.97595, 0.98191667, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97872
	best: 0.9805

Starting e_i: 254
Model ind 665 epoch 254 batch: 0 avg loss -2.888474 avg loss no lamb -2.888474 time 2020-06-26 00:21:49.793121
Model ind 665 epoch 254 batch: 100 avg loss -2.757922 avg loss no lamb -2.757922 time 2020-06-26 00:22:00.439408
Model ind 665 epoch 254 batch: 200 avg loss -2.807000 avg loss no lamb -2.807000 time 2020-06-26 00:22:11.199729
Model ind 665 epoch 254 batch: 300 avg loss -2.796846 avg loss no lamb -2.796846 time 2020-06-26 00:22:22.100122
Model ind 665 epoch 254 batch: 400 avg loss -2.722832 avg loss no lamb -2.722832 time 2020-06-26 00:22:32.869888
Model ind 665 epoch 254 batch: 500 avg loss -2.712924 avg loss no lamb -2.712924 time 2020-06-26 00:22:43.601944
Model ind 665 epoch 254 batch: 600 avg loss -2.812600 avg loss no lamb -2.812600 time 2020-06-26 00:22:54.310655
Model ind 665 epoch 254 batch: 700 avg loss -2.617102 avg loss no lamb -2.617102 time 2020-06-26 00:23:04.999350
Model ind 665 epoch 254 batch: 800 avg loss -2.775889 avg loss no lamb -2.775889 time 2020-06-26 00:23:15.659199
last batch sz 10
Pre: time 2020-06-26 00:23:29.003409: 
 	std: 0.0028751458
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9838, 0.9833, 0.9772, 0.9836, 0.9783]
	train_accs: [0.98331666, 0.9821333, 0.9777833, 0.98325, 0.97868335]
	best_train_sub_head: 0
	worst: 0.9772
	avg: 0.98124
	best: 0.9838

Starting e_i: 255
Model ind 665 epoch 255 batch: 0 avg loss -2.871721 avg loss no lamb -2.871721 time 2020-06-26 00:23:29.777639
Model ind 665 epoch 255 batch: 100 avg loss -2.834494 avg loss no lamb -2.834494 time 2020-06-26 00:23:40.646691
Model ind 665 epoch 255 batch: 200 avg loss -2.807012 avg loss no lamb -2.807012 time 2020-06-26 00:23:51.681956
Model ind 665 epoch 255 batch: 300 avg loss -2.753502 avg loss no lamb -2.753502 time 2020-06-26 00:24:02.568763
Model ind 665 epoch 255 batch: 400 avg loss -2.665023 avg loss no lamb -2.665023 time 2020-06-26 00:24:13.280532
Model ind 665 epoch 255 batch: 500 avg loss -2.729891 avg loss no lamb -2.729891 time 2020-06-26 00:24:23.734682
Model ind 665 epoch 255 batch: 600 avg loss -2.803342 avg loss no lamb -2.803342 time 2020-06-26 00:24:34.387969
Model ind 665 epoch 255 batch: 700 avg loss -2.620980 avg loss no lamb -2.620980 time 2020-06-26 00:24:44.979468
Model ind 665 epoch 255 batch: 800 avg loss -2.767143 avg loss no lamb -2.767143 time 2020-06-26 00:24:55.678252
last batch sz 10
Pre: time 2020-06-26 00:25:09.618827: 
 	std: 0.00227457
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9822, 0.9765, 0.9813, 0.9777]
	train_accs: [0.9821, 0.98186666, 0.97675, 0.98228335, 0.9781]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.9798201
	best: 0.9813

Starting e_i: 256
Model ind 665 epoch 256 batch: 0 avg loss -2.923414 avg loss no lamb -2.923414 time 2020-06-26 00:25:10.457622
Model ind 665 epoch 256 batch: 100 avg loss -2.765270 avg loss no lamb -2.765270 time 2020-06-26 00:25:20.882684
Model ind 665 epoch 256 batch: 200 avg loss -2.717535 avg loss no lamb -2.717535 time 2020-06-26 00:25:31.641789
Model ind 665 epoch 256 batch: 300 avg loss -2.805041 avg loss no lamb -2.805041 time 2020-06-26 00:25:42.268447
Model ind 665 epoch 256 batch: 400 avg loss -2.651520 avg loss no lamb -2.651520 time 2020-06-26 00:25:52.852067
Model ind 665 epoch 256 batch: 500 avg loss -2.720261 avg loss no lamb -2.720261 time 2020-06-26 00:26:03.515813
Model ind 665 epoch 256 batch: 600 avg loss -2.780051 avg loss no lamb -2.780051 time 2020-06-26 00:26:14.328522
Model ind 665 epoch 256 batch: 700 avg loss -2.569410 avg loss no lamb -2.569410 time 2020-06-26 00:26:25.273005
Model ind 665 epoch 256 batch: 800 avg loss -2.739739 avg loss no lamb -2.739739 time 2020-06-26 00:26:36.020689
last batch sz 10
Pre: time 2020-06-26 00:26:49.714229: 
 	std: 0.002490306
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9831, 0.9769, 0.9817, 0.9777]
	train_accs: [0.98233336, 0.98225, 0.97676665, 0.9823667, 0.97826666]
	best_train_sub_head: 3
	worst: 0.9769
	avg: 0.9802799
	best: 0.9817

Starting e_i: 257
Model ind 665 epoch 257 batch: 0 avg loss -2.924005 avg loss no lamb -2.924005 time 2020-06-26 00:26:50.442843
Model ind 665 epoch 257 batch: 100 avg loss -2.742394 avg loss no lamb -2.742394 time 2020-06-26 00:27:01.235919
Model ind 665 epoch 257 batch: 200 avg loss -2.762164 avg loss no lamb -2.762164 time 2020-06-26 00:27:11.956651
Model ind 665 epoch 257 batch: 300 avg loss -2.827572 avg loss no lamb -2.827572 time 2020-06-26 00:27:22.376199
Model ind 665 epoch 257 batch: 400 avg loss -2.692933 avg loss no lamb -2.692933 time 2020-06-26 00:27:33.001145
Model ind 665 epoch 257 batch: 500 avg loss -2.655881 avg loss no lamb -2.655881 time 2020-06-26 00:27:43.606481
Model ind 665 epoch 257 batch: 600 avg loss -2.855601 avg loss no lamb -2.855601 time 2020-06-26 00:27:54.396116
Model ind 665 epoch 257 batch: 700 avg loss -2.664751 avg loss no lamb -2.664751 time 2020-06-26 00:28:05.347552
Model ind 665 epoch 257 batch: 800 avg loss -2.750315 avg loss no lamb -2.750315 time 2020-06-26 00:28:15.951965
last batch sz 10
Pre: time 2020-06-26 00:28:29.934110: 
 	std: 0.0031495988
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9809, 0.9745, 0.9821, 0.9757]
	train_accs: [0.98246664, 0.98081666, 0.97573334, 0.9824167, 0.9767333]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97889996
	best: 0.9813

Starting e_i: 258
Model ind 665 epoch 258 batch: 0 avg loss -2.869534 avg loss no lamb -2.869534 time 2020-06-26 00:28:30.721382
Model ind 665 epoch 258 batch: 100 avg loss -2.857979 avg loss no lamb -2.857979 time 2020-06-26 00:28:41.147276
Model ind 665 epoch 258 batch: 200 avg loss -2.728940 avg loss no lamb -2.728940 time 2020-06-26 00:28:51.868158
Model ind 665 epoch 258 batch: 300 avg loss -2.801773 avg loss no lamb -2.801773 time 2020-06-26 00:29:02.643547
Model ind 665 epoch 258 batch: 400 avg loss -2.689117 avg loss no lamb -2.689117 time 2020-06-26 00:29:13.461788
Model ind 665 epoch 258 batch: 500 avg loss -2.779587 avg loss no lamb -2.779587 time 2020-06-26 00:29:24.173241
Model ind 665 epoch 258 batch: 600 avg loss -2.809772 avg loss no lamb -2.809772 time 2020-06-26 00:29:34.942800
Model ind 665 epoch 258 batch: 700 avg loss -2.681208 avg loss no lamb -2.681208 time 2020-06-26 00:29:45.401060
Model ind 665 epoch 258 batch: 800 avg loss -2.738806 avg loss no lamb -2.738806 time 2020-06-26 00:29:56.266667
last batch sz 10
Pre: time 2020-06-26 00:30:09.967583: 
 	std: 0.0026790895
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9827, 0.9774, 0.9833, 0.9775]
	train_accs: [0.9827, 0.9818, 0.9771, 0.98261666, 0.9774333]
	best_train_sub_head: 0
	worst: 0.9774
	avg: 0.98072004
	best: 0.9827

Starting e_i: 259
Model ind 665 epoch 259 batch: 0 avg loss -2.900212 avg loss no lamb -2.900212 time 2020-06-26 00:30:10.759401
Model ind 665 epoch 259 batch: 100 avg loss -2.801112 avg loss no lamb -2.801112 time 2020-06-26 00:30:21.240126
Model ind 665 epoch 259 batch: 200 avg loss -2.773010 avg loss no lamb -2.773010 time 2020-06-26 00:30:32.142937
Model ind 665 epoch 259 batch: 300 avg loss -2.817661 avg loss no lamb -2.817661 time 2020-06-26 00:30:43.028460
Model ind 665 epoch 259 batch: 400 avg loss -2.695425 avg loss no lamb -2.695425 time 2020-06-26 00:30:54.107891
Model ind 665 epoch 259 batch: 500 avg loss -2.699913 avg loss no lamb -2.699913 time 2020-06-26 00:31:05.097429
Model ind 665 epoch 259 batch: 600 avg loss -2.753556 avg loss no lamb -2.753556 time 2020-06-26 00:31:16.028088
Model ind 665 epoch 259 batch: 700 avg loss -2.623829 avg loss no lamb -2.623829 time 2020-06-26 00:31:26.879472
Model ind 665 epoch 259 batch: 800 avg loss -2.764415 avg loss no lamb -2.764415 time 2020-06-26 00:31:37.780256
last batch sz 10
Pre: time 2020-06-26 00:31:51.510669: 
 	std: 0.0023363198
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9803, 0.9756, 0.9803, 0.9756]
	train_accs: [0.9810167, 0.98108333, 0.97611666, 0.9813167, 0.9765667]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97845995
	best: 0.9803

Starting e_i: 260
Model ind 665 epoch 260 batch: 0 avg loss -2.910135 avg loss no lamb -2.910135 time 2020-06-26 00:31:52.257169
Model ind 665 epoch 260 batch: 100 avg loss -2.799825 avg loss no lamb -2.799825 time 2020-06-26 00:32:03.034738
Model ind 665 epoch 260 batch: 200 avg loss -2.739143 avg loss no lamb -2.739143 time 2020-06-26 00:32:13.795622
Model ind 665 epoch 260 batch: 300 avg loss -2.761021 avg loss no lamb -2.761021 time 2020-06-26 00:32:24.510714
Model ind 665 epoch 260 batch: 400 avg loss -2.655488 avg loss no lamb -2.655488 time 2020-06-26 00:32:35.352352
Model ind 665 epoch 260 batch: 500 avg loss -2.670375 avg loss no lamb -2.670375 time 2020-06-26 00:32:46.270944
Model ind 665 epoch 260 batch: 600 avg loss -2.753799 avg loss no lamb -2.753799 time 2020-06-26 00:32:57.145513
Model ind 665 epoch 260 batch: 700 avg loss -2.585579 avg loss no lamb -2.585579 time 2020-06-26 00:33:08.095164
Model ind 665 epoch 260 batch: 800 avg loss -2.781321 avg loss no lamb -2.781321 time 2020-06-26 00:33:18.929415
last batch sz 10
Pre: time 2020-06-26 00:33:32.777867: 
 	std: 0.003826953
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.982, 0.9743, 0.9813, 0.9736]
	train_accs: [0.9824167, 0.9818, 0.97566664, 0.98218334, 0.97575]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97861993
	best: 0.9819

Starting e_i: 261
Model ind 665 epoch 261 batch: 0 avg loss -2.840573 avg loss no lamb -2.840573 time 2020-06-26 00:33:34.709501
Model ind 665 epoch 261 batch: 100 avg loss -2.794048 avg loss no lamb -2.794048 time 2020-06-26 00:33:45.645326
Model ind 665 epoch 261 batch: 200 avg loss -2.712810 avg loss no lamb -2.712810 time 2020-06-26 00:33:56.241319
Model ind 665 epoch 261 batch: 300 avg loss -2.785136 avg loss no lamb -2.785136 time 2020-06-26 00:34:07.277451
Model ind 665 epoch 261 batch: 400 avg loss -2.736547 avg loss no lamb -2.736547 time 2020-06-26 00:34:18.086019
Model ind 665 epoch 261 batch: 500 avg loss -2.659349 avg loss no lamb -2.659349 time 2020-06-26 00:34:28.833461
Model ind 665 epoch 261 batch: 600 avg loss -2.796507 avg loss no lamb -2.796507 time 2020-06-26 00:34:39.539145
Model ind 665 epoch 261 batch: 700 avg loss -2.690389 avg loss no lamb -2.690389 time 2020-06-26 00:34:50.173499
Model ind 665 epoch 261 batch: 800 avg loss -2.795847 avg loss no lamb -2.795847 time 2020-06-26 00:35:01.018613
last batch sz 10
Pre: time 2020-06-26 00:35:14.951099: 
 	std: 0.0032201859
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9814, 0.9749, 0.9823, 0.9763]
	train_accs: [0.9823667, 0.9813833, 0.97606665, 0.98183334, 0.97715]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97947997
	best: 0.9825

Starting e_i: 262
Model ind 665 epoch 262 batch: 0 avg loss -2.884035 avg loss no lamb -2.884035 time 2020-06-26 00:35:15.743267
Model ind 665 epoch 262 batch: 100 avg loss -2.761106 avg loss no lamb -2.761106 time 2020-06-26 00:35:26.417204
Model ind 665 epoch 262 batch: 200 avg loss -2.759998 avg loss no lamb -2.759998 time 2020-06-26 00:35:37.156965
Model ind 665 epoch 262 batch: 300 avg loss -2.815031 avg loss no lamb -2.815031 time 2020-06-26 00:35:47.696331
Model ind 665 epoch 262 batch: 400 avg loss -2.682094 avg loss no lamb -2.682094 time 2020-06-26 00:35:58.528500
Model ind 665 epoch 262 batch: 500 avg loss -2.712759 avg loss no lamb -2.712759 time 2020-06-26 00:36:09.356110
Model ind 665 epoch 262 batch: 600 avg loss -2.796958 avg loss no lamb -2.796958 time 2020-06-26 00:36:20.079249
Model ind 665 epoch 262 batch: 700 avg loss -2.604569 avg loss no lamb -2.604569 time 2020-06-26 00:36:30.796635
Model ind 665 epoch 262 batch: 800 avg loss -2.704567 avg loss no lamb -2.704567 time 2020-06-26 00:36:41.267591
last batch sz 10
Pre: time 2020-06-26 00:36:55.034769: 
 	std: 0.0042225155
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9816, 0.9728, 0.9815, 0.9729]
	train_accs: [0.9822, 0.98158336, 0.9752, 0.98233336, 0.97566664]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97802
	best: 0.9815

Starting e_i: 263
Model ind 665 epoch 263 batch: 0 avg loss -2.946993 avg loss no lamb -2.946993 time 2020-06-26 00:36:55.783393
Model ind 665 epoch 263 batch: 100 avg loss -2.814234 avg loss no lamb -2.814234 time 2020-06-26 00:37:06.366187
Model ind 665 epoch 263 batch: 200 avg loss -2.821652 avg loss no lamb -2.821652 time 2020-06-26 00:37:16.958477
Model ind 665 epoch 263 batch: 300 avg loss -2.836630 avg loss no lamb -2.836630 time 2020-06-26 00:37:27.511419
Model ind 665 epoch 263 batch: 400 avg loss -2.642656 avg loss no lamb -2.642656 time 2020-06-26 00:37:38.034433
Model ind 665 epoch 263 batch: 500 avg loss -2.670779 avg loss no lamb -2.670779 time 2020-06-26 00:37:48.935804
Model ind 665 epoch 263 batch: 600 avg loss -2.815296 avg loss no lamb -2.815296 time 2020-06-26 00:37:59.676686
Model ind 665 epoch 263 batch: 700 avg loss -2.685251 avg loss no lamb -2.685251 time 2020-06-26 00:38:10.352102
Model ind 665 epoch 263 batch: 800 avg loss -2.763600 avg loss no lamb -2.763600 time 2020-06-26 00:38:21.039327
last batch sz 10
Pre: time 2020-06-26 00:38:34.955037: 
 	std: 0.0035134212
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9832, 0.9758, 0.9828, 0.9758]
	train_accs: [0.9831, 0.98221666, 0.97675, 0.9831167, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.9801
	best: 0.9828

Starting e_i: 264
Model ind 665 epoch 264 batch: 0 avg loss -2.915601 avg loss no lamb -2.915601 time 2020-06-26 00:38:35.711945
Model ind 665 epoch 264 batch: 100 avg loss -2.808454 avg loss no lamb -2.808454 time 2020-06-26 00:38:46.586907
Model ind 665 epoch 264 batch: 200 avg loss -2.732979 avg loss no lamb -2.732979 time 2020-06-26 00:38:57.224221
Model ind 665 epoch 264 batch: 300 avg loss -2.834780 avg loss no lamb -2.834780 time 2020-06-26 00:39:07.935109
Model ind 665 epoch 264 batch: 400 avg loss -2.697854 avg loss no lamb -2.697854 time 2020-06-26 00:39:18.407005
Model ind 665 epoch 264 batch: 500 avg loss -2.743662 avg loss no lamb -2.743662 time 2020-06-26 00:39:29.186149
Model ind 665 epoch 264 batch: 600 avg loss -2.833908 avg loss no lamb -2.833908 time 2020-06-26 00:39:40.031000
Model ind 665 epoch 264 batch: 700 avg loss -2.608966 avg loss no lamb -2.608966 time 2020-06-26 00:39:50.672917
Model ind 665 epoch 264 batch: 800 avg loss -2.750886 avg loss no lamb -2.750886 time 2020-06-26 00:40:01.459833
last batch sz 10
Pre: time 2020-06-26 00:40:15.121549: 
 	std: 0.0026884878
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9812, 0.9749, 0.9813, 0.9769]
	train_accs: [0.9821, 0.98153335, 0.97641665, 0.9821, 0.9774167]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97910005
	best: 0.9812

Starting e_i: 265
Model ind 665 epoch 265 batch: 0 avg loss -2.866765 avg loss no lamb -2.866765 time 2020-06-26 00:40:15.959029
Model ind 665 epoch 265 batch: 100 avg loss -2.795543 avg loss no lamb -2.795543 time 2020-06-26 00:40:26.287241
Model ind 665 epoch 265 batch: 200 avg loss -2.764680 avg loss no lamb -2.764680 time 2020-06-26 00:40:36.939248
Model ind 665 epoch 265 batch: 300 avg loss -2.795151 avg loss no lamb -2.795151 time 2020-06-26 00:40:47.871415
Model ind 665 epoch 265 batch: 400 avg loss -2.698691 avg loss no lamb -2.698691 time 2020-06-26 00:40:58.802760
Model ind 665 epoch 265 batch: 500 avg loss -2.641202 avg loss no lamb -2.641202 time 2020-06-26 00:41:09.449031
Model ind 665 epoch 265 batch: 600 avg loss -2.856967 avg loss no lamb -2.856967 time 2020-06-26 00:41:20.346715
Model ind 665 epoch 265 batch: 700 avg loss -2.714308 avg loss no lamb -2.714308 time 2020-06-26 00:41:30.985757
Model ind 665 epoch 265 batch: 800 avg loss -2.669464 avg loss no lamb -2.669464 time 2020-06-26 00:41:41.621309
last batch sz 10
Pre: time 2020-06-26 00:41:55.355476: 
 	std: 0.00330599
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9824, 0.9752, 0.9824, 0.9764]
	train_accs: [0.98293334, 0.98223335, 0.97713333, 0.9827167, 0.9781333]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97981995
	best: 0.9827

Starting e_i: 266
Model ind 665 epoch 266 batch: 0 avg loss -2.930869 avg loss no lamb -2.930869 time 2020-06-26 00:41:56.084330
Model ind 665 epoch 266 batch: 100 avg loss -2.750729 avg loss no lamb -2.750729 time 2020-06-26 00:42:06.675406
Model ind 665 epoch 266 batch: 200 avg loss -2.801957 avg loss no lamb -2.801957 time 2020-06-26 00:42:17.488769
Model ind 665 epoch 266 batch: 300 avg loss -2.750966 avg loss no lamb -2.750966 time 2020-06-26 00:42:28.340418
Model ind 665 epoch 266 batch: 400 avg loss -2.695263 avg loss no lamb -2.695263 time 2020-06-26 00:42:39.056516
Model ind 665 epoch 266 batch: 500 avg loss -2.768722 avg loss no lamb -2.768722 time 2020-06-26 00:42:49.656207
Model ind 665 epoch 266 batch: 600 avg loss -2.763332 avg loss no lamb -2.763332 time 2020-06-26 00:43:00.159314
Model ind 665 epoch 266 batch: 700 avg loss -2.627330 avg loss no lamb -2.627330 time 2020-06-26 00:43:10.867573
Model ind 665 epoch 266 batch: 800 avg loss -2.735713 avg loss no lamb -2.735713 time 2020-06-26 00:43:21.422693
last batch sz 10
Pre: time 2020-06-26 00:43:35.293133: 
 	std: 0.002310498
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9814, 0.9773, 0.9816, 0.9766]
	train_accs: [0.98228335, 0.9813, 0.9767, 0.98195, 0.97756666]
	best_train_sub_head: 0
	worst: 0.9766
	avg: 0.97976
	best: 0.9819

Starting e_i: 267
Model ind 665 epoch 267 batch: 0 avg loss -2.924623 avg loss no lamb -2.924623 time 2020-06-26 00:43:36.089016
Model ind 665 epoch 267 batch: 100 avg loss -2.902458 avg loss no lamb -2.902458 time 2020-06-26 00:43:46.616445
Model ind 665 epoch 267 batch: 200 avg loss -2.780802 avg loss no lamb -2.780802 time 2020-06-26 00:43:57.321593
Model ind 665 epoch 267 batch: 300 avg loss -2.801988 avg loss no lamb -2.801988 time 2020-06-26 00:44:08.312996
Model ind 665 epoch 267 batch: 400 avg loss -2.681921 avg loss no lamb -2.681921 time 2020-06-26 00:44:18.820225
Model ind 665 epoch 267 batch: 500 avg loss -2.744044 avg loss no lamb -2.744044 time 2020-06-26 00:44:29.547359
Model ind 665 epoch 267 batch: 600 avg loss -2.787513 avg loss no lamb -2.787513 time 2020-06-26 00:44:40.341107
Model ind 665 epoch 267 batch: 700 avg loss -2.608938 avg loss no lamb -2.608938 time 2020-06-26 00:44:51.305238
Model ind 665 epoch 267 batch: 800 avg loss -2.748198 avg loss no lamb -2.748198 time 2020-06-26 00:45:01.556122
last batch sz 10
Pre: time 2020-06-26 00:45:15.151604: 
 	std: 0.0034966264
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9809, 0.9745, 0.9828, 0.9756]
	train_accs: [0.98261666, 0.98135, 0.97613335, 0.9824833, 0.9773667]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97924006
	best: 0.9824

Starting e_i: 268
Model ind 665 epoch 268 batch: 0 avg loss -2.867080 avg loss no lamb -2.867080 time 2020-06-26 00:45:15.998907
Model ind 665 epoch 268 batch: 100 avg loss -2.805068 avg loss no lamb -2.805068 time 2020-06-26 00:45:26.863029
Model ind 665 epoch 268 batch: 200 avg loss -2.745651 avg loss no lamb -2.745651 time 2020-06-26 00:45:37.804803
Model ind 665 epoch 268 batch: 300 avg loss -2.722307 avg loss no lamb -2.722307 time 2020-06-26 00:45:48.693184
Model ind 665 epoch 268 batch: 400 avg loss -2.642025 avg loss no lamb -2.642025 time 2020-06-26 00:45:59.373783
Model ind 665 epoch 268 batch: 500 avg loss -2.785417 avg loss no lamb -2.785417 time 2020-06-26 00:46:10.298802
Model ind 665 epoch 268 batch: 600 avg loss -2.800256 avg loss no lamb -2.800256 time 2020-06-26 00:46:21.231628
Model ind 665 epoch 268 batch: 700 avg loss -2.696392 avg loss no lamb -2.696392 time 2020-06-26 00:46:31.940532
Model ind 665 epoch 268 batch: 800 avg loss -2.770711 avg loss no lamb -2.770711 time 2020-06-26 00:46:42.533619
last batch sz 10
Pre: time 2020-06-26 00:46:56.163753: 
 	std: 0.0024832208
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9822, 0.9764, 0.9824, 0.9784]
	train_accs: [0.9831833, 0.98211664, 0.97686666, 0.98335, 0.9783]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.98034
	best: 0.9824

Starting e_i: 269
Model ind 665 epoch 269 batch: 0 avg loss -2.944667 avg loss no lamb -2.944667 time 2020-06-26 00:46:56.970344
Model ind 665 epoch 269 batch: 100 avg loss -2.773348 avg loss no lamb -2.773348 time 2020-06-26 00:47:07.834612
Model ind 665 epoch 269 batch: 200 avg loss -2.828388 avg loss no lamb -2.828388 time 2020-06-26 00:47:18.251472
Model ind 665 epoch 269 batch: 300 avg loss -2.838737 avg loss no lamb -2.838737 time 2020-06-26 00:47:29.061747
Model ind 665 epoch 269 batch: 400 avg loss -2.744306 avg loss no lamb -2.744306 time 2020-06-26 00:47:39.800172
Model ind 665 epoch 269 batch: 500 avg loss -2.695248 avg loss no lamb -2.695248 time 2020-06-26 00:47:50.270635
Model ind 665 epoch 269 batch: 600 avg loss -2.784875 avg loss no lamb -2.784875 time 2020-06-26 00:48:01.061413
Model ind 665 epoch 269 batch: 700 avg loss -2.594992 avg loss no lamb -2.594992 time 2020-06-26 00:48:11.945547
Model ind 665 epoch 269 batch: 800 avg loss -2.739637 avg loss no lamb -2.739637 time 2020-06-26 00:48:22.679033
last batch sz 10
Pre: time 2020-06-26 00:48:36.466575: 
 	std: 0.003091262
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9802, 0.9732, 0.9805, 0.9757]
	train_accs: [0.9820667, 0.9810333, 0.9745333, 0.98176664, 0.97676665]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.9781
	best: 0.9809

Starting e_i: 270
Model ind 665 epoch 270 batch: 0 avg loss -2.920179 avg loss no lamb -2.920179 time 2020-06-26 00:48:37.244550
Model ind 665 epoch 270 batch: 100 avg loss -2.792089 avg loss no lamb -2.792089 time 2020-06-26 00:48:47.884520
Model ind 665 epoch 270 batch: 200 avg loss -2.751583 avg loss no lamb -2.751583 time 2020-06-26 00:48:58.383333
Model ind 665 epoch 270 batch: 300 avg loss -2.795377 avg loss no lamb -2.795377 time 2020-06-26 00:49:09.329312
Model ind 665 epoch 270 batch: 400 avg loss -2.673171 avg loss no lamb -2.673171 time 2020-06-26 00:49:20.295543
Model ind 665 epoch 270 batch: 500 avg loss -2.754100 avg loss no lamb -2.754100 time 2020-06-26 00:49:31.026471
Model ind 665 epoch 270 batch: 600 avg loss -2.838499 avg loss no lamb -2.838499 time 2020-06-26 00:49:41.560769
Model ind 665 epoch 270 batch: 700 avg loss -2.622821 avg loss no lamb -2.622821 time 2020-06-26 00:49:52.103622
Model ind 665 epoch 270 batch: 800 avg loss -2.761687 avg loss no lamb -2.761687 time 2020-06-26 00:50:02.793918
last batch sz 10
Pre: time 2020-06-26 00:50:16.469392: 
 	std: 0.003364283
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9807, 0.9741, 0.9817, 0.9753]
	train_accs: [0.98245, 0.9812667, 0.9756333, 0.98225, 0.9766]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97875994
	best: 0.982

Starting e_i: 271
Model ind 665 epoch 271 batch: 0 avg loss -2.909320 avg loss no lamb -2.909320 time 2020-06-26 00:50:18.486836
Model ind 665 epoch 271 batch: 100 avg loss -2.826203 avg loss no lamb -2.826203 time 2020-06-26 00:50:29.141577
Model ind 665 epoch 271 batch: 200 avg loss -2.784760 avg loss no lamb -2.784760 time 2020-06-26 00:50:39.754081
Model ind 665 epoch 271 batch: 300 avg loss -2.781174 avg loss no lamb -2.781174 time 2020-06-26 00:50:50.348085
Model ind 665 epoch 271 batch: 400 avg loss -2.700948 avg loss no lamb -2.700948 time 2020-06-26 00:51:01.306688
Model ind 665 epoch 271 batch: 500 avg loss -2.690147 avg loss no lamb -2.690147 time 2020-06-26 00:51:11.951074
Model ind 665 epoch 271 batch: 600 avg loss -2.808359 avg loss no lamb -2.808359 time 2020-06-26 00:51:22.713568
Model ind 665 epoch 271 batch: 700 avg loss -2.684449 avg loss no lamb -2.684449 time 2020-06-26 00:51:33.518855
Model ind 665 epoch 271 batch: 800 avg loss -2.792655 avg loss no lamb -2.792655 time 2020-06-26 00:51:44.435583
last batch sz 10
Pre: time 2020-06-26 00:51:58.007815: 
 	std: 0.002673269
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9807, 0.9756, 0.981, 0.975]
	train_accs: [0.98186666, 0.98085, 0.97601664, 0.98183334, 0.97605]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97856
	best: 0.9805

Starting e_i: 272
Model ind 665 epoch 272 batch: 0 avg loss -2.879636 avg loss no lamb -2.879636 time 2020-06-26 00:51:58.773401
Model ind 665 epoch 272 batch: 100 avg loss -2.811071 avg loss no lamb -2.811071 time 2020-06-26 00:52:09.288888
Model ind 665 epoch 272 batch: 200 avg loss -2.770139 avg loss no lamb -2.770139 time 2020-06-26 00:52:19.839851
Model ind 665 epoch 272 batch: 300 avg loss -2.769910 avg loss no lamb -2.769910 time 2020-06-26 00:52:30.681362
Model ind 665 epoch 272 batch: 400 avg loss -2.666831 avg loss no lamb -2.666831 time 2020-06-26 00:52:41.266509
Model ind 665 epoch 272 batch: 500 avg loss -2.748227 avg loss no lamb -2.748227 time 2020-06-26 00:52:51.846410
Model ind 665 epoch 272 batch: 600 avg loss -2.793415 avg loss no lamb -2.793415 time 2020-06-26 00:53:02.338736
Model ind 665 epoch 272 batch: 700 avg loss -2.620033 avg loss no lamb -2.620033 time 2020-06-26 00:53:12.694367
Model ind 665 epoch 272 batch: 800 avg loss -2.737767 avg loss no lamb -2.737767 time 2020-06-26 00:53:23.592990
last batch sz 10
Pre: time 2020-06-26 00:53:37.343562: 
 	std: 0.0029076443
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9825, 0.9759, 0.9823, 0.9774]
	train_accs: [0.9824833, 0.9816667, 0.97718334, 0.98233336, 0.97765]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.98015994
	best: 0.9827

Starting e_i: 273
Model ind 665 epoch 273 batch: 0 avg loss -2.911850 avg loss no lamb -2.911850 time 2020-06-26 00:53:38.119521
Model ind 665 epoch 273 batch: 100 avg loss -2.773682 avg loss no lamb -2.773682 time 2020-06-26 00:53:48.767222
Model ind 665 epoch 273 batch: 200 avg loss -2.791346 avg loss no lamb -2.791346 time 2020-06-26 00:53:59.468509
Model ind 665 epoch 273 batch: 300 avg loss -2.756076 avg loss no lamb -2.756076 time 2020-06-26 00:54:10.265331
Model ind 665 epoch 273 batch: 400 avg loss -2.704829 avg loss no lamb -2.704829 time 2020-06-26 00:54:20.758156
Model ind 665 epoch 273 batch: 500 avg loss -2.726315 avg loss no lamb -2.726315 time 2020-06-26 00:54:31.431111
Model ind 665 epoch 273 batch: 600 avg loss -2.848347 avg loss no lamb -2.848347 time 2020-06-26 00:54:42.163268
Model ind 665 epoch 273 batch: 700 avg loss -2.653332 avg loss no lamb -2.653332 time 2020-06-26 00:54:52.681443
Model ind 665 epoch 273 batch: 800 avg loss -2.790511 avg loss no lamb -2.790511 time 2020-06-26 00:55:03.483311
last batch sz 10
Pre: time 2020-06-26 00:55:17.045251: 
 	std: 0.0027954185
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9808, 0.9755, 0.9823, 0.9761]
	train_accs: [0.98195, 0.9809833, 0.97546667, 0.98226666, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97916
	best: 0.9823

Starting e_i: 274
Model ind 665 epoch 274 batch: 0 avg loss -2.894497 avg loss no lamb -2.894497 time 2020-06-26 00:55:17.863239
Model ind 665 epoch 274 batch: 100 avg loss -2.826892 avg loss no lamb -2.826892 time 2020-06-26 00:55:28.763016
Model ind 665 epoch 274 batch: 200 avg loss -2.827906 avg loss no lamb -2.827906 time 2020-06-26 00:55:39.477539
Model ind 665 epoch 274 batch: 300 avg loss -2.838898 avg loss no lamb -2.838898 time 2020-06-26 00:55:50.282278
Model ind 665 epoch 274 batch: 400 avg loss -2.648136 avg loss no lamb -2.648136 time 2020-06-26 00:56:01.120098
Model ind 665 epoch 274 batch: 500 avg loss -2.751025 avg loss no lamb -2.751025 time 2020-06-26 00:56:11.906482
Model ind 665 epoch 274 batch: 600 avg loss -2.815502 avg loss no lamb -2.815502 time 2020-06-26 00:56:22.662478
Model ind 665 epoch 274 batch: 700 avg loss -2.695017 avg loss no lamb -2.695017 time 2020-06-26 00:56:33.449183
Model ind 665 epoch 274 batch: 800 avg loss -2.770166 avg loss no lamb -2.770166 time 2020-06-26 00:56:43.987454
last batch sz 10
Pre: time 2020-06-26 00:56:57.743049: 
 	std: 0.003414445
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9829, 0.9752, 0.9819, 0.9756]
	train_accs: [0.9828333, 0.98223335, 0.9763167, 0.9828333, 0.9770333]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97956
	best: 0.9822

Starting e_i: 275
Model ind 665 epoch 275 batch: 0 avg loss -2.903257 avg loss no lamb -2.903257 time 2020-06-26 00:56:58.509731
Model ind 665 epoch 275 batch: 100 avg loss -2.826802 avg loss no lamb -2.826802 time 2020-06-26 00:57:09.262363
Model ind 665 epoch 275 batch: 200 avg loss -2.688621 avg loss no lamb -2.688621 time 2020-06-26 00:57:19.957133
Model ind 665 epoch 275 batch: 300 avg loss -2.843978 avg loss no lamb -2.843978 time 2020-06-26 00:57:30.566891
Model ind 665 epoch 275 batch: 400 avg loss -2.596943 avg loss no lamb -2.596943 time 2020-06-26 00:57:41.255917
Model ind 665 epoch 275 batch: 500 avg loss -2.748060 avg loss no lamb -2.748060 time 2020-06-26 00:57:51.914029
Model ind 665 epoch 275 batch: 600 avg loss -2.792672 avg loss no lamb -2.792672 time 2020-06-26 00:58:02.766464
Model ind 665 epoch 275 batch: 700 avg loss -2.674484 avg loss no lamb -2.674484 time 2020-06-26 00:58:13.269691
Model ind 665 epoch 275 batch: 800 avg loss -2.760494 avg loss no lamb -2.760494 time 2020-06-26 00:58:23.866683
last batch sz 10
Pre: time 2020-06-26 00:58:37.622904: 
 	std: 0.003866258
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9823, 0.9735, 0.9818, 0.9749]
	train_accs: [0.9823833, 0.9817, 0.9745333, 0.9824167, 0.9755333]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97889996
	best: 0.9818

Starting e_i: 276
Model ind 665 epoch 276 batch: 0 avg loss -2.918097 avg loss no lamb -2.918097 time 2020-06-26 00:58:38.351101
Model ind 665 epoch 276 batch: 100 avg loss -2.795979 avg loss no lamb -2.795979 time 2020-06-26 00:58:49.123424
Model ind 665 epoch 276 batch: 200 avg loss -2.740231 avg loss no lamb -2.740231 time 2020-06-26 00:58:59.760562
Model ind 665 epoch 276 batch: 300 avg loss -2.810929 avg loss no lamb -2.810929 time 2020-06-26 00:59:10.573553
Model ind 665 epoch 276 batch: 400 avg loss -2.641373 avg loss no lamb -2.641373 time 2020-06-26 00:59:21.254328
Model ind 665 epoch 276 batch: 500 avg loss -2.785473 avg loss no lamb -2.785473 time 2020-06-26 00:59:32.038011
Model ind 665 epoch 276 batch: 600 avg loss -2.829216 avg loss no lamb -2.829216 time 2020-06-26 00:59:42.840292
Model ind 665 epoch 276 batch: 700 avg loss -2.703983 avg loss no lamb -2.703983 time 2020-06-26 00:59:53.659991
Model ind 665 epoch 276 batch: 800 avg loss -2.737605 avg loss no lamb -2.737605 time 2020-06-26 01:00:04.424521
last batch sz 10
Pre: time 2020-06-26 01:00:18.049932: 
 	std: 0.0034248559
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.983, 0.9755, 0.9832, 0.9766]
	train_accs: [0.98268336, 0.9820167, 0.97613335, 0.9826667, 0.97726667]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.98022
	best: 0.9828

Starting e_i: 277
Model ind 665 epoch 277 batch: 0 avg loss -2.902166 avg loss no lamb -2.902166 time 2020-06-26 01:00:18.843876
Model ind 665 epoch 277 batch: 100 avg loss -2.812907 avg loss no lamb -2.812907 time 2020-06-26 01:00:29.761373
Model ind 665 epoch 277 batch: 200 avg loss -2.787823 avg loss no lamb -2.787823 time 2020-06-26 01:00:40.603754
Model ind 665 epoch 277 batch: 300 avg loss -2.721784 avg loss no lamb -2.721784 time 2020-06-26 01:00:51.340489
Model ind 665 epoch 277 batch: 400 avg loss -2.710910 avg loss no lamb -2.710910 time 2020-06-26 01:01:02.136683
Model ind 665 epoch 277 batch: 500 avg loss -2.734406 avg loss no lamb -2.734406 time 2020-06-26 01:01:13.004359
Model ind 665 epoch 277 batch: 600 avg loss -2.765421 avg loss no lamb -2.765421 time 2020-06-26 01:01:23.832660
Model ind 665 epoch 277 batch: 700 avg loss -2.729532 avg loss no lamb -2.729532 time 2020-06-26 01:01:34.652919
Model ind 665 epoch 277 batch: 800 avg loss -2.759581 avg loss no lamb -2.759581 time 2020-06-26 01:01:45.502186
last batch sz 10
Pre: time 2020-06-26 01:01:59.246950: 
 	std: 0.0038567302
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9841, 0.9823, 0.9748, 0.9833, 0.9762]
	train_accs: [0.98253334, 0.98145, 0.9753, 0.98253334, 0.9759167]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.9801399
	best: 0.9841

Starting e_i: 278
Model ind 665 epoch 278 batch: 0 avg loss -2.878953 avg loss no lamb -2.878953 time 2020-06-26 01:02:00.023328
Model ind 665 epoch 278 batch: 100 avg loss -2.834778 avg loss no lamb -2.834778 time 2020-06-26 01:02:10.757740
Model ind 665 epoch 278 batch: 200 avg loss -2.770184 avg loss no lamb -2.770184 time 2020-06-26 01:02:21.633483
Model ind 665 epoch 278 batch: 300 avg loss -2.765162 avg loss no lamb -2.765162 time 2020-06-26 01:02:32.283874
Model ind 665 epoch 278 batch: 400 avg loss -2.754305 avg loss no lamb -2.754305 time 2020-06-26 01:02:43.027659
Model ind 665 epoch 278 batch: 500 avg loss -2.725213 avg loss no lamb -2.725213 time 2020-06-26 01:02:53.636946
Model ind 665 epoch 278 batch: 600 avg loss -2.784913 avg loss no lamb -2.784913 time 2020-06-26 01:03:04.661385
Model ind 665 epoch 278 batch: 700 avg loss -2.598699 avg loss no lamb -2.598699 time 2020-06-26 01:03:15.647302
Model ind 665 epoch 278 batch: 800 avg loss -2.761454 avg loss no lamb -2.761454 time 2020-06-26 01:03:26.435862
last batch sz 10
Pre: time 2020-06-26 01:03:40.448464: 
 	std: 0.0025752657
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9828, 0.9773, 0.9824, 0.9774]
	train_accs: [0.9830667, 0.9823, 0.97753334, 0.9831333, 0.97785]
	best_train_sub_head: 3
	worst: 0.9773
	avg: 0.9804999
	best: 0.9824

Starting e_i: 279
Model ind 665 epoch 279 batch: 0 avg loss -2.921057 avg loss no lamb -2.921057 time 2020-06-26 01:03:41.229063
Model ind 665 epoch 279 batch: 100 avg loss -2.735424 avg loss no lamb -2.735424 time 2020-06-26 01:03:51.887434
Model ind 665 epoch 279 batch: 200 avg loss -2.786015 avg loss no lamb -2.786015 time 2020-06-26 01:04:02.737209
Model ind 665 epoch 279 batch: 300 avg loss -2.744211 avg loss no lamb -2.744211 time 2020-06-26 01:04:13.688346
Model ind 665 epoch 279 batch: 400 avg loss -2.684472 avg loss no lamb -2.684472 time 2020-06-26 01:04:24.241096
Model ind 665 epoch 279 batch: 500 avg loss -2.730800 avg loss no lamb -2.730800 time 2020-06-26 01:04:34.952291
Model ind 665 epoch 279 batch: 600 avg loss -2.833641 avg loss no lamb -2.833641 time 2020-06-26 01:04:45.618246
Model ind 665 epoch 279 batch: 700 avg loss -2.616055 avg loss no lamb -2.616055 time 2020-06-26 01:04:56.408062
Model ind 665 epoch 279 batch: 800 avg loss -2.803216 avg loss no lamb -2.803216 time 2020-06-26 01:05:07.285202
last batch sz 10
Pre: time 2020-06-26 01:05:21.077219: 
 	std: 0.0033314258
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9822, 0.9752, 0.9833, 0.9771]
	train_accs: [0.98256665, 0.9820167, 0.9763, 0.9827667, 0.97785]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.98014003
	best: 0.9833

Starting e_i: 280
Model ind 665 epoch 280 batch: 0 avg loss -2.862633 avg loss no lamb -2.862633 time 2020-06-26 01:05:21.861310
Model ind 665 epoch 280 batch: 100 avg loss -2.823332 avg loss no lamb -2.823332 time 2020-06-26 01:05:32.529504
Model ind 665 epoch 280 batch: 200 avg loss -2.790168 avg loss no lamb -2.790168 time 2020-06-26 01:05:43.439958
Model ind 665 epoch 280 batch: 300 avg loss -2.831114 avg loss no lamb -2.831114 time 2020-06-26 01:05:54.248119
Model ind 665 epoch 280 batch: 400 avg loss -2.710798 avg loss no lamb -2.710798 time 2020-06-26 01:06:05.004769
Model ind 665 epoch 280 batch: 500 avg loss -2.772331 avg loss no lamb -2.772331 time 2020-06-26 01:06:15.698700
Model ind 665 epoch 280 batch: 600 avg loss -2.784528 avg loss no lamb -2.784528 time 2020-06-26 01:06:26.359716
Model ind 665 epoch 280 batch: 700 avg loss -2.612858 avg loss no lamb -2.612858 time 2020-06-26 01:06:37.103162
Model ind 665 epoch 280 batch: 800 avg loss -2.698869 avg loss no lamb -2.698869 time 2020-06-26 01:06:47.551626
last batch sz 10
Pre: time 2020-06-26 01:07:01.398154: 
 	std: 0.0027853723
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9823, 0.9761, 0.9823, 0.9768]
	train_accs: [0.9823667, 0.9821, 0.97728336, 0.98255, 0.9776667]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.97984
	best: 0.9823

Starting e_i: 281
Model ind 665 epoch 281 batch: 0 avg loss -2.930276 avg loss no lamb -2.930276 time 2020-06-26 01:07:03.303704
Model ind 665 epoch 281 batch: 100 avg loss -2.855679 avg loss no lamb -2.855679 time 2020-06-26 01:07:14.021443
Model ind 665 epoch 281 batch: 200 avg loss -2.799176 avg loss no lamb -2.799176 time 2020-06-26 01:07:24.632844
Model ind 665 epoch 281 batch: 300 avg loss -2.887485 avg loss no lamb -2.887485 time 2020-06-26 01:07:35.298593
Model ind 665 epoch 281 batch: 400 avg loss -2.712223 avg loss no lamb -2.712223 time 2020-06-26 01:07:45.863983
Model ind 665 epoch 281 batch: 500 avg loss -2.755422 avg loss no lamb -2.755422 time 2020-06-26 01:07:56.437278
Model ind 665 epoch 281 batch: 600 avg loss -2.809939 avg loss no lamb -2.809939 time 2020-06-26 01:08:07.166600
Model ind 665 epoch 281 batch: 700 avg loss -2.672913 avg loss no lamb -2.672913 time 2020-06-26 01:08:17.978671
Model ind 665 epoch 281 batch: 800 avg loss -2.728046 avg loss no lamb -2.728046 time 2020-06-26 01:08:28.784832
last batch sz 10
Pre: time 2020-06-26 01:08:42.599290: 
 	std: 0.0030822079
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9836, 0.9827, 0.976, 0.9834, 0.9783]
	train_accs: [0.98251665, 0.98183334, 0.9766333, 0.98265, 0.97746664]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.98080003
	best: 0.9834

Starting e_i: 282
Model ind 665 epoch 282 batch: 0 avg loss -2.958756 avg loss no lamb -2.958756 time 2020-06-26 01:08:43.376276
Model ind 665 epoch 282 batch: 100 avg loss -2.796312 avg loss no lamb -2.796312 time 2020-06-26 01:08:54.082652
Model ind 665 epoch 282 batch: 200 avg loss -2.786412 avg loss no lamb -2.786412 time 2020-06-26 01:09:04.952124
Model ind 665 epoch 282 batch: 300 avg loss -2.770200 avg loss no lamb -2.770200 time 2020-06-26 01:09:15.678585
Model ind 665 epoch 282 batch: 400 avg loss -2.707368 avg loss no lamb -2.707368 time 2020-06-26 01:09:26.547158
Model ind 665 epoch 282 batch: 500 avg loss -2.696644 avg loss no lamb -2.696644 time 2020-06-26 01:09:37.132358
Model ind 665 epoch 282 batch: 600 avg loss -2.805526 avg loss no lamb -2.805526 time 2020-06-26 01:09:47.820469
Model ind 665 epoch 282 batch: 700 avg loss -2.625973 avg loss no lamb -2.625973 time 2020-06-26 01:09:58.645989
Model ind 665 epoch 282 batch: 800 avg loss -2.819210 avg loss no lamb -2.819210 time 2020-06-26 01:10:09.529510
last batch sz 10
Pre: time 2020-06-26 01:10:23.351131: 
 	std: 0.0024944674
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9808, 0.9757, 0.9811, 0.9763]
	train_accs: [0.9820667, 0.9812, 0.9768, 0.98221666, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97903997
	best: 0.9811

Starting e_i: 283
Model ind 665 epoch 283 batch: 0 avg loss -2.883408 avg loss no lamb -2.883408 time 2020-06-26 01:10:24.212078
Model ind 665 epoch 283 batch: 100 avg loss -2.799976 avg loss no lamb -2.799976 time 2020-06-26 01:10:35.133750
Model ind 665 epoch 283 batch: 200 avg loss -2.778344 avg loss no lamb -2.778344 time 2020-06-26 01:10:45.883393
Model ind 665 epoch 283 batch: 300 avg loss -2.786505 avg loss no lamb -2.786505 time 2020-06-26 01:10:56.481185
Model ind 665 epoch 283 batch: 400 avg loss -2.679371 avg loss no lamb -2.679371 time 2020-06-26 01:11:07.402835
Model ind 665 epoch 283 batch: 500 avg loss -2.791450 avg loss no lamb -2.791450 time 2020-06-26 01:11:17.990539
Model ind 665 epoch 283 batch: 600 avg loss -2.804118 avg loss no lamb -2.804118 time 2020-06-26 01:11:28.570181
Model ind 665 epoch 283 batch: 700 avg loss -2.671947 avg loss no lamb -2.671947 time 2020-06-26 01:11:39.349445
Model ind 665 epoch 283 batch: 800 avg loss -2.731260 avg loss no lamb -2.731260 time 2020-06-26 01:11:50.203072
last batch sz 10
Pre: time 2020-06-26 01:12:04.048876: 
 	std: 0.0030322236
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.983, 0.9765, 0.9827, 0.9768]
	train_accs: [0.9827667, 0.98223335, 0.97748333, 0.9827833, 0.9773333]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.98036003
	best: 0.9827

Starting e_i: 284
Model ind 665 epoch 284 batch: 0 avg loss -2.915342 avg loss no lamb -2.915342 time 2020-06-26 01:12:04.821933
Model ind 665 epoch 284 batch: 100 avg loss -2.799691 avg loss no lamb -2.799691 time 2020-06-26 01:12:15.362022
Model ind 665 epoch 284 batch: 200 avg loss -2.781390 avg loss no lamb -2.781390 time 2020-06-26 01:12:25.989801
Model ind 665 epoch 284 batch: 300 avg loss -2.811175 avg loss no lamb -2.811175 time 2020-06-26 01:12:36.824736
Model ind 665 epoch 284 batch: 400 avg loss -2.672319 avg loss no lamb -2.672319 time 2020-06-26 01:12:47.450346
Model ind 665 epoch 284 batch: 500 avg loss -2.727332 avg loss no lamb -2.727332 time 2020-06-26 01:12:57.875264
Model ind 665 epoch 284 batch: 600 avg loss -2.835584 avg loss no lamb -2.835584 time 2020-06-26 01:13:08.832756
Model ind 665 epoch 284 batch: 700 avg loss -2.649861 avg loss no lamb -2.649861 time 2020-06-26 01:13:19.638772
Model ind 665 epoch 284 batch: 800 avg loss -2.716572 avg loss no lamb -2.716572 time 2020-06-26 01:13:30.466596
last batch sz 10
Pre: time 2020-06-26 01:13:44.654335: 
 	std: 0.0029387102
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9814, 0.9753, 0.9821, 0.9766]
	train_accs: [0.98228335, 0.98146665, 0.97601664, 0.98245, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.9795
	best: 0.9821

Starting e_i: 285
Model ind 665 epoch 285 batch: 0 avg loss -2.930330 avg loss no lamb -2.930330 time 2020-06-26 01:13:45.440991
Model ind 665 epoch 285 batch: 100 avg loss -2.840881 avg loss no lamb -2.840881 time 2020-06-26 01:13:56.231606
Model ind 665 epoch 285 batch: 200 avg loss -2.782177 avg loss no lamb -2.782177 time 2020-06-26 01:14:07.083125
Model ind 665 epoch 285 batch: 300 avg loss -2.838928 avg loss no lamb -2.838928 time 2020-06-26 01:14:17.890546
Model ind 665 epoch 285 batch: 400 avg loss -2.634269 avg loss no lamb -2.634269 time 2020-06-26 01:14:28.574085
Model ind 665 epoch 285 batch: 500 avg loss -2.719294 avg loss no lamb -2.719294 time 2020-06-26 01:14:39.235021
Model ind 665 epoch 285 batch: 600 avg loss -2.831635 avg loss no lamb -2.831635 time 2020-06-26 01:14:50.028479
Model ind 665 epoch 285 batch: 700 avg loss -2.639804 avg loss no lamb -2.639804 time 2020-06-26 01:15:01.174663
Model ind 665 epoch 285 batch: 800 avg loss -2.758217 avg loss no lamb -2.758217 time 2020-06-26 01:15:11.901292
last batch sz 10
Pre: time 2020-06-26 01:15:25.482321: 
 	std: 0.002955947
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9814, 0.9745, 0.9811, 0.9757]
	train_accs: [0.9817333, 0.98148334, 0.9766, 0.98226666, 0.97755]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97868
	best: 0.9811

Starting e_i: 286
Model ind 665 epoch 286 batch: 0 avg loss -2.902480 avg loss no lamb -2.902480 time 2020-06-26 01:15:26.327236
Model ind 665 epoch 286 batch: 100 avg loss -2.780708 avg loss no lamb -2.780708 time 2020-06-26 01:15:37.095171
Model ind 665 epoch 286 batch: 200 avg loss -2.794141 avg loss no lamb -2.794141 time 2020-06-26 01:15:47.881155
Model ind 665 epoch 286 batch: 300 avg loss -2.778698 avg loss no lamb -2.778698 time 2020-06-26 01:15:58.487023
Model ind 665 epoch 286 batch: 400 avg loss -2.688887 avg loss no lamb -2.688887 time 2020-06-26 01:16:09.384084
Model ind 665 epoch 286 batch: 500 avg loss -2.728889 avg loss no lamb -2.728889 time 2020-06-26 01:16:20.072231
Model ind 665 epoch 286 batch: 600 avg loss -2.824483 avg loss no lamb -2.824483 time 2020-06-26 01:16:31.100112
Model ind 665 epoch 286 batch: 700 avg loss -2.624746 avg loss no lamb -2.624746 time 2020-06-26 01:16:41.965027
Model ind 665 epoch 286 batch: 800 avg loss -2.807271 avg loss no lamb -2.807271 time 2020-06-26 01:16:52.713369
last batch sz 10
Pre: time 2020-06-26 01:17:06.640756: 
 	std: 0.0027679615
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9819, 0.9776, 0.9832, 0.9764]
	train_accs: [0.9827333, 0.98185, 0.97756666, 0.98301667, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.98032
	best: 0.9832

Starting e_i: 287
Model ind 665 epoch 287 batch: 0 avg loss -2.916320 avg loss no lamb -2.916320 time 2020-06-26 01:17:07.422393
Model ind 665 epoch 287 batch: 100 avg loss -2.877421 avg loss no lamb -2.877421 time 2020-06-26 01:17:18.496045
Model ind 665 epoch 287 batch: 200 avg loss -2.797148 avg loss no lamb -2.797148 time 2020-06-26 01:17:29.347665
Model ind 665 epoch 287 batch: 300 avg loss -2.801581 avg loss no lamb -2.801581 time 2020-06-26 01:17:39.880333
Model ind 665 epoch 287 batch: 400 avg loss -2.713322 avg loss no lamb -2.713322 time 2020-06-26 01:17:50.624199
Model ind 665 epoch 287 batch: 500 avg loss -2.723358 avg loss no lamb -2.723358 time 2020-06-26 01:18:01.420480
Model ind 665 epoch 287 batch: 600 avg loss -2.777107 avg loss no lamb -2.777107 time 2020-06-26 01:18:12.174531
Model ind 665 epoch 287 batch: 700 avg loss -2.642279 avg loss no lamb -2.642279 time 2020-06-26 01:18:22.576647
Model ind 665 epoch 287 batch: 800 avg loss -2.735084 avg loss no lamb -2.735084 time 2020-06-26 01:18:33.142396
last batch sz 10
Pre: time 2020-06-26 01:18:46.929971: 
 	std: 0.0025964654
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9825, 0.9762, 0.9821, 0.9782]
	train_accs: [0.98296666, 0.98211664, 0.97681665, 0.98251665, 0.97823334]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.98028004
	best: 0.9824

Starting e_i: 288
Model ind 665 epoch 288 batch: 0 avg loss -2.861209 avg loss no lamb -2.861209 time 2020-06-26 01:18:47.688452
Model ind 665 epoch 288 batch: 100 avg loss -2.748999 avg loss no lamb -2.748999 time 2020-06-26 01:18:58.349256
Model ind 665 epoch 288 batch: 200 avg loss -2.761659 avg loss no lamb -2.761659 time 2020-06-26 01:19:08.974062
Model ind 665 epoch 288 batch: 300 avg loss -2.799225 avg loss no lamb -2.799225 time 2020-06-26 01:19:19.702740
Model ind 665 epoch 288 batch: 400 avg loss -2.621629 avg loss no lamb -2.621629 time 2020-06-26 01:19:30.464530
Model ind 665 epoch 288 batch: 500 avg loss -2.712122 avg loss no lamb -2.712122 time 2020-06-26 01:19:41.181565
Model ind 665 epoch 288 batch: 600 avg loss -2.780636 avg loss no lamb -2.780636 time 2020-06-26 01:19:51.625066
Model ind 665 epoch 288 batch: 700 avg loss -2.665694 avg loss no lamb -2.665694 time 2020-06-26 01:20:02.342100
Model ind 665 epoch 288 batch: 800 avg loss -2.730695 avg loss no lamb -2.730695 time 2020-06-26 01:20:13.079145
last batch sz 10
Pre: time 2020-06-26 01:20:26.809438: 
 	std: 0.0031814508
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9816, 0.9748, 0.9829, 0.9768]
	train_accs: [0.9820333, 0.98106664, 0.97535, 0.9822, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97958004
	best: 0.9829

Starting e_i: 289
Model ind 665 epoch 289 batch: 0 avg loss -2.908018 avg loss no lamb -2.908018 time 2020-06-26 01:20:27.654096
Model ind 665 epoch 289 batch: 100 avg loss -2.800622 avg loss no lamb -2.800622 time 2020-06-26 01:20:38.437354
Model ind 665 epoch 289 batch: 200 avg loss -2.633024 avg loss no lamb -2.633024 time 2020-06-26 01:20:49.163244
Model ind 665 epoch 289 batch: 300 avg loss -2.799865 avg loss no lamb -2.799865 time 2020-06-26 01:21:00.018154
Model ind 665 epoch 289 batch: 400 avg loss -2.758021 avg loss no lamb -2.758021 time 2020-06-26 01:21:10.655319
Model ind 665 epoch 289 batch: 500 avg loss -2.775344 avg loss no lamb -2.775344 time 2020-06-26 01:21:21.183963
Model ind 665 epoch 289 batch: 600 avg loss -2.774884 avg loss no lamb -2.774884 time 2020-06-26 01:21:31.821597
Model ind 665 epoch 289 batch: 700 avg loss -2.671360 avg loss no lamb -2.671360 time 2020-06-26 01:21:42.483872
Model ind 665 epoch 289 batch: 800 avg loss -2.794948 avg loss no lamb -2.794948 time 2020-06-26 01:21:53.091960
last batch sz 10
Pre: time 2020-06-26 01:22:06.746753: 
 	std: 0.002854754
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9817, 0.976, 0.9811, 0.9751]
	train_accs: [0.98191667, 0.9817, 0.9766667, 0.98185, 0.97685]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97902
	best: 0.9812

Starting e_i: 290
Model ind 665 epoch 290 batch: 0 avg loss -2.882527 avg loss no lamb -2.882527 time 2020-06-26 01:22:07.507838
Model ind 665 epoch 290 batch: 100 avg loss -2.751513 avg loss no lamb -2.751513 time 2020-06-26 01:22:18.149255
Model ind 665 epoch 290 batch: 200 avg loss -2.822928 avg loss no lamb -2.822928 time 2020-06-26 01:22:28.848870
Model ind 665 epoch 290 batch: 300 avg loss -2.803148 avg loss no lamb -2.803148 time 2020-06-26 01:22:39.579619
Model ind 665 epoch 290 batch: 400 avg loss -2.698198 avg loss no lamb -2.698198 time 2020-06-26 01:22:50.176977
Model ind 665 epoch 290 batch: 500 avg loss -2.730912 avg loss no lamb -2.730912 time 2020-06-26 01:23:00.513763
Model ind 665 epoch 290 batch: 600 avg loss -2.811278 avg loss no lamb -2.811278 time 2020-06-26 01:23:11.129060
Model ind 665 epoch 290 batch: 700 avg loss -2.638077 avg loss no lamb -2.638077 time 2020-06-26 01:23:21.947170
Model ind 665 epoch 290 batch: 800 avg loss -2.783481 avg loss no lamb -2.783481 time 2020-06-26 01:23:32.844499
last batch sz 10
Pre: time 2020-06-26 01:23:46.719496: 
 	std: 0.0036810907
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.982, 0.9741, 0.9829, 0.9759]
	train_accs: [0.98226666, 0.98148334, 0.9759833, 0.98235, 0.9773333]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97944003
	best: 0.9829

Starting e_i: 291
Model ind 665 epoch 291 batch: 0 avg loss -2.919505 avg loss no lamb -2.919505 time 2020-06-26 01:23:48.650516
Model ind 665 epoch 291 batch: 100 avg loss -2.784506 avg loss no lamb -2.784506 time 2020-06-26 01:23:59.355070
Model ind 665 epoch 291 batch: 200 avg loss -2.736079 avg loss no lamb -2.736079 time 2020-06-26 01:24:10.124227
Model ind 665 epoch 291 batch: 300 avg loss -2.765515 avg loss no lamb -2.765515 time 2020-06-26 01:24:20.725838
Model ind 665 epoch 291 batch: 400 avg loss -2.656513 avg loss no lamb -2.656513 time 2020-06-26 01:24:31.443301
Model ind 665 epoch 291 batch: 500 avg loss -2.804421 avg loss no lamb -2.804421 time 2020-06-26 01:24:42.219552
Model ind 665 epoch 291 batch: 600 avg loss -2.836058 avg loss no lamb -2.836058 time 2020-06-26 01:24:53.134798
Model ind 665 epoch 291 batch: 700 avg loss -2.660003 avg loss no lamb -2.660003 time 2020-06-26 01:25:04.042214
Model ind 665 epoch 291 batch: 800 avg loss -2.695609 avg loss no lamb -2.695609 time 2020-06-26 01:25:14.749314
last batch sz 10
Pre: time 2020-06-26 01:25:28.887623: 
 	std: 0.0027931468
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9836, 0.9826, 0.9772, 0.9835, 0.978]
	train_accs: [0.98315, 0.9819, 0.97748333, 0.9827833, 0.97805]
	best_train_sub_head: 0
	worst: 0.9772
	avg: 0.98098004
	best: 0.9836

Starting e_i: 292
Model ind 665 epoch 292 batch: 0 avg loss -2.851892 avg loss no lamb -2.851892 time 2020-06-26 01:25:29.761636
Model ind 665 epoch 292 batch: 100 avg loss -2.765773 avg loss no lamb -2.765773 time 2020-06-26 01:25:40.253569
Model ind 665 epoch 292 batch: 200 avg loss -2.775390 avg loss no lamb -2.775390 time 2020-06-26 01:25:51.159185
Model ind 665 epoch 292 batch: 300 avg loss -2.768134 avg loss no lamb -2.768134 time 2020-06-26 01:26:02.304766
Model ind 665 epoch 292 batch: 400 avg loss -2.665688 avg loss no lamb -2.665688 time 2020-06-26 01:26:13.145372
Model ind 665 epoch 292 batch: 500 avg loss -2.787104 avg loss no lamb -2.787104 time 2020-06-26 01:26:23.769818
Model ind 665 epoch 292 batch: 600 avg loss -2.770759 avg loss no lamb -2.770759 time 2020-06-26 01:26:34.407356
Model ind 665 epoch 292 batch: 700 avg loss -2.603528 avg loss no lamb -2.603528 time 2020-06-26 01:26:45.266747
Model ind 665 epoch 292 batch: 800 avg loss -2.747855 avg loss no lamb -2.747855 time 2020-06-26 01:26:56.207229
last batch sz 10
Pre: time 2020-06-26 01:27:10.190370: 
 	std: 0.002778774
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9814, 0.9753, 0.9821, 0.9771]
	train_accs: [0.9816667, 0.98141664, 0.9758833, 0.9819667, 0.97723335]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97952
	best: 0.9821

Starting e_i: 293
Model ind 665 epoch 293 batch: 0 avg loss -2.904882 avg loss no lamb -2.904882 time 2020-06-26 01:27:10.945272
Model ind 665 epoch 293 batch: 100 avg loss -2.844012 avg loss no lamb -2.844012 time 2020-06-26 01:27:21.382827
Model ind 665 epoch 293 batch: 200 avg loss -2.822154 avg loss no lamb -2.822154 time 2020-06-26 01:27:32.315730
Model ind 665 epoch 293 batch: 300 avg loss -2.836535 avg loss no lamb -2.836535 time 2020-06-26 01:27:43.197820
Model ind 665 epoch 293 batch: 400 avg loss -2.738698 avg loss no lamb -2.738698 time 2020-06-26 01:27:53.861148
Model ind 665 epoch 293 batch: 500 avg loss -2.647196 avg loss no lamb -2.647196 time 2020-06-26 01:28:04.491063
Model ind 665 epoch 293 batch: 600 avg loss -2.809465 avg loss no lamb -2.809465 time 2020-06-26 01:28:15.363829
Model ind 665 epoch 293 batch: 700 avg loss -2.567643 avg loss no lamb -2.567643 time 2020-06-26 01:28:25.957282
Model ind 665 epoch 293 batch: 800 avg loss -2.725546 avg loss no lamb -2.725546 time 2020-06-26 01:28:36.564448
last batch sz 10
Pre: time 2020-06-26 01:28:50.739431: 
 	std: 0.0032338286
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9801, 0.974, 0.9817, 0.9753]
	train_accs: [0.98191667, 0.98078334, 0.9762667, 0.98181665, 0.97655]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97852004
	best: 0.9815

Starting e_i: 294
Model ind 665 epoch 294 batch: 0 avg loss -2.930673 avg loss no lamb -2.930673 time 2020-06-26 01:28:51.521943
Model ind 665 epoch 294 batch: 100 avg loss -2.757399 avg loss no lamb -2.757399 time 2020-06-26 01:29:02.404210
Model ind 665 epoch 294 batch: 200 avg loss -2.810004 avg loss no lamb -2.810004 time 2020-06-26 01:29:13.319442
Model ind 665 epoch 294 batch: 300 avg loss -2.801669 avg loss no lamb -2.801669 time 2020-06-26 01:29:24.165385
Model ind 665 epoch 294 batch: 400 avg loss -2.659113 avg loss no lamb -2.659113 time 2020-06-26 01:29:34.795097
Model ind 665 epoch 294 batch: 500 avg loss -2.785137 avg loss no lamb -2.785137 time 2020-06-26 01:29:45.455408
Model ind 665 epoch 294 batch: 600 avg loss -2.794170 avg loss no lamb -2.794170 time 2020-06-26 01:29:56.048290
Model ind 665 epoch 294 batch: 700 avg loss -2.661378 avg loss no lamb -2.661378 time 2020-06-26 01:30:06.958574
Model ind 665 epoch 294 batch: 800 avg loss -2.760808 avg loss no lamb -2.760808 time 2020-06-26 01:30:17.577170
last batch sz 10
Pre: time 2020-06-26 01:30:31.320040: 
 	std: 0.0030786982
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.982, 0.9752, 0.9819, 0.9764]
	train_accs: [0.98235, 0.98151666, 0.9758, 0.98221666, 0.9773167]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97954
	best: 0.9822

Starting e_i: 295
Model ind 665 epoch 295 batch: 0 avg loss -2.904391 avg loss no lamb -2.904391 time 2020-06-26 01:30:32.136905
Model ind 665 epoch 295 batch: 100 avg loss -2.746542 avg loss no lamb -2.746542 time 2020-06-26 01:30:42.945290
Model ind 665 epoch 295 batch: 200 avg loss -2.858567 avg loss no lamb -2.858567 time 2020-06-26 01:30:53.805357
Model ind 665 epoch 295 batch: 300 avg loss -2.806813 avg loss no lamb -2.806813 time 2020-06-26 01:31:04.571685
Model ind 665 epoch 295 batch: 400 avg loss -2.678016 avg loss no lamb -2.678016 time 2020-06-26 01:31:15.596551
Model ind 665 epoch 295 batch: 500 avg loss -2.729020 avg loss no lamb -2.729020 time 2020-06-26 01:31:26.349640
Model ind 665 epoch 295 batch: 600 avg loss -2.821004 avg loss no lamb -2.821004 time 2020-06-26 01:31:36.914403
Model ind 665 epoch 295 batch: 700 avg loss -2.655875 avg loss no lamb -2.655875 time 2020-06-26 01:31:47.383392
Model ind 665 epoch 295 batch: 800 avg loss -2.793582 avg loss no lamb -2.793582 time 2020-06-26 01:31:58.448703
last batch sz 10
Pre: time 2020-06-26 01:32:12.265840: 
 	std: 0.003401766
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9802, 0.9737, 0.98, 0.9728]
	train_accs: [0.98148334, 0.98088336, 0.97595, 0.98125, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.9774
	best: 0.9803

Starting e_i: 296
Model ind 665 epoch 296 batch: 0 avg loss -2.887566 avg loss no lamb -2.887566 time 2020-06-26 01:32:13.053592
Model ind 665 epoch 296 batch: 100 avg loss -2.818267 avg loss no lamb -2.818267 time 2020-06-26 01:32:23.606313
Model ind 665 epoch 296 batch: 200 avg loss -2.837165 avg loss no lamb -2.837165 time 2020-06-26 01:32:34.297148
Model ind 665 epoch 296 batch: 300 avg loss -2.802383 avg loss no lamb -2.802383 time 2020-06-26 01:32:44.833941
Model ind 665 epoch 296 batch: 400 avg loss -2.680132 avg loss no lamb -2.680132 time 2020-06-26 01:32:55.430379
Model ind 665 epoch 296 batch: 500 avg loss -2.674996 avg loss no lamb -2.674996 time 2020-06-26 01:33:06.342059
Model ind 665 epoch 296 batch: 600 avg loss -2.768121 avg loss no lamb -2.768121 time 2020-06-26 01:33:17.082165
Model ind 665 epoch 296 batch: 700 avg loss -2.601190 avg loss no lamb -2.601190 time 2020-06-26 01:33:27.903156
Model ind 665 epoch 296 batch: 800 avg loss -2.766266 avg loss no lamb -2.766266 time 2020-06-26 01:33:38.823589
last batch sz 10
Pre: time 2020-06-26 01:33:52.818515: 
 	std: 0.0034621442
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9814, 0.9735, 0.9802, 0.974]
	train_accs: [0.9820167, 0.98145, 0.97525, 0.9816333, 0.97576666]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97796
	best: 0.9807

Starting e_i: 297
Model ind 665 epoch 297 batch: 0 avg loss -2.895164 avg loss no lamb -2.895164 time 2020-06-26 01:33:53.624066
Model ind 665 epoch 297 batch: 100 avg loss -2.832345 avg loss no lamb -2.832345 time 2020-06-26 01:34:04.472984
Model ind 665 epoch 297 batch: 200 avg loss -2.824362 avg loss no lamb -2.824362 time 2020-06-26 01:34:14.859672
Model ind 665 epoch 297 batch: 300 avg loss -2.797052 avg loss no lamb -2.797052 time 2020-06-26 01:34:25.455012
Model ind 665 epoch 297 batch: 400 avg loss -2.734354 avg loss no lamb -2.734354 time 2020-06-26 01:34:36.479861
Model ind 665 epoch 297 batch: 500 avg loss -2.753045 avg loss no lamb -2.753045 time 2020-06-26 01:34:47.397370
Model ind 665 epoch 297 batch: 600 avg loss -2.810289 avg loss no lamb -2.810289 time 2020-06-26 01:34:57.922591
Model ind 665 epoch 297 batch: 700 avg loss -2.716313 avg loss no lamb -2.716313 time 2020-06-26 01:35:08.741144
Model ind 665 epoch 297 batch: 800 avg loss -2.778543 avg loss no lamb -2.778543 time 2020-06-26 01:35:19.310734
last batch sz 10
Pre: time 2020-06-26 01:35:32.967378: 
 	std: 0.0038793893
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.984, 0.9832, 0.9747, 0.9834, 0.9768]
	train_accs: [0.98296666, 0.98216665, 0.97636664, 0.98245, 0.97745]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.98042
	best: 0.984

Starting e_i: 298
Model ind 665 epoch 298 batch: 0 avg loss -2.906677 avg loss no lamb -2.906677 time 2020-06-26 01:35:33.847740
Model ind 665 epoch 298 batch: 100 avg loss -2.764544 avg loss no lamb -2.764544 time 2020-06-26 01:35:44.650355
Model ind 665 epoch 298 batch: 200 avg loss -2.758616 avg loss no lamb -2.758616 time 2020-06-26 01:35:55.304418
Model ind 665 epoch 298 batch: 300 avg loss -2.818918 avg loss no lamb -2.818918 time 2020-06-26 01:36:06.070873
Model ind 665 epoch 298 batch: 400 avg loss -2.686231 avg loss no lamb -2.686231 time 2020-06-26 01:36:16.502338
Model ind 665 epoch 298 batch: 500 avg loss -2.787080 avg loss no lamb -2.787080 time 2020-06-26 01:36:27.183997
Model ind 665 epoch 298 batch: 600 avg loss -2.778505 avg loss no lamb -2.778505 time 2020-06-26 01:36:37.840914
Model ind 665 epoch 298 batch: 700 avg loss -2.662392 avg loss no lamb -2.662392 time 2020-06-26 01:36:48.494546
Model ind 665 epoch 298 batch: 800 avg loss -2.741120 avg loss no lamb -2.741120 time 2020-06-26 01:36:59.163209
last batch sz 10
Pre: time 2020-06-26 01:37:12.838504: 
 	std: 0.0036935674
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9818, 0.9744, 0.9829, 0.9753]
	train_accs: [0.9823167, 0.9813167, 0.9759167, 0.98253334, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97933996
	best: 0.9829

Starting e_i: 299
Model ind 665 epoch 299 batch: 0 avg loss -2.896403 avg loss no lamb -2.896403 time 2020-06-26 01:37:13.630593
Model ind 665 epoch 299 batch: 100 avg loss -2.804991 avg loss no lamb -2.804991 time 2020-06-26 01:37:25.029155
Model ind 665 epoch 299 batch: 200 avg loss -2.750361 avg loss no lamb -2.750361 time 2020-06-26 01:37:35.752847
Model ind 665 epoch 299 batch: 300 avg loss -2.818109 avg loss no lamb -2.818109 time 2020-06-26 01:37:46.607838
Model ind 665 epoch 299 batch: 400 avg loss -2.698269 avg loss no lamb -2.698269 time 2020-06-26 01:37:57.429285
Model ind 665 epoch 299 batch: 500 avg loss -2.709995 avg loss no lamb -2.709995 time 2020-06-26 01:38:08.289290
Model ind 665 epoch 299 batch: 600 avg loss -2.790704 avg loss no lamb -2.790704 time 2020-06-26 01:38:18.932309
Model ind 665 epoch 299 batch: 700 avg loss -2.682903 avg loss no lamb -2.682903 time 2020-06-26 01:38:29.561957
Model ind 665 epoch 299 batch: 800 avg loss -2.805450 avg loss no lamb -2.805450 time 2020-06-26 01:38:40.225326
last batch sz 10
Pre: time 2020-06-26 01:38:53.813832: 
 	std: 0.0033136164
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.981, 0.9741, 0.9807, 0.9744]
	train_accs: [0.9819833, 0.98106664, 0.9755667, 0.9816167, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.9783
	best: 0.9813

Starting e_i: 300
Model ind 665 epoch 300 batch: 0 avg loss -2.917456 avg loss no lamb -2.917456 time 2020-06-26 01:38:54.603273
Model ind 665 epoch 300 batch: 100 avg loss -2.845306 avg loss no lamb -2.845306 time 2020-06-26 01:39:05.403398
Model ind 665 epoch 300 batch: 200 avg loss -2.763895 avg loss no lamb -2.763895 time 2020-06-26 01:39:16.024869
Model ind 665 epoch 300 batch: 300 avg loss -2.793762 avg loss no lamb -2.793762 time 2020-06-26 01:39:26.589070
Model ind 665 epoch 300 batch: 400 avg loss -2.616777 avg loss no lamb -2.616777 time 2020-06-26 01:39:37.284822
Model ind 665 epoch 300 batch: 500 avg loss -2.738566 avg loss no lamb -2.738566 time 2020-06-26 01:39:47.925944
Model ind 665 epoch 300 batch: 600 avg loss -2.881543 avg loss no lamb -2.881543 time 2020-06-26 01:39:58.874045
Model ind 665 epoch 300 batch: 700 avg loss -2.595439 avg loss no lamb -2.595439 time 2020-06-26 01:40:09.529969
Model ind 665 epoch 300 batch: 800 avg loss -2.757511 avg loss no lamb -2.757511 time 2020-06-26 01:40:20.185044
last batch sz 10
Pre: time 2020-06-26 01:40:33.991218: 
 	std: 0.0032473989
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9821, 0.9748, 0.9824, 0.977]
	train_accs: [0.9824167, 0.9816333, 0.97571665, 0.9820167, 0.9769833]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97978
	best: 0.9826

Starting e_i: 301
Model ind 665 epoch 301 batch: 0 avg loss -2.938430 avg loss no lamb -2.938430 time 2020-06-26 01:40:36.049321
Model ind 665 epoch 301 batch: 100 avg loss -2.762423 avg loss no lamb -2.762423 time 2020-06-26 01:40:46.935534
Model ind 665 epoch 301 batch: 200 avg loss -2.789291 avg loss no lamb -2.789291 time 2020-06-26 01:40:57.640584
Model ind 665 epoch 301 batch: 300 avg loss -2.794088 avg loss no lamb -2.794088 time 2020-06-26 01:41:08.418511
Model ind 665 epoch 301 batch: 400 avg loss -2.660672 avg loss no lamb -2.660672 time 2020-06-26 01:41:19.379438
Model ind 665 epoch 301 batch: 500 avg loss -2.744830 avg loss no lamb -2.744830 time 2020-06-26 01:41:29.956583
Model ind 665 epoch 301 batch: 600 avg loss -2.803932 avg loss no lamb -2.803932 time 2020-06-26 01:41:40.803332
Model ind 665 epoch 301 batch: 700 avg loss -2.657139 avg loss no lamb -2.657139 time 2020-06-26 01:41:51.570050
Model ind 665 epoch 301 batch: 800 avg loss -2.774507 avg loss no lamb -2.774507 time 2020-06-26 01:42:02.550741
last batch sz 10
Pre: time 2020-06-26 01:42:16.429901: 
 	std: 0.0034521748
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9812, 0.9728, 0.9812, 0.9757]
	train_accs: [0.98135, 0.98165, 0.9757, 0.98188335, 0.9771]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97832
	best: 0.9812

Starting e_i: 302
Model ind 665 epoch 302 batch: 0 avg loss -2.876389 avg loss no lamb -2.876389 time 2020-06-26 01:42:17.257738
Model ind 665 epoch 302 batch: 100 avg loss -2.804050 avg loss no lamb -2.804050 time 2020-06-26 01:42:27.941671
Model ind 665 epoch 302 batch: 200 avg loss -2.803255 avg loss no lamb -2.803255 time 2020-06-26 01:42:38.584046
Model ind 665 epoch 302 batch: 300 avg loss -2.839445 avg loss no lamb -2.839445 time 2020-06-26 01:42:49.368769
Model ind 665 epoch 302 batch: 400 avg loss -2.660693 avg loss no lamb -2.660693 time 2020-06-26 01:43:00.342048
Model ind 665 epoch 302 batch: 500 avg loss -2.658664 avg loss no lamb -2.658664 time 2020-06-26 01:43:11.062864
Model ind 665 epoch 302 batch: 600 avg loss -2.794501 avg loss no lamb -2.794501 time 2020-06-26 01:43:21.777212
Model ind 665 epoch 302 batch: 700 avg loss -2.727784 avg loss no lamb -2.727784 time 2020-06-26 01:43:32.521171
Model ind 665 epoch 302 batch: 800 avg loss -2.738146 avg loss no lamb -2.738146 time 2020-06-26 01:43:43.290414
last batch sz 10
Pre: time 2020-06-26 01:43:57.465896: 
 	std: 0.0028280085
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9816, 0.9754, 0.9824, 0.9773]
	train_accs: [0.98195, 0.98118335, 0.97625, 0.9819833, 0.9770167]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97972
	best: 0.9824

Starting e_i: 303
Model ind 665 epoch 303 batch: 0 avg loss -2.910059 avg loss no lamb -2.910059 time 2020-06-26 01:43:58.265338
Model ind 665 epoch 303 batch: 100 avg loss -2.797915 avg loss no lamb -2.797915 time 2020-06-26 01:44:09.080245
Model ind 665 epoch 303 batch: 200 avg loss -2.795391 avg loss no lamb -2.795391 time 2020-06-26 01:44:19.764600
Model ind 665 epoch 303 batch: 300 avg loss -2.819223 avg loss no lamb -2.819223 time 2020-06-26 01:44:30.801236
Model ind 665 epoch 303 batch: 400 avg loss -2.663756 avg loss no lamb -2.663756 time 2020-06-26 01:44:41.552388
Model ind 665 epoch 303 batch: 500 avg loss -2.781067 avg loss no lamb -2.781067 time 2020-06-26 01:44:52.224500
Model ind 665 epoch 303 batch: 600 avg loss -2.794651 avg loss no lamb -2.794651 time 2020-06-26 01:45:02.855095
Model ind 665 epoch 303 batch: 700 avg loss -2.603299 avg loss no lamb -2.603299 time 2020-06-26 01:45:13.541275
Model ind 665 epoch 303 batch: 800 avg loss -2.747317 avg loss no lamb -2.747317 time 2020-06-26 01:45:24.401270
last batch sz 10
Pre: time 2020-06-26 01:45:38.252869: 
 	std: 0.0029328489
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9817, 0.9757, 0.9822, 0.9771]
	train_accs: [0.98263335, 0.9816333, 0.9766833, 0.9824833, 0.97711664]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97992
	best: 0.9829

Starting e_i: 304
Model ind 665 epoch 304 batch: 0 avg loss -2.888984 avg loss no lamb -2.888984 time 2020-06-26 01:45:39.118465
Model ind 665 epoch 304 batch: 100 avg loss -2.783546 avg loss no lamb -2.783546 time 2020-06-26 01:45:49.948854
Model ind 665 epoch 304 batch: 200 avg loss -2.794079 avg loss no lamb -2.794079 time 2020-06-26 01:46:00.768131
Model ind 665 epoch 304 batch: 300 avg loss -2.792267 avg loss no lamb -2.792267 time 2020-06-26 01:46:11.438380
Model ind 665 epoch 304 batch: 400 avg loss -2.703052 avg loss no lamb -2.703052 time 2020-06-26 01:46:22.136920
Model ind 665 epoch 304 batch: 500 avg loss -2.711976 avg loss no lamb -2.711976 time 2020-06-26 01:46:32.870939
Model ind 665 epoch 304 batch: 600 avg loss -2.844874 avg loss no lamb -2.844874 time 2020-06-26 01:46:43.585223
Model ind 665 epoch 304 batch: 700 avg loss -2.619718 avg loss no lamb -2.619718 time 2020-06-26 01:46:54.271459
Model ind 665 epoch 304 batch: 800 avg loss -2.749918 avg loss no lamb -2.749918 time 2020-06-26 01:47:04.995596
last batch sz 10
Pre: time 2020-06-26 01:47:18.814049: 
 	std: 0.0041049314
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9818, 0.9733, 0.9821, 0.9742]
	train_accs: [0.9828333, 0.98215, 0.97568333, 0.9824333, 0.97655]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97875994
	best: 0.9824

Starting e_i: 305
Model ind 665 epoch 305 batch: 0 avg loss -2.883934 avg loss no lamb -2.883934 time 2020-06-26 01:47:19.606803
Model ind 665 epoch 305 batch: 100 avg loss -2.736734 avg loss no lamb -2.736734 time 2020-06-26 01:47:30.384380
Model ind 665 epoch 305 batch: 200 avg loss -2.804770 avg loss no lamb -2.804770 time 2020-06-26 01:47:41.021661
Model ind 665 epoch 305 batch: 300 avg loss -2.805038 avg loss no lamb -2.805038 time 2020-06-26 01:47:51.774190
Model ind 665 epoch 305 batch: 400 avg loss -2.750105 avg loss no lamb -2.750105 time 2020-06-26 01:48:02.545022
Model ind 665 epoch 305 batch: 500 avg loss -2.777324 avg loss no lamb -2.777324 time 2020-06-26 01:48:13.151651
Model ind 665 epoch 305 batch: 600 avg loss -2.741871 avg loss no lamb -2.741871 time 2020-06-26 01:48:24.088325
Model ind 665 epoch 305 batch: 700 avg loss -2.685304 avg loss no lamb -2.685304 time 2020-06-26 01:48:34.962594
Model ind 665 epoch 305 batch: 800 avg loss -2.756227 avg loss no lamb -2.756227 time 2020-06-26 01:48:45.831547
last batch sz 10
Pre: time 2020-06-26 01:48:59.534631: 
 	std: 0.0029223117
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9809, 0.975, 0.9826, 0.9777]
	train_accs: [0.98265, 0.98183334, 0.97611666, 0.9828333, 0.9775]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97969997
	best: 0.9826

Starting e_i: 306
Model ind 665 epoch 306 batch: 0 avg loss -2.915197 avg loss no lamb -2.915197 time 2020-06-26 01:49:00.334612
Model ind 665 epoch 306 batch: 100 avg loss -2.810250 avg loss no lamb -2.810250 time 2020-06-26 01:49:10.966621
Model ind 665 epoch 306 batch: 200 avg loss -2.769562 avg loss no lamb -2.769562 time 2020-06-26 01:49:21.543209
Model ind 665 epoch 306 batch: 300 avg loss -2.835098 avg loss no lamb -2.835098 time 2020-06-26 01:49:32.217142
Model ind 665 epoch 306 batch: 400 avg loss -2.706789 avg loss no lamb -2.706789 time 2020-06-26 01:49:42.852487
Model ind 665 epoch 306 batch: 500 avg loss -2.784650 avg loss no lamb -2.784650 time 2020-06-26 01:49:53.754186
Model ind 665 epoch 306 batch: 600 avg loss -2.829164 avg loss no lamb -2.829164 time 2020-06-26 01:50:04.447795
Model ind 665 epoch 306 batch: 700 avg loss -2.654643 avg loss no lamb -2.654643 time 2020-06-26 01:50:15.097475
Model ind 665 epoch 306 batch: 800 avg loss -2.701113 avg loss no lamb -2.701113 time 2020-06-26 01:50:25.715376
last batch sz 10
Pre: time 2020-06-26 01:50:39.433993: 
 	std: 0.0030314403
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.982, 0.9748, 0.9813, 0.9759]
	train_accs: [0.98216665, 0.98186666, 0.97573334, 0.98188335, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97902
	best: 0.9811

Starting e_i: 307
Model ind 665 epoch 307 batch: 0 avg loss -2.907562 avg loss no lamb -2.907562 time 2020-06-26 01:50:40.275609
Model ind 665 epoch 307 batch: 100 avg loss -2.750248 avg loss no lamb -2.750248 time 2020-06-26 01:50:50.907637
Model ind 665 epoch 307 batch: 200 avg loss -2.778419 avg loss no lamb -2.778419 time 2020-06-26 01:51:01.592633
Model ind 665 epoch 307 batch: 300 avg loss -2.776646 avg loss no lamb -2.776646 time 2020-06-26 01:51:12.276856
Model ind 665 epoch 307 batch: 400 avg loss -2.691628 avg loss no lamb -2.691628 time 2020-06-26 01:51:22.769971
Model ind 665 epoch 307 batch: 500 avg loss -2.738568 avg loss no lamb -2.738568 time 2020-06-26 01:51:33.485567
Model ind 665 epoch 307 batch: 600 avg loss -2.790597 avg loss no lamb -2.790597 time 2020-06-26 01:51:44.167401
Model ind 665 epoch 307 batch: 700 avg loss -2.692936 avg loss no lamb -2.692936 time 2020-06-26 01:51:54.804606
Model ind 665 epoch 307 batch: 800 avg loss -2.819813 avg loss no lamb -2.819813 time 2020-06-26 01:52:05.606087
last batch sz 10
Pre: time 2020-06-26 01:52:19.145396: 
 	std: 0.0033462837
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.98, 0.9729, 0.9809, 0.9746]
	train_accs: [0.9816833, 0.98088336, 0.97583336, 0.9820167, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97778
	best: 0.9809

Starting e_i: 308
Model ind 665 epoch 308 batch: 0 avg loss -2.887478 avg loss no lamb -2.887478 time 2020-06-26 01:52:19.907089
Model ind 665 epoch 308 batch: 100 avg loss -2.864309 avg loss no lamb -2.864309 time 2020-06-26 01:52:30.557874
Model ind 665 epoch 308 batch: 200 avg loss -2.815308 avg loss no lamb -2.815308 time 2020-06-26 01:52:41.236139
Model ind 665 epoch 308 batch: 300 avg loss -2.845115 avg loss no lamb -2.845115 time 2020-06-26 01:52:51.950328
Model ind 665 epoch 308 batch: 400 avg loss -2.675737 avg loss no lamb -2.675737 time 2020-06-26 01:53:02.565943
Model ind 665 epoch 308 batch: 500 avg loss -2.787306 avg loss no lamb -2.787306 time 2020-06-26 01:53:13.243647
Model ind 665 epoch 308 batch: 600 avg loss -2.783584 avg loss no lamb -2.783584 time 2020-06-26 01:53:23.950351
Model ind 665 epoch 308 batch: 700 avg loss -2.647893 avg loss no lamb -2.647893 time 2020-06-26 01:53:34.684633
Model ind 665 epoch 308 batch: 800 avg loss -2.758216 avg loss no lamb -2.758216 time 2020-06-26 01:53:45.577738
last batch sz 10
Pre: time 2020-06-26 01:53:59.530345: 
 	std: 0.0020788456
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9804, 0.976, 0.9809, 0.9766]
	train_accs: [0.98178333, 0.98145, 0.9774167, 0.98185, 0.97815]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.97882
	best: 0.9809

Starting e_i: 309
Model ind 665 epoch 309 batch: 0 avg loss -2.929193 avg loss no lamb -2.929193 time 2020-06-26 01:54:00.354017
Model ind 665 epoch 309 batch: 100 avg loss -2.842824 avg loss no lamb -2.842824 time 2020-06-26 01:54:11.170195
Model ind 665 epoch 309 batch: 200 avg loss -2.789792 avg loss no lamb -2.789792 time 2020-06-26 01:54:21.968469
Model ind 665 epoch 309 batch: 300 avg loss -2.785687 avg loss no lamb -2.785687 time 2020-06-26 01:54:32.794394
Model ind 665 epoch 309 batch: 400 avg loss -2.654805 avg loss no lamb -2.654805 time 2020-06-26 01:54:43.515220
Model ind 665 epoch 309 batch: 500 avg loss -2.701486 avg loss no lamb -2.701486 time 2020-06-26 01:54:54.101971
Model ind 665 epoch 309 batch: 600 avg loss -2.818039 avg loss no lamb -2.818039 time 2020-06-26 01:55:04.872983
Model ind 665 epoch 309 batch: 700 avg loss -2.642413 avg loss no lamb -2.642413 time 2020-06-26 01:55:15.360130
Model ind 665 epoch 309 batch: 800 avg loss -2.786293 avg loss no lamb -2.786293 time 2020-06-26 01:55:25.934526
last batch sz 10
Pre: time 2020-06-26 01:55:39.544822: 
 	std: 0.0025995444
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9808, 0.9752, 0.9815, 0.9765]
	train_accs: [0.9813667, 0.98076665, 0.9763167, 0.98165, 0.97676665]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97897995
	best: 0.9815

Starting e_i: 310
Model ind 665 epoch 310 batch: 0 avg loss -2.888541 avg loss no lamb -2.888541 time 2020-06-26 01:55:40.386044
Model ind 665 epoch 310 batch: 100 avg loss -2.764188 avg loss no lamb -2.764188 time 2020-06-26 01:55:51.191018
Model ind 665 epoch 310 batch: 200 avg loss -2.748456 avg loss no lamb -2.748456 time 2020-06-26 01:56:02.056935
Model ind 665 epoch 310 batch: 300 avg loss -2.861868 avg loss no lamb -2.861868 time 2020-06-26 01:56:12.624340
Model ind 665 epoch 310 batch: 400 avg loss -2.718958 avg loss no lamb -2.718958 time 2020-06-26 01:56:23.240411
Model ind 665 epoch 310 batch: 500 avg loss -2.712433 avg loss no lamb -2.712433 time 2020-06-26 01:56:33.886091
Model ind 665 epoch 310 batch: 600 avg loss -2.788529 avg loss no lamb -2.788529 time 2020-06-26 01:56:44.361365
Model ind 665 epoch 310 batch: 700 avg loss -2.586811 avg loss no lamb -2.586811 time 2020-06-26 01:56:55.070219
Model ind 665 epoch 310 batch: 800 avg loss -2.778050 avg loss no lamb -2.778050 time 2020-06-26 01:57:05.684088
last batch sz 10
Pre: time 2020-06-26 01:57:19.340616: 
 	std: 0.0034301218
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9814, 0.9747, 0.9825, 0.9754]
	train_accs: [0.9829, 0.9817333, 0.97665, 0.9827, 0.9771]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97922003
	best: 0.9821

Starting e_i: 311
Model ind 665 epoch 311 batch: 0 avg loss -2.937890 avg loss no lamb -2.937890 time 2020-06-26 01:57:21.243722
Model ind 665 epoch 311 batch: 100 avg loss -2.782192 avg loss no lamb -2.782192 time 2020-06-26 01:57:32.044102
Model ind 665 epoch 311 batch: 200 avg loss -2.719841 avg loss no lamb -2.719841 time 2020-06-26 01:57:42.860955
Model ind 665 epoch 311 batch: 300 avg loss -2.819745 avg loss no lamb -2.819745 time 2020-06-26 01:57:53.430722
Model ind 665 epoch 311 batch: 400 avg loss -2.652507 avg loss no lamb -2.652507 time 2020-06-26 01:58:04.039262
Model ind 665 epoch 311 batch: 500 avg loss -2.753265 avg loss no lamb -2.753265 time 2020-06-26 01:58:14.766975
Model ind 665 epoch 311 batch: 600 avg loss -2.885334 avg loss no lamb -2.885334 time 2020-06-26 01:58:25.482845
Model ind 665 epoch 311 batch: 700 avg loss -2.694736 avg loss no lamb -2.694736 time 2020-06-26 01:58:36.159327
Model ind 665 epoch 311 batch: 800 avg loss -2.740014 avg loss no lamb -2.740014 time 2020-06-26 01:58:46.941788
last batch sz 10
Pre: time 2020-06-26 01:59:00.991531: 
 	std: 0.0028315333
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9822, 0.9757, 0.982, 0.977]
	train_accs: [0.98268336, 0.9823167, 0.97681665, 0.9828167, 0.9780667]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97978
	best: 0.982

Starting e_i: 312
Model ind 665 epoch 312 batch: 0 avg loss -2.857025 avg loss no lamb -2.857025 time 2020-06-26 01:59:01.794395
Model ind 665 epoch 312 batch: 100 avg loss -2.845238 avg loss no lamb -2.845238 time 2020-06-26 01:59:12.365127
Model ind 665 epoch 312 batch: 200 avg loss -2.849033 avg loss no lamb -2.849033 time 2020-06-26 01:59:22.988107
Model ind 665 epoch 312 batch: 300 avg loss -2.845776 avg loss no lamb -2.845776 time 2020-06-26 01:59:33.538988
Model ind 665 epoch 312 batch: 400 avg loss -2.691147 avg loss no lamb -2.691147 time 2020-06-26 01:59:44.071640
Model ind 665 epoch 312 batch: 500 avg loss -2.742786 avg loss no lamb -2.742786 time 2020-06-26 01:59:54.674587
Model ind 665 epoch 312 batch: 600 avg loss -2.801318 avg loss no lamb -2.801318 time 2020-06-26 02:00:05.361692
Model ind 665 epoch 312 batch: 700 avg loss -2.582948 avg loss no lamb -2.582948 time 2020-06-26 02:00:15.915588
Model ind 665 epoch 312 batch: 800 avg loss -2.794065 avg loss no lamb -2.794065 time 2020-06-26 02:00:26.655258
last batch sz 10
Pre: time 2020-06-26 02:00:40.652482: 
 	std: 0.0026522386
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9806, 0.9764, 0.982, 0.9757]
	train_accs: [0.9819667, 0.98118335, 0.9769833, 0.9824, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97923994
	best: 0.982

Starting e_i: 313
Model ind 665 epoch 313 batch: 0 avg loss -2.925687 avg loss no lamb -2.925687 time 2020-06-26 02:00:41.498989
Model ind 665 epoch 313 batch: 100 avg loss -2.746345 avg loss no lamb -2.746345 time 2020-06-26 02:00:52.161729
Model ind 665 epoch 313 batch: 200 avg loss -2.886046 avg loss no lamb -2.886046 time 2020-06-26 02:01:02.754028
Model ind 665 epoch 313 batch: 300 avg loss -2.891290 avg loss no lamb -2.891290 time 2020-06-26 02:01:13.450831
Model ind 665 epoch 313 batch: 400 avg loss -2.720050 avg loss no lamb -2.720050 time 2020-06-26 02:01:24.049701
Model ind 665 epoch 313 batch: 500 avg loss -2.791845 avg loss no lamb -2.791845 time 2020-06-26 02:01:34.735864
Model ind 665 epoch 313 batch: 600 avg loss -2.799153 avg loss no lamb -2.799153 time 2020-06-26 02:01:45.286052
Model ind 665 epoch 313 batch: 700 avg loss -2.636821 avg loss no lamb -2.636821 time 2020-06-26 02:01:56.050999
Model ind 665 epoch 313 batch: 800 avg loss -2.858561 avg loss no lamb -2.858561 time 2020-06-26 02:02:07.194621
last batch sz 10
Pre: time 2020-06-26 02:02:21.129926: 
 	std: 0.0029990652
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9823, 0.9757, 0.9822, 0.9763]
	train_accs: [0.9823, 0.9813667, 0.97728336, 0.9824333, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97966003
	best: 0.9822

Starting e_i: 314
Model ind 665 epoch 314 batch: 0 avg loss -2.909581 avg loss no lamb -2.909581 time 2020-06-26 02:02:21.910058
Model ind 665 epoch 314 batch: 100 avg loss -2.776679 avg loss no lamb -2.776679 time 2020-06-26 02:02:32.648384
Model ind 665 epoch 314 batch: 200 avg loss -2.821026 avg loss no lamb -2.821026 time 2020-06-26 02:02:43.362146
Model ind 665 epoch 314 batch: 300 avg loss -2.740189 avg loss no lamb -2.740189 time 2020-06-26 02:02:53.944226
Model ind 665 epoch 314 batch: 400 avg loss -2.687502 avg loss no lamb -2.687502 time 2020-06-26 02:03:04.462112
Model ind 665 epoch 314 batch: 500 avg loss -2.787436 avg loss no lamb -2.787436 time 2020-06-26 02:03:15.096404
Model ind 665 epoch 314 batch: 600 avg loss -2.829273 avg loss no lamb -2.829273 time 2020-06-26 02:03:25.862879
Model ind 665 epoch 314 batch: 700 avg loss -2.641615 avg loss no lamb -2.641615 time 2020-06-26 02:03:36.370410
Model ind 665 epoch 314 batch: 800 avg loss -2.707736 avg loss no lamb -2.707736 time 2020-06-26 02:03:47.130591
last batch sz 10
Pre: time 2020-06-26 02:04:01.208836: 
 	std: 0.0030010538
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9809, 0.9743, 0.9816, 0.9767]
	train_accs: [0.9824, 0.9819667, 0.97718334, 0.98221666, 0.9778]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97904
	best: 0.9817

Starting e_i: 315
Model ind 665 epoch 315 batch: 0 avg loss -2.892624 avg loss no lamb -2.892624 time 2020-06-26 02:04:01.990057
Model ind 665 epoch 315 batch: 100 avg loss -2.808959 avg loss no lamb -2.808959 time 2020-06-26 02:04:12.750160
Model ind 665 epoch 315 batch: 200 avg loss -2.726450 avg loss no lamb -2.726450 time 2020-06-26 02:04:23.438969
Model ind 665 epoch 315 batch: 300 avg loss -2.826939 avg loss no lamb -2.826939 time 2020-06-26 02:04:34.036608
Model ind 665 epoch 315 batch: 400 avg loss -2.687149 avg loss no lamb -2.687149 time 2020-06-26 02:04:44.677550
Model ind 665 epoch 315 batch: 500 avg loss -2.709554 avg loss no lamb -2.709554 time 2020-06-26 02:04:55.388932
Model ind 665 epoch 315 batch: 600 avg loss -2.867022 avg loss no lamb -2.867022 time 2020-06-26 02:05:06.020575
Model ind 665 epoch 315 batch: 700 avg loss -2.685529 avg loss no lamb -2.685529 time 2020-06-26 02:05:16.524848
Model ind 665 epoch 315 batch: 800 avg loss -2.775515 avg loss no lamb -2.775515 time 2020-06-26 02:05:27.268007
last batch sz 10
Pre: time 2020-06-26 02:05:41.086967: 
 	std: 0.0032263896
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9819, 0.9759, 0.9827, 0.976]
	train_accs: [0.98286664, 0.98186666, 0.9769833, 0.98303336, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97988
	best: 0.9827

Starting e_i: 316
Model ind 665 epoch 316 batch: 0 avg loss -2.896916 avg loss no lamb -2.896916 time 2020-06-26 02:05:41.939003
Model ind 665 epoch 316 batch: 100 avg loss -2.797613 avg loss no lamb -2.797613 time 2020-06-26 02:05:52.619653
Model ind 665 epoch 316 batch: 200 avg loss -2.773774 avg loss no lamb -2.773774 time 2020-06-26 02:06:03.163827
Model ind 665 epoch 316 batch: 300 avg loss -2.778657 avg loss no lamb -2.778657 time 2020-06-26 02:06:13.747741
Model ind 665 epoch 316 batch: 400 avg loss -2.710193 avg loss no lamb -2.710193 time 2020-06-26 02:06:24.735820
Model ind 665 epoch 316 batch: 500 avg loss -2.786002 avg loss no lamb -2.786002 time 2020-06-26 02:06:35.456321
Model ind 665 epoch 316 batch: 600 avg loss -2.798763 avg loss no lamb -2.798763 time 2020-06-26 02:06:46.225489
Model ind 665 epoch 316 batch: 700 avg loss -2.584531 avg loss no lamb -2.584531 time 2020-06-26 02:06:56.838914
Model ind 665 epoch 316 batch: 800 avg loss -2.725049 avg loss no lamb -2.725049 time 2020-06-26 02:07:07.629938
last batch sz 10
Pre: time 2020-06-26 02:07:21.401009: 
 	std: 0.002193981
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9831, 0.9821, 0.9779, 0.983, 0.9788]
	train_accs: [0.98298335, 0.9813, 0.9773333, 0.983, 0.97825]
	best_train_sub_head: 3
	worst: 0.9779
	avg: 0.98098004
	best: 0.983

Starting e_i: 317
Model ind 665 epoch 317 batch: 0 avg loss -2.883510 avg loss no lamb -2.883510 time 2020-06-26 02:07:22.224938
Model ind 665 epoch 317 batch: 100 avg loss -2.866377 avg loss no lamb -2.866377 time 2020-06-26 02:07:32.896233
Model ind 665 epoch 317 batch: 200 avg loss -2.806082 avg loss no lamb -2.806082 time 2020-06-26 02:07:43.435710
Model ind 665 epoch 317 batch: 300 avg loss -2.817943 avg loss no lamb -2.817943 time 2020-06-26 02:07:54.158406
Model ind 665 epoch 317 batch: 400 avg loss -2.703819 avg loss no lamb -2.703819 time 2020-06-26 02:08:04.857831
Model ind 665 epoch 317 batch: 500 avg loss -2.665553 avg loss no lamb -2.665553 time 2020-06-26 02:08:15.766612
Model ind 665 epoch 317 batch: 600 avg loss -2.820509 avg loss no lamb -2.820509 time 2020-06-26 02:08:26.371844
Model ind 665 epoch 317 batch: 700 avg loss -2.674415 avg loss no lamb -2.674415 time 2020-06-26 02:08:36.878228
Model ind 665 epoch 317 batch: 800 avg loss -2.726132 avg loss no lamb -2.726132 time 2020-06-26 02:08:47.500779
last batch sz 10
Pre: time 2020-06-26 02:09:01.429857: 
 	std: 0.0030921865
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9831, 0.9828, 0.9759, 0.9831, 0.9777]
	train_accs: [0.98285, 0.98233336, 0.97583336, 0.9828333, 0.9770833]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.98051995
	best: 0.9831

Starting e_i: 318
Model ind 665 epoch 318 batch: 0 avg loss -2.926072 avg loss no lamb -2.926072 time 2020-06-26 02:09:02.230371
Model ind 665 epoch 318 batch: 100 avg loss -2.858011 avg loss no lamb -2.858011 time 2020-06-26 02:09:13.049320
Model ind 665 epoch 318 batch: 200 avg loss -2.835456 avg loss no lamb -2.835456 time 2020-06-26 02:09:23.763466
Model ind 665 epoch 318 batch: 300 avg loss -2.812434 avg loss no lamb -2.812434 time 2020-06-26 02:09:34.431210
Model ind 665 epoch 318 batch: 400 avg loss -2.659151 avg loss no lamb -2.659151 time 2020-06-26 02:09:45.030720
Model ind 665 epoch 318 batch: 500 avg loss -2.698772 avg loss no lamb -2.698772 time 2020-06-26 02:09:55.978200
Model ind 665 epoch 318 batch: 600 avg loss -2.773987 avg loss no lamb -2.773987 time 2020-06-26 02:10:06.690901
Model ind 665 epoch 318 batch: 700 avg loss -2.739977 avg loss no lamb -2.739977 time 2020-06-26 02:10:17.583746
Model ind 665 epoch 318 batch: 800 avg loss -2.752096 avg loss no lamb -2.752096 time 2020-06-26 02:10:28.361556
last batch sz 10
Pre: time 2020-06-26 02:10:42.110825: 
 	std: 0.002707458
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9818, 0.9764, 0.9823, 0.9771]
	train_accs: [0.98226666, 0.9813, 0.9759333, 0.9823167, 0.97705]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.9800401
	best: 0.9823

Starting e_i: 319
Model ind 665 epoch 319 batch: 0 avg loss -2.966046 avg loss no lamb -2.966046 time 2020-06-26 02:10:42.946923
Model ind 665 epoch 319 batch: 100 avg loss -2.838120 avg loss no lamb -2.838120 time 2020-06-26 02:10:54.031032
Model ind 665 epoch 319 batch: 200 avg loss -2.797801 avg loss no lamb -2.797801 time 2020-06-26 02:11:04.967184
Model ind 665 epoch 319 batch: 300 avg loss -2.837893 avg loss no lamb -2.837893 time 2020-06-26 02:11:15.831254
Model ind 665 epoch 319 batch: 400 avg loss -2.682167 avg loss no lamb -2.682167 time 2020-06-26 02:11:26.732451
Model ind 665 epoch 319 batch: 500 avg loss -2.704220 avg loss no lamb -2.704220 time 2020-06-26 02:11:37.436276
Model ind 665 epoch 319 batch: 600 avg loss -2.768348 avg loss no lamb -2.768348 time 2020-06-26 02:11:48.238421
Model ind 665 epoch 319 batch: 700 avg loss -2.609968 avg loss no lamb -2.609968 time 2020-06-26 02:11:59.085868
Model ind 665 epoch 319 batch: 800 avg loss -2.773969 avg loss no lamb -2.773969 time 2020-06-26 02:12:10.039126
last batch sz 10
Pre: time 2020-06-26 02:12:23.993571: 
 	std: 0.003114092
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9824, 0.9753, 0.9823, 0.9765]
	train_accs: [0.98256665, 0.98181665, 0.97585, 0.98256665, 0.9771]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97967994
	best: 0.9819

Starting e_i: 320
Model ind 665 epoch 320 batch: 0 avg loss -2.897830 avg loss no lamb -2.897830 time 2020-06-26 02:12:24.841999
Model ind 665 epoch 320 batch: 100 avg loss -2.897865 avg loss no lamb -2.897865 time 2020-06-26 02:12:35.523706
Model ind 665 epoch 320 batch: 200 avg loss -2.734622 avg loss no lamb -2.734622 time 2020-06-26 02:12:46.253580
Model ind 665 epoch 320 batch: 300 avg loss -2.843173 avg loss no lamb -2.843173 time 2020-06-26 02:12:56.951843
Model ind 665 epoch 320 batch: 400 avg loss -2.675488 avg loss no lamb -2.675488 time 2020-06-26 02:13:07.991973
Model ind 665 epoch 320 batch: 500 avg loss -2.734830 avg loss no lamb -2.734830 time 2020-06-26 02:13:18.706181
Model ind 665 epoch 320 batch: 600 avg loss -2.769152 avg loss no lamb -2.769152 time 2020-06-26 02:13:29.204902
Model ind 665 epoch 320 batch: 700 avg loss -2.699605 avg loss no lamb -2.699605 time 2020-06-26 02:13:39.697357
Model ind 665 epoch 320 batch: 800 avg loss -2.732430 avg loss no lamb -2.732430 time 2020-06-26 02:13:50.594313
last batch sz 10
Pre: time 2020-06-26 02:14:04.590533: 
 	std: 0.0040765735
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9789, 0.9722, 0.9808, 0.9719]
	train_accs: [0.9820667, 0.98005, 0.9748333, 0.98215, 0.97508335]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97696
	best: 0.9808

Starting e_i: 321
Model ind 665 epoch 321 batch: 0 avg loss -2.894600 avg loss no lamb -2.894600 time 2020-06-26 02:14:06.523507
Model ind 665 epoch 321 batch: 100 avg loss -2.757971 avg loss no lamb -2.757971 time 2020-06-26 02:14:17.272425
Model ind 665 epoch 321 batch: 200 avg loss -2.744407 avg loss no lamb -2.744407 time 2020-06-26 02:14:27.744617
Model ind 665 epoch 321 batch: 300 avg loss -2.815289 avg loss no lamb -2.815289 time 2020-06-26 02:14:38.689244
Model ind 665 epoch 321 batch: 400 avg loss -2.662291 avg loss no lamb -2.662291 time 2020-06-26 02:14:49.381263
Model ind 665 epoch 321 batch: 500 avg loss -2.738373 avg loss no lamb -2.738373 time 2020-06-26 02:15:00.461952
Model ind 665 epoch 321 batch: 600 avg loss -2.791356 avg loss no lamb -2.791356 time 2020-06-26 02:15:11.041146
Model ind 665 epoch 321 batch: 700 avg loss -2.618979 avg loss no lamb -2.618979 time 2020-06-26 02:15:21.882929
Model ind 665 epoch 321 batch: 800 avg loss -2.684723 avg loss no lamb -2.684723 time 2020-06-26 02:15:32.607405
last batch sz 10
Pre: time 2020-06-26 02:15:46.431057: 
 	std: 0.003214091
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9809, 0.9745, 0.9818, 0.9756]
	train_accs: [0.98211664, 0.9813167, 0.97601664, 0.98218334, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97894
	best: 0.9818

Starting e_i: 322
Model ind 665 epoch 322 batch: 0 avg loss -2.895730 avg loss no lamb -2.895730 time 2020-06-26 02:15:47.326367
Model ind 665 epoch 322 batch: 100 avg loss -2.819613 avg loss no lamb -2.819613 time 2020-06-26 02:15:57.902374
Model ind 665 epoch 322 batch: 200 avg loss -2.821225 avg loss no lamb -2.821225 time 2020-06-26 02:16:08.834971
Model ind 665 epoch 322 batch: 300 avg loss -2.862381 avg loss no lamb -2.862381 time 2020-06-26 02:16:19.630955
Model ind 665 epoch 322 batch: 400 avg loss -2.740016 avg loss no lamb -2.740016 time 2020-06-26 02:16:30.322634
Model ind 665 epoch 322 batch: 500 avg loss -2.746686 avg loss no lamb -2.746686 time 2020-06-26 02:16:41.247182
Model ind 665 epoch 322 batch: 600 avg loss -2.870930 avg loss no lamb -2.870930 time 2020-06-26 02:16:52.147698
Model ind 665 epoch 322 batch: 700 avg loss -2.604756 avg loss no lamb -2.604756 time 2020-06-26 02:17:02.921176
Model ind 665 epoch 322 batch: 800 avg loss -2.814481 avg loss no lamb -2.814481 time 2020-06-26 02:17:13.703828
last batch sz 10
Pre: time 2020-06-26 02:17:27.566879: 
 	std: 0.0025786664
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9794, 0.9746, 0.981, 0.9761]
	train_accs: [0.98185, 0.9806333, 0.97535, 0.9816667, 0.97683334]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97838
	best: 0.9808

Starting e_i: 323
Model ind 665 epoch 323 batch: 0 avg loss -2.897902 avg loss no lamb -2.897902 time 2020-06-26 02:17:28.370809
Model ind 665 epoch 323 batch: 100 avg loss -2.812753 avg loss no lamb -2.812753 time 2020-06-26 02:17:38.873116
Model ind 665 epoch 323 batch: 200 avg loss -2.748993 avg loss no lamb -2.748993 time 2020-06-26 02:17:49.426528
Model ind 665 epoch 323 batch: 300 avg loss -2.826033 avg loss no lamb -2.826033 time 2020-06-26 02:18:00.358801
Model ind 665 epoch 323 batch: 400 avg loss -2.723320 avg loss no lamb -2.723320 time 2020-06-26 02:18:11.141278
Model ind 665 epoch 323 batch: 500 avg loss -2.698452 avg loss no lamb -2.698452 time 2020-06-26 02:18:22.067366
Model ind 665 epoch 323 batch: 600 avg loss -2.845614 avg loss no lamb -2.845614 time 2020-06-26 02:18:33.031943
Model ind 665 epoch 323 batch: 700 avg loss -2.647513 avg loss no lamb -2.647513 time 2020-06-26 02:18:43.838958
Model ind 665 epoch 323 batch: 800 avg loss -2.774115 avg loss no lamb -2.774115 time 2020-06-26 02:18:54.801710
last batch sz 10
Pre: time 2020-06-26 02:19:08.756200: 
 	std: 0.0028826266
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9823, 0.9767, 0.9834, 0.9778]
	train_accs: [0.9831667, 0.9821, 0.977, 0.98315, 0.97828335]
	best_train_sub_head: 0
	worst: 0.9767
	avg: 0.9807199
	best: 0.9834

Starting e_i: 324
Model ind 665 epoch 324 batch: 0 avg loss -2.910653 avg loss no lamb -2.910653 time 2020-06-26 02:19:09.508769
Model ind 665 epoch 324 batch: 100 avg loss -2.816115 avg loss no lamb -2.816115 time 2020-06-26 02:19:20.077677
Model ind 665 epoch 324 batch: 200 avg loss -2.748652 avg loss no lamb -2.748652 time 2020-06-26 02:19:30.792469
Model ind 665 epoch 324 batch: 300 avg loss -2.872295 avg loss no lamb -2.872295 time 2020-06-26 02:19:41.680722
Model ind 665 epoch 324 batch: 400 avg loss -2.667618 avg loss no lamb -2.667618 time 2020-06-26 02:19:52.441207
Model ind 665 epoch 324 batch: 500 avg loss -2.737456 avg loss no lamb -2.737456 time 2020-06-26 02:20:03.090156
Model ind 665 epoch 324 batch: 600 avg loss -2.822632 avg loss no lamb -2.822632 time 2020-06-26 02:20:13.758456
Model ind 665 epoch 324 batch: 700 avg loss -2.688102 avg loss no lamb -2.688102 time 2020-06-26 02:20:24.367880
Model ind 665 epoch 324 batch: 800 avg loss -2.769830 avg loss no lamb -2.769830 time 2020-06-26 02:20:35.277396
last batch sz 10
Pre: time 2020-06-26 02:20:48.895614: 
 	std: 0.0030026715
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9817, 0.9752, 0.9824, 0.977]
	train_accs: [0.9826, 0.9816833, 0.97613335, 0.9827, 0.9777833]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97969997
	best: 0.9824

Starting e_i: 325
Model ind 665 epoch 325 batch: 0 avg loss -2.937093 avg loss no lamb -2.937093 time 2020-06-26 02:20:49.741732
Model ind 665 epoch 325 batch: 100 avg loss -2.857839 avg loss no lamb -2.857839 time 2020-06-26 02:21:00.198363
Model ind 665 epoch 325 batch: 200 avg loss -2.781135 avg loss no lamb -2.781135 time 2020-06-26 02:21:11.019969
Model ind 665 epoch 325 batch: 300 avg loss -2.771108 avg loss no lamb -2.771108 time 2020-06-26 02:21:21.680140
Model ind 665 epoch 325 batch: 400 avg loss -2.657506 avg loss no lamb -2.657506 time 2020-06-26 02:21:32.371720
Model ind 665 epoch 325 batch: 500 avg loss -2.773255 avg loss no lamb -2.773255 time 2020-06-26 02:21:43.167094
Model ind 665 epoch 325 batch: 600 avg loss -2.817923 avg loss no lamb -2.817923 time 2020-06-26 02:21:53.957707
Model ind 665 epoch 325 batch: 700 avg loss -2.721599 avg loss no lamb -2.721599 time 2020-06-26 02:22:04.723741
Model ind 665 epoch 325 batch: 800 avg loss -2.823313 avg loss no lamb -2.823313 time 2020-06-26 02:22:15.704263
last batch sz 10
Pre: time 2020-06-26 02:22:30.153536: 
 	std: 0.0031622744
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9845, 0.9829, 0.9763, 0.9841, 0.9792]
	train_accs: [0.9831167, 0.98233336, 0.97688335, 0.98303336, 0.97863334]
	best_train_sub_head: 0
	worst: 0.9763
	avg: 0.9814
	best: 0.9845

Starting e_i: 326
Model ind 665 epoch 326 batch: 0 avg loss -2.927806 avg loss no lamb -2.927806 time 2020-06-26 02:22:30.953718
Model ind 665 epoch 326 batch: 100 avg loss -2.853190 avg loss no lamb -2.853190 time 2020-06-26 02:22:41.988527
Model ind 665 epoch 326 batch: 200 avg loss -2.751963 avg loss no lamb -2.751963 time 2020-06-26 02:22:52.497275
Model ind 665 epoch 326 batch: 300 avg loss -2.882687 avg loss no lamb -2.882687 time 2020-06-26 02:23:03.513343
Model ind 665 epoch 326 batch: 400 avg loss -2.718740 avg loss no lamb -2.718740 time 2020-06-26 02:23:14.254883
Model ind 665 epoch 326 batch: 500 avg loss -2.727072 avg loss no lamb -2.727072 time 2020-06-26 02:23:24.846526
Model ind 665 epoch 326 batch: 600 avg loss -2.843636 avg loss no lamb -2.843636 time 2020-06-26 02:23:35.460553
Model ind 665 epoch 326 batch: 700 avg loss -2.578077 avg loss no lamb -2.578077 time 2020-06-26 02:23:46.153806
Model ind 665 epoch 326 batch: 800 avg loss -2.753275 avg loss no lamb -2.753275 time 2020-06-26 02:23:57.053634
last batch sz 10
Pre: time 2020-06-26 02:24:11.043268: 
 	std: 0.0029982617
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9833, 0.9834, 0.9771, 0.9831, 0.9772]
	train_accs: [0.98258334, 0.9821, 0.9770333, 0.98275, 0.9776833]
	best_train_sub_head: 3
	worst: 0.9771
	avg: 0.98082
	best: 0.9831

Starting e_i: 327
Model ind 665 epoch 327 batch: 0 avg loss -2.879389 avg loss no lamb -2.879389 time 2020-06-26 02:24:11.857263
Model ind 665 epoch 327 batch: 100 avg loss -2.841904 avg loss no lamb -2.841904 time 2020-06-26 02:24:22.592037
Model ind 665 epoch 327 batch: 200 avg loss -2.859945 avg loss no lamb -2.859945 time 2020-06-26 02:24:33.200168
Model ind 665 epoch 327 batch: 300 avg loss -2.750117 avg loss no lamb -2.750117 time 2020-06-26 02:24:44.034010
Model ind 665 epoch 327 batch: 400 avg loss -2.640406 avg loss no lamb -2.640406 time 2020-06-26 02:24:54.715748
Model ind 665 epoch 327 batch: 500 avg loss -2.778464 avg loss no lamb -2.778464 time 2020-06-26 02:25:05.536969
Model ind 665 epoch 327 batch: 600 avg loss -2.843782 avg loss no lamb -2.843782 time 2020-06-26 02:25:16.231599
Model ind 665 epoch 327 batch: 700 avg loss -2.611709 avg loss no lamb -2.611709 time 2020-06-26 02:25:26.967209
Model ind 665 epoch 327 batch: 800 avg loss -2.796591 avg loss no lamb -2.796591 time 2020-06-26 02:25:37.934658
last batch sz 10
Pre: time 2020-06-26 02:25:51.915556: 
 	std: 0.0033772087
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9808, 0.9738, 0.9811, 0.9743]
	train_accs: [0.98215, 0.98151666, 0.9755667, 0.9820667, 0.9753]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97818005
	best: 0.9809

Starting e_i: 328
Model ind 665 epoch 328 batch: 0 avg loss -2.903975 avg loss no lamb -2.903975 time 2020-06-26 02:25:52.787372
Model ind 665 epoch 328 batch: 100 avg loss -2.815769 avg loss no lamb -2.815769 time 2020-06-26 02:26:03.803421
Model ind 665 epoch 328 batch: 200 avg loss -2.777152 avg loss no lamb -2.777152 time 2020-06-26 02:26:14.719161
Model ind 665 epoch 328 batch: 300 avg loss -2.783843 avg loss no lamb -2.783843 time 2020-06-26 02:26:25.593522
Model ind 665 epoch 328 batch: 400 avg loss -2.703845 avg loss no lamb -2.703845 time 2020-06-26 02:26:36.838957
Model ind 665 epoch 328 batch: 500 avg loss -2.782989 avg loss no lamb -2.782989 time 2020-06-26 02:26:47.780506
Model ind 665 epoch 328 batch: 600 avg loss -2.780715 avg loss no lamb -2.780715 time 2020-06-26 02:26:58.489820
Model ind 665 epoch 328 batch: 700 avg loss -2.591066 avg loss no lamb -2.591066 time 2020-06-26 02:27:09.395824
Model ind 665 epoch 328 batch: 800 avg loss -2.750473 avg loss no lamb -2.750473 time 2020-06-26 02:27:20.242922
last batch sz 10
Pre: time 2020-06-26 02:27:34.371210: 
 	std: 0.0024895098
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9832, 0.9827, 0.9776, 0.9831, 0.9783]
	train_accs: [0.9826, 0.982, 0.97715, 0.9829, 0.9773333]
	best_train_sub_head: 3
	worst: 0.9776
	avg: 0.98098004
	best: 0.9831

Starting e_i: 329
Model ind 665 epoch 329 batch: 0 avg loss -2.918240 avg loss no lamb -2.918240 time 2020-06-26 02:27:35.203348
Model ind 665 epoch 329 batch: 100 avg loss -2.830683 avg loss no lamb -2.830683 time 2020-06-26 02:27:46.333652
Model ind 665 epoch 329 batch: 200 avg loss -2.802444 avg loss no lamb -2.802444 time 2020-06-26 02:27:57.023511
Model ind 665 epoch 329 batch: 300 avg loss -2.828100 avg loss no lamb -2.828100 time 2020-06-26 02:28:07.671736
Model ind 665 epoch 329 batch: 400 avg loss -2.633600 avg loss no lamb -2.633600 time 2020-06-26 02:28:18.404507
Model ind 665 epoch 329 batch: 500 avg loss -2.770675 avg loss no lamb -2.770675 time 2020-06-26 02:28:29.030668
Model ind 665 epoch 329 batch: 600 avg loss -2.865024 avg loss no lamb -2.865024 time 2020-06-26 02:28:39.745176
Model ind 665 epoch 329 batch: 700 avg loss -2.639413 avg loss no lamb -2.639413 time 2020-06-26 02:28:50.624704
Model ind 665 epoch 329 batch: 800 avg loss -2.774925 avg loss no lamb -2.774925 time 2020-06-26 02:29:01.600605
last batch sz 10
Pre: time 2020-06-26 02:29:15.806535: 
 	std: 0.002711899
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9817, 0.9762, 0.9831, 0.9779]
	train_accs: [0.98258334, 0.9819667, 0.9773, 0.98285, 0.97793335]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.98026
	best: 0.9831

Starting e_i: 330
Model ind 665 epoch 330 batch: 0 avg loss -2.876501 avg loss no lamb -2.876501 time 2020-06-26 02:29:16.586744
Model ind 665 epoch 330 batch: 100 avg loss -2.793264 avg loss no lamb -2.793264 time 2020-06-26 02:29:27.304271
Model ind 665 epoch 330 batch: 200 avg loss -2.765347 avg loss no lamb -2.765347 time 2020-06-26 02:29:37.880590
Model ind 665 epoch 330 batch: 300 avg loss -2.801258 avg loss no lamb -2.801258 time 2020-06-26 02:29:48.679141
Model ind 665 epoch 330 batch: 400 avg loss -2.681745 avg loss no lamb -2.681745 time 2020-06-26 02:29:59.454621
Model ind 665 epoch 330 batch: 500 avg loss -2.688703 avg loss no lamb -2.688703 time 2020-06-26 02:30:10.054866
Model ind 665 epoch 330 batch: 600 avg loss -2.851969 avg loss no lamb -2.851969 time 2020-06-26 02:30:20.538280
Model ind 665 epoch 330 batch: 700 avg loss -2.695909 avg loss no lamb -2.695909 time 2020-06-26 02:30:31.255960
Model ind 665 epoch 330 batch: 800 avg loss -2.703301 avg loss no lamb -2.703301 time 2020-06-26 02:30:41.981421
last batch sz 10
Pre: time 2020-06-26 02:30:55.640322: 
 	std: 0.00333671
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9808, 0.9744, 0.9828, 0.9759]
	train_accs: [0.9825, 0.9809667, 0.97568333, 0.98251665, 0.9765667]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97911996
	best: 0.9828

Starting e_i: 331
Model ind 665 epoch 331 batch: 0 avg loss -2.943180 avg loss no lamb -2.943180 time 2020-06-26 02:30:57.730551
Model ind 665 epoch 331 batch: 100 avg loss -2.811610 avg loss no lamb -2.811610 time 2020-06-26 02:31:08.271957
Model ind 665 epoch 331 batch: 200 avg loss -2.815231 avg loss no lamb -2.815231 time 2020-06-26 02:31:18.903759
Model ind 665 epoch 331 batch: 300 avg loss -2.814725 avg loss no lamb -2.814725 time 2020-06-26 02:31:29.783066
Model ind 665 epoch 331 batch: 400 avg loss -2.680087 avg loss no lamb -2.680087 time 2020-06-26 02:31:40.616388
Model ind 665 epoch 331 batch: 500 avg loss -2.748839 avg loss no lamb -2.748839 time 2020-06-26 02:31:51.268780
Model ind 665 epoch 331 batch: 600 avg loss -2.770955 avg loss no lamb -2.770955 time 2020-06-26 02:32:02.072418
Model ind 665 epoch 331 batch: 700 avg loss -2.561585 avg loss no lamb -2.561585 time 2020-06-26 02:32:13.077075
Model ind 665 epoch 331 batch: 800 avg loss -2.817894 avg loss no lamb -2.817894 time 2020-06-26 02:32:23.951963
last batch sz 10
Pre: time 2020-06-26 02:32:37.629797: 
 	std: 0.003114475
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.979, 0.9728, 0.9802, 0.9741]
	train_accs: [0.98073334, 0.97943336, 0.97461665, 0.98071665, 0.97573334]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97720003
	best: 0.9799

Starting e_i: 332
Model ind 665 epoch 332 batch: 0 avg loss -2.904484 avg loss no lamb -2.904484 time 2020-06-26 02:32:38.433357
Model ind 665 epoch 332 batch: 100 avg loss -2.790848 avg loss no lamb -2.790848 time 2020-06-26 02:32:49.479300
Model ind 665 epoch 332 batch: 200 avg loss -2.748906 avg loss no lamb -2.748906 time 2020-06-26 02:33:00.343124
Model ind 665 epoch 332 batch: 300 avg loss -2.838208 avg loss no lamb -2.838208 time 2020-06-26 02:33:11.083225
Model ind 665 epoch 332 batch: 400 avg loss -2.709277 avg loss no lamb -2.709277 time 2020-06-26 02:33:21.582444
Model ind 665 epoch 332 batch: 500 avg loss -2.752823 avg loss no lamb -2.752823 time 2020-06-26 02:33:32.372729
Model ind 665 epoch 332 batch: 600 avg loss -2.804509 avg loss no lamb -2.804509 time 2020-06-26 02:33:43.084637
Model ind 665 epoch 332 batch: 700 avg loss -2.609596 avg loss no lamb -2.609596 time 2020-06-26 02:33:54.038820
Model ind 665 epoch 332 batch: 800 avg loss -2.686902 avg loss no lamb -2.686902 time 2020-06-26 02:34:04.805036
last batch sz 10
Pre: time 2020-06-26 02:34:18.549877: 
 	std: 0.004314351
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9825, 0.9724, 0.9816, 0.9741]
	train_accs: [0.98193336, 0.9814, 0.97456664, 0.98186666, 0.97571665]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97848004
	best: 0.9818

Starting e_i: 333
Model ind 665 epoch 333 batch: 0 avg loss -2.902008 avg loss no lamb -2.902008 time 2020-06-26 02:34:19.355687
Model ind 665 epoch 333 batch: 100 avg loss -2.812498 avg loss no lamb -2.812498 time 2020-06-26 02:34:30.031711
Model ind 665 epoch 333 batch: 200 avg loss -2.765843 avg loss no lamb -2.765843 time 2020-06-26 02:34:40.759238
Model ind 665 epoch 333 batch: 300 avg loss -2.848324 avg loss no lamb -2.848324 time 2020-06-26 02:34:51.324735
Model ind 665 epoch 333 batch: 400 avg loss -2.734840 avg loss no lamb -2.734840 time 2020-06-26 02:35:02.031037
Model ind 665 epoch 333 batch: 500 avg loss -2.736874 avg loss no lamb -2.736874 time 2020-06-26 02:35:12.852134
Model ind 665 epoch 333 batch: 600 avg loss -2.808666 avg loss no lamb -2.808666 time 2020-06-26 02:35:23.484573
Model ind 665 epoch 333 batch: 700 avg loss -2.585989 avg loss no lamb -2.585989 time 2020-06-26 02:35:34.095100
Model ind 665 epoch 333 batch: 800 avg loss -2.802377 avg loss no lamb -2.802377 time 2020-06-26 02:35:44.927138
last batch sz 10
Pre: time 2020-06-26 02:35:58.935157: 
 	std: 0.0018061973
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9824, 0.9782, 0.9819, 0.9792]
	train_accs: [0.9824333, 0.9817333, 0.9778, 0.9819833, 0.97816664]
	best_train_sub_head: 0
	worst: 0.9782
	avg: 0.98085994
	best: 0.9826

Starting e_i: 334
Model ind 665 epoch 334 batch: 0 avg loss -2.908616 avg loss no lamb -2.908616 time 2020-06-26 02:35:59.787764
Model ind 665 epoch 334 batch: 100 avg loss -2.781873 avg loss no lamb -2.781873 time 2020-06-26 02:36:10.368841
Model ind 665 epoch 334 batch: 200 avg loss -2.683115 avg loss no lamb -2.683115 time 2020-06-26 02:36:21.117545
Model ind 665 epoch 334 batch: 300 avg loss -2.819132 avg loss no lamb -2.819132 time 2020-06-26 02:36:31.618072
Model ind 665 epoch 334 batch: 400 avg loss -2.671144 avg loss no lamb -2.671144 time 2020-06-26 02:36:42.578507
Model ind 665 epoch 334 batch: 500 avg loss -2.797290 avg loss no lamb -2.797290 time 2020-06-26 02:36:53.280737
Model ind 665 epoch 334 batch: 600 avg loss -2.847720 avg loss no lamb -2.847720 time 2020-06-26 02:37:03.861083
Model ind 665 epoch 334 batch: 700 avg loss -2.639635 avg loss no lamb -2.639635 time 2020-06-26 02:37:14.406592
Model ind 665 epoch 334 batch: 800 avg loss -2.788732 avg loss no lamb -2.788732 time 2020-06-26 02:37:25.235950
last batch sz 10
Pre: time 2020-06-26 02:37:38.921867: 
 	std: 0.0022114178
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9819, 0.977, 0.9811, 0.9771]
	train_accs: [0.98218334, 0.98165, 0.97716665, 0.98193336, 0.97755]
	best_train_sub_head: 0
	worst: 0.977
	avg: 0.97974
	best: 0.9816

Starting e_i: 335
Model ind 665 epoch 335 batch: 0 avg loss -2.946872 avg loss no lamb -2.946872 time 2020-06-26 02:37:39.696945
Model ind 665 epoch 335 batch: 100 avg loss -2.747747 avg loss no lamb -2.747747 time 2020-06-26 02:37:50.453656
Model ind 665 epoch 335 batch: 200 avg loss -2.769807 avg loss no lamb -2.769807 time 2020-06-26 02:38:01.094376
Model ind 665 epoch 335 batch: 300 avg loss -2.859838 avg loss no lamb -2.859838 time 2020-06-26 02:38:11.935442
Model ind 665 epoch 335 batch: 400 avg loss -2.713940 avg loss no lamb -2.713940 time 2020-06-26 02:38:22.739633
Model ind 665 epoch 335 batch: 500 avg loss -2.776917 avg loss no lamb -2.776917 time 2020-06-26 02:38:33.705411
Model ind 665 epoch 335 batch: 600 avg loss -2.769154 avg loss no lamb -2.769154 time 2020-06-26 02:38:44.364231
Model ind 665 epoch 335 batch: 700 avg loss -2.564550 avg loss no lamb -2.564550 time 2020-06-26 02:38:55.151107
Model ind 665 epoch 335 batch: 800 avg loss -2.753039 avg loss no lamb -2.753039 time 2020-06-26 02:39:06.257651
last batch sz 10
Pre: time 2020-06-26 02:39:20.313523: 
 	std: 0.003015568
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.982, 0.9747, 0.982, 0.9769]
	train_accs: [0.98188335, 0.98181665, 0.97581667, 0.9820167, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97938
	best: 0.982

Starting e_i: 336
Model ind 665 epoch 336 batch: 0 avg loss -2.896562 avg loss no lamb -2.896562 time 2020-06-26 02:39:21.123980
Model ind 665 epoch 336 batch: 100 avg loss -2.821865 avg loss no lamb -2.821865 time 2020-06-26 02:39:31.866583
Model ind 665 epoch 336 batch: 200 avg loss -2.797982 avg loss no lamb -2.797982 time 2020-06-26 02:39:42.598453
Model ind 665 epoch 336 batch: 300 avg loss -2.832259 avg loss no lamb -2.832259 time 2020-06-26 02:39:53.145528
Model ind 665 epoch 336 batch: 400 avg loss -2.684292 avg loss no lamb -2.684292 time 2020-06-26 02:40:03.956207
Model ind 665 epoch 336 batch: 500 avg loss -2.751775 avg loss no lamb -2.751775 time 2020-06-26 02:40:14.727269
Model ind 665 epoch 336 batch: 600 avg loss -2.792022 avg loss no lamb -2.792022 time 2020-06-26 02:40:25.548159
Model ind 665 epoch 336 batch: 700 avg loss -2.650666 avg loss no lamb -2.650666 time 2020-06-26 02:40:36.267707
Model ind 665 epoch 336 batch: 800 avg loss -2.774757 avg loss no lamb -2.774757 time 2020-06-26 02:40:47.048362
last batch sz 10
Pre: time 2020-06-26 02:41:00.877200: 
 	std: 0.0028366204
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9831, 0.9825, 0.9767, 0.9832, 0.9777]
	train_accs: [0.98288333, 0.98183334, 0.97745, 0.98288333, 0.97756666]
	best_train_sub_head: 0
	worst: 0.9767
	avg: 0.98064005
	best: 0.9831

Starting e_i: 337
Model ind 665 epoch 337 batch: 0 avg loss -2.925446 avg loss no lamb -2.925446 time 2020-06-26 02:41:01.736633
Model ind 665 epoch 337 batch: 100 avg loss -2.861071 avg loss no lamb -2.861071 time 2020-06-26 02:41:12.462123
Model ind 665 epoch 337 batch: 200 avg loss -2.751026 avg loss no lamb -2.751026 time 2020-06-26 02:41:23.239374
Model ind 665 epoch 337 batch: 300 avg loss -2.829434 avg loss no lamb -2.829434 time 2020-06-26 02:41:34.169280
Model ind 665 epoch 337 batch: 400 avg loss -2.633087 avg loss no lamb -2.633087 time 2020-06-26 02:41:44.854779
Model ind 665 epoch 337 batch: 500 avg loss -2.711256 avg loss no lamb -2.711256 time 2020-06-26 02:41:55.795239
Model ind 665 epoch 337 batch: 600 avg loss -2.816067 avg loss no lamb -2.816067 time 2020-06-26 02:42:06.853461
Model ind 665 epoch 337 batch: 700 avg loss -2.738990 avg loss no lamb -2.738990 time 2020-06-26 02:42:17.466656
Model ind 665 epoch 337 batch: 800 avg loss -2.761818 avg loss no lamb -2.761818 time 2020-06-26 02:42:28.083042
last batch sz 10
Pre: time 2020-06-26 02:42:41.763442: 
 	std: 0.0027789164
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9809, 0.975, 0.9818, 0.9764]
	train_accs: [0.9823833, 0.9816167, 0.97581667, 0.9823667, 0.9769667]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97904
	best: 0.9811

Starting e_i: 338
Model ind 665 epoch 338 batch: 0 avg loss -2.868586 avg loss no lamb -2.868586 time 2020-06-26 02:42:42.526330
Model ind 665 epoch 338 batch: 100 avg loss -2.849040 avg loss no lamb -2.849040 time 2020-06-26 02:42:53.237490
Model ind 665 epoch 338 batch: 200 avg loss -2.742623 avg loss no lamb -2.742623 time 2020-06-26 02:43:03.862109
Model ind 665 epoch 338 batch: 300 avg loss -2.750406 avg loss no lamb -2.750406 time 2020-06-26 02:43:14.461279
Model ind 665 epoch 338 batch: 400 avg loss -2.709037 avg loss no lamb -2.709037 time 2020-06-26 02:43:24.971087
Model ind 665 epoch 338 batch: 500 avg loss -2.751605 avg loss no lamb -2.751605 time 2020-06-26 02:43:35.544406
Model ind 665 epoch 338 batch: 600 avg loss -2.812118 avg loss no lamb -2.812118 time 2020-06-26 02:43:46.496151
Model ind 665 epoch 338 batch: 700 avg loss -2.626913 avg loss no lamb -2.626913 time 2020-06-26 02:43:57.332902
Model ind 665 epoch 338 batch: 800 avg loss -2.780022 avg loss no lamb -2.780022 time 2020-06-26 02:44:07.941428
last batch sz 10
Pre: time 2020-06-26 02:44:21.885146: 
 	std: 0.0034533557
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9811, 0.9742, 0.9821, 0.9754]
	train_accs: [0.98261666, 0.9814, 0.97506666, 0.98251665, 0.97543335]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97898006
	best: 0.9821

Starting e_i: 339
Model ind 665 epoch 339 batch: 0 avg loss -2.865803 avg loss no lamb -2.865803 time 2020-06-26 02:44:22.676443
Model ind 665 epoch 339 batch: 100 avg loss -2.864527 avg loss no lamb -2.864527 time 2020-06-26 02:44:33.683872
Model ind 665 epoch 339 batch: 200 avg loss -2.836056 avg loss no lamb -2.836056 time 2020-06-26 02:44:44.461990
Model ind 665 epoch 339 batch: 300 avg loss -2.882466 avg loss no lamb -2.882466 time 2020-06-26 02:44:55.167056
Model ind 665 epoch 339 batch: 400 avg loss -2.711181 avg loss no lamb -2.711181 time 2020-06-26 02:45:05.990688
Model ind 665 epoch 339 batch: 500 avg loss -2.722001 avg loss no lamb -2.722001 time 2020-06-26 02:45:16.640299
Model ind 665 epoch 339 batch: 600 avg loss -2.770529 avg loss no lamb -2.770529 time 2020-06-26 02:45:27.540258
Model ind 665 epoch 339 batch: 700 avg loss -2.718417 avg loss no lamb -2.718417 time 2020-06-26 02:45:38.339343
Model ind 665 epoch 339 batch: 800 avg loss -2.741090 avg loss no lamb -2.741090 time 2020-06-26 02:45:49.303906
last batch sz 10
Pre: time 2020-06-26 02:46:03.086526: 
 	std: 0.0033607136
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9827, 0.9754, 0.9817, 0.9757]
	train_accs: [0.98288333, 0.9820167, 0.97581667, 0.9824167, 0.97625]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97964
	best: 0.9827

Starting e_i: 340
Model ind 665 epoch 340 batch: 0 avg loss -2.834232 avg loss no lamb -2.834232 time 2020-06-26 02:46:04.003548
Model ind 665 epoch 340 batch: 100 avg loss -2.818079 avg loss no lamb -2.818079 time 2020-06-26 02:46:14.561956
Model ind 665 epoch 340 batch: 200 avg loss -2.752792 avg loss no lamb -2.752792 time 2020-06-26 02:46:25.453408
Model ind 665 epoch 340 batch: 300 avg loss -2.868260 avg loss no lamb -2.868260 time 2020-06-26 02:46:36.168507
Model ind 665 epoch 340 batch: 400 avg loss -2.653446 avg loss no lamb -2.653446 time 2020-06-26 02:46:46.565035
Model ind 665 epoch 340 batch: 500 avg loss -2.700990 avg loss no lamb -2.700990 time 2020-06-26 02:46:57.240850
Model ind 665 epoch 340 batch: 600 avg loss -2.843188 avg loss no lamb -2.843188 time 2020-06-26 02:47:08.113364
Model ind 665 epoch 340 batch: 700 avg loss -2.690019 avg loss no lamb -2.690019 time 2020-06-26 02:47:18.750705
Model ind 665 epoch 340 batch: 800 avg loss -2.744176 avg loss no lamb -2.744176 time 2020-06-26 02:47:29.337163
last batch sz 10
Pre: time 2020-06-26 02:47:43.269357: 
 	std: 0.0034808028
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9828, 0.9761, 0.9831, 0.9754]
	train_accs: [0.98275, 0.98216665, 0.9766667, 0.9827167, 0.97681665]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.9799999
	best: 0.9826

Starting e_i: 341
Model ind 665 epoch 341 batch: 0 avg loss -2.915021 avg loss no lamb -2.915021 time 2020-06-26 02:47:45.262059
Model ind 665 epoch 341 batch: 100 avg loss -2.766027 avg loss no lamb -2.766027 time 2020-06-26 02:47:56.200952
Model ind 665 epoch 341 batch: 200 avg loss -2.781521 avg loss no lamb -2.781521 time 2020-06-26 02:48:06.981088
Model ind 665 epoch 341 batch: 300 avg loss -2.806648 avg loss no lamb -2.806648 time 2020-06-26 02:48:17.692864
Model ind 665 epoch 341 batch: 400 avg loss -2.698623 avg loss no lamb -2.698623 time 2020-06-26 02:48:28.203903
Model ind 665 epoch 341 batch: 500 avg loss -2.724859 avg loss no lamb -2.724859 time 2020-06-26 02:48:38.745711
Model ind 665 epoch 341 batch: 600 avg loss -2.849677 avg loss no lamb -2.849677 time 2020-06-26 02:48:49.499832
Model ind 665 epoch 341 batch: 700 avg loss -2.630025 avg loss no lamb -2.630025 time 2020-06-26 02:49:00.164357
Model ind 665 epoch 341 batch: 800 avg loss -2.781090 avg loss no lamb -2.781090 time 2020-06-26 02:49:10.854046
last batch sz 10
Pre: time 2020-06-26 02:49:24.709196: 
 	std: 0.0023574561
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9828, 0.9783, 0.9835, 0.9786]
	train_accs: [0.9832, 0.98185, 0.9780167, 0.9831333, 0.9784833]
	best_train_sub_head: 0
	worst: 0.9783
	avg: 0.98132
	best: 0.9834

Starting e_i: 342
Model ind 665 epoch 342 batch: 0 avg loss -2.942513 avg loss no lamb -2.942513 time 2020-06-26 02:49:25.492839
Model ind 665 epoch 342 batch: 100 avg loss -2.815253 avg loss no lamb -2.815253 time 2020-06-26 02:49:35.992782
Model ind 665 epoch 342 batch: 200 avg loss -2.776032 avg loss no lamb -2.776032 time 2020-06-26 02:49:46.364784
Model ind 665 epoch 342 batch: 300 avg loss -2.824172 avg loss no lamb -2.824172 time 2020-06-26 02:49:57.075707
Model ind 665 epoch 342 batch: 400 avg loss -2.656842 avg loss no lamb -2.656842 time 2020-06-26 02:50:07.851669
Model ind 665 epoch 342 batch: 500 avg loss -2.772272 avg loss no lamb -2.772272 time 2020-06-26 02:50:18.313261
Model ind 665 epoch 342 batch: 600 avg loss -2.816506 avg loss no lamb -2.816506 time 2020-06-26 02:50:28.959423
Model ind 665 epoch 342 batch: 700 avg loss -2.639407 avg loss no lamb -2.639407 time 2020-06-26 02:50:39.520279
Model ind 665 epoch 342 batch: 800 avg loss -2.774403 avg loss no lamb -2.774403 time 2020-06-26 02:50:50.230173
last batch sz 10
Pre: time 2020-06-26 02:51:04.254042: 
 	std: 0.003180572
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9831, 0.9758, 0.983, 0.9773]
	train_accs: [0.98256665, 0.9819, 0.9765, 0.9826667, 0.9773833]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.98039997
	best: 0.983

Starting e_i: 343
Model ind 665 epoch 343 batch: 0 avg loss -2.934399 avg loss no lamb -2.934399 time 2020-06-26 02:51:05.136215
Model ind 665 epoch 343 batch: 100 avg loss -2.817336 avg loss no lamb -2.817336 time 2020-06-26 02:51:15.712860
Model ind 665 epoch 343 batch: 200 avg loss -2.714365 avg loss no lamb -2.714365 time 2020-06-26 02:51:26.383266
Model ind 665 epoch 343 batch: 300 avg loss -2.861775 avg loss no lamb -2.861775 time 2020-06-26 02:51:37.261124
Model ind 665 epoch 343 batch: 400 avg loss -2.742397 avg loss no lamb -2.742397 time 2020-06-26 02:51:47.983198
Model ind 665 epoch 343 batch: 500 avg loss -2.818858 avg loss no lamb -2.818858 time 2020-06-26 02:51:58.782174
Model ind 665 epoch 343 batch: 600 avg loss -2.840533 avg loss no lamb -2.840533 time 2020-06-26 02:52:09.702916
Model ind 665 epoch 343 batch: 700 avg loss -2.688282 avg loss no lamb -2.688282 time 2020-06-26 02:52:20.314767
Model ind 665 epoch 343 batch: 800 avg loss -2.760824 avg loss no lamb -2.760824 time 2020-06-26 02:52:31.179103
last batch sz 10
Pre: time 2020-06-26 02:52:45.095571: 
 	std: 0.0025476187
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9835, 0.9831, 0.9779, 0.9834, 0.9784]
	train_accs: [0.9824167, 0.98186666, 0.9777333, 0.98263335, 0.9777167]
	best_train_sub_head: 3
	worst: 0.9779
	avg: 0.98125994
	best: 0.9834

Starting e_i: 344
Model ind 665 epoch 344 batch: 0 avg loss -2.894084 avg loss no lamb -2.894084 time 2020-06-26 02:52:45.929912
Model ind 665 epoch 344 batch: 100 avg loss -2.778461 avg loss no lamb -2.778461 time 2020-06-26 02:52:56.582802
Model ind 665 epoch 344 batch: 200 avg loss -2.763411 avg loss no lamb -2.763411 time 2020-06-26 02:53:07.287846
Model ind 665 epoch 344 batch: 300 avg loss -2.773320 avg loss no lamb -2.773320 time 2020-06-26 02:53:17.959112
Model ind 665 epoch 344 batch: 400 avg loss -2.766674 avg loss no lamb -2.766674 time 2020-06-26 02:53:28.739638
Model ind 665 epoch 344 batch: 500 avg loss -2.754097 avg loss no lamb -2.754097 time 2020-06-26 02:53:39.565931
Model ind 665 epoch 344 batch: 600 avg loss -2.875540 avg loss no lamb -2.875540 time 2020-06-26 02:53:50.141431
Model ind 665 epoch 344 batch: 700 avg loss -2.659712 avg loss no lamb -2.659712 time 2020-06-26 02:54:00.684477
Model ind 665 epoch 344 batch: 800 avg loss -2.791123 avg loss no lamb -2.791123 time 2020-06-26 02:54:11.456465
last batch sz 10
Pre: time 2020-06-26 02:54:25.384264: 
 	std: 0.0031946267
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9826, 0.9758, 0.9831, 0.977]
	train_accs: [0.98268336, 0.9820167, 0.9759667, 0.98246664, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9758
	avg: 0.9802799
	best: 0.9829

Starting e_i: 345
Model ind 665 epoch 345 batch: 0 avg loss -2.830073 avg loss no lamb -2.830073 time 2020-06-26 02:54:26.144962
Model ind 665 epoch 345 batch: 100 avg loss -2.751476 avg loss no lamb -2.751476 time 2020-06-26 02:54:36.940833
Model ind 665 epoch 345 batch: 200 avg loss -2.812435 avg loss no lamb -2.812435 time 2020-06-26 02:54:47.604092
Model ind 665 epoch 345 batch: 300 avg loss -2.755122 avg loss no lamb -2.755122 time 2020-06-26 02:54:58.357094
Model ind 665 epoch 345 batch: 400 avg loss -2.759294 avg loss no lamb -2.759294 time 2020-06-26 02:55:09.130234
Model ind 665 epoch 345 batch: 500 avg loss -2.741212 avg loss no lamb -2.741212 time 2020-06-26 02:55:20.067613
Model ind 665 epoch 345 batch: 600 avg loss -2.804724 avg loss no lamb -2.804724 time 2020-06-26 02:55:30.740722
Model ind 665 epoch 345 batch: 700 avg loss -2.663774 avg loss no lamb -2.663774 time 2020-06-26 02:55:41.847874
Model ind 665 epoch 345 batch: 800 avg loss -2.790709 avg loss no lamb -2.790709 time 2020-06-26 02:55:52.765140
last batch sz 10
Pre: time 2020-06-26 02:56:06.415947: 
 	std: 0.002639245
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9813, 0.9756, 0.9817, 0.977]
	train_accs: [0.98211664, 0.98146665, 0.9763, 0.9823, 0.97705]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97947997
	best: 0.9817

Starting e_i: 346
Model ind 665 epoch 346 batch: 0 avg loss -2.897027 avg loss no lamb -2.897027 time 2020-06-26 02:56:07.274733
Model ind 665 epoch 346 batch: 100 avg loss -2.878515 avg loss no lamb -2.878515 time 2020-06-26 02:56:18.196239
Model ind 665 epoch 346 batch: 200 avg loss -2.848492 avg loss no lamb -2.848492 time 2020-06-26 02:56:28.885912
Model ind 665 epoch 346 batch: 300 avg loss -2.798570 avg loss no lamb -2.798570 time 2020-06-26 02:56:39.560917
Model ind 665 epoch 346 batch: 400 avg loss -2.697389 avg loss no lamb -2.697389 time 2020-06-26 02:56:50.357592
Model ind 665 epoch 346 batch: 500 avg loss -2.744633 avg loss no lamb -2.744633 time 2020-06-26 02:57:01.157101
Model ind 665 epoch 346 batch: 600 avg loss -2.790254 avg loss no lamb -2.790254 time 2020-06-26 02:57:11.837022
Model ind 665 epoch 346 batch: 700 avg loss -2.611171 avg loss no lamb -2.611171 time 2020-06-26 02:57:22.607424
Model ind 665 epoch 346 batch: 800 avg loss -2.758954 avg loss no lamb -2.758954 time 2020-06-26 02:57:33.335523
last batch sz 10
Pre: time 2020-06-26 02:57:46.906498: 
 	std: 0.0035575305
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9807, 0.9737, 0.9827, 0.9759]
	train_accs: [0.9821333, 0.9806, 0.97498333, 0.98215, 0.97651666]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.979
	best: 0.9827

Starting e_i: 347
Model ind 665 epoch 347 batch: 0 avg loss -2.918344 avg loss no lamb -2.918344 time 2020-06-26 02:57:47.727282
Model ind 665 epoch 347 batch: 100 avg loss -2.832209 avg loss no lamb -2.832209 time 2020-06-26 02:57:58.623757
Model ind 665 epoch 347 batch: 200 avg loss -2.808456 avg loss no lamb -2.808456 time 2020-06-26 02:58:09.449498
Model ind 665 epoch 347 batch: 300 avg loss -2.764450 avg loss no lamb -2.764450 time 2020-06-26 02:58:20.249173
Model ind 665 epoch 347 batch: 400 avg loss -2.728458 avg loss no lamb -2.728458 time 2020-06-26 02:58:30.796148
Model ind 665 epoch 347 batch: 500 avg loss -2.703188 avg loss no lamb -2.703188 time 2020-06-26 02:58:41.540641
Model ind 665 epoch 347 batch: 600 avg loss -2.827171 avg loss no lamb -2.827171 time 2020-06-26 02:58:52.187273
Model ind 665 epoch 347 batch: 700 avg loss -2.577958 avg loss no lamb -2.577958 time 2020-06-26 02:59:02.939656
Model ind 665 epoch 347 batch: 800 avg loss -2.818618 avg loss no lamb -2.818618 time 2020-06-26 02:59:13.631965
last batch sz 10
Pre: time 2020-06-26 02:59:27.521586: 
 	std: 0.0029260246
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9809, 0.9748, 0.981, 0.9752]
	train_accs: [0.98193336, 0.98146665, 0.9762, 0.98188335, 0.9767]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97858
	best: 0.981

Starting e_i: 348
Model ind 665 epoch 348 batch: 0 avg loss -2.933142 avg loss no lamb -2.933142 time 2020-06-26 02:59:28.321802
Model ind 665 epoch 348 batch: 100 avg loss -2.854655 avg loss no lamb -2.854655 time 2020-06-26 02:59:39.063704
Model ind 665 epoch 348 batch: 200 avg loss -2.842505 avg loss no lamb -2.842505 time 2020-06-26 02:59:49.725731
Model ind 665 epoch 348 batch: 300 avg loss -2.804431 avg loss no lamb -2.804431 time 2020-06-26 03:00:00.669941
Model ind 665 epoch 348 batch: 400 avg loss -2.740283 avg loss no lamb -2.740283 time 2020-06-26 03:00:11.004059
Model ind 665 epoch 348 batch: 500 avg loss -2.767376 avg loss no lamb -2.767376 time 2020-06-26 03:00:21.482172
Model ind 665 epoch 348 batch: 600 avg loss -2.788624 avg loss no lamb -2.788624 time 2020-06-26 03:00:32.291029
Model ind 665 epoch 348 batch: 700 avg loss -2.573616 avg loss no lamb -2.573616 time 2020-06-26 03:00:43.031910
Model ind 665 epoch 348 batch: 800 avg loss -2.730635 avg loss no lamb -2.730635 time 2020-06-26 03:00:53.794596
last batch sz 10
Pre: time 2020-06-26 03:01:07.580364: 
 	std: 0.0034880422
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9809, 0.9738, 0.9814, 0.9748]
	train_accs: [0.9820833, 0.98116666, 0.97581667, 0.98191667, 0.97578335]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97854006
	best: 0.9818

Starting e_i: 349
Model ind 665 epoch 349 batch: 0 avg loss -2.938945 avg loss no lamb -2.938945 time 2020-06-26 03:01:08.459567
Model ind 665 epoch 349 batch: 100 avg loss -2.845703 avg loss no lamb -2.845703 time 2020-06-26 03:01:19.247737
Model ind 665 epoch 349 batch: 200 avg loss -2.827802 avg loss no lamb -2.827802 time 2020-06-26 03:01:30.064400
Model ind 665 epoch 349 batch: 300 avg loss -2.856164 avg loss no lamb -2.856164 time 2020-06-26 03:01:40.687930
Model ind 665 epoch 349 batch: 400 avg loss -2.692471 avg loss no lamb -2.692471 time 2020-06-26 03:01:51.514798
Model ind 665 epoch 349 batch: 500 avg loss -2.742994 avg loss no lamb -2.742994 time 2020-06-26 03:02:02.173135
Model ind 665 epoch 349 batch: 600 avg loss -2.782407 avg loss no lamb -2.782407 time 2020-06-26 03:02:12.943790
Model ind 665 epoch 349 batch: 700 avg loss -2.649628 avg loss no lamb -2.649628 time 2020-06-26 03:02:23.937187
Model ind 665 epoch 349 batch: 800 avg loss -2.740545 avg loss no lamb -2.740545 time 2020-06-26 03:02:34.954575
last batch sz 10
Pre: time 2020-06-26 03:02:48.838186: 
 	std: 0.0030340631
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9816, 0.9761, 0.9835, 0.9771]
	train_accs: [0.9824833, 0.9813167, 0.97693336, 0.98296666, 0.9776833]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.98022
	best: 0.9835

Starting e_i: 350
Model ind 665 epoch 350 batch: 0 avg loss -2.903359 avg loss no lamb -2.903359 time 2020-06-26 03:02:49.664437
Model ind 665 epoch 350 batch: 100 avg loss -2.875169 avg loss no lamb -2.875169 time 2020-06-26 03:03:00.336980
Model ind 665 epoch 350 batch: 200 avg loss -2.816416 avg loss no lamb -2.816416 time 2020-06-26 03:03:10.962912
Model ind 665 epoch 350 batch: 300 avg loss -2.844551 avg loss no lamb -2.844551 time 2020-06-26 03:03:21.669739
Model ind 665 epoch 350 batch: 400 avg loss -2.690904 avg loss no lamb -2.690904 time 2020-06-26 03:03:32.393896
Model ind 665 epoch 350 batch: 500 avg loss -2.784682 avg loss no lamb -2.784682 time 2020-06-26 03:03:43.218981
Model ind 665 epoch 350 batch: 600 avg loss -2.830454 avg loss no lamb -2.830454 time 2020-06-26 03:03:54.103603
Model ind 665 epoch 350 batch: 700 avg loss -2.655535 avg loss no lamb -2.655535 time 2020-06-26 03:04:05.080676
Model ind 665 epoch 350 batch: 800 avg loss -2.700076 avg loss no lamb -2.700076 time 2020-06-26 03:04:15.965979
last batch sz 10
Pre: time 2020-06-26 03:04:29.939024: 
 	std: 0.0026820952
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9815, 0.9756, 0.9825, 0.9775]
	train_accs: [0.98226666, 0.98195, 0.9769, 0.9827, 0.97785]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97972
	best: 0.9825

Starting e_i: 351
Model ind 665 epoch 351 batch: 0 avg loss -2.879206 avg loss no lamb -2.879206 time 2020-06-26 03:04:32.022322
Model ind 665 epoch 351 batch: 100 avg loss -2.856531 avg loss no lamb -2.856531 time 2020-06-26 03:04:42.915650
Model ind 665 epoch 351 batch: 200 avg loss -2.755651 avg loss no lamb -2.755651 time 2020-06-26 03:04:53.818111
Model ind 665 epoch 351 batch: 300 avg loss -2.857219 avg loss no lamb -2.857219 time 2020-06-26 03:05:04.760247
Model ind 665 epoch 351 batch: 400 avg loss -2.734450 avg loss no lamb -2.734450 time 2020-06-26 03:05:15.548885
Model ind 665 epoch 351 batch: 500 avg loss -2.773536 avg loss no lamb -2.773536 time 2020-06-26 03:05:26.170760
Model ind 665 epoch 351 batch: 600 avg loss -2.792778 avg loss no lamb -2.792778 time 2020-06-26 03:05:37.049690
Model ind 665 epoch 351 batch: 700 avg loss -2.721696 avg loss no lamb -2.721696 time 2020-06-26 03:05:47.751065
Model ind 665 epoch 351 batch: 800 avg loss -2.755662 avg loss no lamb -2.755662 time 2020-06-26 03:05:58.435841
last batch sz 10
Pre: time 2020-06-26 03:06:12.138348: 
 	std: 0.0029846358
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9818, 0.9758, 0.9821, 0.9761]
	train_accs: [0.98205, 0.9818, 0.97685, 0.98225, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.9796001
	best: 0.9821

Starting e_i: 352
Model ind 665 epoch 352 batch: 0 avg loss -2.917623 avg loss no lamb -2.917623 time 2020-06-26 03:06:12.981423
Model ind 665 epoch 352 batch: 100 avg loss -2.805953 avg loss no lamb -2.805953 time 2020-06-26 03:06:23.653935
Model ind 665 epoch 352 batch: 200 avg loss -2.818918 avg loss no lamb -2.818918 time 2020-06-26 03:06:34.332741
Model ind 665 epoch 352 batch: 300 avg loss -2.775462 avg loss no lamb -2.775462 time 2020-06-26 03:06:44.766015
Model ind 665 epoch 352 batch: 400 avg loss -2.632844 avg loss no lamb -2.632844 time 2020-06-26 03:06:55.458732
Model ind 665 epoch 352 batch: 500 avg loss -2.788111 avg loss no lamb -2.788111 time 2020-06-26 03:07:06.070153
Model ind 665 epoch 352 batch: 600 avg loss -2.851661 avg loss no lamb -2.851661 time 2020-06-26 03:07:17.023350
Model ind 665 epoch 352 batch: 700 avg loss -2.684911 avg loss no lamb -2.684911 time 2020-06-26 03:07:27.705707
Model ind 665 epoch 352 batch: 800 avg loss -2.773060 avg loss no lamb -2.773060 time 2020-06-26 03:07:38.483798
last batch sz 10
Pre: time 2020-06-26 03:07:52.589488: 
 	std: 0.0021141456
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9821, 0.9782, 0.9834, 0.979]
	train_accs: [0.98255, 0.98145, 0.97805, 0.9829, 0.9784833]
	best_train_sub_head: 3
	worst: 0.9782
	avg: 0.98112
	best: 0.9834

Starting e_i: 353
Model ind 665 epoch 353 batch: 0 avg loss -2.877492 avg loss no lamb -2.877492 time 2020-06-26 03:07:53.429604
Model ind 665 epoch 353 batch: 100 avg loss -2.779612 avg loss no lamb -2.779612 time 2020-06-26 03:08:04.418773
Model ind 665 epoch 353 batch: 200 avg loss -2.783097 avg loss no lamb -2.783097 time 2020-06-26 03:08:15.289194
Model ind 665 epoch 353 batch: 300 avg loss -2.836747 avg loss no lamb -2.836747 time 2020-06-26 03:08:26.327643
Model ind 665 epoch 353 batch: 400 avg loss -2.691059 avg loss no lamb -2.691059 time 2020-06-26 03:08:37.146808
Model ind 665 epoch 353 batch: 500 avg loss -2.718608 avg loss no lamb -2.718608 time 2020-06-26 03:08:47.862688
Model ind 665 epoch 353 batch: 600 avg loss -2.863719 avg loss no lamb -2.863719 time 2020-06-26 03:08:58.623098
Model ind 665 epoch 353 batch: 700 avg loss -2.675982 avg loss no lamb -2.675982 time 2020-06-26 03:09:09.379137
Model ind 665 epoch 353 batch: 800 avg loss -2.753948 avg loss no lamb -2.753948 time 2020-06-26 03:09:20.000130
last batch sz 10
Pre: time 2020-06-26 03:09:33.825480: 
 	std: 0.0026209936
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9818, 0.9766, 0.9831, 0.9779]
	train_accs: [0.98286664, 0.9819, 0.9771, 0.98275, 0.97816664]
	best_train_sub_head: 0
	worst: 0.9766
	avg: 0.98038006
	best: 0.9825

Starting e_i: 354
Model ind 665 epoch 354 batch: 0 avg loss -2.920856 avg loss no lamb -2.920856 time 2020-06-26 03:09:34.641824
Model ind 665 epoch 354 batch: 100 avg loss -2.855901 avg loss no lamb -2.855901 time 2020-06-26 03:09:45.295790
Model ind 665 epoch 354 batch: 200 avg loss -2.777198 avg loss no lamb -2.777198 time 2020-06-26 03:09:56.051344
Model ind 665 epoch 354 batch: 300 avg loss -2.784490 avg loss no lamb -2.784490 time 2020-06-26 03:10:06.895677
Model ind 665 epoch 354 batch: 400 avg loss -2.670766 avg loss no lamb -2.670766 time 2020-06-26 03:10:17.542353
Model ind 665 epoch 354 batch: 500 avg loss -2.715936 avg loss no lamb -2.715936 time 2020-06-26 03:10:28.177325
Model ind 665 epoch 354 batch: 600 avg loss -2.780304 avg loss no lamb -2.780304 time 2020-06-26 03:10:38.736312
Model ind 665 epoch 354 batch: 700 avg loss -2.735247 avg loss no lamb -2.735247 time 2020-06-26 03:10:49.547629
Model ind 665 epoch 354 batch: 800 avg loss -2.774221 avg loss no lamb -2.774221 time 2020-06-26 03:11:00.330677
last batch sz 10
Pre: time 2020-06-26 03:11:14.102923: 
 	std: 0.002237855
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9823, 0.9776, 0.9833, 0.9791]
	train_accs: [0.9826, 0.98183334, 0.97763336, 0.98258334, 0.97833335]
	best_train_sub_head: 0
	worst: 0.9776
	avg: 0.98099995
	best: 0.9827

Starting e_i: 355
Model ind 665 epoch 355 batch: 0 avg loss -2.919046 avg loss no lamb -2.919046 time 2020-06-26 03:11:14.947457
Model ind 665 epoch 355 batch: 100 avg loss -2.748723 avg loss no lamb -2.748723 time 2020-06-26 03:11:25.777344
Model ind 665 epoch 355 batch: 200 avg loss -2.764443 avg loss no lamb -2.764443 time 2020-06-26 03:11:36.554139
Model ind 665 epoch 355 batch: 300 avg loss -2.774500 avg loss no lamb -2.774500 time 2020-06-26 03:11:47.523646
Model ind 665 epoch 355 batch: 400 avg loss -2.716513 avg loss no lamb -2.716513 time 2020-06-26 03:11:58.166617
Model ind 665 epoch 355 batch: 500 avg loss -2.712707 avg loss no lamb -2.712707 time 2020-06-26 03:12:09.031298
Model ind 665 epoch 355 batch: 600 avg loss -2.807451 avg loss no lamb -2.807451 time 2020-06-26 03:12:19.895004
Model ind 665 epoch 355 batch: 700 avg loss -2.593824 avg loss no lamb -2.593824 time 2020-06-26 03:12:30.878244
Model ind 665 epoch 355 batch: 800 avg loss -2.766733 avg loss no lamb -2.766733 time 2020-06-26 03:12:41.475134
last batch sz 10
Pre: time 2020-06-26 03:12:55.392086: 
 	std: 0.0030055994
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9804, 0.9737, 0.9809, 0.9753]
	train_accs: [0.9813333, 0.98075, 0.9752167, 0.9816, 0.97648335]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97812
	best: 0.9809

Starting e_i: 356
Model ind 665 epoch 356 batch: 0 avg loss -2.902648 avg loss no lamb -2.902648 time 2020-06-26 03:12:56.187928
Model ind 665 epoch 356 batch: 100 avg loss -2.844887 avg loss no lamb -2.844887 time 2020-06-26 03:13:07.138830
Model ind 665 epoch 356 batch: 200 avg loss -2.818165 avg loss no lamb -2.818165 time 2020-06-26 03:13:17.778073
Model ind 665 epoch 356 batch: 300 avg loss -2.781028 avg loss no lamb -2.781028 time 2020-06-26 03:13:28.465012
Model ind 665 epoch 356 batch: 400 avg loss -2.719048 avg loss no lamb -2.719048 time 2020-06-26 03:13:39.060899
Model ind 665 epoch 356 batch: 500 avg loss -2.751490 avg loss no lamb -2.751490 time 2020-06-26 03:13:49.591543
Model ind 665 epoch 356 batch: 600 avg loss -2.791720 avg loss no lamb -2.791720 time 2020-06-26 03:14:00.561354
Model ind 665 epoch 356 batch: 700 avg loss -2.757277 avg loss no lamb -2.757277 time 2020-06-26 03:14:11.302301
Model ind 665 epoch 356 batch: 800 avg loss -2.747125 avg loss no lamb -2.747125 time 2020-06-26 03:14:21.820761
last batch sz 10
Pre: time 2020-06-26 03:14:35.691545: 
 	std: 0.002772294
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9833, 0.9765, 0.9828, 0.9781]
	train_accs: [0.9823, 0.98215, 0.9762, 0.98256665, 0.9774333]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.98062
	best: 0.9828

Starting e_i: 357
Model ind 665 epoch 357 batch: 0 avg loss -2.910608 avg loss no lamb -2.910608 time 2020-06-26 03:14:36.542775
Model ind 665 epoch 357 batch: 100 avg loss -2.796793 avg loss no lamb -2.796793 time 2020-06-26 03:14:47.430641
Model ind 665 epoch 357 batch: 200 avg loss -2.789101 avg loss no lamb -2.789101 time 2020-06-26 03:14:58.159463
Model ind 665 epoch 357 batch: 300 avg loss -2.771785 avg loss no lamb -2.771785 time 2020-06-26 03:15:09.003039
Model ind 665 epoch 357 batch: 400 avg loss -2.736969 avg loss no lamb -2.736969 time 2020-06-26 03:15:19.679640
Model ind 665 epoch 357 batch: 500 avg loss -2.784190 avg loss no lamb -2.784190 time 2020-06-26 03:15:30.497604
Model ind 665 epoch 357 batch: 600 avg loss -2.829932 avg loss no lamb -2.829932 time 2020-06-26 03:15:41.288721
Model ind 665 epoch 357 batch: 700 avg loss -2.716861 avg loss no lamb -2.716861 time 2020-06-26 03:15:51.756067
Model ind 665 epoch 357 batch: 800 avg loss -2.710111 avg loss no lamb -2.710111 time 2020-06-26 03:16:02.280393
last batch sz 10
Pre: time 2020-06-26 03:16:16.239522: 
 	std: 0.0031891048
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.982, 0.9753, 0.9829, 0.9769]
	train_accs: [0.9823667, 0.9813333, 0.9763, 0.98233336, 0.97745]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97994006
	best: 0.9826

Starting e_i: 358
Model ind 665 epoch 358 batch: 0 avg loss -2.860619 avg loss no lamb -2.860619 time 2020-06-26 03:16:17.029061
Model ind 665 epoch 358 batch: 100 avg loss -2.851645 avg loss no lamb -2.851645 time 2020-06-26 03:16:27.399880
Model ind 665 epoch 358 batch: 200 avg loss -2.766626 avg loss no lamb -2.766626 time 2020-06-26 03:16:38.051735
Model ind 665 epoch 358 batch: 300 avg loss -2.808709 avg loss no lamb -2.808709 time 2020-06-26 03:16:48.883268
Model ind 665 epoch 358 batch: 400 avg loss -2.643067 avg loss no lamb -2.643067 time 2020-06-26 03:16:59.700690
Model ind 665 epoch 358 batch: 500 avg loss -2.758565 avg loss no lamb -2.758565 time 2020-06-26 03:17:10.499099
Model ind 665 epoch 358 batch: 600 avg loss -2.812037 avg loss no lamb -2.812037 time 2020-06-26 03:17:21.022069
Model ind 665 epoch 358 batch: 700 avg loss -2.680331 avg loss no lamb -2.680331 time 2020-06-26 03:17:31.618135
Model ind 665 epoch 358 batch: 800 avg loss -2.813846 avg loss no lamb -2.813846 time 2020-06-26 03:17:42.439978
last batch sz 10
Pre: time 2020-06-26 03:17:56.276978: 
 	std: 0.0022824556
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9807, 0.9752, 0.9803, 0.9767]
	train_accs: [0.98158336, 0.9809, 0.9763, 0.98191667, 0.9776833]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97867996
	best: 0.9803

Starting e_i: 359
Model ind 665 epoch 359 batch: 0 avg loss -2.936303 avg loss no lamb -2.936303 time 2020-06-26 03:17:57.094772
Model ind 665 epoch 359 batch: 100 avg loss -2.842967 avg loss no lamb -2.842967 time 2020-06-26 03:18:07.862286
Model ind 665 epoch 359 batch: 200 avg loss -2.809688 avg loss no lamb -2.809688 time 2020-06-26 03:18:18.570371
Model ind 665 epoch 359 batch: 300 avg loss -2.852857 avg loss no lamb -2.852857 time 2020-06-26 03:18:29.331934
Model ind 665 epoch 359 batch: 400 avg loss -2.695225 avg loss no lamb -2.695225 time 2020-06-26 03:18:39.986521
Model ind 665 epoch 359 batch: 500 avg loss -2.715705 avg loss no lamb -2.715705 time 2020-06-26 03:18:50.719419
Model ind 665 epoch 359 batch: 600 avg loss -2.797536 avg loss no lamb -2.797536 time 2020-06-26 03:19:01.390533
Model ind 665 epoch 359 batch: 700 avg loss -2.652661 avg loss no lamb -2.652661 time 2020-06-26 03:19:12.081726
Model ind 665 epoch 359 batch: 800 avg loss -2.748230 avg loss no lamb -2.748230 time 2020-06-26 03:19:22.882236
last batch sz 10
Pre: time 2020-06-26 03:19:37.061856: 
 	std: 0.0018832069
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9797, 0.9762, 0.9797, 0.9755]
	train_accs: [0.98155, 0.9806833, 0.9770167, 0.9813833, 0.9774333]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97814006
	best: 0.9796

Starting e_i: 360
Model ind 665 epoch 360 batch: 0 avg loss -2.940556 avg loss no lamb -2.940556 time 2020-06-26 03:19:37.855911
Model ind 665 epoch 360 batch: 100 avg loss -2.780747 avg loss no lamb -2.780747 time 2020-06-26 03:19:48.332769
Model ind 665 epoch 360 batch: 200 avg loss -2.799918 avg loss no lamb -2.799918 time 2020-06-26 03:19:59.023309
Model ind 665 epoch 360 batch: 300 avg loss -2.823206 avg loss no lamb -2.823206 time 2020-06-26 03:20:09.929686
Model ind 665 epoch 360 batch: 400 avg loss -2.739534 avg loss no lamb -2.739534 time 2020-06-26 03:20:20.567123
Model ind 665 epoch 360 batch: 500 avg loss -2.732270 avg loss no lamb -2.732270 time 2020-06-26 03:20:31.111712
Model ind 665 epoch 360 batch: 600 avg loss -2.805236 avg loss no lamb -2.805236 time 2020-06-26 03:20:41.755849
Model ind 665 epoch 360 batch: 700 avg loss -2.682521 avg loss no lamb -2.682521 time 2020-06-26 03:20:52.551930
Model ind 665 epoch 360 batch: 800 avg loss -2.713367 avg loss no lamb -2.713367 time 2020-06-26 03:21:03.445978
last batch sz 10
Pre: time 2020-06-26 03:21:17.251205: 
 	std: 0.0022512174
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9803, 0.9753, 0.9806, 0.9767]
	train_accs: [0.9817, 0.98073334, 0.97595, 0.9813833, 0.97728336]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.9787
	best: 0.9806

Starting e_i: 361
Model ind 665 epoch 361 batch: 0 avg loss -2.900252 avg loss no lamb -2.900252 time 2020-06-26 03:21:19.257681
Model ind 665 epoch 361 batch: 100 avg loss -2.822321 avg loss no lamb -2.822321 time 2020-06-26 03:21:29.986233
Model ind 665 epoch 361 batch: 200 avg loss -2.800229 avg loss no lamb -2.800229 time 2020-06-26 03:21:40.638389
Model ind 665 epoch 361 batch: 300 avg loss -2.802733 avg loss no lamb -2.802733 time 2020-06-26 03:21:51.449914
Model ind 665 epoch 361 batch: 400 avg loss -2.704183 avg loss no lamb -2.704183 time 2020-06-26 03:22:01.980401
Model ind 665 epoch 361 batch: 500 avg loss -2.713797 avg loss no lamb -2.713797 time 2020-06-26 03:22:12.602527
Model ind 665 epoch 361 batch: 600 avg loss -2.767245 avg loss no lamb -2.767245 time 2020-06-26 03:22:23.385793
Model ind 665 epoch 361 batch: 700 avg loss -2.675866 avg loss no lamb -2.675866 time 2020-06-26 03:22:34.118691
Model ind 665 epoch 361 batch: 800 avg loss -2.708262 avg loss no lamb -2.708262 time 2020-06-26 03:22:44.959964
last batch sz 10
Pre: time 2020-06-26 03:22:59.034065: 
 	std: 0.0035116882
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9812, 0.9741, 0.9811, 0.9739]
	train_accs: [0.98205, 0.98123336, 0.9759833, 0.98185, 0.9765667]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.9783
	best: 0.9812

Starting e_i: 362
Model ind 665 epoch 362 batch: 0 avg loss -2.842379 avg loss no lamb -2.842379 time 2020-06-26 03:22:59.824508
Model ind 665 epoch 362 batch: 100 avg loss -2.868505 avg loss no lamb -2.868505 time 2020-06-26 03:23:10.577010
Model ind 665 epoch 362 batch: 200 avg loss -2.765628 avg loss no lamb -2.765628 time 2020-06-26 03:23:21.257934
Model ind 665 epoch 362 batch: 300 avg loss -2.842093 avg loss no lamb -2.842093 time 2020-06-26 03:23:31.930047
Model ind 665 epoch 362 batch: 400 avg loss -2.691303 avg loss no lamb -2.691303 time 2020-06-26 03:23:42.651507
Model ind 665 epoch 362 batch: 500 avg loss -2.725863 avg loss no lamb -2.725863 time 2020-06-26 03:23:53.262405
Model ind 665 epoch 362 batch: 600 avg loss -2.806023 avg loss no lamb -2.806023 time 2020-06-26 03:24:04.103741
Model ind 665 epoch 362 batch: 700 avg loss -2.708146 avg loss no lamb -2.708146 time 2020-06-26 03:24:14.561356
Model ind 665 epoch 362 batch: 800 avg loss -2.791403 avg loss no lamb -2.791403 time 2020-06-26 03:24:25.226302
last batch sz 10
Pre: time 2020-06-26 03:24:39.063888: 
 	std: 0.0028088372
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9836, 0.9833, 0.9771, 0.9837, 0.9787]
	train_accs: [0.98298335, 0.9821333, 0.97775, 0.9830833, 0.9786]
	best_train_sub_head: 3
	worst: 0.9771
	avg: 0.98128
	best: 0.9837

Starting e_i: 363
Model ind 665 epoch 363 batch: 0 avg loss -2.930840 avg loss no lamb -2.930840 time 2020-06-26 03:24:39.993460
Model ind 665 epoch 363 batch: 100 avg loss -2.771368 avg loss no lamb -2.771368 time 2020-06-26 03:24:50.743507
Model ind 665 epoch 363 batch: 200 avg loss -2.827094 avg loss no lamb -2.827094 time 2020-06-26 03:25:01.334819
Model ind 665 epoch 363 batch: 300 avg loss -2.811005 avg loss no lamb -2.811005 time 2020-06-26 03:25:12.185966
Model ind 665 epoch 363 batch: 400 avg loss -2.686030 avg loss no lamb -2.686030 time 2020-06-26 03:25:23.051246
Model ind 665 epoch 363 batch: 500 avg loss -2.729089 avg loss no lamb -2.729089 time 2020-06-26 03:25:33.583952
Model ind 665 epoch 363 batch: 600 avg loss -2.733325 avg loss no lamb -2.733325 time 2020-06-26 03:25:44.236881
Model ind 665 epoch 363 batch: 700 avg loss -2.650098 avg loss no lamb -2.650098 time 2020-06-26 03:25:54.917398
Model ind 665 epoch 363 batch: 800 avg loss -2.750157 avg loss no lamb -2.750157 time 2020-06-26 03:26:05.768488
last batch sz 10
Pre: time 2020-06-26 03:26:19.447853: 
 	std: 0.0035016502
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9808, 0.973, 0.9819, 0.9763]
	train_accs: [0.98221666, 0.9812, 0.97491664, 0.9823167, 0.97643334]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97872
	best: 0.9819

Starting e_i: 364
Model ind 665 epoch 364 batch: 0 avg loss -2.880157 avg loss no lamb -2.880157 time 2020-06-26 03:26:20.255905
Model ind 665 epoch 364 batch: 100 avg loss -2.795702 avg loss no lamb -2.795702 time 2020-06-26 03:26:30.877147
Model ind 665 epoch 364 batch: 200 avg loss -2.818957 avg loss no lamb -2.818957 time 2020-06-26 03:26:41.458297
Model ind 665 epoch 364 batch: 300 avg loss -2.810502 avg loss no lamb -2.810502 time 2020-06-26 03:26:52.469652
Model ind 665 epoch 364 batch: 400 avg loss -2.743423 avg loss no lamb -2.743423 time 2020-06-26 03:27:03.273330
Model ind 665 epoch 364 batch: 500 avg loss -2.758682 avg loss no lamb -2.758682 time 2020-06-26 03:27:13.665137
Model ind 665 epoch 364 batch: 600 avg loss -2.849540 avg loss no lamb -2.849540 time 2020-06-26 03:27:24.339119
Model ind 665 epoch 364 batch: 700 avg loss -2.701919 avg loss no lamb -2.701919 time 2020-06-26 03:27:35.230386
Model ind 665 epoch 364 batch: 800 avg loss -2.745613 avg loss no lamb -2.745613 time 2020-06-26 03:27:46.105862
last batch sz 10
Pre: time 2020-06-26 03:28:00.050155: 
 	std: 0.0031783008
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9821, 0.9755, 0.9827, 0.9764]
	train_accs: [0.98265, 0.98205, 0.97665, 0.9825, 0.9772]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97981995
	best: 0.9824

Starting e_i: 365
Model ind 665 epoch 365 batch: 0 avg loss -2.925088 avg loss no lamb -2.925088 time 2020-06-26 03:28:00.884408
Model ind 665 epoch 365 batch: 100 avg loss -2.857636 avg loss no lamb -2.857636 time 2020-06-26 03:28:11.394391
Model ind 665 epoch 365 batch: 200 avg loss -2.859820 avg loss no lamb -2.859820 time 2020-06-26 03:28:22.224832
Model ind 665 epoch 365 batch: 300 avg loss -2.821787 avg loss no lamb -2.821787 time 2020-06-26 03:28:32.952536
Model ind 665 epoch 365 batch: 400 avg loss -2.728652 avg loss no lamb -2.728652 time 2020-06-26 03:28:43.654357
Model ind 665 epoch 365 batch: 500 avg loss -2.787225 avg loss no lamb -2.787225 time 2020-06-26 03:28:54.356901
Model ind 665 epoch 365 batch: 600 avg loss -2.790479 avg loss no lamb -2.790479 time 2020-06-26 03:29:05.180571
Model ind 665 epoch 365 batch: 700 avg loss -2.634837 avg loss no lamb -2.634837 time 2020-06-26 03:29:16.167745
Model ind 665 epoch 365 batch: 800 avg loss -2.751663 avg loss no lamb -2.751663 time 2020-06-26 03:29:27.046533
last batch sz 10
Pre: time 2020-06-26 03:29:41.095002: 
 	std: 0.0030506407
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9807, 0.9738, 0.9805, 0.9749]
	train_accs: [0.98146665, 0.9814, 0.9752667, 0.9816, 0.97585]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97805995
	best: 0.9805

Starting e_i: 366
Model ind 665 epoch 366 batch: 0 avg loss -2.876161 avg loss no lamb -2.876161 time 2020-06-26 03:29:41.978082
Model ind 665 epoch 366 batch: 100 avg loss -2.819194 avg loss no lamb -2.819194 time 2020-06-26 03:29:52.459976
Model ind 665 epoch 366 batch: 200 avg loss -2.753595 avg loss no lamb -2.753595 time 2020-06-26 03:30:03.038342
Model ind 665 epoch 366 batch: 300 avg loss -2.834408 avg loss no lamb -2.834408 time 2020-06-26 03:30:13.941524
Model ind 665 epoch 366 batch: 400 avg loss -2.721375 avg loss no lamb -2.721375 time 2020-06-26 03:30:24.816729
Model ind 665 epoch 366 batch: 500 avg loss -2.723381 avg loss no lamb -2.723381 time 2020-06-26 03:30:35.551989
Model ind 665 epoch 366 batch: 600 avg loss -2.812027 avg loss no lamb -2.812027 time 2020-06-26 03:30:46.242022
Model ind 665 epoch 366 batch: 700 avg loss -2.729868 avg loss no lamb -2.729868 time 2020-06-26 03:30:57.334849
Model ind 665 epoch 366 batch: 800 avg loss -2.805754 avg loss no lamb -2.805754 time 2020-06-26 03:31:08.475593
last batch sz 10
Pre: time 2020-06-26 03:31:22.328443: 
 	std: 0.002865235
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9807, 0.9756, 0.9826, 0.9774]
	train_accs: [0.98275, 0.9813333, 0.97715, 0.98286664, 0.97798336]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97981995
	best: 0.9826

Starting e_i: 367
Model ind 665 epoch 367 batch: 0 avg loss -2.862207 avg loss no lamb -2.862207 time 2020-06-26 03:31:23.112421
Model ind 665 epoch 367 batch: 100 avg loss -2.850819 avg loss no lamb -2.850819 time 2020-06-26 03:31:34.101606
Model ind 665 epoch 367 batch: 200 avg loss -2.843454 avg loss no lamb -2.843454 time 2020-06-26 03:31:44.822071
Model ind 665 epoch 367 batch: 300 avg loss -2.866675 avg loss no lamb -2.866675 time 2020-06-26 03:31:55.592891
Model ind 665 epoch 367 batch: 400 avg loss -2.781701 avg loss no lamb -2.781701 time 2020-06-26 03:32:06.714989
Model ind 665 epoch 367 batch: 500 avg loss -2.709260 avg loss no lamb -2.709260 time 2020-06-26 03:32:17.432236
Model ind 665 epoch 367 batch: 600 avg loss -2.811171 avg loss no lamb -2.811171 time 2020-06-26 03:32:28.109613
Model ind 665 epoch 367 batch: 700 avg loss -2.709095 avg loss no lamb -2.709095 time 2020-06-26 03:32:38.900159
Model ind 665 epoch 367 batch: 800 avg loss -2.803726 avg loss no lamb -2.803726 time 2020-06-26 03:32:49.698494
last batch sz 10
Pre: time 2020-06-26 03:33:03.873275: 
 	std: 0.0037512463
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9833, 0.981, 0.9746, 0.9826, 0.975]
	train_accs: [0.9831333, 0.98216665, 0.97575, 0.98296666, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.9793
	best: 0.9833

Starting e_i: 368
Model ind 665 epoch 368 batch: 0 avg loss -2.932627 avg loss no lamb -2.932627 time 2020-06-26 03:33:04.657312
Model ind 665 epoch 368 batch: 100 avg loss -2.771014 avg loss no lamb -2.771014 time 2020-06-26 03:33:15.395218
Model ind 665 epoch 368 batch: 200 avg loss -2.801024 avg loss no lamb -2.801024 time 2020-06-26 03:33:26.145926
Model ind 665 epoch 368 batch: 300 avg loss -2.812855 avg loss no lamb -2.812855 time 2020-06-26 03:33:36.898827
Model ind 665 epoch 368 batch: 400 avg loss -2.758885 avg loss no lamb -2.758885 time 2020-06-26 03:33:47.861550
Model ind 665 epoch 368 batch: 500 avg loss -2.750656 avg loss no lamb -2.750656 time 2020-06-26 03:33:58.566519
Model ind 665 epoch 368 batch: 600 avg loss -2.808361 avg loss no lamb -2.808361 time 2020-06-26 03:34:09.396474
Model ind 665 epoch 368 batch: 700 avg loss -2.604968 avg loss no lamb -2.604968 time 2020-06-26 03:34:20.065531
Model ind 665 epoch 368 batch: 800 avg loss -2.788686 avg loss no lamb -2.788686 time 2020-06-26 03:34:30.771378
last batch sz 10
Pre: time 2020-06-26 03:34:44.456473: 
 	std: 0.002692514
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9799, 0.9745, 0.9805, 0.9751]
	train_accs: [0.9816833, 0.98111665, 0.9765667, 0.9818, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97808
	best: 0.9805

Starting e_i: 369
Model ind 665 epoch 369 batch: 0 avg loss -2.890541 avg loss no lamb -2.890541 time 2020-06-26 03:34:45.404631
Model ind 665 epoch 369 batch: 100 avg loss -2.842644 avg loss no lamb -2.842644 time 2020-06-26 03:34:55.995280
Model ind 665 epoch 369 batch: 200 avg loss -2.811847 avg loss no lamb -2.811847 time 2020-06-26 03:35:06.866731
Model ind 665 epoch 369 batch: 300 avg loss -2.856778 avg loss no lamb -2.856778 time 2020-06-26 03:35:17.701586
Model ind 665 epoch 369 batch: 400 avg loss -2.732538 avg loss no lamb -2.732538 time 2020-06-26 03:35:28.485968
Model ind 665 epoch 369 batch: 500 avg loss -2.739334 avg loss no lamb -2.739334 time 2020-06-26 03:35:38.958386
Model ind 665 epoch 369 batch: 600 avg loss -2.819943 avg loss no lamb -2.819943 time 2020-06-26 03:35:49.638059
Model ind 665 epoch 369 batch: 700 avg loss -2.667703 avg loss no lamb -2.667703 time 2020-06-26 03:36:00.694868
Model ind 665 epoch 369 batch: 800 avg loss -2.743917 avg loss no lamb -2.743917 time 2020-06-26 03:36:11.373995
last batch sz 10
Pre: time 2020-06-26 03:36:25.326338: 
 	std: 0.0026004708
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9803, 0.976, 0.9818, 0.9765]
	train_accs: [0.98181665, 0.9806, 0.9762333, 0.98195, 0.97653335]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.97933996
	best: 0.9818

Starting e_i: 370
Model ind 665 epoch 370 batch: 0 avg loss -2.923168 avg loss no lamb -2.923168 time 2020-06-26 03:36:26.112720
Model ind 665 epoch 370 batch: 100 avg loss -2.820739 avg loss no lamb -2.820739 time 2020-06-26 03:36:36.781307
Model ind 665 epoch 370 batch: 200 avg loss -2.871644 avg loss no lamb -2.871644 time 2020-06-26 03:36:47.347086
Model ind 665 epoch 370 batch: 300 avg loss -2.838510 avg loss no lamb -2.838510 time 2020-06-26 03:36:58.262097
Model ind 665 epoch 370 batch: 400 avg loss -2.659979 avg loss no lamb -2.659979 time 2020-06-26 03:37:09.352284
Model ind 665 epoch 370 batch: 500 avg loss -2.768466 avg loss no lamb -2.768466 time 2020-06-26 03:37:20.017781
Model ind 665 epoch 370 batch: 600 avg loss -2.784708 avg loss no lamb -2.784708 time 2020-06-26 03:37:30.760193
Model ind 665 epoch 370 batch: 700 avg loss -2.630455 avg loss no lamb -2.630455 time 2020-06-26 03:37:41.464054
Model ind 665 epoch 370 batch: 800 avg loss -2.767312 avg loss no lamb -2.767312 time 2020-06-26 03:37:52.503370
last batch sz 10
Pre: time 2020-06-26 03:38:06.769046: 
 	std: 0.0023588105
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.983, 0.9774, 0.9835, 0.9791]
	train_accs: [0.98221666, 0.98183334, 0.97783333, 0.9830667, 0.97868335]
	best_train_sub_head: 3
	worst: 0.9774
	avg: 0.98099995
	best: 0.9835

Starting e_i: 371
Model ind 665 epoch 371 batch: 0 avg loss -2.894600 avg loss no lamb -2.894600 time 2020-06-26 03:38:08.751692
Model ind 665 epoch 371 batch: 100 avg loss -2.800450 avg loss no lamb -2.800450 time 2020-06-26 03:38:19.750185
Model ind 665 epoch 371 batch: 200 avg loss -2.790289 avg loss no lamb -2.790289 time 2020-06-26 03:38:30.393726
Model ind 665 epoch 371 batch: 300 avg loss -2.825327 avg loss no lamb -2.825327 time 2020-06-26 03:38:41.249466
Model ind 665 epoch 371 batch: 400 avg loss -2.744800 avg loss no lamb -2.744800 time 2020-06-26 03:38:51.890464
Model ind 665 epoch 371 batch: 500 avg loss -2.721078 avg loss no lamb -2.721078 time 2020-06-26 03:39:02.579880
Model ind 665 epoch 371 batch: 600 avg loss -2.776280 avg loss no lamb -2.776280 time 2020-06-26 03:39:13.424980
Model ind 665 epoch 371 batch: 700 avg loss -2.640105 avg loss no lamb -2.640105 time 2020-06-26 03:39:24.118136
Model ind 665 epoch 371 batch: 800 avg loss -2.763422 avg loss no lamb -2.763422 time 2020-06-26 03:39:35.043112
last batch sz 10
Pre: time 2020-06-26 03:39:48.785570: 
 	std: 0.0031065051
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9824, 0.9753, 0.9824, 0.9771]
	train_accs: [0.9823, 0.98188335, 0.9767333, 0.9826, 0.9771]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97994006
	best: 0.9824

Starting e_i: 372
Model ind 665 epoch 372 batch: 0 avg loss -2.886348 avg loss no lamb -2.886348 time 2020-06-26 03:39:49.693548
Model ind 665 epoch 372 batch: 100 avg loss -2.832940 avg loss no lamb -2.832940 time 2020-06-26 03:40:00.442035
Model ind 665 epoch 372 batch: 200 avg loss -2.810066 avg loss no lamb -2.810066 time 2020-06-26 03:40:11.003074
Model ind 665 epoch 372 batch: 300 avg loss -2.843607 avg loss no lamb -2.843607 time 2020-06-26 03:40:21.597979
Model ind 665 epoch 372 batch: 400 avg loss -2.732437 avg loss no lamb -2.732437 time 2020-06-26 03:40:32.634268
Model ind 665 epoch 372 batch: 500 avg loss -2.757533 avg loss no lamb -2.757533 time 2020-06-26 03:40:43.541529
Model ind 665 epoch 372 batch: 600 avg loss -2.790556 avg loss no lamb -2.790556 time 2020-06-26 03:40:54.201374
Model ind 665 epoch 372 batch: 700 avg loss -2.594090 avg loss no lamb -2.594090 time 2020-06-26 03:41:04.833397
Model ind 665 epoch 372 batch: 800 avg loss -2.824979 avg loss no lamb -2.824979 time 2020-06-26 03:41:15.702217
last batch sz 10
Pre: time 2020-06-26 03:41:29.742746: 
 	std: 0.002767972
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9824, 0.9763, 0.9825, 0.9772]
	train_accs: [0.98245, 0.98226666, 0.97705, 0.98263335, 0.9775]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.98012
	best: 0.9825

Starting e_i: 373
Model ind 665 epoch 373 batch: 0 avg loss -2.864598 avg loss no lamb -2.864598 time 2020-06-26 03:41:30.556478
Model ind 665 epoch 373 batch: 100 avg loss -2.840832 avg loss no lamb -2.840832 time 2020-06-26 03:41:41.069369
Model ind 665 epoch 373 batch: 200 avg loss -2.757089 avg loss no lamb -2.757089 time 2020-06-26 03:41:51.770040
Model ind 665 epoch 373 batch: 300 avg loss -2.845814 avg loss no lamb -2.845814 time 2020-06-26 03:42:02.468322
Model ind 665 epoch 373 batch: 400 avg loss -2.757844 avg loss no lamb -2.757844 time 2020-06-26 03:42:13.370944
Model ind 665 epoch 373 batch: 500 avg loss -2.728709 avg loss no lamb -2.728709 time 2020-06-26 03:42:24.054355
Model ind 665 epoch 373 batch: 600 avg loss -2.812037 avg loss no lamb -2.812037 time 2020-06-26 03:42:34.747739
Model ind 665 epoch 373 batch: 700 avg loss -2.743229 avg loss no lamb -2.743229 time 2020-06-26 03:42:45.486670
Model ind 665 epoch 373 batch: 800 avg loss -2.773540 avg loss no lamb -2.773540 time 2020-06-26 03:42:56.188695
last batch sz 10
Pre: time 2020-06-26 03:43:10.113834: 
 	std: 0.0028684484
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9809, 0.9764, 0.9831, 0.9766]
	train_accs: [0.98246664, 0.98165, 0.9774, 0.9827833, 0.97763336]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.9799
	best: 0.9831

Starting e_i: 374
Model ind 665 epoch 374 batch: 0 avg loss -2.932280 avg loss no lamb -2.932280 time 2020-06-26 03:43:10.955679
Model ind 665 epoch 374 batch: 100 avg loss -2.780399 avg loss no lamb -2.780399 time 2020-06-26 03:43:21.868814
Model ind 665 epoch 374 batch: 200 avg loss -2.799728 avg loss no lamb -2.799728 time 2020-06-26 03:43:32.600325
Model ind 665 epoch 374 batch: 300 avg loss -2.792511 avg loss no lamb -2.792511 time 2020-06-26 03:43:43.295905
Model ind 665 epoch 374 batch: 400 avg loss -2.640237 avg loss no lamb -2.640237 time 2020-06-26 03:43:54.049705
Model ind 665 epoch 374 batch: 500 avg loss -2.733950 avg loss no lamb -2.733950 time 2020-06-26 03:44:04.984773
Model ind 665 epoch 374 batch: 600 avg loss -2.806291 avg loss no lamb -2.806291 time 2020-06-26 03:44:15.693467
Model ind 665 epoch 374 batch: 700 avg loss -2.613464 avg loss no lamb -2.613464 time 2020-06-26 03:44:26.483317
Model ind 665 epoch 374 batch: 800 avg loss -2.774670 avg loss no lamb -2.774670 time 2020-06-26 03:44:37.351579
last batch sz 10
Pre: time 2020-06-26 03:44:51.352357: 
 	std: 0.003433366
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9804, 0.973, 0.9801, 0.9734]
	train_accs: [0.98135, 0.9810167, 0.97511667, 0.9814, 0.9755833]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.9774
	best: 0.9801

Starting e_i: 375
Model ind 665 epoch 375 batch: 0 avg loss -2.945894 avg loss no lamb -2.945894 time 2020-06-26 03:44:52.259448
Model ind 665 epoch 375 batch: 100 avg loss -2.876819 avg loss no lamb -2.876819 time 2020-06-26 03:45:03.031821
Model ind 665 epoch 375 batch: 200 avg loss -2.686056 avg loss no lamb -2.686056 time 2020-06-26 03:45:13.831185
Model ind 665 epoch 375 batch: 300 avg loss -2.808449 avg loss no lamb -2.808449 time 2020-06-26 03:45:24.641822
Model ind 665 epoch 375 batch: 400 avg loss -2.755307 avg loss no lamb -2.755307 time 2020-06-26 03:45:35.296672
Model ind 665 epoch 375 batch: 500 avg loss -2.729794 avg loss no lamb -2.729794 time 2020-06-26 03:45:45.936846
Model ind 665 epoch 375 batch: 600 avg loss -2.848316 avg loss no lamb -2.848316 time 2020-06-26 03:45:56.825048
Model ind 665 epoch 375 batch: 700 avg loss -2.633224 avg loss no lamb -2.633224 time 2020-06-26 03:46:07.490732
Model ind 665 epoch 375 batch: 800 avg loss -2.764901 avg loss no lamb -2.764901 time 2020-06-26 03:46:18.160747
last batch sz 10
Pre: time 2020-06-26 03:46:31.967923: 
 	std: 0.0031052087
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9823, 0.9757, 0.9826, 0.9764]
	train_accs: [0.98245, 0.9822, 0.9770167, 0.98246664, 0.97728336]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97984
	best: 0.9826

Starting e_i: 376
Model ind 665 epoch 376 batch: 0 avg loss -2.862942 avg loss no lamb -2.862942 time 2020-06-26 03:46:32.772050
Model ind 665 epoch 376 batch: 100 avg loss -2.892195 avg loss no lamb -2.892195 time 2020-06-26 03:46:43.451262
Model ind 665 epoch 376 batch: 200 avg loss -2.814742 avg loss no lamb -2.814742 time 2020-06-26 03:46:54.203787
Model ind 665 epoch 376 batch: 300 avg loss -2.852407 avg loss no lamb -2.852407 time 2020-06-26 03:47:05.103720
Model ind 665 epoch 376 batch: 400 avg loss -2.716552 avg loss no lamb -2.716552 time 2020-06-26 03:47:15.627559
Model ind 665 epoch 376 batch: 500 avg loss -2.741556 avg loss no lamb -2.741556 time 2020-06-26 03:47:26.251196
Model ind 665 epoch 376 batch: 600 avg loss -2.819346 avg loss no lamb -2.819346 time 2020-06-26 03:47:36.994230
Model ind 665 epoch 376 batch: 700 avg loss -2.724377 avg loss no lamb -2.724377 time 2020-06-26 03:47:47.594568
Model ind 665 epoch 376 batch: 800 avg loss -2.803946 avg loss no lamb -2.803946 time 2020-06-26 03:47:58.423723
last batch sz 10
Pre: time 2020-06-26 03:48:12.382046: 
 	std: 0.003044735
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9826, 0.9765, 0.9834, 0.977]
	train_accs: [0.98268336, 0.98191667, 0.97653335, 0.9827333, 0.9770833]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.98046
	best: 0.9834

Starting e_i: 377
Model ind 665 epoch 377 batch: 0 avg loss -2.863340 avg loss no lamb -2.863340 time 2020-06-26 03:48:13.177978
Model ind 665 epoch 377 batch: 100 avg loss -2.845828 avg loss no lamb -2.845828 time 2020-06-26 03:48:24.067406
Model ind 665 epoch 377 batch: 200 avg loss -2.723464 avg loss no lamb -2.723464 time 2020-06-26 03:48:34.906205
Model ind 665 epoch 377 batch: 300 avg loss -2.737405 avg loss no lamb -2.737405 time 2020-06-26 03:48:45.888670
Model ind 665 epoch 377 batch: 400 avg loss -2.766534 avg loss no lamb -2.766534 time 2020-06-26 03:48:56.602616
Model ind 665 epoch 377 batch: 500 avg loss -2.759494 avg loss no lamb -2.759494 time 2020-06-26 03:49:07.404480
Model ind 665 epoch 377 batch: 600 avg loss -2.765245 avg loss no lamb -2.765245 time 2020-06-26 03:49:18.061397
Model ind 665 epoch 377 batch: 700 avg loss -2.646028 avg loss no lamb -2.646028 time 2020-06-26 03:49:28.811399
Model ind 665 epoch 377 batch: 800 avg loss -2.735441 avg loss no lamb -2.735441 time 2020-06-26 03:49:39.395645
last batch sz 10
Pre: time 2020-06-26 03:49:53.153663: 
 	std: 0.0026384834
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9817, 0.9763, 0.9827, 0.9781]
	train_accs: [0.98233336, 0.9813833, 0.97653335, 0.98256665, 0.97745]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.98032
	best: 0.9827

Starting e_i: 378
Model ind 665 epoch 378 batch: 0 avg loss -2.933118 avg loss no lamb -2.933118 time 2020-06-26 03:49:54.075152
Model ind 665 epoch 378 batch: 100 avg loss -2.910966 avg loss no lamb -2.910966 time 2020-06-26 03:50:04.714539
Model ind 665 epoch 378 batch: 200 avg loss -2.826817 avg loss no lamb -2.826817 time 2020-06-26 03:50:15.433003
Model ind 665 epoch 378 batch: 300 avg loss -2.842208 avg loss no lamb -2.842208 time 2020-06-26 03:50:26.118377
Model ind 665 epoch 378 batch: 400 avg loss -2.732769 avg loss no lamb -2.732769 time 2020-06-26 03:50:36.744281
Model ind 665 epoch 378 batch: 500 avg loss -2.762472 avg loss no lamb -2.762472 time 2020-06-26 03:50:47.570314
Model ind 665 epoch 378 batch: 600 avg loss -2.778441 avg loss no lamb -2.778441 time 2020-06-26 03:50:58.339544
Model ind 665 epoch 378 batch: 700 avg loss -2.707237 avg loss no lamb -2.707237 time 2020-06-26 03:51:08.981579
Model ind 665 epoch 378 batch: 800 avg loss -2.750445 avg loss no lamb -2.750445 time 2020-06-26 03:51:19.768701
last batch sz 10
Pre: time 2020-06-26 03:51:33.743989: 
 	std: 0.0025953674
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9813, 0.9763, 0.9826, 0.9775]
	train_accs: [0.9827667, 0.982, 0.9773333, 0.98301667, 0.97761667]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.98
	best: 0.9826

Starting e_i: 379
Model ind 665 epoch 379 batch: 0 avg loss -2.928977 avg loss no lamb -2.928977 time 2020-06-26 03:51:34.559540
Model ind 665 epoch 379 batch: 100 avg loss -2.821398 avg loss no lamb -2.821398 time 2020-06-26 03:51:45.152122
Model ind 665 epoch 379 batch: 200 avg loss -2.817330 avg loss no lamb -2.817330 time 2020-06-26 03:51:55.688394
Model ind 665 epoch 379 batch: 300 avg loss -2.828109 avg loss no lamb -2.828109 time 2020-06-26 03:52:06.197938
Model ind 665 epoch 379 batch: 400 avg loss -2.690183 avg loss no lamb -2.690183 time 2020-06-26 03:52:16.933415
Model ind 665 epoch 379 batch: 500 avg loss -2.746389 avg loss no lamb -2.746389 time 2020-06-26 03:52:27.653083
Model ind 665 epoch 379 batch: 600 avg loss -2.773244 avg loss no lamb -2.773244 time 2020-06-26 03:52:38.436288
Model ind 665 epoch 379 batch: 700 avg loss -2.680845 avg loss no lamb -2.680845 time 2020-06-26 03:52:48.996245
Model ind 665 epoch 379 batch: 800 avg loss -2.756613 avg loss no lamb -2.756613 time 2020-06-26 03:52:59.918954
last batch sz 10
Pre: time 2020-06-26 03:53:13.951748: 
 	std: 0.0025432224
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9807, 0.975, 0.9812, 0.9765]
	train_accs: [0.98188335, 0.98145, 0.976, 0.98205, 0.9770167]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.9788
	best: 0.9812

Starting e_i: 380
Model ind 665 epoch 380 batch: 0 avg loss -2.889261 avg loss no lamb -2.889261 time 2020-06-26 03:53:14.765543
Model ind 665 epoch 380 batch: 100 avg loss -2.904060 avg loss no lamb -2.904060 time 2020-06-26 03:53:25.257903
Model ind 665 epoch 380 batch: 200 avg loss -2.746063 avg loss no lamb -2.746063 time 2020-06-26 03:53:36.041576
Model ind 665 epoch 380 batch: 300 avg loss -2.729642 avg loss no lamb -2.729642 time 2020-06-26 03:53:46.679000
Model ind 665 epoch 380 batch: 400 avg loss -2.673122 avg loss no lamb -2.673122 time 2020-06-26 03:53:57.629506
Model ind 665 epoch 380 batch: 500 avg loss -2.745664 avg loss no lamb -2.745664 time 2020-06-26 03:54:08.482619
Model ind 665 epoch 380 batch: 600 avg loss -2.874395 avg loss no lamb -2.874395 time 2020-06-26 03:54:19.359621
Model ind 665 epoch 380 batch: 700 avg loss -2.732496 avg loss no lamb -2.732496 time 2020-06-26 03:54:30.156918
Model ind 665 epoch 380 batch: 800 avg loss -2.752568 avg loss no lamb -2.752568 time 2020-06-26 03:54:40.662401
last batch sz 10
Pre: time 2020-06-26 03:54:54.397224: 
 	std: 0.002966068
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9825, 0.9764, 0.983, 0.977]
	train_accs: [0.98298335, 0.98185, 0.97711664, 0.98265, 0.9777333]
	best_train_sub_head: 0
	worst: 0.9764
	avg: 0.98032
	best: 0.9827

Starting e_i: 381
Model ind 665 epoch 381 batch: 0 avg loss -2.925992 avg loss no lamb -2.925992 time 2020-06-26 03:54:56.518839
Model ind 665 epoch 381 batch: 100 avg loss -2.802665 avg loss no lamb -2.802665 time 2020-06-26 03:55:07.274053
Model ind 665 epoch 381 batch: 200 avg loss -2.799199 avg loss no lamb -2.799199 time 2020-06-26 03:55:18.043517
Model ind 665 epoch 381 batch: 300 avg loss -2.826204 avg loss no lamb -2.826204 time 2020-06-26 03:55:28.930855
Model ind 665 epoch 381 batch: 400 avg loss -2.678324 avg loss no lamb -2.678324 time 2020-06-26 03:55:39.840267
Model ind 665 epoch 381 batch: 500 avg loss -2.813226 avg loss no lamb -2.813226 time 2020-06-26 03:55:50.651979
Model ind 665 epoch 381 batch: 600 avg loss -2.769626 avg loss no lamb -2.769626 time 2020-06-26 03:56:01.444219
Model ind 665 epoch 381 batch: 700 avg loss -2.647859 avg loss no lamb -2.647859 time 2020-06-26 03:56:12.007673
Model ind 665 epoch 381 batch: 800 avg loss -2.762892 avg loss no lamb -2.762892 time 2020-06-26 03:56:22.623185
last batch sz 10
Pre: time 2020-06-26 03:56:36.511911: 
 	std: 0.0028819432
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9818, 0.9762, 0.9824, 0.9764]
	train_accs: [0.9823167, 0.98156667, 0.97645, 0.98226666, 0.97665]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.97981995
	best: 0.9823

Starting e_i: 382
Model ind 665 epoch 382 batch: 0 avg loss -2.863163 avg loss no lamb -2.863163 time 2020-06-26 03:56:37.353424
Model ind 665 epoch 382 batch: 100 avg loss -2.848560 avg loss no lamb -2.848560 time 2020-06-26 03:56:47.869772
Model ind 665 epoch 382 batch: 200 avg loss -2.801388 avg loss no lamb -2.801388 time 2020-06-26 03:56:58.489333
Model ind 665 epoch 382 batch: 300 avg loss -2.819441 avg loss no lamb -2.819441 time 2020-06-26 03:57:09.089731
Model ind 665 epoch 382 batch: 400 avg loss -2.698079 avg loss no lamb -2.698079 time 2020-06-26 03:57:19.624009
Model ind 665 epoch 382 batch: 500 avg loss -2.760849 avg loss no lamb -2.760849 time 2020-06-26 03:57:30.168252
Model ind 665 epoch 382 batch: 600 avg loss -2.846655 avg loss no lamb -2.846655 time 2020-06-26 03:57:40.878642
Model ind 665 epoch 382 batch: 700 avg loss -2.699971 avg loss no lamb -2.699971 time 2020-06-26 03:57:51.721768
Model ind 665 epoch 382 batch: 800 avg loss -2.723023 avg loss no lamb -2.723023 time 2020-06-26 03:58:02.456940
last batch sz 10
Pre: time 2020-06-26 03:58:16.197727: 
 	std: 0.0031676996
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9833, 0.9764, 0.9823, 0.9764]
	train_accs: [0.9817167, 0.9813833, 0.97555, 0.9813833, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9764
	avg: 0.98026
	best: 0.9829

Starting e_i: 383
Model ind 665 epoch 383 batch: 0 avg loss -2.953471 avg loss no lamb -2.953471 time 2020-06-26 03:58:17.034769
Model ind 665 epoch 383 batch: 100 avg loss -2.808906 avg loss no lamb -2.808906 time 2020-06-26 03:58:27.856305
Model ind 665 epoch 383 batch: 200 avg loss -2.866807 avg loss no lamb -2.866807 time 2020-06-26 03:58:38.687278
Model ind 665 epoch 383 batch: 300 avg loss -2.846816 avg loss no lamb -2.846816 time 2020-06-26 03:58:49.367953
Model ind 665 epoch 383 batch: 400 avg loss -2.659928 avg loss no lamb -2.659928 time 2020-06-26 03:59:00.194481
Model ind 665 epoch 383 batch: 500 avg loss -2.682537 avg loss no lamb -2.682537 time 2020-06-26 03:59:10.951862
Model ind 665 epoch 383 batch: 600 avg loss -2.809561 avg loss no lamb -2.809561 time 2020-06-26 03:59:21.793676
Model ind 665 epoch 383 batch: 700 avg loss -2.710058 avg loss no lamb -2.710058 time 2020-06-26 03:59:32.454486
Model ind 665 epoch 383 batch: 800 avg loss -2.798708 avg loss no lamb -2.798708 time 2020-06-26 03:59:43.053770
last batch sz 10
Pre: time 2020-06-26 03:59:56.484587: 
 	std: 0.00297362
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9822, 0.9764, 0.9835, 0.9773]
	train_accs: [0.9824833, 0.98215, 0.9770833, 0.9827, 0.97783333]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.98044
	best: 0.9835

Starting e_i: 384
Model ind 665 epoch 384 batch: 0 avg loss -2.929818 avg loss no lamb -2.929818 time 2020-06-26 03:59:57.299581
Model ind 665 epoch 384 batch: 100 avg loss -2.829027 avg loss no lamb -2.829027 time 2020-06-26 04:00:07.930690
Model ind 665 epoch 384 batch: 200 avg loss -2.823010 avg loss no lamb -2.823010 time 2020-06-26 04:00:18.371887
Model ind 665 epoch 384 batch: 300 avg loss -2.816666 avg loss no lamb -2.816666 time 2020-06-26 04:00:28.949360
Model ind 665 epoch 384 batch: 400 avg loss -2.690129 avg loss no lamb -2.690129 time 2020-06-26 04:00:39.643013
Model ind 665 epoch 384 batch: 500 avg loss -2.775518 avg loss no lamb -2.775518 time 2020-06-26 04:00:50.166495
Model ind 665 epoch 384 batch: 600 avg loss -2.839470 avg loss no lamb -2.839470 time 2020-06-26 04:01:00.879191
Model ind 665 epoch 384 batch: 700 avg loss -2.645267 avg loss no lamb -2.645267 time 2020-06-26 04:01:11.369689
Model ind 665 epoch 384 batch: 800 avg loss -2.800397 avg loss no lamb -2.800397 time 2020-06-26 04:01:22.255453
last batch sz 10
Pre: time 2020-06-26 04:01:36.327465: 
 	std: 0.0030928305
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9807, 0.9742, 0.9816, 0.9758]
	train_accs: [0.9822, 0.9813167, 0.9758833, 0.9823167, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97872
	best: 0.9816

Starting e_i: 385
Model ind 665 epoch 385 batch: 0 avg loss -2.928767 avg loss no lamb -2.928767 time 2020-06-26 04:01:37.147304
Model ind 665 epoch 385 batch: 100 avg loss -2.852320 avg loss no lamb -2.852320 time 2020-06-26 04:01:47.850487
Model ind 665 epoch 385 batch: 200 avg loss -2.822633 avg loss no lamb -2.822633 time 2020-06-26 04:01:58.530609
Model ind 665 epoch 385 batch: 300 avg loss -2.844146 avg loss no lamb -2.844146 time 2020-06-26 04:02:08.998018
Model ind 665 epoch 385 batch: 400 avg loss -2.687593 avg loss no lamb -2.687593 time 2020-06-26 04:02:19.789924
Model ind 665 epoch 385 batch: 500 avg loss -2.729684 avg loss no lamb -2.729684 time 2020-06-26 04:02:30.365924
Model ind 665 epoch 385 batch: 600 avg loss -2.769010 avg loss no lamb -2.769010 time 2020-06-26 04:02:41.064347
Model ind 665 epoch 385 batch: 700 avg loss -2.702705 avg loss no lamb -2.702705 time 2020-06-26 04:02:51.633351
Model ind 665 epoch 385 batch: 800 avg loss -2.837793 avg loss no lamb -2.837793 time 2020-06-26 04:03:02.437034
last batch sz 10
Pre: time 2020-06-26 04:03:16.202107: 
 	std: 0.003128328
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9821, 0.9749, 0.982, 0.9762]
	train_accs: [0.9826, 0.9824167, 0.9762167, 0.9827667, 0.9773333]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97933996
	best: 0.982

Starting e_i: 386
Model ind 665 epoch 386 batch: 0 avg loss -2.914251 avg loss no lamb -2.914251 time 2020-06-26 04:03:17.116734
Model ind 665 epoch 386 batch: 100 avg loss -2.856874 avg loss no lamb -2.856874 time 2020-06-26 04:03:27.531596
Model ind 665 epoch 386 batch: 200 avg loss -2.750940 avg loss no lamb -2.750940 time 2020-06-26 04:03:38.242763
Model ind 665 epoch 386 batch: 300 avg loss -2.769068 avg loss no lamb -2.769068 time 2020-06-26 04:03:48.979599
Model ind 665 epoch 386 batch: 400 avg loss -2.699563 avg loss no lamb -2.699563 time 2020-06-26 04:03:59.793987
Model ind 665 epoch 386 batch: 500 avg loss -2.776576 avg loss no lamb -2.776576 time 2020-06-26 04:04:10.479981
Model ind 665 epoch 386 batch: 600 avg loss -2.804281 avg loss no lamb -2.804281 time 2020-06-26 04:04:20.931904
Model ind 665 epoch 386 batch: 700 avg loss -2.696919 avg loss no lamb -2.696919 time 2020-06-26 04:04:31.458022
Model ind 665 epoch 386 batch: 800 avg loss -2.790993 avg loss no lamb -2.790993 time 2020-06-26 04:04:42.257406
last batch sz 10
Pre: time 2020-06-26 04:04:56.193973: 
 	std: 0.003066592
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9814, 0.9748, 0.9813, 0.9753]
	train_accs: [0.98191667, 0.98165, 0.97566664, 0.9820833, 0.97635]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97879994
	best: 0.9813

Starting e_i: 387
Model ind 665 epoch 387 batch: 0 avg loss -2.833360 avg loss no lamb -2.833360 time 2020-06-26 04:04:57.043581
Model ind 665 epoch 387 batch: 100 avg loss -2.830933 avg loss no lamb -2.830933 time 2020-06-26 04:05:07.676968
Model ind 665 epoch 387 batch: 200 avg loss -2.763166 avg loss no lamb -2.763166 time 2020-06-26 04:05:18.348960
Model ind 665 epoch 387 batch: 300 avg loss -2.800735 avg loss no lamb -2.800735 time 2020-06-26 04:05:29.054393
Model ind 665 epoch 387 batch: 400 avg loss -2.719223 avg loss no lamb -2.719223 time 2020-06-26 04:05:39.584736
Model ind 665 epoch 387 batch: 500 avg loss -2.761286 avg loss no lamb -2.761286 time 2020-06-26 04:05:50.147392
Model ind 665 epoch 387 batch: 600 avg loss -2.802251 avg loss no lamb -2.802251 time 2020-06-26 04:06:01.070888
Model ind 665 epoch 387 batch: 700 avg loss -2.712665 avg loss no lamb -2.712665 time 2020-06-26 04:06:11.878830
Model ind 665 epoch 387 batch: 800 avg loss -2.767677 avg loss no lamb -2.767677 time 2020-06-26 04:06:22.486571
last batch sz 10
Pre: time 2020-06-26 04:06:36.364512: 
 	std: 0.002465283
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9821, 0.9763, 0.9816, 0.9769]
	train_accs: [0.98181665, 0.98175, 0.97658336, 0.98216665, 0.9774333]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.97958004
	best: 0.9816

Starting e_i: 388
Model ind 665 epoch 388 batch: 0 avg loss -2.931638 avg loss no lamb -2.931638 time 2020-06-26 04:06:37.179597
Model ind 665 epoch 388 batch: 100 avg loss -2.854168 avg loss no lamb -2.854168 time 2020-06-26 04:06:47.802817
Model ind 665 epoch 388 batch: 200 avg loss -2.788897 avg loss no lamb -2.788897 time 2020-06-26 04:06:58.390439
Model ind 665 epoch 388 batch: 300 avg loss -2.779984 avg loss no lamb -2.779984 time 2020-06-26 04:07:09.041701
Model ind 665 epoch 388 batch: 400 avg loss -2.653219 avg loss no lamb -2.653219 time 2020-06-26 04:07:19.470329
Model ind 665 epoch 388 batch: 500 avg loss -2.776666 avg loss no lamb -2.776666 time 2020-06-26 04:07:30.089409
Model ind 665 epoch 388 batch: 600 avg loss -2.769456 avg loss no lamb -2.769456 time 2020-06-26 04:07:40.824759
Model ind 665 epoch 388 batch: 700 avg loss -2.639831 avg loss no lamb -2.639831 time 2020-06-26 04:07:51.449845
Model ind 665 epoch 388 batch: 800 avg loss -2.723583 avg loss no lamb -2.723583 time 2020-06-26 04:08:02.223763
last batch sz 10
Pre: time 2020-06-26 04:08:15.821635: 
 	std: 0.0028527842
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9822, 0.9761, 0.9828, 0.9771]
	train_accs: [0.9820167, 0.9820167, 0.97643334, 0.9823, 0.97745]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.98006
	best: 0.9828

Starting e_i: 389
Model ind 665 epoch 389 batch: 0 avg loss -2.953696 avg loss no lamb -2.953696 time 2020-06-26 04:08:16.710631
Model ind 665 epoch 389 batch: 100 avg loss -2.832037 avg loss no lamb -2.832037 time 2020-06-26 04:08:27.427561
Model ind 665 epoch 389 batch: 200 avg loss -2.887652 avg loss no lamb -2.887652 time 2020-06-26 04:08:38.038767
Model ind 665 epoch 389 batch: 300 avg loss -2.800696 avg loss no lamb -2.800696 time 2020-06-26 04:08:48.787042
Model ind 665 epoch 389 batch: 400 avg loss -2.727402 avg loss no lamb -2.727402 time 2020-06-26 04:08:59.275568
Model ind 665 epoch 389 batch: 500 avg loss -2.714115 avg loss no lamb -2.714115 time 2020-06-26 04:09:10.039871
Model ind 665 epoch 389 batch: 600 avg loss -2.857469 avg loss no lamb -2.857469 time 2020-06-26 04:09:20.691023
Model ind 665 epoch 389 batch: 700 avg loss -2.718222 avg loss no lamb -2.718222 time 2020-06-26 04:09:31.290603
Model ind 665 epoch 389 batch: 800 avg loss -2.774219 avg loss no lamb -2.774219 time 2020-06-26 04:09:41.749639
last batch sz 10
Pre: time 2020-06-26 04:09:55.587383: 
 	std: 0.0026003139
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9829, 0.9778, 0.9831, 0.9773]
	train_accs: [0.9823167, 0.9818, 0.9773833, 0.98228335, 0.9777667]
	best_train_sub_head: 0
	worst: 0.9773
	avg: 0.98072004
	best: 0.9825

Starting e_i: 390
Model ind 665 epoch 390 batch: 0 avg loss -2.911425 avg loss no lamb -2.911425 time 2020-06-26 04:09:56.454483
Model ind 665 epoch 390 batch: 100 avg loss -2.844867 avg loss no lamb -2.844867 time 2020-06-26 04:10:07.019235
Model ind 665 epoch 390 batch: 200 avg loss -2.779315 avg loss no lamb -2.779315 time 2020-06-26 04:10:17.436105
Model ind 665 epoch 390 batch: 300 avg loss -2.819983 avg loss no lamb -2.819983 time 2020-06-26 04:10:28.136675
Model ind 665 epoch 390 batch: 400 avg loss -2.775426 avg loss no lamb -2.775426 time 2020-06-26 04:10:38.905433
Model ind 665 epoch 390 batch: 500 avg loss -2.772521 avg loss no lamb -2.772521 time 2020-06-26 04:10:49.739542
Model ind 665 epoch 390 batch: 600 avg loss -2.779365 avg loss no lamb -2.779365 time 2020-06-26 04:11:00.327203
Model ind 665 epoch 390 batch: 700 avg loss -2.650127 avg loss no lamb -2.650127 time 2020-06-26 04:11:10.848116
Model ind 665 epoch 390 batch: 800 avg loss -2.787104 avg loss no lamb -2.787104 time 2020-06-26 04:11:21.450970
last batch sz 10
Pre: time 2020-06-26 04:11:35.490844: 
 	std: 0.00280171
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9837, 0.9842, 0.9781, 0.9835, 0.9781]
	train_accs: [0.98335, 0.9824833, 0.9777333, 0.98326665, 0.9781333]
	best_train_sub_head: 0
	worst: 0.9781
	avg: 0.98152
	best: 0.9837

Starting e_i: 391
Model ind 665 epoch 391 batch: 0 avg loss -2.928891 avg loss no lamb -2.928891 time 2020-06-26 04:11:37.470231
Model ind 665 epoch 391 batch: 100 avg loss -2.843468 avg loss no lamb -2.843468 time 2020-06-26 04:11:48.116135
Model ind 665 epoch 391 batch: 200 avg loss -2.836144 avg loss no lamb -2.836144 time 2020-06-26 04:11:58.838843
Model ind 665 epoch 391 batch: 300 avg loss -2.851606 avg loss no lamb -2.851606 time 2020-06-26 04:12:09.524080
Model ind 665 epoch 391 batch: 400 avg loss -2.660310 avg loss no lamb -2.660310 time 2020-06-26 04:12:19.918427
Model ind 665 epoch 391 batch: 500 avg loss -2.832345 avg loss no lamb -2.832345 time 2020-06-26 04:12:30.664787
Model ind 665 epoch 391 batch: 600 avg loss -2.803330 avg loss no lamb -2.803330 time 2020-06-26 04:12:41.669919
Model ind 665 epoch 391 batch: 700 avg loss -2.654117 avg loss no lamb -2.654117 time 2020-06-26 04:12:52.055173
Model ind 665 epoch 391 batch: 800 avg loss -2.849314 avg loss no lamb -2.849314 time 2020-06-26 04:13:02.504384
last batch sz 10
Pre: time 2020-06-26 04:13:16.566354: 
 	std: 0.0034517308
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9825, 0.9748, 0.9821, 0.9757]
	train_accs: [0.98226666, 0.9817333, 0.97606665, 0.98233336, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97945994
	best: 0.9821

Starting e_i: 392
Model ind 665 epoch 392 batch: 0 avg loss -2.874214 avg loss no lamb -2.874214 time 2020-06-26 04:13:17.501838
Model ind 665 epoch 392 batch: 100 avg loss -2.813978 avg loss no lamb -2.813978 time 2020-06-26 04:13:28.267427
Model ind 665 epoch 392 batch: 200 avg loss -2.835811 avg loss no lamb -2.835811 time 2020-06-26 04:13:39.126140
Model ind 665 epoch 392 batch: 300 avg loss -2.783447 avg loss no lamb -2.783447 time 2020-06-26 04:13:49.842882
Model ind 665 epoch 392 batch: 400 avg loss -2.695460 avg loss no lamb -2.695460 time 2020-06-26 04:14:00.750880
Model ind 665 epoch 392 batch: 500 avg loss -2.740228 avg loss no lamb -2.740228 time 2020-06-26 04:14:11.516253
Model ind 665 epoch 392 batch: 600 avg loss -2.783253 avg loss no lamb -2.783253 time 2020-06-26 04:14:22.224318
Model ind 665 epoch 392 batch: 700 avg loss -2.679234 avg loss no lamb -2.679234 time 2020-06-26 04:14:32.907298
Model ind 665 epoch 392 batch: 800 avg loss -2.750294 avg loss no lamb -2.750294 time 2020-06-26 04:14:43.546846
last batch sz 10
Pre: time 2020-06-26 04:14:57.297066: 
 	std: 0.0030955523
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.983, 0.9765, 0.9828, 0.9766]
	train_accs: [0.9827167, 0.98211664, 0.97723335, 0.98286664, 0.9775]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.98034
	best: 0.9828

Starting e_i: 393
Model ind 665 epoch 393 batch: 0 avg loss -2.920424 avg loss no lamb -2.920424 time 2020-06-26 04:14:58.139917
Model ind 665 epoch 393 batch: 100 avg loss -2.749273 avg loss no lamb -2.749273 time 2020-06-26 04:15:08.864774
Model ind 665 epoch 393 batch: 200 avg loss -2.800750 avg loss no lamb -2.800750 time 2020-06-26 04:15:19.570792
Model ind 665 epoch 393 batch: 300 avg loss -2.773999 avg loss no lamb -2.773999 time 2020-06-26 04:15:30.243183
Model ind 665 epoch 393 batch: 400 avg loss -2.678856 avg loss no lamb -2.678856 time 2020-06-26 04:15:40.920852
Model ind 665 epoch 393 batch: 500 avg loss -2.718335 avg loss no lamb -2.718335 time 2020-06-26 04:15:51.722040
Model ind 665 epoch 393 batch: 600 avg loss -2.774651 avg loss no lamb -2.774651 time 2020-06-26 04:16:02.573864
Model ind 665 epoch 393 batch: 700 avg loss -2.718625 avg loss no lamb -2.718625 time 2020-06-26 04:16:13.064487
Model ind 665 epoch 393 batch: 800 avg loss -2.765545 avg loss no lamb -2.765545 time 2020-06-26 04:16:23.670107
last batch sz 10
Pre: time 2020-06-26 04:16:37.380446: 
 	std: 0.0031295945
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9812, 0.9749, 0.9821, 0.976]
	train_accs: [0.9819, 0.98155, 0.976, 0.9821, 0.9766]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97923994
	best: 0.9821

Starting e_i: 394
Model ind 665 epoch 394 batch: 0 avg loss -2.888064 avg loss no lamb -2.888064 time 2020-06-26 04:16:38.229245
Model ind 665 epoch 394 batch: 100 avg loss -2.760047 avg loss no lamb -2.760047 time 2020-06-26 04:16:48.951535
Model ind 665 epoch 394 batch: 200 avg loss -2.801104 avg loss no lamb -2.801104 time 2020-06-26 04:16:59.724061
Model ind 665 epoch 394 batch: 300 avg loss -2.807056 avg loss no lamb -2.807056 time 2020-06-26 04:17:10.522979
Model ind 665 epoch 394 batch: 400 avg loss -2.734418 avg loss no lamb -2.734418 time 2020-06-26 04:17:21.281337
Model ind 665 epoch 394 batch: 500 avg loss -2.708999 avg loss no lamb -2.708999 time 2020-06-26 04:17:32.266435
Model ind 665 epoch 394 batch: 600 avg loss -2.768594 avg loss no lamb -2.768594 time 2020-06-26 04:17:43.184855
Model ind 665 epoch 394 batch: 700 avg loss -2.681885 avg loss no lamb -2.681885 time 2020-06-26 04:17:54.089149
Model ind 665 epoch 394 batch: 800 avg loss -2.757987 avg loss no lamb -2.757987 time 2020-06-26 04:18:04.823696
last batch sz 10
Pre: time 2020-06-26 04:18:18.365654: 
 	std: 0.0039839745
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9796, 0.9719, 0.9813, 0.9733]
	train_accs: [0.9817167, 0.98038334, 0.97435, 0.98211664, 0.9751667]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.9774
	best: 0.9813

Starting e_i: 395
Model ind 665 epoch 395 batch: 0 avg loss -2.925070 avg loss no lamb -2.925070 time 2020-06-26 04:18:19.290752
Model ind 665 epoch 395 batch: 100 avg loss -2.787169 avg loss no lamb -2.787169 time 2020-06-26 04:18:30.208426
Model ind 665 epoch 395 batch: 200 avg loss -2.795092 avg loss no lamb -2.795092 time 2020-06-26 04:18:40.984873
Model ind 665 epoch 395 batch: 300 avg loss -2.821851 avg loss no lamb -2.821851 time 2020-06-26 04:18:52.073887
Model ind 665 epoch 395 batch: 400 avg loss -2.717427 avg loss no lamb -2.717427 time 2020-06-26 04:19:02.863932
Model ind 665 epoch 395 batch: 500 avg loss -2.696777 avg loss no lamb -2.696777 time 2020-06-26 04:19:13.698518
Model ind 665 epoch 395 batch: 600 avg loss -2.772402 avg loss no lamb -2.772402 time 2020-06-26 04:19:24.571971
Model ind 665 epoch 395 batch: 700 avg loss -2.665437 avg loss no lamb -2.665437 time 2020-06-26 04:19:35.285296
Model ind 665 epoch 395 batch: 800 avg loss -2.787073 avg loss no lamb -2.787073 time 2020-06-26 04:19:45.890233
last batch sz 10
Pre: time 2020-06-26 04:19:59.675758: 
 	std: 0.0033294973
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9794, 0.973, 0.9806, 0.9739]
	train_accs: [0.98185, 0.98118335, 0.9751, 0.98193336, 0.97603333]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97747993
	best: 0.9806

Starting e_i: 396
Model ind 665 epoch 396 batch: 0 avg loss -2.902110 avg loss no lamb -2.902110 time 2020-06-26 04:20:00.543693
Model ind 665 epoch 396 batch: 100 avg loss -2.827321 avg loss no lamb -2.827321 time 2020-06-26 04:20:11.284797
Model ind 665 epoch 396 batch: 200 avg loss -2.829151 avg loss no lamb -2.829151 time 2020-06-26 04:20:21.995917
Model ind 665 epoch 396 batch: 300 avg loss -2.844263 avg loss no lamb -2.844263 time 2020-06-26 04:20:32.644075
Model ind 665 epoch 396 batch: 400 avg loss -2.762806 avg loss no lamb -2.762806 time 2020-06-26 04:20:43.148581
Model ind 665 epoch 396 batch: 500 avg loss -2.727921 avg loss no lamb -2.727921 time 2020-06-26 04:20:53.735930
Model ind 665 epoch 396 batch: 600 avg loss -2.750229 avg loss no lamb -2.750229 time 2020-06-26 04:21:04.297638
Model ind 665 epoch 396 batch: 700 avg loss -2.650639 avg loss no lamb -2.650639 time 2020-06-26 04:21:15.095597
Model ind 665 epoch 396 batch: 800 avg loss -2.820170 avg loss no lamb -2.820170 time 2020-06-26 04:21:26.009179
last batch sz 10
Pre: time 2020-06-26 04:21:40.029371: 
 	std: 0.0026642892
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9834, 0.9832, 0.977, 0.9829, 0.9787]
	train_accs: [0.98285, 0.98228335, 0.9779, 0.9827833, 0.9786]
	best_train_sub_head: 0
	worst: 0.977
	avg: 0.98104
	best: 0.9834

Starting e_i: 397
Model ind 665 epoch 397 batch: 0 avg loss -2.874529 avg loss no lamb -2.874529 time 2020-06-26 04:21:40.860245
Model ind 665 epoch 397 batch: 100 avg loss -2.810224 avg loss no lamb -2.810224 time 2020-06-26 04:21:51.190892
Model ind 665 epoch 397 batch: 200 avg loss -2.815405 avg loss no lamb -2.815405 time 2020-06-26 04:22:01.885236
Model ind 665 epoch 397 batch: 300 avg loss -2.768455 avg loss no lamb -2.768455 time 2020-06-26 04:22:12.550649
Model ind 665 epoch 397 batch: 400 avg loss -2.644024 avg loss no lamb -2.644024 time 2020-06-26 04:22:23.338366
Model ind 665 epoch 397 batch: 500 avg loss -2.762466 avg loss no lamb -2.762466 time 2020-06-26 04:22:33.872508
Model ind 665 epoch 397 batch: 600 avg loss -2.774343 avg loss no lamb -2.774343 time 2020-06-26 04:22:44.347624
Model ind 665 epoch 397 batch: 700 avg loss -2.699118 avg loss no lamb -2.699118 time 2020-06-26 04:22:55.134651
Model ind 665 epoch 397 batch: 800 avg loss -2.793207 avg loss no lamb -2.793207 time 2020-06-26 04:23:05.952382
last batch sz 10
Pre: time 2020-06-26 04:23:19.825758: 
 	std: 0.0030162286
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9793, 0.9739, 0.981, 0.9751]
	train_accs: [0.98216665, 0.98065, 0.97568333, 0.98215, 0.97646666]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97808
	best: 0.9811

Starting e_i: 398
Model ind 665 epoch 398 batch: 0 avg loss -2.895298 avg loss no lamb -2.895298 time 2020-06-26 04:23:20.716562
Model ind 665 epoch 398 batch: 100 avg loss -2.853707 avg loss no lamb -2.853707 time 2020-06-26 04:23:31.441059
Model ind 665 epoch 398 batch: 200 avg loss -2.757800 avg loss no lamb -2.757800 time 2020-06-26 04:23:42.223440
Model ind 665 epoch 398 batch: 300 avg loss -2.803248 avg loss no lamb -2.803248 time 2020-06-26 04:23:52.803619
Model ind 665 epoch 398 batch: 400 avg loss -2.713255 avg loss no lamb -2.713255 time 2020-06-26 04:24:03.732555
Model ind 665 epoch 398 batch: 500 avg loss -2.730730 avg loss no lamb -2.730730 time 2020-06-26 04:24:14.424382
Model ind 665 epoch 398 batch: 600 avg loss -2.766510 avg loss no lamb -2.766510 time 2020-06-26 04:24:25.313593
Model ind 665 epoch 398 batch: 700 avg loss -2.703535 avg loss no lamb -2.703535 time 2020-06-26 04:24:35.899931
Model ind 665 epoch 398 batch: 800 avg loss -2.791023 avg loss no lamb -2.791023 time 2020-06-26 04:24:46.620376
last batch sz 10
Pre: time 2020-06-26 04:25:00.484972: 
 	std: 0.0023701587
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9796, 0.9754, 0.9812, 0.9766]
	train_accs: [0.9817167, 0.9809667, 0.97671664, 0.98185, 0.97713333]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97878
	best: 0.9812

Starting e_i: 399
Model ind 665 epoch 399 batch: 0 avg loss -2.960526 avg loss no lamb -2.960526 time 2020-06-26 04:25:01.358511
Model ind 665 epoch 399 batch: 100 avg loss -2.873657 avg loss no lamb -2.873657 time 2020-06-26 04:25:12.105258
Model ind 665 epoch 399 batch: 200 avg loss -2.864535 avg loss no lamb -2.864535 time 2020-06-26 04:25:22.997927
Model ind 665 epoch 399 batch: 300 avg loss -2.841843 avg loss no lamb -2.841843 time 2020-06-26 04:25:33.790823
Model ind 665 epoch 399 batch: 400 avg loss -2.698708 avg loss no lamb -2.698708 time 2020-06-26 04:25:44.667480
Model ind 665 epoch 399 batch: 500 avg loss -2.741112 avg loss no lamb -2.741112 time 2020-06-26 04:25:55.395967
Model ind 665 epoch 399 batch: 600 avg loss -2.755882 avg loss no lamb -2.755882 time 2020-06-26 04:26:06.151709
Model ind 665 epoch 399 batch: 700 avg loss -2.681116 avg loss no lamb -2.681116 time 2020-06-26 04:26:16.834756
Model ind 665 epoch 399 batch: 800 avg loss -2.795645 avg loss no lamb -2.795645 time 2020-06-26 04:26:27.598032
last batch sz 10
Pre: time 2020-06-26 04:26:41.845002: 
 	std: 0.0028011454
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9818, 0.9752, 0.9819, 0.9772]
	train_accs: [0.98228335, 0.98158336, 0.9766667, 0.9822, 0.9777]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97954
	best: 0.9816

Starting e_i: 400
Model ind 665 epoch 400 batch: 0 avg loss -2.948921 avg loss no lamb -2.948921 time 2020-06-26 04:26:42.675830
Model ind 665 epoch 400 batch: 100 avg loss -2.855834 avg loss no lamb -2.855834 time 2020-06-26 04:26:53.240761
Model ind 665 epoch 400 batch: 200 avg loss -2.812739 avg loss no lamb -2.812739 time 2020-06-26 04:27:04.167364
Model ind 665 epoch 400 batch: 300 avg loss -2.855424 avg loss no lamb -2.855424 time 2020-06-26 04:27:15.034478
Model ind 665 epoch 400 batch: 400 avg loss -2.761022 avg loss no lamb -2.761022 time 2020-06-26 04:27:25.881517
Model ind 665 epoch 400 batch: 500 avg loss -2.788961 avg loss no lamb -2.788961 time 2020-06-26 04:27:36.749903
Model ind 665 epoch 400 batch: 600 avg loss -2.859038 avg loss no lamb -2.859038 time 2020-06-26 04:27:47.606704
Model ind 665 epoch 400 batch: 700 avg loss -2.672961 avg loss no lamb -2.672961 time 2020-06-26 04:27:58.347728
Model ind 665 epoch 400 batch: 800 avg loss -2.752180 avg loss no lamb -2.752180 time 2020-06-26 04:28:09.242812
last batch sz 10
Pre: time 2020-06-26 04:28:22.945492: 
 	std: 0.0028673334
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9801, 0.9737, 0.9804, 0.975]
	train_accs: [0.9816333, 0.98123336, 0.9762333, 0.98185, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97782004
	best: 0.9804

Starting e_i: 401
Model ind 665 epoch 401 batch: 0 avg loss -2.925339 avg loss no lamb -2.925339 time 2020-06-26 04:28:25.011901
Model ind 665 epoch 401 batch: 100 avg loss -2.795622 avg loss no lamb -2.795622 time 2020-06-26 04:28:35.630389
Model ind 665 epoch 401 batch: 200 avg loss -2.817691 avg loss no lamb -2.817691 time 2020-06-26 04:28:46.351468
Model ind 665 epoch 401 batch: 300 avg loss -2.811790 avg loss no lamb -2.811790 time 2020-06-26 04:28:56.978577
Model ind 665 epoch 401 batch: 400 avg loss -2.624673 avg loss no lamb -2.624673 time 2020-06-26 04:29:07.736701
Model ind 665 epoch 401 batch: 500 avg loss -2.727979 avg loss no lamb -2.727979 time 2020-06-26 04:29:18.580709
Model ind 665 epoch 401 batch: 600 avg loss -2.879753 avg loss no lamb -2.879753 time 2020-06-26 04:29:29.286830
Model ind 665 epoch 401 batch: 700 avg loss -2.756099 avg loss no lamb -2.756099 time 2020-06-26 04:29:39.931443
Model ind 665 epoch 401 batch: 800 avg loss -2.821021 avg loss no lamb -2.821021 time 2020-06-26 04:29:50.738779
last batch sz 10
Pre: time 2020-06-26 04:30:04.837747: 
 	std: 0.0024154494
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9824, 0.9767, 0.9801, 0.9762]
	train_accs: [0.98205, 0.98155, 0.97693336, 0.98176664, 0.97726667]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.97925997
	best: 0.9809

Starting e_i: 402
Model ind 665 epoch 402 batch: 0 avg loss -2.904927 avg loss no lamb -2.904927 time 2020-06-26 04:30:05.689419
Model ind 665 epoch 402 batch: 100 avg loss -2.801127 avg loss no lamb -2.801127 time 2020-06-26 04:30:16.323686
Model ind 665 epoch 402 batch: 200 avg loss -2.807336 avg loss no lamb -2.807336 time 2020-06-26 04:30:27.083364
Model ind 665 epoch 402 batch: 300 avg loss -2.830658 avg loss no lamb -2.830658 time 2020-06-26 04:30:37.522083
Model ind 665 epoch 402 batch: 400 avg loss -2.692199 avg loss no lamb -2.692199 time 2020-06-26 04:30:48.154625
Model ind 665 epoch 402 batch: 500 avg loss -2.816115 avg loss no lamb -2.816115 time 2020-06-26 04:30:58.942790
Model ind 665 epoch 402 batch: 600 avg loss -2.782588 avg loss no lamb -2.782588 time 2020-06-26 04:31:09.840991
Model ind 665 epoch 402 batch: 700 avg loss -2.669126 avg loss no lamb -2.669126 time 2020-06-26 04:31:20.221043
Model ind 665 epoch 402 batch: 800 avg loss -2.770780 avg loss no lamb -2.770780 time 2020-06-26 04:31:30.941981
last batch sz 10
Pre: time 2020-06-26 04:31:44.929850: 
 	std: 0.002787544
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9808, 0.9763, 0.9818, 0.9757]
	train_accs: [0.9826, 0.9816, 0.9769, 0.9824833, 0.9768]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97936
	best: 0.9822

Starting e_i: 403
Model ind 665 epoch 403 batch: 0 avg loss -2.962185 avg loss no lamb -2.962185 time 2020-06-26 04:31:45.758514
Model ind 665 epoch 403 batch: 100 avg loss -2.826940 avg loss no lamb -2.826940 time 2020-06-26 04:31:56.230286
Model ind 665 epoch 403 batch: 200 avg loss -2.810467 avg loss no lamb -2.810467 time 2020-06-26 04:32:06.934392
Model ind 665 epoch 403 batch: 300 avg loss -2.849112 avg loss no lamb -2.849112 time 2020-06-26 04:32:17.555981
Model ind 665 epoch 403 batch: 400 avg loss -2.778959 avg loss no lamb -2.778959 time 2020-06-26 04:32:28.294489
Model ind 665 epoch 403 batch: 500 avg loss -2.772230 avg loss no lamb -2.772230 time 2020-06-26 04:32:38.885762
Model ind 665 epoch 403 batch: 600 avg loss -2.831586 avg loss no lamb -2.831586 time 2020-06-26 04:32:49.651306
Model ind 665 epoch 403 batch: 700 avg loss -2.621719 avg loss no lamb -2.621719 time 2020-06-26 04:33:00.544474
Model ind 665 epoch 403 batch: 800 avg loss -2.803896 avg loss no lamb -2.803896 time 2020-06-26 04:33:11.283176
last batch sz 10
Pre: time 2020-06-26 04:33:25.056199: 
 	std: 0.0027051086
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9816, 0.9755, 0.9816, 0.9767]
	train_accs: [0.98226666, 0.9815, 0.9762667, 0.98228335, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97938
	best: 0.9816

Starting e_i: 404
Model ind 665 epoch 404 batch: 0 avg loss -2.888551 avg loss no lamb -2.888551 time 2020-06-26 04:33:25.939939
Model ind 665 epoch 404 batch: 100 avg loss -2.790184 avg loss no lamb -2.790184 time 2020-06-26 04:33:36.501466
Model ind 665 epoch 404 batch: 200 avg loss -2.752683 avg loss no lamb -2.752683 time 2020-06-26 04:33:47.092224
Model ind 665 epoch 404 batch: 300 avg loss -2.772650 avg loss no lamb -2.772650 time 2020-06-26 04:33:57.897505
Model ind 665 epoch 404 batch: 400 avg loss -2.665829 avg loss no lamb -2.665829 time 2020-06-26 04:34:08.751635
Model ind 665 epoch 404 batch: 500 avg loss -2.716683 avg loss no lamb -2.716683 time 2020-06-26 04:34:19.452647
Model ind 665 epoch 404 batch: 600 avg loss -2.793102 avg loss no lamb -2.793102 time 2020-06-26 04:34:30.120959
Model ind 665 epoch 404 batch: 700 avg loss -2.683341 avg loss no lamb -2.683341 time 2020-06-26 04:34:40.830960
Model ind 665 epoch 404 batch: 800 avg loss -2.735737 avg loss no lamb -2.735737 time 2020-06-26 04:34:51.412641
last batch sz 10
Pre: time 2020-06-26 04:35:05.574526: 
 	std: 0.0027139597
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9825, 0.9764, 0.9828, 0.9779]
	train_accs: [0.98211664, 0.98186666, 0.9762667, 0.98216665, 0.9773667]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.98042
	best: 0.9828

Starting e_i: 405
Model ind 665 epoch 405 batch: 0 avg loss -2.892828 avg loss no lamb -2.892828 time 2020-06-26 04:35:06.394131
Model ind 665 epoch 405 batch: 100 avg loss -2.810471 avg loss no lamb -2.810471 time 2020-06-26 04:35:17.222930
Model ind 665 epoch 405 batch: 200 avg loss -2.819124 avg loss no lamb -2.819124 time 2020-06-26 04:35:27.774458
Model ind 665 epoch 405 batch: 300 avg loss -2.804169 avg loss no lamb -2.804169 time 2020-06-26 04:35:38.499993
Model ind 665 epoch 405 batch: 400 avg loss -2.696419 avg loss no lamb -2.696419 time 2020-06-26 04:35:49.064569
Model ind 665 epoch 405 batch: 500 avg loss -2.752260 avg loss no lamb -2.752260 time 2020-06-26 04:35:59.853106
Model ind 665 epoch 405 batch: 600 avg loss -2.817433 avg loss no lamb -2.817433 time 2020-06-26 04:36:10.437517
Model ind 665 epoch 405 batch: 700 avg loss -2.741905 avg loss no lamb -2.741905 time 2020-06-26 04:36:21.090951
Model ind 665 epoch 405 batch: 800 avg loss -2.770050 avg loss no lamb -2.770050 time 2020-06-26 04:36:31.837134
last batch sz 10
Pre: time 2020-06-26 04:36:45.847022: 
 	std: 0.0025107851
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9818, 0.9764, 0.9822, 0.9773]
	train_accs: [0.98225, 0.9817167, 0.9770833, 0.98246664, 0.9781167]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.97990006
	best: 0.9822

Starting e_i: 406
Model ind 665 epoch 406 batch: 0 avg loss -2.929953 avg loss no lamb -2.929953 time 2020-06-26 04:36:46.690461
Model ind 665 epoch 406 batch: 100 avg loss -2.800211 avg loss no lamb -2.800211 time 2020-06-26 04:36:57.387487
Model ind 665 epoch 406 batch: 200 avg loss -2.810719 avg loss no lamb -2.810719 time 2020-06-26 04:37:08.090698
Model ind 665 epoch 406 batch: 300 avg loss -2.790676 avg loss no lamb -2.790676 time 2020-06-26 04:37:18.965066
Model ind 665 epoch 406 batch: 400 avg loss -2.717925 avg loss no lamb -2.717925 time 2020-06-26 04:37:29.548917
Model ind 665 epoch 406 batch: 500 avg loss -2.748835 avg loss no lamb -2.748835 time 2020-06-26 04:37:40.069250
Model ind 665 epoch 406 batch: 600 avg loss -2.797391 avg loss no lamb -2.797391 time 2020-06-26 04:37:50.497978
Model ind 665 epoch 406 batch: 700 avg loss -2.682047 avg loss no lamb -2.682047 time 2020-06-26 04:38:01.267345
Model ind 665 epoch 406 batch: 800 avg loss -2.816985 avg loss no lamb -2.816985 time 2020-06-26 04:38:11.996771
last batch sz 10
Pre: time 2020-06-26 04:38:25.816447: 
 	std: 0.003135215
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9805, 0.9741, 0.9816, 0.9756]
	train_accs: [0.9824, 0.98183334, 0.97615, 0.9824833, 0.9769]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97861993
	best: 0.9816

Starting e_i: 407
Model ind 665 epoch 407 batch: 0 avg loss -2.901802 avg loss no lamb -2.901802 time 2020-06-26 04:38:26.671850
Model ind 665 epoch 407 batch: 100 avg loss -2.847030 avg loss no lamb -2.847030 time 2020-06-26 04:38:37.470381
Model ind 665 epoch 407 batch: 200 avg loss -2.795838 avg loss no lamb -2.795838 time 2020-06-26 04:38:48.137772
Model ind 665 epoch 407 batch: 300 avg loss -2.820934 avg loss no lamb -2.820934 time 2020-06-26 04:38:58.897282
Model ind 665 epoch 407 batch: 400 avg loss -2.703045 avg loss no lamb -2.703045 time 2020-06-26 04:39:09.603287
Model ind 665 epoch 407 batch: 500 avg loss -2.765947 avg loss no lamb -2.765947 time 2020-06-26 04:39:20.253062
Model ind 665 epoch 407 batch: 600 avg loss -2.807550 avg loss no lamb -2.807550 time 2020-06-26 04:39:30.769021
Model ind 665 epoch 407 batch: 700 avg loss -2.608760 avg loss no lamb -2.608760 time 2020-06-26 04:39:41.443141
Model ind 665 epoch 407 batch: 800 avg loss -2.803510 avg loss no lamb -2.803510 time 2020-06-26 04:39:52.332758
last batch sz 10
Pre: time 2020-06-26 04:40:06.261633: 
 	std: 0.0033958775
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9811, 0.9739, 0.9808, 0.9744]
	train_accs: [0.98226666, 0.9816833, 0.97603333, 0.9819667, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.9783
	best: 0.9813

Starting e_i: 408
Model ind 665 epoch 408 batch: 0 avg loss -2.976574 avg loss no lamb -2.976574 time 2020-06-26 04:40:07.058950
Model ind 665 epoch 408 batch: 100 avg loss -2.803260 avg loss no lamb -2.803260 time 2020-06-26 04:40:17.919721
Model ind 665 epoch 408 batch: 200 avg loss -2.847749 avg loss no lamb -2.847749 time 2020-06-26 04:40:28.506695
Model ind 665 epoch 408 batch: 300 avg loss -2.801395 avg loss no lamb -2.801395 time 2020-06-26 04:40:39.135491
Model ind 665 epoch 408 batch: 400 avg loss -2.646403 avg loss no lamb -2.646403 time 2020-06-26 04:40:49.760409
Model ind 665 epoch 408 batch: 500 avg loss -2.787012 avg loss no lamb -2.787012 time 2020-06-26 04:41:00.208044
Model ind 665 epoch 408 batch: 600 avg loss -2.819191 avg loss no lamb -2.819191 time 2020-06-26 04:41:10.685931
Model ind 665 epoch 408 batch: 700 avg loss -2.729276 avg loss no lamb -2.729276 time 2020-06-26 04:41:21.375053
Model ind 665 epoch 408 batch: 800 avg loss -2.812252 avg loss no lamb -2.812252 time 2020-06-26 04:41:32.195992
last batch sz 10
Pre: time 2020-06-26 04:41:46.016528: 
 	std: 0.0022771836
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9821, 0.9775, 0.9824, 0.9779]
	train_accs: [0.98286664, 0.9816, 0.97765, 0.9827, 0.97828335]
	best_train_sub_head: 0
	worst: 0.9775
	avg: 0.98048
	best: 0.9825

Starting e_i: 409
Model ind 665 epoch 409 batch: 0 avg loss -2.917340 avg loss no lamb -2.917340 time 2020-06-26 04:41:46.907570
Model ind 665 epoch 409 batch: 100 avg loss -2.865906 avg loss no lamb -2.865906 time 2020-06-26 04:41:57.304374
Model ind 665 epoch 409 batch: 200 avg loss -2.726902 avg loss no lamb -2.726902 time 2020-06-26 04:42:07.894196
Model ind 665 epoch 409 batch: 300 avg loss -2.788090 avg loss no lamb -2.788090 time 2020-06-26 04:42:18.395017
Model ind 665 epoch 409 batch: 400 avg loss -2.709699 avg loss no lamb -2.709699 time 2020-06-26 04:42:29.145670
Model ind 665 epoch 409 batch: 500 avg loss -2.787732 avg loss no lamb -2.787732 time 2020-06-26 04:42:39.887919
Model ind 665 epoch 409 batch: 600 avg loss -2.839707 avg loss no lamb -2.839707 time 2020-06-26 04:42:50.546460
Model ind 665 epoch 409 batch: 700 avg loss -2.693088 avg loss no lamb -2.693088 time 2020-06-26 04:43:01.455771
Model ind 665 epoch 409 batch: 800 avg loss -2.793553 avg loss no lamb -2.793553 time 2020-06-26 04:43:12.175648
last batch sz 10
Pre: time 2020-06-26 04:43:26.102768: 
 	std: 0.0023894839
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9817, 0.9766, 0.9815, 0.9766]
	train_accs: [0.98261666, 0.9820667, 0.97725, 0.98251665, 0.9773167]
	best_train_sub_head: 0
	worst: 0.9766
	avg: 0.97952
	best: 0.9812

Starting e_i: 410
Model ind 665 epoch 410 batch: 0 avg loss -2.881103 avg loss no lamb -2.881103 time 2020-06-26 04:43:26.966074
Model ind 665 epoch 410 batch: 100 avg loss -2.824733 avg loss no lamb -2.824733 time 2020-06-26 04:43:37.725145
Model ind 665 epoch 410 batch: 200 avg loss -2.860397 avg loss no lamb -2.860397 time 2020-06-26 04:43:48.602332
Model ind 665 epoch 410 batch: 300 avg loss -2.783059 avg loss no lamb -2.783059 time 2020-06-26 04:43:59.143298
Model ind 665 epoch 410 batch: 400 avg loss -2.671005 avg loss no lamb -2.671005 time 2020-06-26 04:44:09.793375
Model ind 665 epoch 410 batch: 500 avg loss -2.777956 avg loss no lamb -2.777956 time 2020-06-26 04:44:20.657820
Model ind 665 epoch 410 batch: 600 avg loss -2.869789 avg loss no lamb -2.869789 time 2020-06-26 04:44:31.337934
Model ind 665 epoch 410 batch: 700 avg loss -2.737768 avg loss no lamb -2.737768 time 2020-06-26 04:44:41.951075
Model ind 665 epoch 410 batch: 800 avg loss -2.742128 avg loss no lamb -2.742128 time 2020-06-26 04:44:52.451175
last batch sz 10
Pre: time 2020-06-26 04:45:06.283713: 
 	std: 0.0023591376
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9808, 0.9753, 0.9808, 0.9768]
	train_accs: [0.98116666, 0.9807, 0.97608334, 0.98123336, 0.9771]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97888005
	best: 0.9808

Starting e_i: 411
Model ind 665 epoch 411 batch: 0 avg loss -2.827937 avg loss no lamb -2.827937 time 2020-06-26 04:45:08.397995
Model ind 665 epoch 411 batch: 100 avg loss -2.822819 avg loss no lamb -2.822819 time 2020-06-26 04:45:19.288821
Model ind 665 epoch 411 batch: 200 avg loss -2.838351 avg loss no lamb -2.838351 time 2020-06-26 04:45:30.055166
Model ind 665 epoch 411 batch: 300 avg loss -2.832301 avg loss no lamb -2.832301 time 2020-06-26 04:45:40.598999
Model ind 665 epoch 411 batch: 400 avg loss -2.726410 avg loss no lamb -2.726410 time 2020-06-26 04:45:51.139611
Model ind 665 epoch 411 batch: 500 avg loss -2.750555 avg loss no lamb -2.750555 time 2020-06-26 04:46:01.952492
Model ind 665 epoch 411 batch: 600 avg loss -2.851522 avg loss no lamb -2.851522 time 2020-06-26 04:46:12.666341
Model ind 665 epoch 411 batch: 700 avg loss -2.697501 avg loss no lamb -2.697501 time 2020-06-26 04:46:23.387489
Model ind 665 epoch 411 batch: 800 avg loss -2.790159 avg loss no lamb -2.790159 time 2020-06-26 04:46:34.043676
last batch sz 10
Pre: time 2020-06-26 04:46:47.789287: 
 	std: 0.0026567702
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9821, 0.9764, 0.9829, 0.9784]
	train_accs: [0.9825, 0.98178333, 0.97718334, 0.98251665, 0.97805]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.9805401
	best: 0.9829

Starting e_i: 412
Model ind 665 epoch 412 batch: 0 avg loss -2.892420 avg loss no lamb -2.892420 time 2020-06-26 04:46:48.704879
Model ind 665 epoch 412 batch: 100 avg loss -2.865958 avg loss no lamb -2.865958 time 2020-06-26 04:46:59.409765
Model ind 665 epoch 412 batch: 200 avg loss -2.835873 avg loss no lamb -2.835873 time 2020-06-26 04:47:10.266648
Model ind 665 epoch 412 batch: 300 avg loss -2.801372 avg loss no lamb -2.801372 time 2020-06-26 04:47:20.832994
Model ind 665 epoch 412 batch: 400 avg loss -2.735665 avg loss no lamb -2.735665 time 2020-06-26 04:47:31.596115
Model ind 665 epoch 412 batch: 500 avg loss -2.783906 avg loss no lamb -2.783906 time 2020-06-26 04:47:42.382724
Model ind 665 epoch 412 batch: 600 avg loss -2.860207 avg loss no lamb -2.860207 time 2020-06-26 04:47:53.039913
Model ind 665 epoch 412 batch: 700 avg loss -2.689728 avg loss no lamb -2.689728 time 2020-06-26 04:48:03.737385
Model ind 665 epoch 412 batch: 800 avg loss -2.736719 avg loss no lamb -2.736719 time 2020-06-26 04:48:14.491012
last batch sz 10
Pre: time 2020-06-26 04:48:28.252473: 
 	std: 0.0021629632
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9789, 0.9745, 0.9802, 0.9773]
	train_accs: [0.98121667, 0.9801, 0.9755167, 0.9813, 0.97723335]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97824
	best: 0.9802

Starting e_i: 413
Model ind 665 epoch 413 batch: 0 avg loss -2.893422 avg loss no lamb -2.893422 time 2020-06-26 04:48:29.030012
Model ind 665 epoch 413 batch: 100 avg loss -2.889600 avg loss no lamb -2.889600 time 2020-06-26 04:48:39.766118
Model ind 665 epoch 413 batch: 200 avg loss -2.811937 avg loss no lamb -2.811937 time 2020-06-26 04:48:50.464373
Model ind 665 epoch 413 batch: 300 avg loss -2.859320 avg loss no lamb -2.859320 time 2020-06-26 04:49:01.244987
Model ind 665 epoch 413 batch: 400 avg loss -2.721021 avg loss no lamb -2.721021 time 2020-06-26 04:49:12.039203
Model ind 665 epoch 413 batch: 500 avg loss -2.732713 avg loss no lamb -2.732713 time 2020-06-26 04:49:22.747997
Model ind 665 epoch 413 batch: 600 avg loss -2.791823 avg loss no lamb -2.791823 time 2020-06-26 04:49:33.275347
Model ind 665 epoch 413 batch: 700 avg loss -2.739910 avg loss no lamb -2.739910 time 2020-06-26 04:49:44.021355
Model ind 665 epoch 413 batch: 800 avg loss -2.784512 avg loss no lamb -2.784512 time 2020-06-26 04:49:54.796627
last batch sz 10
Pre: time 2020-06-26 04:50:08.891465: 
 	std: 0.0028074114
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9814, 0.975, 0.982, 0.9771]
	train_accs: [0.9819, 0.9813167, 0.9759333, 0.9817333, 0.9770833]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97937995
	best: 0.9814

Starting e_i: 414
Model ind 665 epoch 414 batch: 0 avg loss -2.842946 avg loss no lamb -2.842946 time 2020-06-26 04:50:09.715412
Model ind 665 epoch 414 batch: 100 avg loss -2.771527 avg loss no lamb -2.771527 time 2020-06-26 04:50:20.345064
Model ind 665 epoch 414 batch: 200 avg loss -2.810022 avg loss no lamb -2.810022 time 2020-06-26 04:50:30.963898
Model ind 665 epoch 414 batch: 300 avg loss -2.917810 avg loss no lamb -2.917810 time 2020-06-26 04:50:41.639177
Model ind 665 epoch 414 batch: 400 avg loss -2.740704 avg loss no lamb -2.740704 time 2020-06-26 04:50:52.236498
Model ind 665 epoch 414 batch: 500 avg loss -2.788780 avg loss no lamb -2.788780 time 2020-06-26 04:51:02.897123
Model ind 665 epoch 414 batch: 600 avg loss -2.869960 avg loss no lamb -2.869960 time 2020-06-26 04:51:13.533202
Model ind 665 epoch 414 batch: 700 avg loss -2.716102 avg loss no lamb -2.716102 time 2020-06-26 04:51:24.250890
Model ind 665 epoch 414 batch: 800 avg loss -2.807641 avg loss no lamb -2.807641 time 2020-06-26 04:51:35.157357
last batch sz 10
Pre: time 2020-06-26 04:51:48.872578: 
 	std: 0.0023961547
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9808, 0.9753, 0.9806, 0.9768]
	train_accs: [0.9813833, 0.98081666, 0.97585, 0.9814, 0.97665]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97892
	best: 0.9806

Starting e_i: 415
Model ind 665 epoch 415 batch: 0 avg loss -2.904464 avg loss no lamb -2.904464 time 2020-06-26 04:51:49.706282
Model ind 665 epoch 415 batch: 100 avg loss -2.824640 avg loss no lamb -2.824640 time 2020-06-26 04:52:00.332401
Model ind 665 epoch 415 batch: 200 avg loss -2.801780 avg loss no lamb -2.801780 time 2020-06-26 04:52:11.133315
Model ind 665 epoch 415 batch: 300 avg loss -2.819242 avg loss no lamb -2.819242 time 2020-06-26 04:52:21.843395
Model ind 665 epoch 415 batch: 400 avg loss -2.699038 avg loss no lamb -2.699038 time 2020-06-26 04:52:32.584049
Model ind 665 epoch 415 batch: 500 avg loss -2.735939 avg loss no lamb -2.735939 time 2020-06-26 04:52:43.430961
Model ind 665 epoch 415 batch: 600 avg loss -2.820230 avg loss no lamb -2.820230 time 2020-06-26 04:52:54.225283
Model ind 665 epoch 415 batch: 700 avg loss -2.747768 avg loss no lamb -2.747768 time 2020-06-26 04:53:04.859022
Model ind 665 epoch 415 batch: 800 avg loss -2.771744 avg loss no lamb -2.771744 time 2020-06-26 04:53:15.608251
last batch sz 10
Pre: time 2020-06-26 04:53:29.781074: 
 	std: 0.002367779
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9828, 0.9774, 0.9829, 0.9786]
	train_accs: [0.98255, 0.9822, 0.9782, 0.98265, 0.97785]
	best_train_sub_head: 3
	worst: 0.9774
	avg: 0.98086005
	best: 0.9829

Starting e_i: 416
Model ind 665 epoch 416 batch: 0 avg loss -2.838374 avg loss no lamb -2.838374 time 2020-06-26 04:53:30.595155
Model ind 665 epoch 416 batch: 100 avg loss -2.793912 avg loss no lamb -2.793912 time 2020-06-26 04:53:41.286749
Model ind 665 epoch 416 batch: 200 avg loss -2.776112 avg loss no lamb -2.776112 time 2020-06-26 04:53:52.256680
Model ind 665 epoch 416 batch: 300 avg loss -2.794754 avg loss no lamb -2.794754 time 2020-06-26 04:54:03.330719
Model ind 665 epoch 416 batch: 400 avg loss -2.714627 avg loss no lamb -2.714627 time 2020-06-26 04:54:14.006283
Model ind 665 epoch 416 batch: 500 avg loss -2.803145 avg loss no lamb -2.803145 time 2020-06-26 04:54:24.750517
Model ind 665 epoch 416 batch: 600 avg loss -2.786590 avg loss no lamb -2.786590 time 2020-06-26 04:54:35.354292
Model ind 665 epoch 416 batch: 700 avg loss -2.659969 avg loss no lamb -2.659969 time 2020-06-26 04:54:46.090198
Model ind 665 epoch 416 batch: 800 avg loss -2.742415 avg loss no lamb -2.742415 time 2020-06-26 04:54:56.834952
last batch sz 10
Pre: time 2020-06-26 04:55:10.845497: 
 	std: 0.0024548909
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9802, 0.9747, 0.9804, 0.9769]
	train_accs: [0.98148334, 0.98066664, 0.97583336, 0.98125, 0.97715]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97866
	best: 0.9811

Starting e_i: 417
Model ind 665 epoch 417 batch: 0 avg loss -2.931569 avg loss no lamb -2.931569 time 2020-06-26 04:55:11.763214
Model ind 665 epoch 417 batch: 100 avg loss -2.854081 avg loss no lamb -2.854081 time 2020-06-26 04:55:22.531049
Model ind 665 epoch 417 batch: 200 avg loss -2.861487 avg loss no lamb -2.861487 time 2020-06-26 04:55:33.065008
Model ind 665 epoch 417 batch: 300 avg loss -2.769252 avg loss no lamb -2.769252 time 2020-06-26 04:55:43.625057
Model ind 665 epoch 417 batch: 400 avg loss -2.717932 avg loss no lamb -2.717932 time 2020-06-26 04:55:54.293513
Model ind 665 epoch 417 batch: 500 avg loss -2.778575 avg loss no lamb -2.778575 time 2020-06-26 04:56:05.142789
Model ind 665 epoch 417 batch: 600 avg loss -2.815339 avg loss no lamb -2.815339 time 2020-06-26 04:56:15.920738
Model ind 665 epoch 417 batch: 700 avg loss -2.702410 avg loss no lamb -2.702410 time 2020-06-26 04:56:26.690225
Model ind 665 epoch 417 batch: 800 avg loss -2.831427 avg loss no lamb -2.831427 time 2020-06-26 04:56:37.183203
last batch sz 10
Pre: time 2020-06-26 04:56:51.181471: 
 	std: 0.002805997
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9812, 0.9748, 0.9818, 0.977]
	train_accs: [0.98158336, 0.98095, 0.9759, 0.9813833, 0.9766167]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97922003
	best: 0.9813

Starting e_i: 418
Model ind 665 epoch 418 batch: 0 avg loss -2.859572 avg loss no lamb -2.859572 time 2020-06-26 04:56:52.011240
Model ind 665 epoch 418 batch: 100 avg loss -2.847037 avg loss no lamb -2.847037 time 2020-06-26 04:57:02.865612
Model ind 665 epoch 418 batch: 200 avg loss -2.868340 avg loss no lamb -2.868340 time 2020-06-26 04:57:13.688057
Model ind 665 epoch 418 batch: 300 avg loss -2.848366 avg loss no lamb -2.848366 time 2020-06-26 04:57:24.466624
Model ind 665 epoch 418 batch: 400 avg loss -2.720525 avg loss no lamb -2.720525 time 2020-06-26 04:57:35.313209
Model ind 665 epoch 418 batch: 500 avg loss -2.695259 avg loss no lamb -2.695259 time 2020-06-26 04:57:46.204394
Model ind 665 epoch 418 batch: 600 avg loss -2.801687 avg loss no lamb -2.801687 time 2020-06-26 04:57:56.945803
Model ind 665 epoch 418 batch: 700 avg loss -2.635772 avg loss no lamb -2.635772 time 2020-06-26 04:58:07.870757
Model ind 665 epoch 418 batch: 800 avg loss -2.773775 avg loss no lamb -2.773775 time 2020-06-26 04:58:18.592558
last batch sz 10
Pre: time 2020-06-26 04:58:32.833507: 
 	std: 0.003559008
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9804, 0.9729, 0.9811, 0.9744]
	train_accs: [0.98071665, 0.9805667, 0.97443336, 0.98111665, 0.9751667]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97796
	best: 0.9811

Starting e_i: 419
Model ind 665 epoch 419 batch: 0 avg loss -2.924912 avg loss no lamb -2.924912 time 2020-06-26 04:58:33.662892
Model ind 665 epoch 419 batch: 100 avg loss -2.820920 avg loss no lamb -2.820920 time 2020-06-26 04:58:44.395470
Model ind 665 epoch 419 batch: 200 avg loss -2.836694 avg loss no lamb -2.836694 time 2020-06-26 04:58:55.073166
Model ind 665 epoch 419 batch: 300 avg loss -2.828569 avg loss no lamb -2.828569 time 2020-06-26 04:59:05.718896
Model ind 665 epoch 419 batch: 400 avg loss -2.705019 avg loss no lamb -2.705019 time 2020-06-26 04:59:16.073863
Model ind 665 epoch 419 batch: 500 avg loss -2.760777 avg loss no lamb -2.760777 time 2020-06-26 04:59:26.802184
Model ind 665 epoch 419 batch: 600 avg loss -2.825186 avg loss no lamb -2.825186 time 2020-06-26 04:59:37.732386
Model ind 665 epoch 419 batch: 700 avg loss -2.613904 avg loss no lamb -2.613904 time 2020-06-26 04:59:48.544064
Model ind 665 epoch 419 batch: 800 avg loss -2.784166 avg loss no lamb -2.784166 time 2020-06-26 04:59:59.238512
last batch sz 10
Pre: time 2020-06-26 05:00:12.946741: 
 	std: 0.0036602793
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9802, 0.9727, 0.9822, 0.9755]
	train_accs: [0.9812667, 0.9805, 0.97456664, 0.9813833, 0.97573334]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97838
	best: 0.9822

Starting e_i: 420
Model ind 665 epoch 420 batch: 0 avg loss -2.903506 avg loss no lamb -2.903506 time 2020-06-26 05:00:13.783692
Model ind 665 epoch 420 batch: 100 avg loss -2.857552 avg loss no lamb -2.857552 time 2020-06-26 05:00:24.297313
Model ind 665 epoch 420 batch: 200 avg loss -2.872163 avg loss no lamb -2.872163 time 2020-06-26 05:00:34.966573
Model ind 665 epoch 420 batch: 300 avg loss -2.711167 avg loss no lamb -2.711167 time 2020-06-26 05:00:45.562066
Model ind 665 epoch 420 batch: 400 avg loss -2.690305 avg loss no lamb -2.690305 time 2020-06-26 05:00:56.123499
Model ind 665 epoch 420 batch: 500 avg loss -2.793573 avg loss no lamb -2.793573 time 2020-06-26 05:01:06.943426
Model ind 665 epoch 420 batch: 600 avg loss -2.798488 avg loss no lamb -2.798488 time 2020-06-26 05:01:17.594389
Model ind 665 epoch 420 batch: 700 avg loss -2.660403 avg loss no lamb -2.660403 time 2020-06-26 05:01:28.312543
Model ind 665 epoch 420 batch: 800 avg loss -2.818724 avg loss no lamb -2.818724 time 2020-06-26 05:01:39.058200
last batch sz 10
Pre: time 2020-06-26 05:01:52.794525: 
 	std: 0.0031147536
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9821, 0.9754, 0.9828, 0.9773]
	train_accs: [0.98256665, 0.98176664, 0.97651666, 0.9824833, 0.97735]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.98008
	best: 0.9828

Starting e_i: 421
Model ind 665 epoch 421 batch: 0 avg loss -2.935191 avg loss no lamb -2.935191 time 2020-06-26 05:01:54.801591
Model ind 665 epoch 421 batch: 100 avg loss -2.853831 avg loss no lamb -2.853831 time 2020-06-26 05:02:05.464467
Model ind 665 epoch 421 batch: 200 avg loss -2.774838 avg loss no lamb -2.774838 time 2020-06-26 05:02:16.133581
Model ind 665 epoch 421 batch: 300 avg loss -2.808368 avg loss no lamb -2.808368 time 2020-06-26 05:02:26.713114
Model ind 665 epoch 421 batch: 400 avg loss -2.713093 avg loss no lamb -2.713093 time 2020-06-26 05:02:37.544567
Model ind 665 epoch 421 batch: 500 avg loss -2.741042 avg loss no lamb -2.741042 time 2020-06-26 05:02:48.422100
Model ind 665 epoch 421 batch: 600 avg loss -2.799428 avg loss no lamb -2.799428 time 2020-06-26 05:02:59.366232
Model ind 665 epoch 421 batch: 700 avg loss -2.666297 avg loss no lamb -2.666297 time 2020-06-26 05:03:10.273210
Model ind 665 epoch 421 batch: 800 avg loss -2.737779 avg loss no lamb -2.737779 time 2020-06-26 05:03:20.924356
last batch sz 10
Pre: time 2020-06-26 05:03:34.786257: 
 	std: 0.0026280088
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9816, 0.9762, 0.9821, 0.9771]
	train_accs: [0.98246664, 0.9819, 0.9769167, 0.9824167, 0.9774167]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.97984
	best: 0.9822

Starting e_i: 422
Model ind 665 epoch 422 batch: 0 avg loss -2.909629 avg loss no lamb -2.909629 time 2020-06-26 05:03:35.595057
Model ind 665 epoch 422 batch: 100 avg loss -2.852716 avg loss no lamb -2.852716 time 2020-06-26 05:03:46.307022
Model ind 665 epoch 422 batch: 200 avg loss -2.785282 avg loss no lamb -2.785282 time 2020-06-26 05:03:56.848574
Model ind 665 epoch 422 batch: 300 avg loss -2.825852 avg loss no lamb -2.825852 time 2020-06-26 05:04:07.594237
Model ind 665 epoch 422 batch: 400 avg loss -2.682034 avg loss no lamb -2.682034 time 2020-06-26 05:04:18.097639
Model ind 665 epoch 422 batch: 500 avg loss -2.812754 avg loss no lamb -2.812754 time 2020-06-26 05:04:28.580653
Model ind 665 epoch 422 batch: 600 avg loss -2.779546 avg loss no lamb -2.779546 time 2020-06-26 05:04:39.280860
Model ind 665 epoch 422 batch: 700 avg loss -2.749307 avg loss no lamb -2.749307 time 2020-06-26 05:04:49.995406
Model ind 665 epoch 422 batch: 800 avg loss -2.838274 avg loss no lamb -2.838274 time 2020-06-26 05:05:00.636317
last batch sz 10
Pre: time 2020-06-26 05:05:14.735393: 
 	std: 0.0022085353
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9818, 0.9776, 0.9828, 0.9781]
	train_accs: [0.9827667, 0.98216665, 0.9774333, 0.98293334, 0.97786665]
	best_train_sub_head: 3
	worst: 0.9776
	avg: 0.98051995
	best: 0.9828

Starting e_i: 423
Model ind 665 epoch 423 batch: 0 avg loss -2.876306 avg loss no lamb -2.876306 time 2020-06-26 05:05:15.583819
Model ind 665 epoch 423 batch: 100 avg loss -2.809423 avg loss no lamb -2.809423 time 2020-06-26 05:05:26.377814
Model ind 665 epoch 423 batch: 200 avg loss -2.853392 avg loss no lamb -2.853392 time 2020-06-26 05:05:37.315588
Model ind 665 epoch 423 batch: 300 avg loss -2.818367 avg loss no lamb -2.818367 time 2020-06-26 05:05:48.029533
Model ind 665 epoch 423 batch: 400 avg loss -2.797058 avg loss no lamb -2.797058 time 2020-06-26 05:05:58.795427
Model ind 665 epoch 423 batch: 500 avg loss -2.736958 avg loss no lamb -2.736958 time 2020-06-26 05:06:09.466180
Model ind 665 epoch 423 batch: 600 avg loss -2.813572 avg loss no lamb -2.813572 time 2020-06-26 05:06:20.087825
Model ind 665 epoch 423 batch: 700 avg loss -2.712262 avg loss no lamb -2.712262 time 2020-06-26 05:06:30.644876
Model ind 665 epoch 423 batch: 800 avg loss -2.782255 avg loss no lamb -2.782255 time 2020-06-26 05:06:41.285378
last batch sz 10
Pre: time 2020-06-26 05:06:55.279495: 
 	std: 0.002006983
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9809, 0.9766, 0.9805, 0.9774]
	train_accs: [0.98176664, 0.98106664, 0.97688335, 0.9816, 0.97726667]
	best_train_sub_head: 0
	worst: 0.9766
	avg: 0.9794
	best: 0.9816

Starting e_i: 424
Model ind 665 epoch 424 batch: 0 avg loss -2.874187 avg loss no lamb -2.874187 time 2020-06-26 05:06:56.065111
Model ind 665 epoch 424 batch: 100 avg loss -2.839752 avg loss no lamb -2.839752 time 2020-06-26 05:07:06.752471
Model ind 665 epoch 424 batch: 200 avg loss -2.766769 avg loss no lamb -2.766769 time 2020-06-26 05:07:17.495059
Model ind 665 epoch 424 batch: 300 avg loss -2.730831 avg loss no lamb -2.730831 time 2020-06-26 05:07:28.081388
Model ind 665 epoch 424 batch: 400 avg loss -2.781774 avg loss no lamb -2.781774 time 2020-06-26 05:07:38.740305
Model ind 665 epoch 424 batch: 500 avg loss -2.751189 avg loss no lamb -2.751189 time 2020-06-26 05:07:49.503651
Model ind 665 epoch 424 batch: 600 avg loss -2.840895 avg loss no lamb -2.840895 time 2020-06-26 05:08:00.343087
Model ind 665 epoch 424 batch: 700 avg loss -2.766034 avg loss no lamb -2.766034 time 2020-06-26 05:08:10.943218
Model ind 665 epoch 424 batch: 800 avg loss -2.792442 avg loss no lamb -2.792442 time 2020-06-26 05:08:21.744570
last batch sz 10
Pre: time 2020-06-26 05:08:35.752638: 
 	std: 0.0034252082
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9813, 0.9742, 0.9818, 0.9755]
	train_accs: [0.98221666, 0.9813833, 0.9759, 0.98205, 0.9762]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.979
	best: 0.9822

Starting e_i: 425
Model ind 665 epoch 425 batch: 0 avg loss -2.924938 avg loss no lamb -2.924938 time 2020-06-26 05:08:36.641925
Model ind 665 epoch 425 batch: 100 avg loss -2.870059 avg loss no lamb -2.870059 time 2020-06-26 05:08:47.420883
Model ind 665 epoch 425 batch: 200 avg loss -2.836122 avg loss no lamb -2.836122 time 2020-06-26 05:08:58.083332
Model ind 665 epoch 425 batch: 300 avg loss -2.854679 avg loss no lamb -2.854679 time 2020-06-26 05:09:08.901056
Model ind 665 epoch 425 batch: 400 avg loss -2.770995 avg loss no lamb -2.770995 time 2020-06-26 05:09:19.720538
Model ind 665 epoch 425 batch: 500 avg loss -2.767483 avg loss no lamb -2.767483 time 2020-06-26 05:09:30.395069
Model ind 665 epoch 425 batch: 600 avg loss -2.779175 avg loss no lamb -2.779175 time 2020-06-26 05:09:41.169815
Model ind 665 epoch 425 batch: 700 avg loss -2.696210 avg loss no lamb -2.696210 time 2020-06-26 05:09:51.751629
Model ind 665 epoch 425 batch: 800 avg loss -2.807373 avg loss no lamb -2.807373 time 2020-06-26 05:10:02.370487
last batch sz 10
Pre: time 2020-06-26 05:10:15.947946: 
 	std: 0.0023795785
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9832, 0.9826, 0.9773, 0.983, 0.9792]
	train_accs: [0.9829, 0.9824833, 0.9776833, 0.9829, 0.97858334]
	best_train_sub_head: 0
	worst: 0.9773
	avg: 0.98106
	best: 0.9832

Starting e_i: 426
Model ind 665 epoch 426 batch: 0 avg loss -2.938631 avg loss no lamb -2.938631 time 2020-06-26 05:10:16.737729
Model ind 665 epoch 426 batch: 100 avg loss -2.847640 avg loss no lamb -2.847640 time 2020-06-26 05:10:27.381261
Model ind 665 epoch 426 batch: 200 avg loss -2.798759 avg loss no lamb -2.798759 time 2020-06-26 05:10:38.086759
Model ind 665 epoch 426 batch: 300 avg loss -2.840887 avg loss no lamb -2.840887 time 2020-06-26 05:10:48.896549
Model ind 665 epoch 426 batch: 400 avg loss -2.715153 avg loss no lamb -2.715153 time 2020-06-26 05:10:59.495833
Model ind 665 epoch 426 batch: 500 avg loss -2.715792 avg loss no lamb -2.715792 time 2020-06-26 05:11:10.449066
Model ind 665 epoch 426 batch: 600 avg loss -2.770149 avg loss no lamb -2.770149 time 2020-06-26 05:11:21.164924
Model ind 665 epoch 426 batch: 700 avg loss -2.709683 avg loss no lamb -2.709683 time 2020-06-26 05:11:31.871083
Model ind 665 epoch 426 batch: 800 avg loss -2.772758 avg loss no lamb -2.772758 time 2020-06-26 05:11:42.580167
last batch sz 10
Pre: time 2020-06-26 05:11:56.423714: 
 	std: 0.0027206044
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9823, 0.9765, 0.9829, 0.978]
	train_accs: [0.9819667, 0.9816333, 0.9770167, 0.9820333, 0.97763336]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.98052007
	best: 0.9829

Starting e_i: 427
Model ind 665 epoch 427 batch: 0 avg loss -2.890920 avg loss no lamb -2.890920 time 2020-06-26 05:11:57.209355
Model ind 665 epoch 427 batch: 100 avg loss -2.829548 avg loss no lamb -2.829548 time 2020-06-26 05:12:07.833200
Model ind 665 epoch 427 batch: 200 avg loss -2.845309 avg loss no lamb -2.845309 time 2020-06-26 05:12:18.564978
Model ind 665 epoch 427 batch: 300 avg loss -2.856573 avg loss no lamb -2.856573 time 2020-06-26 05:12:29.482568
Model ind 665 epoch 427 batch: 400 avg loss -2.654618 avg loss no lamb -2.654618 time 2020-06-26 05:12:40.182893
Model ind 665 epoch 427 batch: 500 avg loss -2.796472 avg loss no lamb -2.796472 time 2020-06-26 05:12:50.667838
Model ind 665 epoch 427 batch: 600 avg loss -2.847963 avg loss no lamb -2.847963 time 2020-06-26 05:13:01.341096
Model ind 665 epoch 427 batch: 700 avg loss -2.696295 avg loss no lamb -2.696295 time 2020-06-26 05:13:12.064903
Model ind 665 epoch 427 batch: 800 avg loss -2.743073 avg loss no lamb -2.743073 time 2020-06-26 05:13:22.991290
last batch sz 10
Pre: time 2020-06-26 05:13:36.861648: 
 	std: 0.0028138233
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.981, 0.975, 0.9811, 0.9765]
	train_accs: [0.98195, 0.98121667, 0.97575, 0.98191667, 0.9767]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.9791201
	best: 0.982

Starting e_i: 428
Model ind 665 epoch 428 batch: 0 avg loss -2.938440 avg loss no lamb -2.938440 time 2020-06-26 05:13:37.663271
Model ind 665 epoch 428 batch: 100 avg loss -2.820494 avg loss no lamb -2.820494 time 2020-06-26 05:13:48.220865
Model ind 665 epoch 428 batch: 200 avg loss -2.785850 avg loss no lamb -2.785850 time 2020-06-26 05:13:59.145167
Model ind 665 epoch 428 batch: 300 avg loss -2.860334 avg loss no lamb -2.860334 time 2020-06-26 05:14:10.049221
Model ind 665 epoch 428 batch: 400 avg loss -2.805391 avg loss no lamb -2.805391 time 2020-06-26 05:14:20.798100
Model ind 665 epoch 428 batch: 500 avg loss -2.739666 avg loss no lamb -2.739666 time 2020-06-26 05:14:31.501642
Model ind 665 epoch 428 batch: 600 avg loss -2.764393 avg loss no lamb -2.764393 time 2020-06-26 05:14:42.137191
Model ind 665 epoch 428 batch: 700 avg loss -2.730083 avg loss no lamb -2.730083 time 2020-06-26 05:14:52.904785
Model ind 665 epoch 428 batch: 800 avg loss -2.790412 avg loss no lamb -2.790412 time 2020-06-26 05:15:03.645440
last batch sz 10
Pre: time 2020-06-26 05:15:17.494205: 
 	std: 0.0036235873
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.98, 0.9727, 0.9809, 0.9743]
	train_accs: [0.98165, 0.98115, 0.9755167, 0.98158336, 0.9758]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.9778601
	best: 0.9814

Starting e_i: 429
Model ind 665 epoch 429 batch: 0 avg loss -2.931343 avg loss no lamb -2.931343 time 2020-06-26 05:15:18.305623
Model ind 665 epoch 429 batch: 100 avg loss -2.888558 avg loss no lamb -2.888558 time 2020-06-26 05:15:28.888639
Model ind 665 epoch 429 batch: 200 avg loss -2.805946 avg loss no lamb -2.805946 time 2020-06-26 05:15:39.544558
Model ind 665 epoch 429 batch: 300 avg loss -2.847854 avg loss no lamb -2.847854 time 2020-06-26 05:15:50.159147
Model ind 665 epoch 429 batch: 400 avg loss -2.770794 avg loss no lamb -2.770794 time 2020-06-26 05:16:00.843664
Model ind 665 epoch 429 batch: 500 avg loss -2.807081 avg loss no lamb -2.807081 time 2020-06-26 05:16:11.443125
Model ind 665 epoch 429 batch: 600 avg loss -2.776282 avg loss no lamb -2.776282 time 2020-06-26 05:16:21.968870
Model ind 665 epoch 429 batch: 700 avg loss -2.662909 avg loss no lamb -2.662909 time 2020-06-26 05:16:32.537616
Model ind 665 epoch 429 batch: 800 avg loss -2.737439 avg loss no lamb -2.737439 time 2020-06-26 05:16:43.361029
last batch sz 10
Pre: time 2020-06-26 05:16:57.349215: 
 	std: 0.002900749
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9819, 0.9753, 0.9821, 0.977]
	train_accs: [0.9817, 0.9816, 0.9766333, 0.98215, 0.97723335]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97964
	best: 0.9821

Starting e_i: 430
Model ind 665 epoch 430 batch: 0 avg loss -2.930290 avg loss no lamb -2.930290 time 2020-06-26 05:16:58.276586
Model ind 665 epoch 430 batch: 100 avg loss -2.833905 avg loss no lamb -2.833905 time 2020-06-26 05:17:08.915681
Model ind 665 epoch 430 batch: 200 avg loss -2.790912 avg loss no lamb -2.790912 time 2020-06-26 05:17:19.828511
Model ind 665 epoch 430 batch: 300 avg loss -2.804331 avg loss no lamb -2.804331 time 2020-06-26 05:17:30.752888
Model ind 665 epoch 430 batch: 400 avg loss -2.734216 avg loss no lamb -2.734216 time 2020-06-26 05:17:41.564294
Model ind 665 epoch 430 batch: 500 avg loss -2.772007 avg loss no lamb -2.772007 time 2020-06-26 05:17:52.067795
Model ind 665 epoch 430 batch: 600 avg loss -2.818125 avg loss no lamb -2.818125 time 2020-06-26 05:18:02.563110
Model ind 665 epoch 430 batch: 700 avg loss -2.668051 avg loss no lamb -2.668051 time 2020-06-26 05:18:12.943909
Model ind 665 epoch 430 batch: 800 avg loss -2.778107 avg loss no lamb -2.778107 time 2020-06-26 05:18:23.830289
last batch sz 10
Pre: time 2020-06-26 05:18:37.677418: 
 	std: 0.0029448224
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9801, 0.9745, 0.9805, 0.9741]
	train_accs: [0.98081666, 0.9799333, 0.9759167, 0.9809833, 0.97581667]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.9779
	best: 0.9805

Starting e_i: 431
Model ind 665 epoch 431 batch: 0 avg loss -2.943251 avg loss no lamb -2.943251 time 2020-06-26 05:18:39.639222
Model ind 665 epoch 431 batch: 100 avg loss -2.869215 avg loss no lamb -2.869215 time 2020-06-26 05:18:50.361855
Model ind 665 epoch 431 batch: 200 avg loss -2.803753 avg loss no lamb -2.803753 time 2020-06-26 05:19:00.869689
Model ind 665 epoch 431 batch: 300 avg loss -2.782898 avg loss no lamb -2.782898 time 2020-06-26 05:19:11.593738
Model ind 665 epoch 431 batch: 400 avg loss -2.775805 avg loss no lamb -2.775805 time 2020-06-26 05:19:22.174854
Model ind 665 epoch 431 batch: 500 avg loss -2.787156 avg loss no lamb -2.787156 time 2020-06-26 05:19:32.868173
Model ind 665 epoch 431 batch: 600 avg loss -2.837937 avg loss no lamb -2.837937 time 2020-06-26 05:19:43.471362
Model ind 665 epoch 431 batch: 700 avg loss -2.680977 avg loss no lamb -2.680977 time 2020-06-26 05:19:53.947897
Model ind 665 epoch 431 batch: 800 avg loss -2.732403 avg loss no lamb -2.732403 time 2020-06-26 05:20:04.496425
last batch sz 10
Pre: time 2020-06-26 05:20:18.323820: 
 	std: 0.0025227016
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9811, 0.9767, 0.9822, 0.9768]
	train_accs: [0.98165, 0.9809, 0.97705, 0.9819, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9767
	avg: 0.97980005
	best: 0.9822

Starting e_i: 432
Model ind 665 epoch 432 batch: 0 avg loss -2.855825 avg loss no lamb -2.855825 time 2020-06-26 05:20:19.140833
Model ind 665 epoch 432 batch: 100 avg loss -2.879709 avg loss no lamb -2.879709 time 2020-06-26 05:20:29.501642
Model ind 665 epoch 432 batch: 200 avg loss -2.807628 avg loss no lamb -2.807628 time 2020-06-26 05:20:40.051965
Model ind 665 epoch 432 batch: 300 avg loss -2.829785 avg loss no lamb -2.829785 time 2020-06-26 05:20:50.633854
Model ind 665 epoch 432 batch: 400 avg loss -2.720107 avg loss no lamb -2.720107 time 2020-06-26 05:21:01.232367
Model ind 665 epoch 432 batch: 500 avg loss -2.826796 avg loss no lamb -2.826796 time 2020-06-26 05:21:11.597932
Model ind 665 epoch 432 batch: 600 avg loss -2.824565 avg loss no lamb -2.824565 time 2020-06-26 05:21:22.133890
Model ind 665 epoch 432 batch: 700 avg loss -2.663031 avg loss no lamb -2.663031 time 2020-06-26 05:21:32.738311
Model ind 665 epoch 432 batch: 800 avg loss -2.814560 avg loss no lamb -2.814560 time 2020-06-26 05:21:43.469734
last batch sz 10
Pre: time 2020-06-26 05:21:57.295387: 
 	std: 0.0025226988
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9799, 0.9754, 0.9819, 0.9778]
	train_accs: [0.98181665, 0.98081666, 0.97655, 0.982, 0.97798336]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.9794
	best: 0.9819

Starting e_i: 433
Model ind 665 epoch 433 batch: 0 avg loss -2.956079 avg loss no lamb -2.956079 time 2020-06-26 05:21:58.111605
Model ind 665 epoch 433 batch: 100 avg loss -2.795694 avg loss no lamb -2.795694 time 2020-06-26 05:22:08.762582
Model ind 665 epoch 433 batch: 200 avg loss -2.795691 avg loss no lamb -2.795691 time 2020-06-26 05:22:19.608214
Model ind 665 epoch 433 batch: 300 avg loss -2.864674 avg loss no lamb -2.864674 time 2020-06-26 05:22:30.101531
Model ind 665 epoch 433 batch: 400 avg loss -2.695964 avg loss no lamb -2.695964 time 2020-06-26 05:22:40.774937
Model ind 665 epoch 433 batch: 500 avg loss -2.776041 avg loss no lamb -2.776041 time 2020-06-26 05:22:51.375360
Model ind 665 epoch 433 batch: 600 avg loss -2.737205 avg loss no lamb -2.737205 time 2020-06-26 05:23:02.255139
Model ind 665 epoch 433 batch: 700 avg loss -2.682924 avg loss no lamb -2.682924 time 2020-06-26 05:23:12.922382
Model ind 665 epoch 433 batch: 800 avg loss -2.774499 avg loss no lamb -2.774499 time 2020-06-26 05:23:23.567323
last batch sz 10
Pre: time 2020-06-26 05:23:37.464960: 
 	std: 0.0029869033
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9795, 0.9743, 0.9815, 0.9748]
	train_accs: [0.98155, 0.98075, 0.9753, 0.9820333, 0.97603333]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97812
	best: 0.9815

Starting e_i: 434
Model ind 665 epoch 434 batch: 0 avg loss -2.974365 avg loss no lamb -2.974365 time 2020-06-26 05:23:38.278408
Model ind 665 epoch 434 batch: 100 avg loss -2.825471 avg loss no lamb -2.825471 time 2020-06-26 05:23:48.796497
Model ind 665 epoch 434 batch: 200 avg loss -2.810634 avg loss no lamb -2.810634 time 2020-06-26 05:23:59.364027
Model ind 665 epoch 434 batch: 300 avg loss -2.824445 avg loss no lamb -2.824445 time 2020-06-26 05:24:10.242141
Model ind 665 epoch 434 batch: 400 avg loss -2.698399 avg loss no lamb -2.698399 time 2020-06-26 05:24:21.041666
Model ind 665 epoch 434 batch: 500 avg loss -2.753335 avg loss no lamb -2.753335 time 2020-06-26 05:24:31.760403
Model ind 665 epoch 434 batch: 600 avg loss -2.832041 avg loss no lamb -2.832041 time 2020-06-26 05:24:42.323193
Model ind 665 epoch 434 batch: 700 avg loss -2.693018 avg loss no lamb -2.693018 time 2020-06-26 05:24:53.107033
Model ind 665 epoch 434 batch: 800 avg loss -2.812534 avg loss no lamb -2.812534 time 2020-06-26 05:25:03.946126
last batch sz 10
Pre: time 2020-06-26 05:25:18.073319: 
 	std: 0.0027672371
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9829, 0.9772, 0.9826, 0.9768]
	train_accs: [0.98265, 0.9820167, 0.97723335, 0.98258334, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9768
	avg: 0.98038006
	best: 0.9824

Starting e_i: 435
Model ind 665 epoch 435 batch: 0 avg loss -2.849512 avg loss no lamb -2.849512 time 2020-06-26 05:25:18.910258
Model ind 665 epoch 435 batch: 100 avg loss -2.862474 avg loss no lamb -2.862474 time 2020-06-26 05:25:29.491390
Model ind 665 epoch 435 batch: 200 avg loss -2.786520 avg loss no lamb -2.786520 time 2020-06-26 05:25:40.348260
Model ind 665 epoch 435 batch: 300 avg loss -2.818228 avg loss no lamb -2.818228 time 2020-06-26 05:25:51.049857
Model ind 665 epoch 435 batch: 400 avg loss -2.782178 avg loss no lamb -2.782178 time 2020-06-26 05:26:01.963328
Model ind 665 epoch 435 batch: 500 avg loss -2.764356 avg loss no lamb -2.764356 time 2020-06-26 05:26:12.595750
Model ind 665 epoch 435 batch: 600 avg loss -2.805473 avg loss no lamb -2.805473 time 2020-06-26 05:26:23.339858
Model ind 665 epoch 435 batch: 700 avg loss -2.689149 avg loss no lamb -2.689149 time 2020-06-26 05:26:34.096327
Model ind 665 epoch 435 batch: 800 avg loss -2.783236 avg loss no lamb -2.783236 time 2020-06-26 05:26:44.831457
last batch sz 10
Pre: time 2020-06-26 05:26:58.851367: 
 	std: 0.0026768616
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9809, 0.9751, 0.9816, 0.9774]
	train_accs: [0.98125, 0.9805, 0.9755, 0.9815, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97938
	best: 0.9816

Starting e_i: 436
Model ind 665 epoch 436 batch: 0 avg loss -2.941673 avg loss no lamb -2.941673 time 2020-06-26 05:26:59.642708
Model ind 665 epoch 436 batch: 100 avg loss -2.817560 avg loss no lamb -2.817560 time 2020-06-26 05:27:10.603436
Model ind 665 epoch 436 batch: 200 avg loss -2.827088 avg loss no lamb -2.827088 time 2020-06-26 05:27:21.063712
Model ind 665 epoch 436 batch: 300 avg loss -2.795851 avg loss no lamb -2.795851 time 2020-06-26 05:27:31.877727
Model ind 665 epoch 436 batch: 400 avg loss -2.719619 avg loss no lamb -2.719619 time 2020-06-26 05:27:43.005574
Model ind 665 epoch 436 batch: 500 avg loss -2.742592 avg loss no lamb -2.742592 time 2020-06-26 05:27:53.833418
Model ind 665 epoch 436 batch: 600 avg loss -2.831726 avg loss no lamb -2.831726 time 2020-06-26 05:28:04.770930
Model ind 665 epoch 436 batch: 700 avg loss -2.670697 avg loss no lamb -2.670697 time 2020-06-26 05:28:15.395230
Model ind 665 epoch 436 batch: 800 avg loss -2.785161 avg loss no lamb -2.785161 time 2020-06-26 05:28:26.342186
last batch sz 10
Pre: time 2020-06-26 05:28:40.371671: 
 	std: 0.0025199985
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.982, 0.9768, 0.9822, 0.9778]
	train_accs: [0.98186666, 0.98158336, 0.97711664, 0.982, 0.97753334]
	best_train_sub_head: 3
	worst: 0.9768
	avg: 0.98034
	best: 0.9822

Starting e_i: 437
Model ind 665 epoch 437 batch: 0 avg loss -2.932850 avg loss no lamb -2.932850 time 2020-06-26 05:28:41.186196
Model ind 665 epoch 437 batch: 100 avg loss -2.874684 avg loss no lamb -2.874684 time 2020-06-26 05:28:51.760507
Model ind 665 epoch 437 batch: 200 avg loss -2.822328 avg loss no lamb -2.822328 time 2020-06-26 05:29:02.302432
Model ind 665 epoch 437 batch: 300 avg loss -2.842417 avg loss no lamb -2.842417 time 2020-06-26 05:29:12.981471
Model ind 665 epoch 437 batch: 400 avg loss -2.719064 avg loss no lamb -2.719064 time 2020-06-26 05:29:23.908294
Model ind 665 epoch 437 batch: 500 avg loss -2.791319 avg loss no lamb -2.791319 time 2020-06-26 05:29:34.675522
Model ind 665 epoch 437 batch: 600 avg loss -2.858929 avg loss no lamb -2.858929 time 2020-06-26 05:29:44.987486
Model ind 665 epoch 437 batch: 700 avg loss -2.675736 avg loss no lamb -2.675736 time 2020-06-26 05:29:56.090226
Model ind 665 epoch 437 batch: 800 avg loss -2.770787 avg loss no lamb -2.770787 time 2020-06-26 05:30:07.034986
last batch sz 10
Pre: time 2020-06-26 05:30:21.318657: 
 	std: 0.0030934769
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9807, 0.9742, 0.9815, 0.9757]
	train_accs: [0.9816167, 0.9809667, 0.9754, 0.98176664, 0.9762]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97868
	best: 0.9815

Starting e_i: 438
Model ind 665 epoch 438 batch: 0 avg loss -2.911969 avg loss no lamb -2.911969 time 2020-06-26 05:30:22.223599
Model ind 665 epoch 438 batch: 100 avg loss -2.854543 avg loss no lamb -2.854543 time 2020-06-26 05:30:32.885097
Model ind 665 epoch 438 batch: 200 avg loss -2.840090 avg loss no lamb -2.840090 time 2020-06-26 05:30:43.799776
Model ind 665 epoch 438 batch: 300 avg loss -2.761647 avg loss no lamb -2.761647 time 2020-06-26 05:30:54.658731
Model ind 665 epoch 438 batch: 400 avg loss -2.734948 avg loss no lamb -2.734948 time 2020-06-26 05:31:05.758960
Model ind 665 epoch 438 batch: 500 avg loss -2.748212 avg loss no lamb -2.748212 time 2020-06-26 05:31:16.509264
Model ind 665 epoch 438 batch: 600 avg loss -2.812298 avg loss no lamb -2.812298 time 2020-06-26 05:31:27.399198
Model ind 665 epoch 438 batch: 700 avg loss -2.641898 avg loss no lamb -2.641898 time 2020-06-26 05:31:38.348555
Model ind 665 epoch 438 batch: 800 avg loss -2.798691 avg loss no lamb -2.798691 time 2020-06-26 05:31:48.975312
last batch sz 10
Pre: time 2020-06-26 05:32:02.872823: 
 	std: 0.0026558614
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9815, 0.9759, 0.9819, 0.9768]
	train_accs: [0.98211664, 0.98135, 0.9765667, 0.98226666, 0.9773667]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97958004
	best: 0.9819

Starting e_i: 439
Model ind 665 epoch 439 batch: 0 avg loss -2.956223 avg loss no lamb -2.956223 time 2020-06-26 05:32:03.730595
Model ind 665 epoch 439 batch: 100 avg loss -2.827796 avg loss no lamb -2.827796 time 2020-06-26 05:32:14.540687
Model ind 665 epoch 439 batch: 200 avg loss -2.817257 avg loss no lamb -2.817257 time 2020-06-26 05:32:25.279165
Model ind 665 epoch 439 batch: 300 avg loss -2.819545 avg loss no lamb -2.819545 time 2020-06-26 05:32:36.139269
Model ind 665 epoch 439 batch: 400 avg loss -2.704940 avg loss no lamb -2.704940 time 2020-06-26 05:32:46.801072
Model ind 665 epoch 439 batch: 500 avg loss -2.748964 avg loss no lamb -2.748964 time 2020-06-26 05:32:57.363733
Model ind 665 epoch 439 batch: 600 avg loss -2.839755 avg loss no lamb -2.839755 time 2020-06-26 05:33:08.168621
Model ind 665 epoch 439 batch: 700 avg loss -2.671381 avg loss no lamb -2.671381 time 2020-06-26 05:33:19.045077
Model ind 665 epoch 439 batch: 800 avg loss -2.755893 avg loss no lamb -2.755893 time 2020-06-26 05:33:29.740417
last batch sz 10
Pre: time 2020-06-26 05:33:43.471278: 
 	std: 0.00220454
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9827, 0.9824, 0.9781, 0.9835, 0.9788]
	train_accs: [0.9827167, 0.98221666, 0.97775, 0.98331666, 0.978]
	best_train_sub_head: 3
	worst: 0.9781
	avg: 0.98109996
	best: 0.9835

Starting e_i: 440
Model ind 665 epoch 440 batch: 0 avg loss -2.936125 avg loss no lamb -2.936125 time 2020-06-26 05:33:44.287051
Model ind 665 epoch 440 batch: 100 avg loss -2.819559 avg loss no lamb -2.819559 time 2020-06-26 05:33:55.167997
Model ind 665 epoch 440 batch: 200 avg loss -2.818001 avg loss no lamb -2.818001 time 2020-06-26 05:34:06.101475
Model ind 665 epoch 440 batch: 300 avg loss -2.808978 avg loss no lamb -2.808978 time 2020-06-26 05:34:16.768641
Model ind 665 epoch 440 batch: 400 avg loss -2.716485 avg loss no lamb -2.716485 time 2020-06-26 05:34:27.501962
Model ind 665 epoch 440 batch: 500 avg loss -2.734588 avg loss no lamb -2.734588 time 2020-06-26 05:34:38.068939
Model ind 665 epoch 440 batch: 600 avg loss -2.856755 avg loss no lamb -2.856755 time 2020-06-26 05:34:48.849265
Model ind 665 epoch 440 batch: 700 avg loss -2.701418 avg loss no lamb -2.701418 time 2020-06-26 05:34:59.748644
Model ind 665 epoch 440 batch: 800 avg loss -2.717228 avg loss no lamb -2.717228 time 2020-06-26 05:35:10.421289
last batch sz 10
Pre: time 2020-06-26 05:35:24.280175: 
 	std: 0.0026336284
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9813, 0.9753, 0.981, 0.9763]
	train_accs: [0.98151666, 0.9813667, 0.9765, 0.9816667, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.979
	best: 0.981

Starting e_i: 441
Model ind 665 epoch 441 batch: 0 avg loss -2.965336 avg loss no lamb -2.965336 time 2020-06-26 05:35:26.289328
Model ind 665 epoch 441 batch: 100 avg loss -2.849680 avg loss no lamb -2.849680 time 2020-06-26 05:35:36.975893
Model ind 665 epoch 441 batch: 200 avg loss -2.820912 avg loss no lamb -2.820912 time 2020-06-26 05:35:47.569099
Model ind 665 epoch 441 batch: 300 avg loss -2.849928 avg loss no lamb -2.849928 time 2020-06-26 05:35:58.342630
Model ind 665 epoch 441 batch: 400 avg loss -2.733526 avg loss no lamb -2.733526 time 2020-06-26 05:36:08.981081
Model ind 665 epoch 441 batch: 500 avg loss -2.737650 avg loss no lamb -2.737650 time 2020-06-26 05:36:19.780077
Model ind 665 epoch 441 batch: 600 avg loss -2.778502 avg loss no lamb -2.778502 time 2020-06-26 05:36:30.288185
Model ind 665 epoch 441 batch: 700 avg loss -2.640584 avg loss no lamb -2.640584 time 2020-06-26 05:36:41.003613
Model ind 665 epoch 441 batch: 800 avg loss -2.825199 avg loss no lamb -2.825199 time 2020-06-26 05:36:51.656579
last batch sz 10
Pre: time 2020-06-26 05:37:05.733109: 
 	std: 0.0030986473
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9819, 0.9751, 0.9821, 0.9769]
	train_accs: [0.98235, 0.98193336, 0.976, 0.9821, 0.9767]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97972
	best: 0.9826

Starting e_i: 442
Model ind 665 epoch 442 batch: 0 avg loss -2.913208 avg loss no lamb -2.913208 time 2020-06-26 05:37:06.575729
Model ind 665 epoch 442 batch: 100 avg loss -2.873715 avg loss no lamb -2.873715 time 2020-06-26 05:37:17.174294
Model ind 665 epoch 442 batch: 200 avg loss -2.817244 avg loss no lamb -2.817244 time 2020-06-26 05:37:27.914717
Model ind 665 epoch 442 batch: 300 avg loss -2.825557 avg loss no lamb -2.825557 time 2020-06-26 05:37:38.636771
Model ind 665 epoch 442 batch: 400 avg loss -2.688318 avg loss no lamb -2.688318 time 2020-06-26 05:37:49.379448
Model ind 665 epoch 442 batch: 500 avg loss -2.726418 avg loss no lamb -2.726418 time 2020-06-26 05:38:00.154668
Model ind 665 epoch 442 batch: 600 avg loss -2.790653 avg loss no lamb -2.790653 time 2020-06-26 05:38:10.751284
Model ind 665 epoch 442 batch: 700 avg loss -2.672590 avg loss no lamb -2.672590 time 2020-06-26 05:38:21.297176
Model ind 665 epoch 442 batch: 800 avg loss -2.774603 avg loss no lamb -2.774603 time 2020-06-26 05:38:31.905673
last batch sz 10
Pre: time 2020-06-26 05:38:45.641817: 
 	std: 0.0033051572
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9819, 0.9747, 0.9828, 0.9773]
	train_accs: [0.98255, 0.98193336, 0.9763167, 0.9823667, 0.9772]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.9799
	best: 0.9828

Starting e_i: 443
Model ind 665 epoch 443 batch: 0 avg loss -2.946421 avg loss no lamb -2.946421 time 2020-06-26 05:38:46.462715
Model ind 665 epoch 443 batch: 100 avg loss -2.892100 avg loss no lamb -2.892100 time 2020-06-26 05:38:57.305381
Model ind 665 epoch 443 batch: 200 avg loss -2.810209 avg loss no lamb -2.810209 time 2020-06-26 05:39:08.104800
Model ind 665 epoch 443 batch: 300 avg loss -2.786911 avg loss no lamb -2.786911 time 2020-06-26 05:39:19.100832
Model ind 665 epoch 443 batch: 400 avg loss -2.751846 avg loss no lamb -2.751846 time 2020-06-26 05:39:29.940568
Model ind 665 epoch 443 batch: 500 avg loss -2.810695 avg loss no lamb -2.810695 time 2020-06-26 05:39:40.678923
Model ind 665 epoch 443 batch: 600 avg loss -2.813344 avg loss no lamb -2.813344 time 2020-06-26 05:39:51.439402
Model ind 665 epoch 443 batch: 700 avg loss -2.741387 avg loss no lamb -2.741387 time 2020-06-26 05:40:02.328199
Model ind 665 epoch 443 batch: 800 avg loss -2.810347 avg loss no lamb -2.810347 time 2020-06-26 05:40:13.065857
last batch sz 10
Pre: time 2020-06-26 05:40:26.587426: 
 	std: 0.0026176274
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9823, 0.9767, 0.982, 0.9767]
	train_accs: [0.9820833, 0.9817167, 0.97685, 0.9822, 0.9771]
	best_train_sub_head: 3
	worst: 0.9767
	avg: 0.9799
	best: 0.982

Starting e_i: 444
Model ind 665 epoch 444 batch: 0 avg loss -2.920242 avg loss no lamb -2.920242 time 2020-06-26 05:40:27.427068
Model ind 665 epoch 444 batch: 100 avg loss -2.890881 avg loss no lamb -2.890881 time 2020-06-26 05:40:38.277078
Model ind 665 epoch 444 batch: 200 avg loss -2.779658 avg loss no lamb -2.779658 time 2020-06-26 05:40:48.892139
Model ind 665 epoch 444 batch: 300 avg loss -2.828495 avg loss no lamb -2.828495 time 2020-06-26 05:40:59.641697
Model ind 665 epoch 444 batch: 400 avg loss -2.715125 avg loss no lamb -2.715125 time 2020-06-26 05:41:10.241086
Model ind 665 epoch 444 batch: 500 avg loss -2.755862 avg loss no lamb -2.755862 time 2020-06-26 05:41:20.875025
Model ind 665 epoch 444 batch: 600 avg loss -2.850913 avg loss no lamb -2.850913 time 2020-06-26 05:41:31.649658
Model ind 665 epoch 444 batch: 700 avg loss -2.659904 avg loss no lamb -2.659904 time 2020-06-26 05:41:42.298989
Model ind 665 epoch 444 batch: 800 avg loss -2.793574 avg loss no lamb -2.793574 time 2020-06-26 05:41:53.070433
last batch sz 10
Pre: time 2020-06-26 05:42:07.259803: 
 	std: 0.0028202087
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9799, 0.9746, 0.9809, 0.9757]
	train_accs: [0.9821333, 0.98125, 0.9762167, 0.982, 0.9769167]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97852004
	best: 0.9815

Starting e_i: 445
Model ind 665 epoch 445 batch: 0 avg loss -2.976484 avg loss no lamb -2.976484 time 2020-06-26 05:42:08.106101
Model ind 665 epoch 445 batch: 100 avg loss -2.814983 avg loss no lamb -2.814983 time 2020-06-26 05:42:18.661414
Model ind 665 epoch 445 batch: 200 avg loss -2.811161 avg loss no lamb -2.811161 time 2020-06-26 05:42:29.556173
Model ind 665 epoch 445 batch: 300 avg loss -2.855086 avg loss no lamb -2.855086 time 2020-06-26 05:42:40.473304
Model ind 665 epoch 445 batch: 400 avg loss -2.706477 avg loss no lamb -2.706477 time 2020-06-26 05:42:51.168101
Model ind 665 epoch 445 batch: 500 avg loss -2.724286 avg loss no lamb -2.724286 time 2020-06-26 05:43:01.953311
Model ind 665 epoch 445 batch: 600 avg loss -2.852444 avg loss no lamb -2.852444 time 2020-06-26 05:43:12.343969
Model ind 665 epoch 445 batch: 700 avg loss -2.665215 avg loss no lamb -2.665215 time 2020-06-26 05:43:23.052497
Model ind 665 epoch 445 batch: 800 avg loss -2.740630 avg loss no lamb -2.740630 time 2020-06-26 05:43:33.934265
last batch sz 10
Pre: time 2020-06-26 05:43:47.989662: 
 	std: 0.0030837625
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9805, 0.9741, 0.9812, 0.976]
	train_accs: [0.98176664, 0.9806833, 0.97548336, 0.9817167, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97872
	best: 0.9818

Starting e_i: 446
Model ind 665 epoch 446 batch: 0 avg loss -2.917405 avg loss no lamb -2.917405 time 2020-06-26 05:43:48.928883
Model ind 665 epoch 446 batch: 100 avg loss -2.835226 avg loss no lamb -2.835226 time 2020-06-26 05:43:59.652476
Model ind 665 epoch 446 batch: 200 avg loss -2.818513 avg loss no lamb -2.818513 time 2020-06-26 05:44:10.361645
Model ind 665 epoch 446 batch: 300 avg loss -2.824844 avg loss no lamb -2.824844 time 2020-06-26 05:44:21.236182
Model ind 665 epoch 446 batch: 400 avg loss -2.762249 avg loss no lamb -2.762249 time 2020-06-26 05:44:32.020641
Model ind 665 epoch 446 batch: 500 avg loss -2.762031 avg loss no lamb -2.762031 time 2020-06-26 05:44:42.757766
Model ind 665 epoch 446 batch: 600 avg loss -2.881057 avg loss no lamb -2.881057 time 2020-06-26 05:44:53.520136
Model ind 665 epoch 446 batch: 700 avg loss -2.651624 avg loss no lamb -2.651624 time 2020-06-26 05:45:04.268031
Model ind 665 epoch 446 batch: 800 avg loss -2.801386 avg loss no lamb -2.801386 time 2020-06-26 05:45:14.773183
last batch sz 10
Pre: time 2020-06-26 05:45:28.515554: 
 	std: 0.0031464465
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9815, 0.9751, 0.9817, 0.9754]
	train_accs: [0.9816333, 0.9812, 0.9759833, 0.98188335, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97910005
	best: 0.9817

Starting e_i: 447
Model ind 665 epoch 447 batch: 0 avg loss -2.919058 avg loss no lamb -2.919058 time 2020-06-26 05:45:29.350204
Model ind 665 epoch 447 batch: 100 avg loss -2.788752 avg loss no lamb -2.788752 time 2020-06-26 05:45:40.241526
Model ind 665 epoch 447 batch: 200 avg loss -2.871731 avg loss no lamb -2.871731 time 2020-06-26 05:45:51.055819
Model ind 665 epoch 447 batch: 300 avg loss -2.809016 avg loss no lamb -2.809016 time 2020-06-26 05:46:01.560853
Model ind 665 epoch 447 batch: 400 avg loss -2.724894 avg loss no lamb -2.724894 time 2020-06-26 05:46:12.107928
Model ind 665 epoch 447 batch: 500 avg loss -2.778474 avg loss no lamb -2.778474 time 2020-06-26 05:46:22.803788
Model ind 665 epoch 447 batch: 600 avg loss -2.785207 avg loss no lamb -2.785207 time 2020-06-26 05:46:33.739871
Model ind 665 epoch 447 batch: 700 avg loss -2.753128 avg loss no lamb -2.753128 time 2020-06-26 05:46:44.432891
Model ind 665 epoch 447 batch: 800 avg loss -2.779940 avg loss no lamb -2.779940 time 2020-06-26 05:46:55.274879
last batch sz 10
Pre: time 2020-06-26 05:47:09.471796: 
 	std: 0.0025388247
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9807, 0.9765, 0.9816, 0.976]
	train_accs: [0.9823, 0.98111665, 0.97725, 0.9823667, 0.9774333]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.97932005
	best: 0.9816

Starting e_i: 448
Model ind 665 epoch 448 batch: 0 avg loss -2.889444 avg loss no lamb -2.889444 time 2020-06-26 05:47:10.346802
Model ind 665 epoch 448 batch: 100 avg loss -2.855676 avg loss no lamb -2.855676 time 2020-06-26 05:47:21.230316
Model ind 665 epoch 448 batch: 200 avg loss -2.853369 avg loss no lamb -2.853369 time 2020-06-26 05:47:32.040516
Model ind 665 epoch 448 batch: 300 avg loss -2.793649 avg loss no lamb -2.793649 time 2020-06-26 05:47:42.689572
Model ind 665 epoch 448 batch: 400 avg loss -2.727764 avg loss no lamb -2.727764 time 2020-06-26 05:47:53.064982
Model ind 665 epoch 448 batch: 500 avg loss -2.801544 avg loss no lamb -2.801544 time 2020-06-26 05:48:03.722295
Model ind 665 epoch 448 batch: 600 avg loss -2.850692 avg loss no lamb -2.850692 time 2020-06-26 05:48:14.745548
Model ind 665 epoch 448 batch: 700 avg loss -2.728087 avg loss no lamb -2.728087 time 2020-06-26 05:48:25.430053
Model ind 665 epoch 448 batch: 800 avg loss -2.770343 avg loss no lamb -2.770343 time 2020-06-26 05:48:36.061633
last batch sz 10
Pre: time 2020-06-26 05:48:49.874923: 
 	std: 0.0021859494
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9806, 0.9753, 0.9803, 0.9768]
	train_accs: [0.98153335, 0.9806167, 0.9759167, 0.9816333, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97866
	best: 0.9803

Starting e_i: 449
Model ind 665 epoch 449 batch: 0 avg loss -2.890319 avg loss no lamb -2.890319 time 2020-06-26 05:48:50.721361
Model ind 665 epoch 449 batch: 100 avg loss -2.803239 avg loss no lamb -2.803239 time 2020-06-26 05:49:01.465123
Model ind 665 epoch 449 batch: 200 avg loss -2.796103 avg loss no lamb -2.796103 time 2020-06-26 05:49:12.446321
Model ind 665 epoch 449 batch: 300 avg loss -2.774630 avg loss no lamb -2.774630 time 2020-06-26 05:49:23.161883
Model ind 665 epoch 449 batch: 400 avg loss -2.737975 avg loss no lamb -2.737975 time 2020-06-26 05:49:33.725775
Model ind 665 epoch 449 batch: 500 avg loss -2.741601 avg loss no lamb -2.741601 time 2020-06-26 05:49:44.397521
Model ind 665 epoch 449 batch: 600 avg loss -2.816542 avg loss no lamb -2.816542 time 2020-06-26 05:49:55.201509
Model ind 665 epoch 449 batch: 700 avg loss -2.730039 avg loss no lamb -2.730039 time 2020-06-26 05:50:05.974661
Model ind 665 epoch 449 batch: 800 avg loss -2.820619 avg loss no lamb -2.820619 time 2020-06-26 05:50:16.770664
last batch sz 10
Pre: time 2020-06-26 05:50:30.604765: 
 	std: 0.0023120535
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9818, 0.9767, 0.9817, 0.9771]
	train_accs: [0.98183334, 0.98153335, 0.9766667, 0.98215, 0.97693336]
	best_train_sub_head: 3
	worst: 0.9767
	avg: 0.97972
	best: 0.9817

Starting e_i: 450
Model ind 665 epoch 450 batch: 0 avg loss -2.934429 avg loss no lamb -2.934429 time 2020-06-26 05:50:31.428616
Model ind 665 epoch 450 batch: 100 avg loss -2.964694 avg loss no lamb -2.964694 time 2020-06-26 05:50:41.942394
Model ind 665 epoch 450 batch: 200 avg loss -2.850658 avg loss no lamb -2.850658 time 2020-06-26 05:50:52.607723
Model ind 665 epoch 450 batch: 300 avg loss -2.793204 avg loss no lamb -2.793204 time 2020-06-26 05:51:03.497730
Model ind 665 epoch 450 batch: 400 avg loss -2.666814 avg loss no lamb -2.666814 time 2020-06-26 05:51:14.348807
Model ind 665 epoch 450 batch: 500 avg loss -2.780422 avg loss no lamb -2.780422 time 2020-06-26 05:51:25.192874
Model ind 665 epoch 450 batch: 600 avg loss -2.843020 avg loss no lamb -2.843020 time 2020-06-26 05:51:35.792483
Model ind 665 epoch 450 batch: 700 avg loss -2.694107 avg loss no lamb -2.694107 time 2020-06-26 05:51:46.337637
Model ind 665 epoch 450 batch: 800 avg loss -2.790792 avg loss no lamb -2.790792 time 2020-06-26 05:51:57.086452
last batch sz 10
Pre: time 2020-06-26 05:52:11.219842: 
 	std: 0.002471106
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9808, 0.9756, 0.9803, 0.9764]
	train_accs: [0.98143333, 0.9811, 0.97641665, 0.98146665, 0.9773333]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.9789599
	best: 0.9803

Starting e_i: 451
Model ind 665 epoch 451 batch: 0 avg loss -2.885701 avg loss no lamb -2.885701 time 2020-06-26 05:52:13.296379
Model ind 665 epoch 451 batch: 100 avg loss -2.913546 avg loss no lamb -2.913546 time 2020-06-26 05:52:23.885669
Model ind 665 epoch 451 batch: 200 avg loss -2.803591 avg loss no lamb -2.803591 time 2020-06-26 05:52:34.241703
Model ind 665 epoch 451 batch: 300 avg loss -2.859199 avg loss no lamb -2.859199 time 2020-06-26 05:52:45.031243
Model ind 665 epoch 451 batch: 400 avg loss -2.735425 avg loss no lamb -2.735425 time 2020-06-26 05:52:55.696796
Model ind 665 epoch 451 batch: 500 avg loss -2.825834 avg loss no lamb -2.825834 time 2020-06-26 05:53:06.392384
Model ind 665 epoch 451 batch: 600 avg loss -2.855992 avg loss no lamb -2.855992 time 2020-06-26 05:53:16.969581
Model ind 665 epoch 451 batch: 700 avg loss -2.657336 avg loss no lamb -2.657336 time 2020-06-26 05:53:27.384447
Model ind 665 epoch 451 batch: 800 avg loss -2.727731 avg loss no lamb -2.727731 time 2020-06-26 05:53:37.958429
last batch sz 10
Pre: time 2020-06-26 05:53:51.932924: 
 	std: 0.0025833384
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9815, 0.9763, 0.9821, 0.9773]
	train_accs: [0.98235, 0.98181665, 0.9773167, 0.9826, 0.9776667]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.97992
	best: 0.9821

Starting e_i: 452
Model ind 665 epoch 452 batch: 0 avg loss -2.964964 avg loss no lamb -2.964964 time 2020-06-26 05:53:52.759128
Model ind 665 epoch 452 batch: 100 avg loss -2.902477 avg loss no lamb -2.902477 time 2020-06-26 05:54:03.757029
Model ind 665 epoch 452 batch: 200 avg loss -2.784340 avg loss no lamb -2.784340 time 2020-06-26 05:54:14.085432
Model ind 665 epoch 452 batch: 300 avg loss -2.790381 avg loss no lamb -2.790381 time 2020-06-26 05:54:24.836127
Model ind 665 epoch 452 batch: 400 avg loss -2.709330 avg loss no lamb -2.709330 time 2020-06-26 05:54:35.676268
Model ind 665 epoch 452 batch: 500 avg loss -2.688927 avg loss no lamb -2.688927 time 2020-06-26 05:54:46.344172
Model ind 665 epoch 452 batch: 600 avg loss -2.828056 avg loss no lamb -2.828056 time 2020-06-26 05:54:56.698047
Model ind 665 epoch 452 batch: 700 avg loss -2.725404 avg loss no lamb -2.725404 time 2020-06-26 05:55:07.580802
Model ind 665 epoch 452 batch: 800 avg loss -2.821894 avg loss no lamb -2.821894 time 2020-06-26 05:55:18.249411
last batch sz 10
Pre: time 2020-06-26 05:55:32.097663: 
 	std: 0.0028638497
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.98, 0.9748, 0.9812, 0.9751]
	train_accs: [0.98258334, 0.98125, 0.97615, 0.9829, 0.9770833]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97841996
	best: 0.9812

Starting e_i: 453
Model ind 665 epoch 453 batch: 0 avg loss -2.907668 avg loss no lamb -2.907668 time 2020-06-26 05:55:32.926984
Model ind 665 epoch 453 batch: 100 avg loss -2.812071 avg loss no lamb -2.812071 time 2020-06-26 05:55:43.446258
Model ind 665 epoch 453 batch: 200 avg loss -2.856841 avg loss no lamb -2.856841 time 2020-06-26 05:55:53.967153
Model ind 665 epoch 453 batch: 300 avg loss -2.808003 avg loss no lamb -2.808003 time 2020-06-26 05:56:04.739961
Model ind 665 epoch 453 batch: 400 avg loss -2.689696 avg loss no lamb -2.689696 time 2020-06-26 05:56:15.488169
Model ind 665 epoch 453 batch: 500 avg loss -2.779943 avg loss no lamb -2.779943 time 2020-06-26 05:56:26.269449
Model ind 665 epoch 453 batch: 600 avg loss -2.832214 avg loss no lamb -2.832214 time 2020-06-26 05:56:36.930799
Model ind 665 epoch 453 batch: 700 avg loss -2.619869 avg loss no lamb -2.619869 time 2020-06-26 05:56:47.558099
Model ind 665 epoch 453 batch: 800 avg loss -2.773037 avg loss no lamb -2.773037 time 2020-06-26 05:56:58.527261
last batch sz 10
Pre: time 2020-06-26 05:57:12.256798: 
 	std: 0.0023953302
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9819, 0.9765, 0.9816, 0.9771]
	train_accs: [0.98215, 0.9819667, 0.97758335, 0.9825, 0.9781]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.97972
	best: 0.9816

Starting e_i: 454
Model ind 665 epoch 454 batch: 0 avg loss -2.941133 avg loss no lamb -2.941133 time 2020-06-26 05:57:13.094363
Model ind 665 epoch 454 batch: 100 avg loss -2.810678 avg loss no lamb -2.810678 time 2020-06-26 05:57:23.819549
Model ind 665 epoch 454 batch: 200 avg loss -2.837045 avg loss no lamb -2.837045 time 2020-06-26 05:57:34.593553
Model ind 665 epoch 454 batch: 300 avg loss -2.849288 avg loss no lamb -2.849288 time 2020-06-26 05:57:44.957368
Model ind 665 epoch 454 batch: 400 avg loss -2.691342 avg loss no lamb -2.691342 time 2020-06-26 05:57:55.841631
Model ind 665 epoch 454 batch: 500 avg loss -2.773462 avg loss no lamb -2.773462 time 2020-06-26 05:58:06.631078
Model ind 665 epoch 454 batch: 600 avg loss -2.780181 avg loss no lamb -2.780181 time 2020-06-26 05:58:17.167519
Model ind 665 epoch 454 batch: 700 avg loss -2.768422 avg loss no lamb -2.768422 time 2020-06-26 05:58:27.889104
Model ind 665 epoch 454 batch: 800 avg loss -2.852853 avg loss no lamb -2.852853 time 2020-06-26 05:58:38.705254
last batch sz 10
Pre: time 2020-06-26 05:58:52.832113: 
 	std: 0.0022480264
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.98, 0.9749, 0.9802, 0.9765]
	train_accs: [0.9818, 0.98111665, 0.97655, 0.98178333, 0.9777833]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97838
	best: 0.9803

Starting e_i: 455
Model ind 665 epoch 455 batch: 0 avg loss -2.887968 avg loss no lamb -2.887968 time 2020-06-26 05:58:53.640672
Model ind 665 epoch 455 batch: 100 avg loss -2.750860 avg loss no lamb -2.750860 time 2020-06-26 05:59:04.785542
Model ind 665 epoch 455 batch: 200 avg loss -2.810301 avg loss no lamb -2.810301 time 2020-06-26 05:59:15.400384
Model ind 665 epoch 455 batch: 300 avg loss -2.823069 avg loss no lamb -2.823069 time 2020-06-26 05:59:26.198285
Model ind 665 epoch 455 batch: 400 avg loss -2.735384 avg loss no lamb -2.735384 time 2020-06-26 05:59:37.092791
Model ind 665 epoch 455 batch: 500 avg loss -2.775832 avg loss no lamb -2.775832 time 2020-06-26 05:59:48.145830
Model ind 665 epoch 455 batch: 600 avg loss -2.853729 avg loss no lamb -2.853729 time 2020-06-26 05:59:58.955497
Model ind 665 epoch 455 batch: 700 avg loss -2.664351 avg loss no lamb -2.664351 time 2020-06-26 06:00:09.701339
Model ind 665 epoch 455 batch: 800 avg loss -2.767078 avg loss no lamb -2.767078 time 2020-06-26 06:00:20.337387
last batch sz 10
Pre: time 2020-06-26 06:00:34.005496: 
 	std: 0.0030467012
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9803, 0.9743, 0.981, 0.9752]
	train_accs: [0.9817333, 0.9806, 0.97595, 0.98165, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97844
	best: 0.9814

Starting e_i: 456
Model ind 665 epoch 456 batch: 0 avg loss -2.966248 avg loss no lamb -2.966248 time 2020-06-26 06:00:34.913727
Model ind 665 epoch 456 batch: 100 avg loss -2.800875 avg loss no lamb -2.800875 time 2020-06-26 06:00:45.457063
Model ind 665 epoch 456 batch: 200 avg loss -2.858193 avg loss no lamb -2.858193 time 2020-06-26 06:00:56.392205
Model ind 665 epoch 456 batch: 300 avg loss -2.811562 avg loss no lamb -2.811562 time 2020-06-26 06:01:07.201623
Model ind 665 epoch 456 batch: 400 avg loss -2.724966 avg loss no lamb -2.724966 time 2020-06-26 06:01:17.896186
Model ind 665 epoch 456 batch: 500 avg loss -2.753571 avg loss no lamb -2.753571 time 2020-06-26 06:01:28.665729
Model ind 665 epoch 456 batch: 600 avg loss -2.852978 avg loss no lamb -2.852978 time 2020-06-26 06:01:39.487139
Model ind 665 epoch 456 batch: 700 avg loss -2.630038 avg loss no lamb -2.630038 time 2020-06-26 06:01:50.206776
Model ind 665 epoch 456 batch: 800 avg loss -2.825447 avg loss no lamb -2.825447 time 2020-06-26 06:02:00.973616
last batch sz 10
Pre: time 2020-06-26 06:02:14.958301: 
 	std: 0.0027183692
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9808, 0.9743, 0.9805, 0.9758]
	train_accs: [0.9821, 0.98113334, 0.97643334, 0.9819667, 0.9773333]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97831994
	best: 0.9802

Starting e_i: 457
Model ind 665 epoch 457 batch: 0 avg loss -2.933867 avg loss no lamb -2.933867 time 2020-06-26 06:02:15.770739
Model ind 665 epoch 457 batch: 100 avg loss -2.798346 avg loss no lamb -2.798346 time 2020-06-26 06:02:26.451909
Model ind 665 epoch 457 batch: 200 avg loss -2.811270 avg loss no lamb -2.811270 time 2020-06-26 06:02:37.090380
Model ind 665 epoch 457 batch: 300 avg loss -2.882599 avg loss no lamb -2.882599 time 2020-06-26 06:02:47.678748
Model ind 665 epoch 457 batch: 400 avg loss -2.719497 avg loss no lamb -2.719497 time 2020-06-26 06:02:58.195883
Model ind 665 epoch 457 batch: 500 avg loss -2.805116 avg loss no lamb -2.805116 time 2020-06-26 06:03:08.837207
Model ind 665 epoch 457 batch: 600 avg loss -2.807907 avg loss no lamb -2.807907 time 2020-06-26 06:03:19.433244
Model ind 665 epoch 457 batch: 700 avg loss -2.656764 avg loss no lamb -2.656764 time 2020-06-26 06:03:30.149252
Model ind 665 epoch 457 batch: 800 avg loss -2.787261 avg loss no lamb -2.787261 time 2020-06-26 06:03:40.786903
last batch sz 10
Pre: time 2020-06-26 06:03:54.762248: 
 	std: 0.0026118252
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9803, 0.9748, 0.981, 0.9764]
	train_accs: [0.9820667, 0.98156667, 0.97648335, 0.9819833, 0.9773667]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97872
	best: 0.9811

Starting e_i: 458
Model ind 665 epoch 458 batch: 0 avg loss -2.913020 avg loss no lamb -2.913020 time 2020-06-26 06:03:55.571354
Model ind 665 epoch 458 batch: 100 avg loss -2.870339 avg loss no lamb -2.870339 time 2020-06-26 06:04:06.300753
Model ind 665 epoch 458 batch: 200 avg loss -2.844470 avg loss no lamb -2.844470 time 2020-06-26 06:04:16.972231
Model ind 665 epoch 458 batch: 300 avg loss -2.870067 avg loss no lamb -2.870067 time 2020-06-26 06:04:27.314270
Model ind 665 epoch 458 batch: 400 avg loss -2.775275 avg loss no lamb -2.775275 time 2020-06-26 06:04:37.950264
Model ind 665 epoch 458 batch: 500 avg loss -2.816393 avg loss no lamb -2.816393 time 2020-06-26 06:04:48.721468
Model ind 665 epoch 458 batch: 600 avg loss -2.803250 avg loss no lamb -2.803250 time 2020-06-26 06:04:59.480714
Model ind 665 epoch 458 batch: 700 avg loss -2.646610 avg loss no lamb -2.646610 time 2020-06-26 06:05:10.158993
Model ind 665 epoch 458 batch: 800 avg loss -2.801429 avg loss no lamb -2.801429 time 2020-06-26 06:05:20.666222
last batch sz 10
Pre: time 2020-06-26 06:05:34.521891: 
 	std: 0.0030869932
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9805, 0.975, 0.9826, 0.9766]
	train_accs: [0.9820333, 0.98081666, 0.9763167, 0.9821333, 0.9770333]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97942
	best: 0.9826

Starting e_i: 459
Model ind 665 epoch 459 batch: 0 avg loss -2.900412 avg loss no lamb -2.900412 time 2020-06-26 06:05:35.315263
Model ind 665 epoch 459 batch: 100 avg loss -2.888925 avg loss no lamb -2.888925 time 2020-06-26 06:05:46.004620
Model ind 665 epoch 459 batch: 200 avg loss -2.834439 avg loss no lamb -2.834439 time 2020-06-26 06:05:56.761999
Model ind 665 epoch 459 batch: 300 avg loss -2.811681 avg loss no lamb -2.811681 time 2020-06-26 06:06:07.479554
Model ind 665 epoch 459 batch: 400 avg loss -2.725788 avg loss no lamb -2.725788 time 2020-06-26 06:06:17.976956
Model ind 665 epoch 459 batch: 500 avg loss -2.704689 avg loss no lamb -2.704689 time 2020-06-26 06:06:28.913748
Model ind 665 epoch 459 batch: 600 avg loss -2.815248 avg loss no lamb -2.815248 time 2020-06-26 06:06:39.683415
Model ind 665 epoch 459 batch: 700 avg loss -2.658905 avg loss no lamb -2.658905 time 2020-06-26 06:06:50.635154
Model ind 665 epoch 459 batch: 800 avg loss -2.739224 avg loss no lamb -2.739224 time 2020-06-26 06:07:01.245095
last batch sz 10
Pre: time 2020-06-26 06:07:15.268639: 
 	std: 0.0023007735
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9821, 0.9771, 0.982, 0.9781]
	train_accs: [0.98211664, 0.98158336, 0.97753334, 0.9822, 0.97831666]
	best_train_sub_head: 3
	worst: 0.9771
	avg: 0.98037994
	best: 0.982

Starting e_i: 460
Model ind 665 epoch 460 batch: 0 avg loss -2.898401 avg loss no lamb -2.898401 time 2020-06-26 06:07:16.158815
Model ind 665 epoch 460 batch: 100 avg loss -2.858214 avg loss no lamb -2.858214 time 2020-06-26 06:07:26.831724
Model ind 665 epoch 460 batch: 200 avg loss -2.818329 avg loss no lamb -2.818329 time 2020-06-26 06:07:37.578658
Model ind 665 epoch 460 batch: 300 avg loss -2.873996 avg loss no lamb -2.873996 time 2020-06-26 06:07:48.192027
Model ind 665 epoch 460 batch: 400 avg loss -2.707002 avg loss no lamb -2.707002 time 2020-06-26 06:07:58.735587
Model ind 665 epoch 460 batch: 500 avg loss -2.763873 avg loss no lamb -2.763873 time 2020-06-26 06:08:09.504114
Model ind 665 epoch 460 batch: 600 avg loss -2.793145 avg loss no lamb -2.793145 time 2020-06-26 06:08:20.353867
Model ind 665 epoch 460 batch: 700 avg loss -2.738070 avg loss no lamb -2.738070 time 2020-06-26 06:08:31.054843
Model ind 665 epoch 460 batch: 800 avg loss -2.770091 avg loss no lamb -2.770091 time 2020-06-26 06:08:41.766447
last batch sz 10
Pre: time 2020-06-26 06:08:55.828107: 
 	std: 0.0025987595
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9806, 0.9749, 0.9812, 0.976]
	train_accs: [0.98105, 0.98088336, 0.97645, 0.9814, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97858
	best: 0.9812

Starting e_i: 461
Model ind 665 epoch 461 batch: 0 avg loss -2.964059 avg loss no lamb -2.964059 time 2020-06-26 06:08:57.793007
Model ind 665 epoch 461 batch: 100 avg loss -2.822658 avg loss no lamb -2.822658 time 2020-06-26 06:09:08.559032
Model ind 665 epoch 461 batch: 200 avg loss -2.820292 avg loss no lamb -2.820292 time 2020-06-26 06:09:19.265088
Model ind 665 epoch 461 batch: 300 avg loss -2.796182 avg loss no lamb -2.796182 time 2020-06-26 06:09:30.038042
Model ind 665 epoch 461 batch: 400 avg loss -2.710956 avg loss no lamb -2.710956 time 2020-06-26 06:09:40.465997
Model ind 665 epoch 461 batch: 500 avg loss -2.785269 avg loss no lamb -2.785269 time 2020-06-26 06:09:51.303654
Model ind 665 epoch 461 batch: 600 avg loss -2.800641 avg loss no lamb -2.800641 time 2020-06-26 06:10:02.141087
Model ind 665 epoch 461 batch: 700 avg loss -2.696214 avg loss no lamb -2.696214 time 2020-06-26 06:10:12.938209
Model ind 665 epoch 461 batch: 800 avg loss -2.768761 avg loss no lamb -2.768761 time 2020-06-26 06:10:23.355941
last batch sz 10
Pre: time 2020-06-26 06:10:37.104583: 
 	std: 0.0024481672
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9816, 0.9768, 0.983, 0.9784]
	train_accs: [0.98186666, 0.98141664, 0.97705, 0.98226666, 0.9773667]
	best_train_sub_head: 3
	worst: 0.9768
	avg: 0.98048
	best: 0.983

Starting e_i: 462
Model ind 665 epoch 462 batch: 0 avg loss -2.931058 avg loss no lamb -2.931058 time 2020-06-26 06:10:37.949673
Model ind 665 epoch 462 batch: 100 avg loss -2.854196 avg loss no lamb -2.854196 time 2020-06-26 06:10:48.638225
Model ind 665 epoch 462 batch: 200 avg loss -2.765548 avg loss no lamb -2.765548 time 2020-06-26 06:10:59.343446
Model ind 665 epoch 462 batch: 300 avg loss -2.809691 avg loss no lamb -2.809691 time 2020-06-26 06:11:10.360252
Model ind 665 epoch 462 batch: 400 avg loss -2.756051 avg loss no lamb -2.756051 time 2020-06-26 06:11:21.043624
Model ind 665 epoch 462 batch: 500 avg loss -2.874177 avg loss no lamb -2.874177 time 2020-06-26 06:11:31.984334
Model ind 665 epoch 462 batch: 600 avg loss -2.793170 avg loss no lamb -2.793170 time 2020-06-26 06:11:42.857942
Model ind 665 epoch 462 batch: 700 avg loss -2.618481 avg loss no lamb -2.618481 time 2020-06-26 06:11:53.561436
Model ind 665 epoch 462 batch: 800 avg loss -2.824894 avg loss no lamb -2.824894 time 2020-06-26 06:12:04.331669
last batch sz 10
Pre: time 2020-06-26 06:12:18.254495: 
 	std: 0.0031189728
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9813, 0.9749, 0.9813, 0.9755]
	train_accs: [0.98186666, 0.9817333, 0.9759833, 0.98195, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.979
	best: 0.9813

Starting e_i: 463
Model ind 665 epoch 463 batch: 0 avg loss -2.878805 avg loss no lamb -2.878805 time 2020-06-26 06:12:19.081754
Model ind 665 epoch 463 batch: 100 avg loss -2.861066 avg loss no lamb -2.861066 time 2020-06-26 06:12:29.942611
Model ind 665 epoch 463 batch: 200 avg loss -2.828693 avg loss no lamb -2.828693 time 2020-06-26 06:12:40.746217
Model ind 665 epoch 463 batch: 300 avg loss -2.847088 avg loss no lamb -2.847088 time 2020-06-26 06:12:51.619407
Model ind 665 epoch 463 batch: 400 avg loss -2.795015 avg loss no lamb -2.795015 time 2020-06-26 06:13:02.465655
Model ind 665 epoch 463 batch: 500 avg loss -2.802931 avg loss no lamb -2.802931 time 2020-06-26 06:13:13.096079
Model ind 665 epoch 463 batch: 600 avg loss -2.828047 avg loss no lamb -2.828047 time 2020-06-26 06:13:24.046463
Model ind 665 epoch 463 batch: 700 avg loss -2.635102 avg loss no lamb -2.635102 time 2020-06-26 06:13:34.917429
Model ind 665 epoch 463 batch: 800 avg loss -2.742103 avg loss no lamb -2.742103 time 2020-06-26 06:13:45.763490
last batch sz 10
Pre: time 2020-06-26 06:13:59.727904: 
 	std: 0.0033731938
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9801, 0.9733, 0.9808, 0.9744]
	train_accs: [0.9816, 0.9808667, 0.97506666, 0.9815, 0.9762]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97793996
	best: 0.9811

Starting e_i: 464
Model ind 665 epoch 464 batch: 0 avg loss -2.850082 avg loss no lamb -2.850082 time 2020-06-26 06:14:00.661964
Model ind 665 epoch 464 batch: 100 avg loss -2.884832 avg loss no lamb -2.884832 time 2020-06-26 06:14:11.300078
Model ind 665 epoch 464 batch: 200 avg loss -2.835882 avg loss no lamb -2.835882 time 2020-06-26 06:14:22.048131
Model ind 665 epoch 464 batch: 300 avg loss -2.833506 avg loss no lamb -2.833506 time 2020-06-26 06:14:33.042843
Model ind 665 epoch 464 batch: 400 avg loss -2.755453 avg loss no lamb -2.755453 time 2020-06-26 06:14:43.725270
Model ind 665 epoch 464 batch: 500 avg loss -2.778085 avg loss no lamb -2.778085 time 2020-06-26 06:14:54.268364
Model ind 665 epoch 464 batch: 600 avg loss -2.837516 avg loss no lamb -2.837516 time 2020-06-26 06:15:05.026280
Model ind 665 epoch 464 batch: 700 avg loss -2.736776 avg loss no lamb -2.736776 time 2020-06-26 06:15:15.770478
Model ind 665 epoch 464 batch: 800 avg loss -2.794619 avg loss no lamb -2.794619 time 2020-06-26 06:15:26.533356
last batch sz 10
Pre: time 2020-06-26 06:15:40.556895: 
 	std: 0.0031005861
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9813, 0.9746, 0.9807, 0.9744]
	train_accs: [0.9816667, 0.9814, 0.97608334, 0.98195, 0.9763833]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97827995
	best: 0.9807

Starting e_i: 465
Model ind 665 epoch 465 batch: 0 avg loss -2.874577 avg loss no lamb -2.874577 time 2020-06-26 06:15:41.372423
Model ind 665 epoch 465 batch: 100 avg loss -2.883401 avg loss no lamb -2.883401 time 2020-06-26 06:15:52.008479
Model ind 665 epoch 465 batch: 200 avg loss -2.791078 avg loss no lamb -2.791078 time 2020-06-26 06:16:02.663265
Model ind 665 epoch 465 batch: 300 avg loss -2.858805 avg loss no lamb -2.858805 time 2020-06-26 06:16:13.273543
Model ind 665 epoch 465 batch: 400 avg loss -2.718713 avg loss no lamb -2.718713 time 2020-06-26 06:16:24.050980
Model ind 665 epoch 465 batch: 500 avg loss -2.787129 avg loss no lamb -2.787129 time 2020-06-26 06:16:34.714453
Model ind 665 epoch 465 batch: 600 avg loss -2.847115 avg loss no lamb -2.847115 time 2020-06-26 06:16:45.343818
Model ind 665 epoch 465 batch: 700 avg loss -2.673800 avg loss no lamb -2.673800 time 2020-06-26 06:16:55.921514
Model ind 665 epoch 465 batch: 800 avg loss -2.811709 avg loss no lamb -2.811709 time 2020-06-26 06:17:06.574424
last batch sz 10
Pre: time 2020-06-26 06:17:20.493524: 
 	std: 0.0023971712
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.981, 0.9808, 0.9759, 0.9815, 0.9766]
	train_accs: [0.9813, 0.98135, 0.97685, 0.98135, 0.9773]
	best_train_sub_head: 1
	worst: 0.9759
	avg: 0.97916
	best: 0.9808

Starting e_i: 466
Model ind 665 epoch 466 batch: 0 avg loss -2.892246 avg loss no lamb -2.892246 time 2020-06-26 06:17:21.347011
Model ind 665 epoch 466 batch: 100 avg loss -2.842056 avg loss no lamb -2.842056 time 2020-06-26 06:17:31.900517
Model ind 665 epoch 466 batch: 200 avg loss -2.824789 avg loss no lamb -2.824789 time 2020-06-26 06:17:42.641060
Model ind 665 epoch 466 batch: 300 avg loss -2.853099 avg loss no lamb -2.853099 time 2020-06-26 06:17:53.836706
Model ind 665 epoch 466 batch: 400 avg loss -2.728560 avg loss no lamb -2.728560 time 2020-06-26 06:18:04.484352
Model ind 665 epoch 466 batch: 500 avg loss -2.789160 avg loss no lamb -2.789160 time 2020-06-26 06:18:14.906812
Model ind 665 epoch 466 batch: 600 avg loss -2.791121 avg loss no lamb -2.791121 time 2020-06-26 06:18:25.519117
Model ind 665 epoch 466 batch: 700 avg loss -2.682980 avg loss no lamb -2.682980 time 2020-06-26 06:18:36.138648
Model ind 665 epoch 466 batch: 800 avg loss -2.783976 avg loss no lamb -2.783976 time 2020-06-26 06:18:46.823782
last batch sz 10
Pre: time 2020-06-26 06:19:02.545027: 
 	std: 0.0025716932
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9816, 0.9759, 0.9823, 0.978]
	train_accs: [0.98195, 0.9813667, 0.97678334, 0.9819667, 0.9777]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97998
	best: 0.9823

Starting e_i: 467
Model ind 665 epoch 467 batch: 0 avg loss -2.926418 avg loss no lamb -2.926418 time 2020-06-26 06:19:03.396693
Model ind 665 epoch 467 batch: 100 avg loss -2.862825 avg loss no lamb -2.862825 time 2020-06-26 06:19:14.137993
Model ind 665 epoch 467 batch: 200 avg loss -2.859519 avg loss no lamb -2.859519 time 2020-06-26 06:19:24.963399
Model ind 665 epoch 467 batch: 300 avg loss -2.851285 avg loss no lamb -2.851285 time 2020-06-26 06:19:35.913503
Model ind 665 epoch 467 batch: 400 avg loss -2.753367 avg loss no lamb -2.753367 time 2020-06-26 06:19:46.726649
Model ind 665 epoch 467 batch: 500 avg loss -2.770278 avg loss no lamb -2.770278 time 2020-06-26 06:19:57.383224
Model ind 665 epoch 467 batch: 600 avg loss -2.802891 avg loss no lamb -2.802891 time 2020-06-26 06:20:08.143858
Model ind 665 epoch 467 batch: 700 avg loss -2.684896 avg loss no lamb -2.684896 time 2020-06-26 06:20:18.905703
Model ind 665 epoch 467 batch: 800 avg loss -2.789185 avg loss no lamb -2.789185 time 2020-06-26 06:20:29.499948
last batch sz 10
Pre: time 2020-06-26 06:20:43.445896: 
 	std: 0.0030835113
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9824, 0.9755, 0.9824, 0.9766]
	train_accs: [0.9816167, 0.9813167, 0.97595, 0.9816167, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97980005
	best: 0.9821

Starting e_i: 468
Model ind 665 epoch 468 batch: 0 avg loss -2.889173 avg loss no lamb -2.889173 time 2020-06-26 06:20:44.273020
Model ind 665 epoch 468 batch: 100 avg loss -2.837952 avg loss no lamb -2.837952 time 2020-06-26 06:20:54.966714
Model ind 665 epoch 468 batch: 200 avg loss -2.787875 avg loss no lamb -2.787875 time 2020-06-26 06:21:05.667809
Model ind 665 epoch 468 batch: 300 avg loss -2.793424 avg loss no lamb -2.793424 time 2020-06-26 06:21:16.333646
Model ind 665 epoch 468 batch: 400 avg loss -2.707522 avg loss no lamb -2.707522 time 2020-06-26 06:21:26.731582
Model ind 665 epoch 468 batch: 500 avg loss -2.761993 avg loss no lamb -2.761993 time 2020-06-26 06:21:37.182905
Model ind 665 epoch 468 batch: 600 avg loss -2.819017 avg loss no lamb -2.819017 time 2020-06-26 06:21:49.641055
Model ind 665 epoch 468 batch: 700 avg loss -2.668166 avg loss no lamb -2.668166 time 2020-06-26 06:22:00.441273
Model ind 665 epoch 468 batch: 800 avg loss -2.799646 avg loss no lamb -2.799646 time 2020-06-26 06:22:11.033994
last batch sz 10
Pre: time 2020-06-26 06:22:25.191034: 
 	std: 0.0026183876
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9823, 0.9761, 0.9821, 0.9776]
	train_accs: [0.98191667, 0.9815, 0.9769167, 0.98178333, 0.9777]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.98
	best: 0.9819

Starting e_i: 469
Model ind 665 epoch 469 batch: 0 avg loss -2.889936 avg loss no lamb -2.889936 time 2020-06-26 06:22:26.079671
Model ind 665 epoch 469 batch: 100 avg loss -2.812753 avg loss no lamb -2.812753 time 2020-06-26 06:22:37.232211
Model ind 665 epoch 469 batch: 200 avg loss -2.820867 avg loss no lamb -2.820867 time 2020-06-26 06:22:48.112846
Model ind 665 epoch 469 batch: 300 avg loss -2.810130 avg loss no lamb -2.810130 time 2020-06-26 06:22:58.902383
Model ind 665 epoch 469 batch: 400 avg loss -2.824380 avg loss no lamb -2.824380 time 2020-06-26 06:23:09.471145
Model ind 665 epoch 469 batch: 500 avg loss -2.820683 avg loss no lamb -2.820683 time 2020-06-26 06:23:20.021147
Model ind 665 epoch 469 batch: 600 avg loss -2.742888 avg loss no lamb -2.742888 time 2020-06-26 06:23:30.680854
Model ind 665 epoch 469 batch: 700 avg loss -2.611659 avg loss no lamb -2.611659 time 2020-06-26 06:23:41.439068
Model ind 665 epoch 469 batch: 800 avg loss -2.808961 avg loss no lamb -2.808961 time 2020-06-26 06:23:52.132850
last batch sz 10
Pre: time 2020-06-26 06:24:05.934393: 
 	std: 0.0024385257
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9826, 0.9769, 0.982, 0.9777]
	train_accs: [0.98211664, 0.98186666, 0.9770667, 0.98205, 0.9774]
	best_train_sub_head: 0
	worst: 0.9769
	avg: 0.9802599
	best: 0.9821

Starting e_i: 470
Model ind 665 epoch 470 batch: 0 avg loss -2.941939 avg loss no lamb -2.941939 time 2020-06-26 06:24:06.764419
Model ind 665 epoch 470 batch: 100 avg loss -2.829492 avg loss no lamb -2.829492 time 2020-06-26 06:24:17.284310
Model ind 665 epoch 470 batch: 200 avg loss -2.847162 avg loss no lamb -2.847162 time 2020-06-26 06:24:28.248659
Model ind 665 epoch 470 batch: 300 avg loss -2.809430 avg loss no lamb -2.809430 time 2020-06-26 06:24:38.849113
Model ind 665 epoch 470 batch: 400 avg loss -2.688834 avg loss no lamb -2.688834 time 2020-06-26 06:24:49.266408
Model ind 665 epoch 470 batch: 500 avg loss -2.778334 avg loss no lamb -2.778334 time 2020-06-26 06:24:59.981670
Model ind 665 epoch 470 batch: 600 avg loss -2.869380 avg loss no lamb -2.869380 time 2020-06-26 06:25:10.688932
Model ind 665 epoch 470 batch: 700 avg loss -2.725508 avg loss no lamb -2.725508 time 2020-06-26 06:25:21.283522
Model ind 665 epoch 470 batch: 800 avg loss -2.854175 avg loss no lamb -2.854175 time 2020-06-26 06:25:32.261909
last batch sz 10
Pre: time 2020-06-26 06:25:46.372294: 
 	std: 0.002713364
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9807, 0.9746, 0.981, 0.9775]
	train_accs: [0.9812833, 0.9802667, 0.9758667, 0.98113334, 0.9769167]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97914
	best: 0.9819

Starting e_i: 471
Model ind 665 epoch 471 batch: 0 avg loss -2.928679 avg loss no lamb -2.928679 time 2020-06-26 06:25:48.352135
Model ind 665 epoch 471 batch: 100 avg loss -2.902425 avg loss no lamb -2.902425 time 2020-06-26 06:25:58.857962
Model ind 665 epoch 471 batch: 200 avg loss -2.756740 avg loss no lamb -2.756740 time 2020-06-26 06:26:09.698842
Model ind 665 epoch 471 batch: 300 avg loss -2.806777 avg loss no lamb -2.806777 time 2020-06-26 06:26:20.660321
Model ind 665 epoch 471 batch: 400 avg loss -2.794275 avg loss no lamb -2.794275 time 2020-06-26 06:26:31.368198
Model ind 665 epoch 471 batch: 500 avg loss -2.749258 avg loss no lamb -2.749258 time 2020-06-26 06:26:42.139076
Model ind 665 epoch 471 batch: 600 avg loss -2.803490 avg loss no lamb -2.803490 time 2020-06-26 06:26:52.777835
Model ind 665 epoch 471 batch: 700 avg loss -2.722122 avg loss no lamb -2.722122 time 2020-06-26 06:27:03.781232
Model ind 665 epoch 471 batch: 800 avg loss -2.826674 avg loss no lamb -2.826674 time 2020-06-26 06:27:14.475389
last batch sz 10
Pre: time 2020-06-26 06:27:28.455783: 
 	std: 0.0023301558
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9822, 0.9769, 0.9819, 0.9778]
	train_accs: [0.9818, 0.98145, 0.9765667, 0.9817167, 0.97728336]
	best_train_sub_head: 0
	worst: 0.9769
	avg: 0.98017997
	best: 0.9821

Starting e_i: 472
Model ind 665 epoch 472 batch: 0 avg loss -2.916357 avg loss no lamb -2.916357 time 2020-06-26 06:27:29.285902
Model ind 665 epoch 472 batch: 100 avg loss -2.905109 avg loss no lamb -2.905109 time 2020-06-26 06:27:39.713730
Model ind 665 epoch 472 batch: 200 avg loss -2.788324 avg loss no lamb -2.788324 time 2020-06-26 06:27:50.353854
Model ind 665 epoch 472 batch: 300 avg loss -2.886299 avg loss no lamb -2.886299 time 2020-06-26 06:28:01.089991
Model ind 665 epoch 472 batch: 400 avg loss -2.690086 avg loss no lamb -2.690086 time 2020-06-26 06:28:11.875431
Model ind 665 epoch 472 batch: 500 avg loss -2.782576 avg loss no lamb -2.782576 time 2020-06-26 06:28:22.364538
Model ind 665 epoch 472 batch: 600 avg loss -2.807658 avg loss no lamb -2.807658 time 2020-06-26 06:28:32.842600
Model ind 665 epoch 472 batch: 700 avg loss -2.665315 avg loss no lamb -2.665315 time 2020-06-26 06:28:43.332817
Model ind 665 epoch 472 batch: 800 avg loss -2.726993 avg loss no lamb -2.726993 time 2020-06-26 06:28:54.133014
last batch sz 10
Pre: time 2020-06-26 06:29:08.043032: 
 	std: 0.002296611
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9822, 0.9765, 0.982, 0.9786]
	train_accs: [0.9820333, 0.9816667, 0.9772, 0.98205, 0.9780833]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.98024
	best: 0.982

Starting e_i: 473
Model ind 665 epoch 473 batch: 0 avg loss -2.918154 avg loss no lamb -2.918154 time 2020-06-26 06:29:08.838138
Model ind 665 epoch 473 batch: 100 avg loss -2.950370 avg loss no lamb -2.950370 time 2020-06-26 06:29:19.541285
Model ind 665 epoch 473 batch: 200 avg loss -2.826170 avg loss no lamb -2.826170 time 2020-06-26 06:29:30.208122
Model ind 665 epoch 473 batch: 300 avg loss -2.849433 avg loss no lamb -2.849433 time 2020-06-26 06:29:40.901448
Model ind 665 epoch 473 batch: 400 avg loss -2.665014 avg loss no lamb -2.665014 time 2020-06-26 06:29:51.622941
Model ind 665 epoch 473 batch: 500 avg loss -2.793091 avg loss no lamb -2.793091 time 2020-06-26 06:30:02.372664
Model ind 665 epoch 473 batch: 600 avg loss -2.857290 avg loss no lamb -2.857290 time 2020-06-26 06:30:12.993037
Model ind 665 epoch 473 batch: 700 avg loss -2.693963 avg loss no lamb -2.693963 time 2020-06-26 06:30:23.872732
Model ind 665 epoch 473 batch: 800 avg loss -2.781220 avg loss no lamb -2.781220 time 2020-06-26 06:30:34.637922
last batch sz 10
Pre: time 2020-06-26 06:30:48.407439: 
 	std: 0.0030135715
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9816, 0.9754, 0.982, 0.9768]
	train_accs: [0.98183334, 0.9816167, 0.97653335, 0.9816, 0.9773]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97972
	best: 0.9828

Starting e_i: 474
Model ind 665 epoch 474 batch: 0 avg loss -2.952960 avg loss no lamb -2.952960 time 2020-06-26 06:30:49.343727
Model ind 665 epoch 474 batch: 100 avg loss -2.885641 avg loss no lamb -2.885641 time 2020-06-26 06:30:59.981178
Model ind 665 epoch 474 batch: 200 avg loss -2.830789 avg loss no lamb -2.830789 time 2020-06-26 06:31:10.755259
Model ind 665 epoch 474 batch: 300 avg loss -2.782090 avg loss no lamb -2.782090 time 2020-06-26 06:31:21.499181
Model ind 665 epoch 474 batch: 400 avg loss -2.745436 avg loss no lamb -2.745436 time 2020-06-26 06:31:32.191534
Model ind 665 epoch 474 batch: 500 avg loss -2.749716 avg loss no lamb -2.749716 time 2020-06-26 06:31:42.841077
Model ind 665 epoch 474 batch: 600 avg loss -2.829754 avg loss no lamb -2.829754 time 2020-06-26 06:31:53.502309
Model ind 665 epoch 474 batch: 700 avg loss -2.720044 avg loss no lamb -2.720044 time 2020-06-26 06:32:04.255471
Model ind 665 epoch 474 batch: 800 avg loss -2.768270 avg loss no lamb -2.768270 time 2020-06-26 06:32:14.891674
last batch sz 10
Pre: time 2020-06-26 06:32:28.694907: 
 	std: 0.003009721
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9813, 0.9752, 0.9826, 0.9772]
	train_accs: [0.98205, 0.98116666, 0.97678334, 0.9821333, 0.97725]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97976
	best: 0.9826

Starting e_i: 475
Model ind 665 epoch 475 batch: 0 avg loss -2.848311 avg loss no lamb -2.848311 time 2020-06-26 06:32:29.542120
Model ind 665 epoch 475 batch: 100 avg loss -2.837733 avg loss no lamb -2.837733 time 2020-06-26 06:32:40.429692
Model ind 665 epoch 475 batch: 200 avg loss -2.802279 avg loss no lamb -2.802279 time 2020-06-26 06:32:51.405231
Model ind 665 epoch 475 batch: 300 avg loss -2.815771 avg loss no lamb -2.815771 time 2020-06-26 06:33:02.220753
Model ind 665 epoch 475 batch: 400 avg loss -2.751061 avg loss no lamb -2.751061 time 2020-06-26 06:33:12.940091
Model ind 665 epoch 475 batch: 500 avg loss -2.749390 avg loss no lamb -2.749390 time 2020-06-26 06:33:23.723285
Model ind 665 epoch 475 batch: 600 avg loss -2.822263 avg loss no lamb -2.822263 time 2020-06-26 06:33:34.714678
Model ind 665 epoch 475 batch: 700 avg loss -2.699172 avg loss no lamb -2.699172 time 2020-06-26 06:33:45.554831
Model ind 665 epoch 475 batch: 800 avg loss -2.814423 avg loss no lamb -2.814423 time 2020-06-26 06:33:56.331666
last batch sz 10
Pre: time 2020-06-26 06:34:10.458027: 
 	std: 0.0027119028
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9838, 0.983, 0.978, 0.9841, 0.9783]
	train_accs: [0.98263335, 0.98175, 0.97748333, 0.9824333, 0.97791666]
	best_train_sub_head: 0
	worst: 0.978
	avg: 0.98143995
	best: 0.9838

Starting e_i: 476
Model ind 665 epoch 476 batch: 0 avg loss -2.931559 avg loss no lamb -2.931559 time 2020-06-26 06:34:11.293037
Model ind 665 epoch 476 batch: 100 avg loss -2.828201 avg loss no lamb -2.828201 time 2020-06-26 06:34:21.981447
Model ind 665 epoch 476 batch: 200 avg loss -2.873212 avg loss no lamb -2.873212 time 2020-06-26 06:34:32.672877
Model ind 665 epoch 476 batch: 300 avg loss -2.840115 avg loss no lamb -2.840115 time 2020-06-26 06:34:43.183998
Model ind 665 epoch 476 batch: 400 avg loss -2.736278 avg loss no lamb -2.736278 time 2020-06-26 06:34:53.934951
Model ind 665 epoch 476 batch: 500 avg loss -2.823676 avg loss no lamb -2.823676 time 2020-06-26 06:35:04.740030
Model ind 665 epoch 476 batch: 600 avg loss -2.883197 avg loss no lamb -2.883197 time 2020-06-26 06:35:15.265379
Model ind 665 epoch 476 batch: 700 avg loss -2.650277 avg loss no lamb -2.650277 time 2020-06-26 06:35:25.914136
Model ind 665 epoch 476 batch: 800 avg loss -2.768197 avg loss no lamb -2.768197 time 2020-06-26 06:35:36.609185
last batch sz 10
Pre: time 2020-06-26 06:35:50.521532: 
 	std: 0.00312705
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9808, 0.9747, 0.9816, 0.9752]
	train_accs: [0.9815, 0.98105, 0.97576666, 0.9816167, 0.9758]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97876006
	best: 0.9816

Starting e_i: 477
Model ind 665 epoch 477 batch: 0 avg loss -2.919487 avg loss no lamb -2.919487 time 2020-06-26 06:35:51.362725
Model ind 665 epoch 477 batch: 100 avg loss -2.804780 avg loss no lamb -2.804780 time 2020-06-26 06:36:01.770820
Model ind 665 epoch 477 batch: 200 avg loss -2.832619 avg loss no lamb -2.832619 time 2020-06-26 06:36:12.234121
Model ind 665 epoch 477 batch: 300 avg loss -2.871017 avg loss no lamb -2.871017 time 2020-06-26 06:36:22.996310
Model ind 665 epoch 477 batch: 400 avg loss -2.692550 avg loss no lamb -2.692550 time 2020-06-26 06:36:33.744024
Model ind 665 epoch 477 batch: 500 avg loss -2.767957 avg loss no lamb -2.767957 time 2020-06-26 06:36:44.556158
Model ind 665 epoch 477 batch: 600 avg loss -2.878561 avg loss no lamb -2.878561 time 2020-06-26 06:36:55.281440
Model ind 665 epoch 477 batch: 700 avg loss -2.722286 avg loss no lamb -2.722286 time 2020-06-26 06:37:05.962339
Model ind 665 epoch 477 batch: 800 avg loss -2.782956 avg loss no lamb -2.782956 time 2020-06-26 06:37:16.598171
last batch sz 10
Pre: time 2020-06-26 06:37:30.730017: 
 	std: 0.002498316
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9824, 0.9767, 0.9821, 0.9779]
	train_accs: [0.9823, 0.98205, 0.9776667, 0.9821333, 0.97775]
	best_train_sub_head: 0
	worst: 0.9767
	avg: 0.98032
	best: 0.9825

Starting e_i: 478
Model ind 665 epoch 478 batch: 0 avg loss -2.945539 avg loss no lamb -2.945539 time 2020-06-26 06:37:31.548215
Model ind 665 epoch 478 batch: 100 avg loss -2.873801 avg loss no lamb -2.873801 time 2020-06-26 06:37:42.541324
Model ind 665 epoch 478 batch: 200 avg loss -2.834767 avg loss no lamb -2.834767 time 2020-06-26 06:37:53.245358
Model ind 665 epoch 478 batch: 300 avg loss -2.765387 avg loss no lamb -2.765387 time 2020-06-26 06:38:03.941292
Model ind 665 epoch 478 batch: 400 avg loss -2.676431 avg loss no lamb -2.676431 time 2020-06-26 06:38:14.648233
Model ind 665 epoch 478 batch: 500 avg loss -2.856129 avg loss no lamb -2.856129 time 2020-06-26 06:38:25.534439
Model ind 665 epoch 478 batch: 600 avg loss -2.777438 avg loss no lamb -2.777438 time 2020-06-26 06:38:36.348658
Model ind 665 epoch 478 batch: 700 avg loss -2.656532 avg loss no lamb -2.656532 time 2020-06-26 06:38:47.037856
Model ind 665 epoch 478 batch: 800 avg loss -2.774823 avg loss no lamb -2.774823 time 2020-06-26 06:38:57.766445
last batch sz 10
Pre: time 2020-06-26 06:39:11.414031: 
 	std: 0.0033498541
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9806, 0.9743, 0.981, 0.9743]
	train_accs: [0.98156667, 0.98111665, 0.97576666, 0.98146665, 0.97555]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97838
	best: 0.9817

Starting e_i: 479
Model ind 665 epoch 479 batch: 0 avg loss -2.917264 avg loss no lamb -2.917264 time 2020-06-26 06:39:12.303604
Model ind 665 epoch 479 batch: 100 avg loss -2.795261 avg loss no lamb -2.795261 time 2020-06-26 06:39:22.949802
Model ind 665 epoch 479 batch: 200 avg loss -2.811063 avg loss no lamb -2.811063 time 2020-06-26 06:39:33.554499
Model ind 665 epoch 479 batch: 300 avg loss -2.827983 avg loss no lamb -2.827983 time 2020-06-26 06:39:44.284502
Model ind 665 epoch 479 batch: 400 avg loss -2.727854 avg loss no lamb -2.727854 time 2020-06-26 06:39:55.006270
Model ind 665 epoch 479 batch: 500 avg loss -2.809182 avg loss no lamb -2.809182 time 2020-06-26 06:40:05.585522
Model ind 665 epoch 479 batch: 600 avg loss -2.780319 avg loss no lamb -2.780319 time 2020-06-26 06:40:16.280326
Model ind 665 epoch 479 batch: 700 avg loss -2.623324 avg loss no lamb -2.623324 time 2020-06-26 06:40:27.034084
Model ind 665 epoch 479 batch: 800 avg loss -2.828776 avg loss no lamb -2.828776 time 2020-06-26 06:40:37.915592
last batch sz 10
Pre: time 2020-06-26 06:40:51.903322: 
 	std: 0.0027524608
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.981, 0.9758, 0.9818, 0.9761]
	train_accs: [0.9819667, 0.9812667, 0.97603333, 0.98175, 0.9763]
	best_train_sub_head: 0
	worst: 0.9758
	avg: 0.9793
	best: 0.9818

Starting e_i: 480
Model ind 665 epoch 480 batch: 0 avg loss -2.953298 avg loss no lamb -2.953298 time 2020-06-26 06:40:52.746315
Model ind 665 epoch 480 batch: 100 avg loss -2.818418 avg loss no lamb -2.818418 time 2020-06-26 06:41:03.137262
Model ind 665 epoch 480 batch: 200 avg loss -2.839369 avg loss no lamb -2.839369 time 2020-06-26 06:41:13.881568
Model ind 665 epoch 480 batch: 300 avg loss -2.782451 avg loss no lamb -2.782451 time 2020-06-26 06:41:24.479205
Model ind 665 epoch 480 batch: 400 avg loss -2.718601 avg loss no lamb -2.718601 time 2020-06-26 06:41:35.112070
Model ind 665 epoch 480 batch: 500 avg loss -2.804698 avg loss no lamb -2.804698 time 2020-06-26 06:41:46.124355
Model ind 665 epoch 480 batch: 600 avg loss -2.842458 avg loss no lamb -2.842458 time 2020-06-26 06:41:57.051649
Model ind 665 epoch 480 batch: 700 avg loss -2.722237 avg loss no lamb -2.722237 time 2020-06-26 06:42:07.843283
Model ind 665 epoch 480 batch: 800 avg loss -2.778671 avg loss no lamb -2.778671 time 2020-06-26 06:42:18.574476
last batch sz 10
Pre: time 2020-06-26 06:42:32.436537: 
 	std: 0.0029722806
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9793, 0.9739, 0.9807, 0.9744]
	train_accs: [0.98143333, 0.9802333, 0.9756, 0.98156667, 0.97595]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97774
	best: 0.9807

Starting e_i: 481
Model ind 665 epoch 481 batch: 0 avg loss -2.896068 avg loss no lamb -2.896068 time 2020-06-26 06:42:34.401803
Model ind 665 epoch 481 batch: 100 avg loss -2.810094 avg loss no lamb -2.810094 time 2020-06-26 06:42:45.173386
Model ind 665 epoch 481 batch: 200 avg loss -2.829115 avg loss no lamb -2.829115 time 2020-06-26 06:42:55.791710
Model ind 665 epoch 481 batch: 300 avg loss -2.875325 avg loss no lamb -2.875325 time 2020-06-26 06:43:06.608105
Model ind 665 epoch 481 batch: 400 avg loss -2.751639 avg loss no lamb -2.751639 time 2020-06-26 06:43:17.262589
Model ind 665 epoch 481 batch: 500 avg loss -2.808730 avg loss no lamb -2.808730 time 2020-06-26 06:43:28.007875
Model ind 665 epoch 481 batch: 600 avg loss -2.876013 avg loss no lamb -2.876013 time 2020-06-26 06:43:38.795546
Model ind 665 epoch 481 batch: 700 avg loss -2.610220 avg loss no lamb -2.610220 time 2020-06-26 06:43:49.359683
Model ind 665 epoch 481 batch: 800 avg loss -2.785642 avg loss no lamb -2.785642 time 2020-06-26 06:44:00.033040
last batch sz 10
Pre: time 2020-06-26 06:44:13.935698: 
 	std: 0.0020803772
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9812, 0.9763, 0.9816, 0.9788]
	train_accs: [0.9816833, 0.9813167, 0.97678334, 0.98178333, 0.97775]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.9799
	best: 0.9816

Starting e_i: 482
Model ind 665 epoch 482 batch: 0 avg loss -2.901852 avg loss no lamb -2.901852 time 2020-06-26 06:44:14.792676
Model ind 665 epoch 482 batch: 100 avg loss -2.852253 avg loss no lamb -2.852253 time 2020-06-26 06:44:25.472974
Model ind 665 epoch 482 batch: 200 avg loss -2.822525 avg loss no lamb -2.822525 time 2020-06-26 06:44:36.244105
Model ind 665 epoch 482 batch: 300 avg loss -2.796235 avg loss no lamb -2.796235 time 2020-06-26 06:44:46.846922
Model ind 665 epoch 482 batch: 400 avg loss -2.711919 avg loss no lamb -2.711919 time 2020-06-26 06:44:57.522565
Model ind 665 epoch 482 batch: 500 avg loss -2.770103 avg loss no lamb -2.770103 time 2020-06-26 06:45:08.490718
Model ind 665 epoch 482 batch: 600 avg loss -2.855785 avg loss no lamb -2.855785 time 2020-06-26 06:45:19.227666
Model ind 665 epoch 482 batch: 700 avg loss -2.678349 avg loss no lamb -2.678349 time 2020-06-26 06:45:30.050969
Model ind 665 epoch 482 batch: 800 avg loss -2.775820 avg loss no lamb -2.775820 time 2020-06-26 06:45:40.457493
last batch sz 10
Pre: time 2020-06-26 06:45:54.127397: 
 	std: 0.00286399
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9814, 0.9752, 0.9815, 0.9761]
	train_accs: [0.98156667, 0.98148334, 0.97581667, 0.98153335, 0.97643334]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97914
	best: 0.9815

Starting e_i: 483
Model ind 665 epoch 483 batch: 0 avg loss -2.954989 avg loss no lamb -2.954989 time 2020-06-26 06:45:54.974164
Model ind 665 epoch 483 batch: 100 avg loss -2.848432 avg loss no lamb -2.848432 time 2020-06-26 06:46:05.873831
Model ind 665 epoch 483 batch: 200 avg loss -2.865864 avg loss no lamb -2.865864 time 2020-06-26 06:46:16.643033
Model ind 665 epoch 483 batch: 300 avg loss -2.823579 avg loss no lamb -2.823579 time 2020-06-26 06:46:27.172345
Model ind 665 epoch 483 batch: 400 avg loss -2.736996 avg loss no lamb -2.736996 time 2020-06-26 06:46:37.761122
Model ind 665 epoch 483 batch: 500 avg loss -2.795463 avg loss no lamb -2.795463 time 2020-06-26 06:46:48.399370
Model ind 665 epoch 483 batch: 600 avg loss -2.824330 avg loss no lamb -2.824330 time 2020-06-26 06:46:59.461872
Model ind 665 epoch 483 batch: 700 avg loss -2.703631 avg loss no lamb -2.703631 time 2020-06-26 06:47:10.262441
Model ind 665 epoch 483 batch: 800 avg loss -2.744912 avg loss no lamb -2.744912 time 2020-06-26 06:47:20.949486
last batch sz 10
Pre: time 2020-06-26 06:47:34.816362: 
 	std: 0.0025136499
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9819, 0.9765, 0.9819, 0.9773]
	train_accs: [0.98193336, 0.9817, 0.97646666, 0.98205, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9765
	avg: 0.97995996
	best: 0.9819

Starting e_i: 484
Model ind 665 epoch 484 batch: 0 avg loss -2.859097 avg loss no lamb -2.859097 time 2020-06-26 06:47:35.765314
Model ind 665 epoch 484 batch: 100 avg loss -2.870608 avg loss no lamb -2.870608 time 2020-06-26 06:47:46.373116
Model ind 665 epoch 484 batch: 200 avg loss -2.749254 avg loss no lamb -2.749254 time 2020-06-26 06:47:57.251212
Model ind 665 epoch 484 batch: 300 avg loss -2.796251 avg loss no lamb -2.796251 time 2020-06-26 06:48:07.756693
Model ind 665 epoch 484 batch: 400 avg loss -2.784000 avg loss no lamb -2.784000 time 2020-06-26 06:48:18.649990
Model ind 665 epoch 484 batch: 500 avg loss -2.832265 avg loss no lamb -2.832265 time 2020-06-26 06:48:29.471663
Model ind 665 epoch 484 batch: 600 avg loss -2.868195 avg loss no lamb -2.868195 time 2020-06-26 06:48:40.163633
Model ind 665 epoch 484 batch: 700 avg loss -2.662925 avg loss no lamb -2.662925 time 2020-06-26 06:48:50.882596
Model ind 665 epoch 484 batch: 800 avg loss -2.742435 avg loss no lamb -2.742435 time 2020-06-26 06:49:01.698613
last batch sz 10
Pre: time 2020-06-26 06:49:15.471457: 
 	std: 0.002931622
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9823, 0.9758, 0.9823, 0.977]
	train_accs: [0.9826667, 0.98223335, 0.9768, 0.9828333, 0.97758335]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.9799601
	best: 0.9823

Starting e_i: 485
Model ind 665 epoch 485 batch: 0 avg loss -2.926801 avg loss no lamb -2.926801 time 2020-06-26 06:49:16.281259
Model ind 665 epoch 485 batch: 100 avg loss -2.869601 avg loss no lamb -2.869601 time 2020-06-26 06:49:27.281657
Model ind 665 epoch 485 batch: 200 avg loss -2.777554 avg loss no lamb -2.777554 time 2020-06-26 06:49:38.173019
Model ind 665 epoch 485 batch: 300 avg loss -2.778122 avg loss no lamb -2.778122 time 2020-06-26 06:49:48.876431
Model ind 665 epoch 485 batch: 400 avg loss -2.718114 avg loss no lamb -2.718114 time 2020-06-26 06:49:59.690143
Model ind 665 epoch 485 batch: 500 avg loss -2.782127 avg loss no lamb -2.782127 time 2020-06-26 06:50:10.545244
Model ind 665 epoch 485 batch: 600 avg loss -2.817528 avg loss no lamb -2.817528 time 2020-06-26 06:50:21.769087
Model ind 665 epoch 485 batch: 700 avg loss -2.721519 avg loss no lamb -2.721519 time 2020-06-26 06:50:32.437668
Model ind 665 epoch 485 batch: 800 avg loss -2.809566 avg loss no lamb -2.809566 time 2020-06-26 06:50:43.165357
last batch sz 10
Pre: time 2020-06-26 06:50:57.047005: 
 	std: 0.0030462993
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.981, 0.9753, 0.9819, 0.9757]
	train_accs: [0.982, 0.98115, 0.9759667, 0.9819833, 0.97641665]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.9792
	best: 0.9821

Starting e_i: 486
Model ind 665 epoch 486 batch: 0 avg loss -2.910446 avg loss no lamb -2.910446 time 2020-06-26 06:50:57.861625
Model ind 665 epoch 486 batch: 100 avg loss -2.801987 avg loss no lamb -2.801987 time 2020-06-26 06:51:08.423113
Model ind 665 epoch 486 batch: 200 avg loss -2.811529 avg loss no lamb -2.811529 time 2020-06-26 06:51:19.074948
Model ind 665 epoch 486 batch: 300 avg loss -2.816686 avg loss no lamb -2.816686 time 2020-06-26 06:51:29.633352
Model ind 665 epoch 486 batch: 400 avg loss -2.785954 avg loss no lamb -2.785954 time 2020-06-26 06:51:40.289992
Model ind 665 epoch 486 batch: 500 avg loss -2.696190 avg loss no lamb -2.696190 time 2020-06-26 06:51:50.981816
Model ind 665 epoch 486 batch: 600 avg loss -2.817125 avg loss no lamb -2.817125 time 2020-06-26 06:52:01.577529
Model ind 665 epoch 486 batch: 700 avg loss -2.614653 avg loss no lamb -2.614653 time 2020-06-26 06:52:12.305240
Model ind 665 epoch 486 batch: 800 avg loss -2.779080 avg loss no lamb -2.779080 time 2020-06-26 06:52:23.072357
last batch sz 10
Pre: time 2020-06-26 06:52:36.986461: 
 	std: 0.0027918573
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9811, 0.9747, 0.9809, 0.9756]
	train_accs: [0.98153335, 0.98135, 0.9762833, 0.9818, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97854006
	best: 0.9809

Starting e_i: 487
Model ind 665 epoch 487 batch: 0 avg loss -2.897509 avg loss no lamb -2.897509 time 2020-06-26 06:52:37.835816
Model ind 665 epoch 487 batch: 100 avg loss -2.879250 avg loss no lamb -2.879250 time 2020-06-26 06:52:48.554129
Model ind 665 epoch 487 batch: 200 avg loss -2.870825 avg loss no lamb -2.870825 time 2020-06-26 06:52:59.268271
Model ind 665 epoch 487 batch: 300 avg loss -2.820578 avg loss no lamb -2.820578 time 2020-06-26 06:53:10.109113
Model ind 665 epoch 487 batch: 400 avg loss -2.730494 avg loss no lamb -2.730494 time 2020-06-26 06:53:20.832249
Model ind 665 epoch 487 batch: 500 avg loss -2.765001 avg loss no lamb -2.765001 time 2020-06-26 06:53:31.922551
Model ind 665 epoch 487 batch: 600 avg loss -2.802641 avg loss no lamb -2.802641 time 2020-06-26 06:53:42.765518
Model ind 665 epoch 487 batch: 700 avg loss -2.692598 avg loss no lamb -2.692598 time 2020-06-26 06:53:53.582013
Model ind 665 epoch 487 batch: 800 avg loss -2.825739 avg loss no lamb -2.825739 time 2020-06-26 06:54:04.285484
last batch sz 10
Pre: time 2020-06-26 06:54:18.169515: 
 	std: 0.002946592
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9816, 0.9752, 0.9818, 0.9761]
	train_accs: [0.9817, 0.98146665, 0.97645, 0.98153335, 0.97695]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97924006
	best: 0.9815

Starting e_i: 488
Model ind 665 epoch 488 batch: 0 avg loss -2.880193 avg loss no lamb -2.880193 time 2020-06-26 06:54:19.008760
Model ind 665 epoch 488 batch: 100 avg loss -2.825641 avg loss no lamb -2.825641 time 2020-06-26 06:54:29.410244
Model ind 665 epoch 488 batch: 200 avg loss -2.823048 avg loss no lamb -2.823048 time 2020-06-26 06:54:40.353383
Model ind 665 epoch 488 batch: 300 avg loss -2.814206 avg loss no lamb -2.814206 time 2020-06-26 06:54:51.225953
Model ind 665 epoch 488 batch: 400 avg loss -2.724704 avg loss no lamb -2.724704 time 2020-06-26 06:55:01.988765
Model ind 665 epoch 488 batch: 500 avg loss -2.811326 avg loss no lamb -2.811326 time 2020-06-26 06:55:12.577628
Model ind 665 epoch 488 batch: 600 avg loss -2.840776 avg loss no lamb -2.840776 time 2020-06-26 06:55:23.240853
Model ind 665 epoch 488 batch: 700 avg loss -2.705527 avg loss no lamb -2.705527 time 2020-06-26 06:55:33.915487
Model ind 665 epoch 488 batch: 800 avg loss -2.748135 avg loss no lamb -2.748135 time 2020-06-26 06:55:44.668278
last batch sz 10
Pre: time 2020-06-26 06:55:58.693081: 
 	std: 0.0026419694
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9801, 0.9749, 0.9808, 0.9762]
	train_accs: [0.98181665, 0.98123336, 0.9762333, 0.9819, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.9787
	best: 0.9808

Starting e_i: 489
Model ind 665 epoch 489 batch: 0 avg loss -2.956784 avg loss no lamb -2.956784 time 2020-06-26 06:55:59.663342
Model ind 665 epoch 489 batch: 100 avg loss -2.903285 avg loss no lamb -2.903285 time 2020-06-26 06:56:10.180445
Model ind 665 epoch 489 batch: 200 avg loss -2.836628 avg loss no lamb -2.836628 time 2020-06-26 06:56:20.885661
Model ind 665 epoch 489 batch: 300 avg loss -2.787917 avg loss no lamb -2.787917 time 2020-06-26 06:56:31.754686
Model ind 665 epoch 489 batch: 400 avg loss -2.697602 avg loss no lamb -2.697602 time 2020-06-26 06:56:42.648249
Model ind 665 epoch 489 batch: 500 avg loss -2.787823 avg loss no lamb -2.787823 time 2020-06-26 06:56:53.356451
Model ind 665 epoch 489 batch: 600 avg loss -2.804277 avg loss no lamb -2.804277 time 2020-06-26 06:57:04.040334
Model ind 665 epoch 489 batch: 700 avg loss -2.689751 avg loss no lamb -2.689751 time 2020-06-26 06:57:15.029796
Model ind 665 epoch 489 batch: 800 avg loss -2.787331 avg loss no lamb -2.787331 time 2020-06-26 06:57:25.860931
last batch sz 10
Pre: time 2020-06-26 06:57:39.837728: 
 	std: 0.0029949308
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9811, 0.9745, 0.9821, 0.9773]
	train_accs: [0.9816167, 0.98078334, 0.9756167, 0.9816667, 0.97675]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97938
	best: 0.9821

Starting e_i: 490
Model ind 665 epoch 490 batch: 0 avg loss -2.898757 avg loss no lamb -2.898757 time 2020-06-26 06:57:40.733152
Model ind 665 epoch 490 batch: 100 avg loss -2.863850 avg loss no lamb -2.863850 time 2020-06-26 06:57:51.670068
Model ind 665 epoch 490 batch: 200 avg loss -2.826420 avg loss no lamb -2.826420 time 2020-06-26 06:58:02.139806
Model ind 665 epoch 490 batch: 300 avg loss -2.827436 avg loss no lamb -2.827436 time 2020-06-26 06:58:12.634225
Model ind 665 epoch 490 batch: 400 avg loss -2.705403 avg loss no lamb -2.705403 time 2020-06-26 06:58:23.324297
Model ind 665 epoch 490 batch: 500 avg loss -2.760772 avg loss no lamb -2.760772 time 2020-06-26 06:58:34.057616
Model ind 665 epoch 490 batch: 600 avg loss -2.867590 avg loss no lamb -2.867590 time 2020-06-26 06:58:44.690826
Model ind 665 epoch 490 batch: 700 avg loss -2.750242 avg loss no lamb -2.750242 time 2020-06-26 06:58:55.204420
Model ind 665 epoch 490 batch: 800 avg loss -2.809166 avg loss no lamb -2.809166 time 2020-06-26 06:59:06.040120
last batch sz 10
Pre: time 2020-06-26 06:59:19.872493: 
 	std: 0.0026203883
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.981, 0.9756, 0.9818, 0.9773]
	train_accs: [0.98218334, 0.98143333, 0.97646666, 0.9820167, 0.97758335]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97956
	best: 0.9821

Starting e_i: 491
Model ind 665 epoch 491 batch: 0 avg loss -2.897748 avg loss no lamb -2.897748 time 2020-06-26 06:59:21.903406
Model ind 665 epoch 491 batch: 100 avg loss -2.826406 avg loss no lamb -2.826406 time 2020-06-26 06:59:32.478453
Model ind 665 epoch 491 batch: 200 avg loss -2.826506 avg loss no lamb -2.826506 time 2020-06-26 06:59:42.875393
Model ind 665 epoch 491 batch: 300 avg loss -2.874833 avg loss no lamb -2.874833 time 2020-06-26 06:59:53.620351
Model ind 665 epoch 491 batch: 400 avg loss -2.739116 avg loss no lamb -2.739116 time 2020-06-26 07:00:04.606691
Model ind 665 epoch 491 batch: 500 avg loss -2.781024 avg loss no lamb -2.781024 time 2020-06-26 07:00:15.313716
Model ind 665 epoch 491 batch: 600 avg loss -2.772979 avg loss no lamb -2.772979 time 2020-06-26 07:00:26.206388
Model ind 665 epoch 491 batch: 700 avg loss -2.725343 avg loss no lamb -2.725343 time 2020-06-26 07:00:36.804996
Model ind 665 epoch 491 batch: 800 avg loss -2.814179 avg loss no lamb -2.814179 time 2020-06-26 07:00:47.575615
last batch sz 10
Pre: time 2020-06-26 07:01:01.526041: 
 	std: 0.00425084
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9796, 0.9715, 0.9806, 0.9719]
	train_accs: [0.9812, 0.9802, 0.97356665, 0.9809167, 0.9745]
	best_train_sub_head: 0
	worst: 0.9715
	avg: 0.97687995
	best: 0.9808

Starting e_i: 492
Model ind 665 epoch 492 batch: 0 avg loss -2.941013 avg loss no lamb -2.941013 time 2020-06-26 07:01:02.406241
Model ind 665 epoch 492 batch: 100 avg loss -2.789077 avg loss no lamb -2.789077 time 2020-06-26 07:01:13.090865
Model ind 665 epoch 492 batch: 200 avg loss -2.767302 avg loss no lamb -2.767302 time 2020-06-26 07:01:23.782196
Model ind 665 epoch 492 batch: 300 avg loss -2.807159 avg loss no lamb -2.807159 time 2020-06-26 07:01:34.436465
Model ind 665 epoch 492 batch: 400 avg loss -2.752023 avg loss no lamb -2.752023 time 2020-06-26 07:01:45.348969
Model ind 665 epoch 492 batch: 500 avg loss -2.773399 avg loss no lamb -2.773399 time 2020-06-26 07:01:56.076741
Model ind 665 epoch 492 batch: 600 avg loss -2.897392 avg loss no lamb -2.897392 time 2020-06-26 07:02:06.654419
Model ind 665 epoch 492 batch: 700 avg loss -2.651204 avg loss no lamb -2.651204 time 2020-06-26 07:02:17.075500
Model ind 665 epoch 492 batch: 800 avg loss -2.752947 avg loss no lamb -2.752947 time 2020-06-26 07:02:27.886685
last batch sz 10
Pre: time 2020-06-26 07:02:41.772243: 
 	std: 0.003026947
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9819, 0.9748, 0.9817, 0.9766]
	train_accs: [0.98233336, 0.9820333, 0.97611666, 0.98233336, 0.97728336]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97933996
	best: 0.9817

Starting e_i: 493
Model ind 665 epoch 493 batch: 0 avg loss -2.924931 avg loss no lamb -2.924931 time 2020-06-26 07:02:42.630222
Model ind 665 epoch 493 batch: 100 avg loss -2.841743 avg loss no lamb -2.841743 time 2020-06-26 07:02:53.291707
Model ind 665 epoch 493 batch: 200 avg loss -2.803971 avg loss no lamb -2.803971 time 2020-06-26 07:03:04.104112
Model ind 665 epoch 493 batch: 300 avg loss -2.849493 avg loss no lamb -2.849493 time 2020-06-26 07:03:14.643334
Model ind 665 epoch 493 batch: 400 avg loss -2.737921 avg loss no lamb -2.737921 time 2020-06-26 07:03:25.458998
Model ind 665 epoch 493 batch: 500 avg loss -2.865899 avg loss no lamb -2.865899 time 2020-06-26 07:03:36.233175
Model ind 665 epoch 493 batch: 600 avg loss -2.767373 avg loss no lamb -2.767373 time 2020-06-26 07:03:46.973754
Model ind 665 epoch 493 batch: 700 avg loss -2.693067 avg loss no lamb -2.693067 time 2020-06-26 07:03:57.826196
Model ind 665 epoch 493 batch: 800 avg loss -2.808600 avg loss no lamb -2.808600 time 2020-06-26 07:04:08.428126
last batch sz 10
Pre: time 2020-06-26 07:04:22.198539: 
 	std: 0.002994275
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9804, 0.9738, 0.9807, 0.9751]
	train_accs: [0.9808, 0.98071665, 0.9752667, 0.98106664, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97808
	best: 0.9807

Starting e_i: 494
Model ind 665 epoch 494 batch: 0 avg loss -2.891686 avg loss no lamb -2.891686 time 2020-06-26 07:04:23.150355
Model ind 665 epoch 494 batch: 100 avg loss -2.793133 avg loss no lamb -2.793133 time 2020-06-26 07:04:34.094544
Model ind 665 epoch 494 batch: 200 avg loss -2.805685 avg loss no lamb -2.805685 time 2020-06-26 07:04:44.869632
Model ind 665 epoch 494 batch: 300 avg loss -2.839895 avg loss no lamb -2.839895 time 2020-06-26 07:04:55.831685
Model ind 665 epoch 494 batch: 400 avg loss -2.759923 avg loss no lamb -2.759923 time 2020-06-26 07:05:06.581913
Model ind 665 epoch 494 batch: 500 avg loss -2.790377 avg loss no lamb -2.790377 time 2020-06-26 07:05:17.346244
Model ind 665 epoch 494 batch: 600 avg loss -2.813992 avg loss no lamb -2.813992 time 2020-06-26 07:05:28.058046
Model ind 665 epoch 494 batch: 700 avg loss -2.712374 avg loss no lamb -2.712374 time 2020-06-26 07:05:38.691156
Model ind 665 epoch 494 batch: 800 avg loss -2.816343 avg loss no lamb -2.816343 time 2020-06-26 07:05:49.353699
last batch sz 10
Pre: time 2020-06-26 07:06:03.078113: 
 	std: 0.002409969
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9802, 0.9752, 0.9809, 0.9764]
	train_accs: [0.9816167, 0.9810167, 0.9763833, 0.9813667, 0.97725]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.9787
	best: 0.9808

Starting e_i: 495
Model ind 665 epoch 495 batch: 0 avg loss -2.866385 avg loss no lamb -2.866385 time 2020-06-26 07:06:03.956049
Model ind 665 epoch 495 batch: 100 avg loss -2.872684 avg loss no lamb -2.872684 time 2020-06-26 07:06:14.411131
Model ind 665 epoch 495 batch: 200 avg loss -2.840828 avg loss no lamb -2.840828 time 2020-06-26 07:06:25.235979
Model ind 665 epoch 495 batch: 300 avg loss -2.922239 avg loss no lamb -2.922239 time 2020-06-26 07:06:35.988896
Model ind 665 epoch 495 batch: 400 avg loss -2.719431 avg loss no lamb -2.719431 time 2020-06-26 07:06:46.566221
Model ind 665 epoch 495 batch: 500 avg loss -2.796245 avg loss no lamb -2.796245 time 2020-06-26 07:06:56.892711
Model ind 665 epoch 495 batch: 600 avg loss -2.828310 avg loss no lamb -2.828310 time 2020-06-26 07:07:07.992601
Model ind 665 epoch 495 batch: 700 avg loss -2.774205 avg loss no lamb -2.774205 time 2020-06-26 07:07:18.764088
Model ind 665 epoch 495 batch: 800 avg loss -2.845073 avg loss no lamb -2.845073 time 2020-06-26 07:07:29.460380
last batch sz 10
Pre: time 2020-06-26 07:07:43.101573: 
 	std: 0.0028378898
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9812, 0.9755, 0.982, 0.9767]
	train_accs: [0.98181665, 0.98135, 0.97611666, 0.9817333, 0.977]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97951996
	best: 0.9822

Starting e_i: 496
Model ind 665 epoch 496 batch: 0 avg loss -2.892082 avg loss no lamb -2.892082 time 2020-06-26 07:07:43.914079
Model ind 665 epoch 496 batch: 100 avg loss -2.834965 avg loss no lamb -2.834965 time 2020-06-26 07:07:54.536234
Model ind 665 epoch 496 batch: 200 avg loss -2.817529 avg loss no lamb -2.817529 time 2020-06-26 07:08:05.458381
Model ind 665 epoch 496 batch: 300 avg loss -2.798846 avg loss no lamb -2.798846 time 2020-06-26 07:08:16.134571
Model ind 665 epoch 496 batch: 400 avg loss -2.720668 avg loss no lamb -2.720668 time 2020-06-26 07:08:26.952255
Model ind 665 epoch 496 batch: 500 avg loss -2.771098 avg loss no lamb -2.771098 time 2020-06-26 07:08:37.675220
Model ind 665 epoch 496 batch: 600 avg loss -2.781777 avg loss no lamb -2.781777 time 2020-06-26 07:08:48.287350
Model ind 665 epoch 496 batch: 700 avg loss -2.679110 avg loss no lamb -2.679110 time 2020-06-26 07:08:58.866571
Model ind 665 epoch 496 batch: 800 avg loss -2.826555 avg loss no lamb -2.826555 time 2020-06-26 07:09:09.573753
last batch sz 10
Pre: time 2020-06-26 07:09:23.280871: 
 	std: 0.0032358028
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9824, 0.9756, 0.9828, 0.9766]
	train_accs: [0.98186666, 0.9817167, 0.97625, 0.98186666, 0.9770333]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.98003995
	best: 0.9828

Starting e_i: 497
Model ind 665 epoch 497 batch: 0 avg loss -2.929341 avg loss no lamb -2.929341 time 2020-06-26 07:09:24.163620
Model ind 665 epoch 497 batch: 100 avg loss -2.918845 avg loss no lamb -2.918845 time 2020-06-26 07:09:34.771592
Model ind 665 epoch 497 batch: 200 avg loss -2.831057 avg loss no lamb -2.831057 time 2020-06-26 07:09:45.374630
Model ind 665 epoch 497 batch: 300 avg loss -2.820361 avg loss no lamb -2.820361 time 2020-06-26 07:09:56.309289
Model ind 665 epoch 497 batch: 400 avg loss -2.772513 avg loss no lamb -2.772513 time 2020-06-26 07:10:06.975132
Model ind 665 epoch 497 batch: 500 avg loss -2.832614 avg loss no lamb -2.832614 time 2020-06-26 07:10:17.635903
Model ind 665 epoch 497 batch: 600 avg loss -2.823847 avg loss no lamb -2.823847 time 2020-06-26 07:10:28.697809
Model ind 665 epoch 497 batch: 700 avg loss -2.635838 avg loss no lamb -2.635838 time 2020-06-26 07:10:39.473710
Model ind 665 epoch 497 batch: 800 avg loss -2.767232 avg loss no lamb -2.767232 time 2020-06-26 07:10:50.145155
last batch sz 10
Pre: time 2020-06-26 07:11:04.066942: 
 	std: 0.0026502742
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9821, 0.9757, 0.9812, 0.9767]
	train_accs: [0.98178333, 0.98165, 0.9763833, 0.98181665, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.9794
	best: 0.9812

Starting e_i: 498
Model ind 665 epoch 498 batch: 0 avg loss -2.932072 avg loss no lamb -2.932072 time 2020-06-26 07:11:04.929838
Model ind 665 epoch 498 batch: 100 avg loss -2.830492 avg loss no lamb -2.830492 time 2020-06-26 07:11:15.642581
Model ind 665 epoch 498 batch: 200 avg loss -2.777664 avg loss no lamb -2.777664 time 2020-06-26 07:11:26.464195
Model ind 665 epoch 498 batch: 300 avg loss -2.833555 avg loss no lamb -2.833555 time 2020-06-26 07:11:37.037617
Model ind 665 epoch 498 batch: 400 avg loss -2.763078 avg loss no lamb -2.763078 time 2020-06-26 07:11:47.688913
Model ind 665 epoch 498 batch: 500 avg loss -2.843907 avg loss no lamb -2.843907 time 2020-06-26 07:11:58.283838
Model ind 665 epoch 498 batch: 600 avg loss -2.900804 avg loss no lamb -2.900804 time 2020-06-26 07:12:09.142754
Model ind 665 epoch 498 batch: 700 avg loss -2.716390 avg loss no lamb -2.716390 time 2020-06-26 07:12:19.799067
Model ind 665 epoch 498 batch: 800 avg loss -2.806607 avg loss no lamb -2.806607 time 2020-06-26 07:12:30.640996
last batch sz 10
Pre: time 2020-06-26 07:12:44.561535: 
 	std: 0.0025353367
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.981, 0.9757, 0.9812, 0.9761]
	train_accs: [0.98191667, 0.9815, 0.977, 0.98211664, 0.9772]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.979
	best: 0.9812

Starting e_i: 499
Model ind 665 epoch 499 batch: 0 avg loss -2.891903 avg loss no lamb -2.891903 time 2020-06-26 07:12:45.477780
Model ind 665 epoch 499 batch: 100 avg loss -2.868778 avg loss no lamb -2.868778 time 2020-06-26 07:12:55.792122
Model ind 665 epoch 499 batch: 200 avg loss -2.870145 avg loss no lamb -2.870145 time 2020-06-26 07:13:06.626136
Model ind 665 epoch 499 batch: 300 avg loss -2.850698 avg loss no lamb -2.850698 time 2020-06-26 07:13:17.824078
Model ind 665 epoch 499 batch: 400 avg loss -2.732180 avg loss no lamb -2.732180 time 2020-06-26 07:13:28.600367
Model ind 665 epoch 499 batch: 500 avg loss -2.781990 avg loss no lamb -2.781990 time 2020-06-26 07:13:39.459236
Model ind 665 epoch 499 batch: 600 avg loss -2.777916 avg loss no lamb -2.777916 time 2020-06-26 07:13:50.139122
Model ind 665 epoch 499 batch: 700 avg loss -2.747010 avg loss no lamb -2.747010 time 2020-06-26 07:14:01.048981
Model ind 665 epoch 499 batch: 800 avg loss -2.799300 avg loss no lamb -2.799300 time 2020-06-26 07:14:11.964087
last batch sz 10
Pre: time 2020-06-26 07:14:25.844555: 
 	std: 0.0028210678
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9798, 0.9738, 0.9804, 0.9751]
	train_accs: [0.9816333, 0.98075, 0.9754, 0.98151666, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97786
	best: 0.9802

Starting e_i: 500
Model ind 665 epoch 500 batch: 0 avg loss -2.903202 avg loss no lamb -2.903202 time 2020-06-26 07:14:26.716795
Model ind 665 epoch 500 batch: 100 avg loss -2.825268 avg loss no lamb -2.825268 time 2020-06-26 07:14:37.339767
Model ind 665 epoch 500 batch: 200 avg loss -2.778793 avg loss no lamb -2.778793 time 2020-06-26 07:14:47.900919
Model ind 665 epoch 500 batch: 300 avg loss -2.832922 avg loss no lamb -2.832922 time 2020-06-26 07:14:58.654448
Model ind 665 epoch 500 batch: 400 avg loss -2.709783 avg loss no lamb -2.709783 time 2020-06-26 07:15:09.550199
Model ind 665 epoch 500 batch: 500 avg loss -2.810771 avg loss no lamb -2.810771 time 2020-06-26 07:15:20.229325
Model ind 665 epoch 500 batch: 600 avg loss -2.788744 avg loss no lamb -2.788744 time 2020-06-26 07:15:30.787420
Model ind 665 epoch 500 batch: 700 avg loss -2.696385 avg loss no lamb -2.696385 time 2020-06-26 07:15:41.633763
Model ind 665 epoch 500 batch: 800 avg loss -2.820571 avg loss no lamb -2.820571 time 2020-06-26 07:15:52.166274
last batch sz 10
Pre: time 2020-06-26 07:16:06.474320: 
 	std: 0.003155944
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9801, 0.974, 0.9809, 0.9743]
	train_accs: [0.98123336, 0.98045, 0.97546667, 0.98143333, 0.9762667]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.978
	best: 0.9809

Starting e_i: 501
Model ind 665 epoch 501 batch: 0 avg loss -2.927724 avg loss no lamb -2.927724 time 2020-06-26 07:16:08.459576
Model ind 665 epoch 501 batch: 100 avg loss -2.863470 avg loss no lamb -2.863470 time 2020-06-26 07:16:19.175375
Model ind 665 epoch 501 batch: 200 avg loss -2.815013 avg loss no lamb -2.815013 time 2020-06-26 07:16:29.782994
Model ind 665 epoch 501 batch: 300 avg loss -2.818167 avg loss no lamb -2.818167 time 2020-06-26 07:16:40.530273
Model ind 665 epoch 501 batch: 400 avg loss -2.774569 avg loss no lamb -2.774569 time 2020-06-26 07:16:51.261902
Model ind 665 epoch 501 batch: 500 avg loss -2.712548 avg loss no lamb -2.712548 time 2020-06-26 07:17:02.062381
Model ind 665 epoch 501 batch: 600 avg loss -2.801585 avg loss no lamb -2.801585 time 2020-06-26 07:17:12.669373
Model ind 665 epoch 501 batch: 700 avg loss -2.666319 avg loss no lamb -2.666319 time 2020-06-26 07:17:23.157868
Model ind 665 epoch 501 batch: 800 avg loss -2.834237 avg loss no lamb -2.834237 time 2020-06-26 07:17:34.029523
last batch sz 10
Pre: time 2020-06-26 07:17:48.049107: 
 	std: 0.0031805702
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.981, 0.9745, 0.9822, 0.9763]
	train_accs: [0.98233336, 0.98146665, 0.97565, 0.98235, 0.9771]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.9792
	best: 0.9822

Starting e_i: 502
Model ind 665 epoch 502 batch: 0 avg loss -2.844187 avg loss no lamb -2.844187 time 2020-06-26 07:17:48.923368
Model ind 665 epoch 502 batch: 100 avg loss -2.867144 avg loss no lamb -2.867144 time 2020-06-26 07:17:59.622508
Model ind 665 epoch 502 batch: 200 avg loss -2.793372 avg loss no lamb -2.793372 time 2020-06-26 07:18:10.124834
Model ind 665 epoch 502 batch: 300 avg loss -2.860916 avg loss no lamb -2.860916 time 2020-06-26 07:18:21.040867
Model ind 665 epoch 502 batch: 400 avg loss -2.780396 avg loss no lamb -2.780396 time 2020-06-26 07:18:31.793251
Model ind 665 epoch 502 batch: 500 avg loss -2.761298 avg loss no lamb -2.761298 time 2020-06-26 07:18:42.554570
Model ind 665 epoch 502 batch: 600 avg loss -2.823537 avg loss no lamb -2.823537 time 2020-06-26 07:18:53.060866
Model ind 665 epoch 502 batch: 700 avg loss -2.675349 avg loss no lamb -2.675349 time 2020-06-26 07:19:03.734718
Model ind 665 epoch 502 batch: 800 avg loss -2.796471 avg loss no lamb -2.796471 time 2020-06-26 07:19:14.351048
last batch sz 10
Pre: time 2020-06-26 07:19:28.343075: 
 	std: 0.0025765111
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9816, 0.9759, 0.9816, 0.9772]
	train_accs: [0.9828167, 0.9820833, 0.97765, 0.98255, 0.97816664]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.9796599
	best: 0.982

Starting e_i: 503
Model ind 665 epoch 503 batch: 0 avg loss -2.873301 avg loss no lamb -2.873301 time 2020-06-26 07:19:29.240302
Model ind 665 epoch 503 batch: 100 avg loss -2.821525 avg loss no lamb -2.821525 time 2020-06-26 07:19:39.883024
Model ind 665 epoch 503 batch: 200 avg loss -2.764054 avg loss no lamb -2.764054 time 2020-06-26 07:19:50.698145
Model ind 665 epoch 503 batch: 300 avg loss -2.787670 avg loss no lamb -2.787670 time 2020-06-26 07:20:01.604739
Model ind 665 epoch 503 batch: 400 avg loss -2.738287 avg loss no lamb -2.738287 time 2020-06-26 07:20:12.277359
Model ind 665 epoch 503 batch: 500 avg loss -2.791011 avg loss no lamb -2.791011 time 2020-06-26 07:20:22.920463
Model ind 665 epoch 503 batch: 600 avg loss -2.802766 avg loss no lamb -2.802766 time 2020-06-26 07:20:33.754261
Model ind 665 epoch 503 batch: 700 avg loss -2.724007 avg loss no lamb -2.724007 time 2020-06-26 07:20:44.490349
Model ind 665 epoch 503 batch: 800 avg loss -2.821650 avg loss no lamb -2.821650 time 2020-06-26 07:20:55.407503
last batch sz 10
Pre: time 2020-06-26 07:21:09.322139: 
 	std: 0.0028124012
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9799, 0.9741, 0.9811, 0.976]
	train_accs: [0.98178333, 0.98088336, 0.97583336, 0.9816667, 0.97695]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97838
	best: 0.9808

Starting e_i: 504
Model ind 665 epoch 504 batch: 0 avg loss -2.862579 avg loss no lamb -2.862579 time 2020-06-26 07:21:10.287877
Model ind 665 epoch 504 batch: 100 avg loss -2.827885 avg loss no lamb -2.827885 time 2020-06-26 07:21:20.931736
Model ind 665 epoch 504 batch: 200 avg loss -2.812222 avg loss no lamb -2.812222 time 2020-06-26 07:21:31.792994
Model ind 665 epoch 504 batch: 300 avg loss -2.767921 avg loss no lamb -2.767921 time 2020-06-26 07:21:42.770006
Model ind 665 epoch 504 batch: 400 avg loss -2.792603 avg loss no lamb -2.792603 time 2020-06-26 07:21:53.552055
Model ind 665 epoch 504 batch: 500 avg loss -2.789839 avg loss no lamb -2.789839 time 2020-06-26 07:22:04.374636
Model ind 665 epoch 504 batch: 600 avg loss -2.770672 avg loss no lamb -2.770672 time 2020-06-26 07:22:14.915459
Model ind 665 epoch 504 batch: 700 avg loss -2.692395 avg loss no lamb -2.692395 time 2020-06-26 07:22:25.798976
Model ind 665 epoch 504 batch: 800 avg loss -2.805774 avg loss no lamb -2.805774 time 2020-06-26 07:22:36.682813
last batch sz 10
Pre: time 2020-06-26 07:22:50.648225: 
 	std: 0.0033944615
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9825, 0.9749, 0.9827, 0.9768]
	train_accs: [0.9823833, 0.9821, 0.97608334, 0.98245, 0.97705]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97994006
	best: 0.9827

Starting e_i: 505
Model ind 665 epoch 505 batch: 0 avg loss -2.892135 avg loss no lamb -2.892135 time 2020-06-26 07:22:51.461117
Model ind 665 epoch 505 batch: 100 avg loss -2.872691 avg loss no lamb -2.872691 time 2020-06-26 07:23:02.116679
Model ind 665 epoch 505 batch: 200 avg loss -2.887862 avg loss no lamb -2.887862 time 2020-06-26 07:23:12.771892
Model ind 665 epoch 505 batch: 300 avg loss -2.890042 avg loss no lamb -2.890042 time 2020-06-26 07:23:23.523367
Model ind 665 epoch 505 batch: 400 avg loss -2.745397 avg loss no lamb -2.745397 time 2020-06-26 07:23:34.394356
Model ind 665 epoch 505 batch: 500 avg loss -2.815007 avg loss no lamb -2.815007 time 2020-06-26 07:23:45.171566
Model ind 665 epoch 505 batch: 600 avg loss -2.848291 avg loss no lamb -2.848291 time 2020-06-26 07:23:55.901285
Model ind 665 epoch 505 batch: 700 avg loss -2.741370 avg loss no lamb -2.741370 time 2020-06-26 07:24:06.543286
Model ind 665 epoch 505 batch: 800 avg loss -2.802844 avg loss no lamb -2.802844 time 2020-06-26 07:24:17.293792
last batch sz 10
Pre: time 2020-06-26 07:24:31.186151: 
 	std: 0.0025879845
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9796, 0.9748, 0.9809, 0.9754]
	train_accs: [0.98121667, 0.98055, 0.9757, 0.98165, 0.9763833]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97822
	best: 0.9809

Starting e_i: 506
Model ind 665 epoch 506 batch: 0 avg loss -2.931155 avg loss no lamb -2.931155 time 2020-06-26 07:24:32.027081
Model ind 665 epoch 506 batch: 100 avg loss -2.821583 avg loss no lamb -2.821583 time 2020-06-26 07:24:42.748387
Model ind 665 epoch 506 batch: 200 avg loss -2.839294 avg loss no lamb -2.839294 time 2020-06-26 07:24:53.580881
Model ind 665 epoch 506 batch: 300 avg loss -2.872482 avg loss no lamb -2.872482 time 2020-06-26 07:25:04.364434
Model ind 665 epoch 506 batch: 400 avg loss -2.752064 avg loss no lamb -2.752064 time 2020-06-26 07:25:15.051669
Model ind 665 epoch 506 batch: 500 avg loss -2.820600 avg loss no lamb -2.820600 time 2020-06-26 07:25:25.683413
Model ind 665 epoch 506 batch: 600 avg loss -2.807893 avg loss no lamb -2.807893 time 2020-06-26 07:25:36.752595
Model ind 665 epoch 506 batch: 700 avg loss -2.606760 avg loss no lamb -2.606760 time 2020-06-26 07:25:47.549897
Model ind 665 epoch 506 batch: 800 avg loss -2.824543 avg loss no lamb -2.824543 time 2020-06-26 07:25:58.373615
last batch sz 10
Pre: time 2020-06-26 07:26:12.235157: 
 	std: 0.002998279
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.98, 0.9737, 0.9806, 0.9751]
	train_accs: [0.9817167, 0.9813833, 0.9759167, 0.9819667, 0.97665]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97802
	best: 0.9806

Starting e_i: 507
Model ind 665 epoch 507 batch: 0 avg loss -2.885782 avg loss no lamb -2.885782 time 2020-06-26 07:26:13.124767
Model ind 665 epoch 507 batch: 100 avg loss -2.893507 avg loss no lamb -2.893507 time 2020-06-26 07:26:23.622040
Model ind 665 epoch 507 batch: 200 avg loss -2.867486 avg loss no lamb -2.867486 time 2020-06-26 07:26:34.219781
Model ind 665 epoch 507 batch: 300 avg loss -2.810957 avg loss no lamb -2.810957 time 2020-06-26 07:26:44.763105
Model ind 665 epoch 507 batch: 400 avg loss -2.743927 avg loss no lamb -2.743927 time 2020-06-26 07:26:55.394926
Model ind 665 epoch 507 batch: 500 avg loss -2.809585 avg loss no lamb -2.809585 time 2020-06-26 07:27:06.317916
Model ind 665 epoch 507 batch: 600 avg loss -2.821255 avg loss no lamb -2.821255 time 2020-06-26 07:27:17.041831
Model ind 665 epoch 507 batch: 700 avg loss -2.706934 avg loss no lamb -2.706934 time 2020-06-26 07:27:27.838697
Model ind 665 epoch 507 batch: 800 avg loss -2.736710 avg loss no lamb -2.736710 time 2020-06-26 07:27:38.774368
last batch sz 10
Pre: time 2020-06-26 07:27:53.033730: 
 	std: 0.0034633027
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.98, 0.9729, 0.9805, 0.9742]
	train_accs: [0.9810167, 0.9806833, 0.9745833, 0.9806, 0.97508335]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97774
	best: 0.9811

Starting e_i: 508
Model ind 665 epoch 508 batch: 0 avg loss -2.917733 avg loss no lamb -2.917733 time 2020-06-26 07:27:53.917059
Model ind 665 epoch 508 batch: 100 avg loss -2.865712 avg loss no lamb -2.865712 time 2020-06-26 07:28:04.596701
Model ind 665 epoch 508 batch: 200 avg loss -2.841092 avg loss no lamb -2.841092 time 2020-06-26 07:28:15.168379
Model ind 665 epoch 508 batch: 300 avg loss -2.792768 avg loss no lamb -2.792768 time 2020-06-26 07:28:25.823790
Model ind 665 epoch 508 batch: 400 avg loss -2.781524 avg loss no lamb -2.781524 time 2020-06-26 07:28:36.543593
Model ind 665 epoch 508 batch: 500 avg loss -2.840986 avg loss no lamb -2.840986 time 2020-06-26 07:28:47.190110
Model ind 665 epoch 508 batch: 600 avg loss -2.801687 avg loss no lamb -2.801687 time 2020-06-26 07:28:58.032933
Model ind 665 epoch 508 batch: 700 avg loss -2.704447 avg loss no lamb -2.704447 time 2020-06-26 07:29:08.581286
Model ind 665 epoch 508 batch: 800 avg loss -2.787559 avg loss no lamb -2.787559 time 2020-06-26 07:29:19.464228
last batch sz 10
Pre: time 2020-06-26 07:29:33.272719: 
 	std: 0.0026810493
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9804, 0.9749, 0.9812, 0.9758]
	train_accs: [0.98141664, 0.98071665, 0.9759833, 0.98145, 0.9765667]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.9786
	best: 0.9812

Starting e_i: 509
Model ind 665 epoch 509 batch: 0 avg loss -2.934488 avg loss no lamb -2.934488 time 2020-06-26 07:29:34.231511
Model ind 665 epoch 509 batch: 100 avg loss -2.862098 avg loss no lamb -2.862098 time 2020-06-26 07:29:44.731863
Model ind 665 epoch 509 batch: 200 avg loss -2.791261 avg loss no lamb -2.791261 time 2020-06-26 07:29:55.307267
Model ind 665 epoch 509 batch: 300 avg loss -2.874426 avg loss no lamb -2.874426 time 2020-06-26 07:30:06.050498
Model ind 665 epoch 509 batch: 400 avg loss -2.668723 avg loss no lamb -2.668723 time 2020-06-26 07:30:16.765454
Model ind 665 epoch 509 batch: 500 avg loss -2.835882 avg loss no lamb -2.835882 time 2020-06-26 07:30:27.398024
Model ind 665 epoch 509 batch: 600 avg loss -2.807810 avg loss no lamb -2.807810 time 2020-06-26 07:30:38.120641
Model ind 665 epoch 509 batch: 700 avg loss -2.671001 avg loss no lamb -2.671001 time 2020-06-26 07:30:48.747475
Model ind 665 epoch 509 batch: 800 avg loss -2.785311 avg loss no lamb -2.785311 time 2020-06-26 07:30:59.507733
last batch sz 10
Pre: time 2020-06-26 07:31:13.512532: 
 	std: 0.003625689
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9807, 0.9727, 0.9819, 0.9755]
	train_accs: [0.9816667, 0.98106664, 0.97478336, 0.98188335, 0.9759]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97838
	best: 0.9819

Starting e_i: 510
Model ind 665 epoch 510 batch: 0 avg loss -2.957047 avg loss no lamb -2.957047 time 2020-06-26 07:31:14.371467
Model ind 665 epoch 510 batch: 100 avg loss -2.847067 avg loss no lamb -2.847067 time 2020-06-26 07:31:25.134984
Model ind 665 epoch 510 batch: 200 avg loss -2.785260 avg loss no lamb -2.785260 time 2020-06-26 07:31:35.938252
Model ind 665 epoch 510 batch: 300 avg loss -2.860538 avg loss no lamb -2.860538 time 2020-06-26 07:31:46.588614
Model ind 665 epoch 510 batch: 400 avg loss -2.768542 avg loss no lamb -2.768542 time 2020-06-26 07:31:57.243251
Model ind 665 epoch 510 batch: 500 avg loss -2.812924 avg loss no lamb -2.812924 time 2020-06-26 07:32:08.134999
Model ind 665 epoch 510 batch: 600 avg loss -2.846864 avg loss no lamb -2.846864 time 2020-06-26 07:32:18.942700
Model ind 665 epoch 510 batch: 700 avg loss -2.631598 avg loss no lamb -2.631598 time 2020-06-26 07:32:29.591878
Model ind 665 epoch 510 batch: 800 avg loss -2.843604 avg loss no lamb -2.843604 time 2020-06-26 07:32:40.234492
last batch sz 10
Pre: time 2020-06-26 07:32:54.132051: 
 	std: 0.002840145
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9797, 0.9738, 0.9804, 0.9753]
	train_accs: [0.98105, 0.9807, 0.9759833, 0.9812833, 0.9766]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97796
	best: 0.9804

Starting e_i: 511
Model ind 665 epoch 511 batch: 0 avg loss -2.967098 avg loss no lamb -2.967098 time 2020-06-26 07:32:56.123932
Model ind 665 epoch 511 batch: 100 avg loss -2.870688 avg loss no lamb -2.870688 time 2020-06-26 07:33:06.766933
Model ind 665 epoch 511 batch: 200 avg loss -2.848914 avg loss no lamb -2.848914 time 2020-06-26 07:33:17.465519
Model ind 665 epoch 511 batch: 300 avg loss -2.815603 avg loss no lamb -2.815603 time 2020-06-26 07:33:28.078863
Model ind 665 epoch 511 batch: 400 avg loss -2.757120 avg loss no lamb -2.757120 time 2020-06-26 07:33:38.501600
Model ind 665 epoch 511 batch: 500 avg loss -2.770967 avg loss no lamb -2.770967 time 2020-06-26 07:33:49.279626
Model ind 665 epoch 511 batch: 600 avg loss -2.816718 avg loss no lamb -2.816718 time 2020-06-26 07:34:00.186658
Model ind 665 epoch 511 batch: 700 avg loss -2.676939 avg loss no lamb -2.676939 time 2020-06-26 07:34:10.887517
Model ind 665 epoch 511 batch: 800 avg loss -2.815788 avg loss no lamb -2.815788 time 2020-06-26 07:34:21.556796
last batch sz 10
Pre: time 2020-06-26 07:34:35.032691: 
 	std: 0.0025004006
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.981, 0.9759, 0.9812, 0.9764]
	train_accs: [0.98156667, 0.9810167, 0.97676665, 0.9816833, 0.97761667]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.9792
	best: 0.9812

Starting e_i: 512
Model ind 665 epoch 512 batch: 0 avg loss -2.890883 avg loss no lamb -2.890883 time 2020-06-26 07:34:35.892717
Model ind 665 epoch 512 batch: 100 avg loss -2.837798 avg loss no lamb -2.837798 time 2020-06-26 07:34:46.789703
Model ind 665 epoch 512 batch: 200 avg loss -2.790596 avg loss no lamb -2.790596 time 2020-06-26 07:34:57.662540
Model ind 665 epoch 512 batch: 300 avg loss -2.851238 avg loss no lamb -2.851238 time 2020-06-26 07:35:08.559045
Model ind 665 epoch 512 batch: 400 avg loss -2.765732 avg loss no lamb -2.765732 time 2020-06-26 07:35:19.211314
Model ind 665 epoch 512 batch: 500 avg loss -2.793971 avg loss no lamb -2.793971 time 2020-06-26 07:35:29.892438
Model ind 665 epoch 512 batch: 600 avg loss -2.869451 avg loss no lamb -2.869451 time 2020-06-26 07:35:40.692804
Model ind 665 epoch 512 batch: 700 avg loss -2.698667 avg loss no lamb -2.698667 time 2020-06-26 07:35:51.538899
Model ind 665 epoch 512 batch: 800 avg loss -2.787965 avg loss no lamb -2.787965 time 2020-06-26 07:36:02.311607
last batch sz 10
Pre: time 2020-06-26 07:36:16.397215: 
 	std: 0.003024313
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9786, 0.9734, 0.9804, 0.9741]
	train_accs: [0.98113334, 0.9798833, 0.97583336, 0.9813, 0.97615]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97736007
	best: 0.9804

Starting e_i: 513
Model ind 665 epoch 513 batch: 0 avg loss -2.961504 avg loss no lamb -2.961504 time 2020-06-26 07:36:17.213217
Model ind 665 epoch 513 batch: 100 avg loss -2.853788 avg loss no lamb -2.853788 time 2020-06-26 07:36:28.043855
Model ind 665 epoch 513 batch: 200 avg loss -2.790456 avg loss no lamb -2.790456 time 2020-06-26 07:36:38.660460
Model ind 665 epoch 513 batch: 300 avg loss -2.862496 avg loss no lamb -2.862496 time 2020-06-26 07:36:49.394837
Model ind 665 epoch 513 batch: 400 avg loss -2.721396 avg loss no lamb -2.721396 time 2020-06-26 07:37:00.202452
Model ind 665 epoch 513 batch: 500 avg loss -2.869502 avg loss no lamb -2.869502 time 2020-06-26 07:37:10.773179
Model ind 665 epoch 513 batch: 600 avg loss -2.794418 avg loss no lamb -2.794418 time 2020-06-26 07:37:21.456108
Model ind 665 epoch 513 batch: 700 avg loss -2.671595 avg loss no lamb -2.671595 time 2020-06-26 07:37:32.186367
Model ind 665 epoch 513 batch: 800 avg loss -2.848358 avg loss no lamb -2.848358 time 2020-06-26 07:37:42.953857
last batch sz 10
Pre: time 2020-06-26 07:37:56.703802: 
 	std: 0.0028498361
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9814, 0.976, 0.982, 0.976]
	train_accs: [0.98185, 0.98116666, 0.9764, 0.98185, 0.97713333]
	best_train_sub_head: 0
	worst: 0.976
	avg: 0.97947997
	best: 0.982

Starting e_i: 514
Model ind 665 epoch 514 batch: 0 avg loss -2.920728 avg loss no lamb -2.920728 time 2020-06-26 07:37:57.607133
Model ind 665 epoch 514 batch: 100 avg loss -2.846486 avg loss no lamb -2.846486 time 2020-06-26 07:38:08.301028
Model ind 665 epoch 514 batch: 200 avg loss -2.780792 avg loss no lamb -2.780792 time 2020-06-26 07:38:18.900064
Model ind 665 epoch 514 batch: 300 avg loss -2.831672 avg loss no lamb -2.831672 time 2020-06-26 07:38:29.547818
Model ind 665 epoch 514 batch: 400 avg loss -2.756793 avg loss no lamb -2.756793 time 2020-06-26 07:38:40.180456
Model ind 665 epoch 514 batch: 500 avg loss -2.803111 avg loss no lamb -2.803111 time 2020-06-26 07:38:50.879786
Model ind 665 epoch 514 batch: 600 avg loss -2.855853 avg loss no lamb -2.855853 time 2020-06-26 07:39:01.632249
Model ind 665 epoch 514 batch: 700 avg loss -2.667896 avg loss no lamb -2.667896 time 2020-06-26 07:39:12.480969
Model ind 665 epoch 514 batch: 800 avg loss -2.830658 avg loss no lamb -2.830658 time 2020-06-26 07:39:23.203694
last batch sz 10
Pre: time 2020-06-26 07:39:37.067762: 
 	std: 0.0022005483
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9801, 0.9762, 0.9811, 0.9764]
	train_accs: [0.98165, 0.98048335, 0.9770167, 0.98178333, 0.97728336]
	best_train_sub_head: 3
	worst: 0.9762
	avg: 0.97896004
	best: 0.9811

Starting e_i: 515
Model ind 665 epoch 515 batch: 0 avg loss -2.931809 avg loss no lamb -2.931809 time 2020-06-26 07:39:37.886288
Model ind 665 epoch 515 batch: 100 avg loss -2.861894 avg loss no lamb -2.861894 time 2020-06-26 07:39:48.171230
Model ind 665 epoch 515 batch: 200 avg loss -2.874341 avg loss no lamb -2.874341 time 2020-06-26 07:39:58.886933
Model ind 665 epoch 515 batch: 300 avg loss -2.878512 avg loss no lamb -2.878512 time 2020-06-26 07:40:10.044214
Model ind 665 epoch 515 batch: 400 avg loss -2.672500 avg loss no lamb -2.672500 time 2020-06-26 07:40:20.827133
Model ind 665 epoch 515 batch: 500 avg loss -2.764278 avg loss no lamb -2.764278 time 2020-06-26 07:40:31.607313
Model ind 665 epoch 515 batch: 600 avg loss -2.845338 avg loss no lamb -2.845338 time 2020-06-26 07:40:42.267710
Model ind 665 epoch 515 batch: 700 avg loss -2.696247 avg loss no lamb -2.696247 time 2020-06-26 07:40:52.867195
Model ind 665 epoch 515 batch: 800 avg loss -2.810737 avg loss no lamb -2.810737 time 2020-06-26 07:41:03.408080
last batch sz 10
Pre: time 2020-06-26 07:41:17.546114: 
 	std: 0.002753484
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9804, 0.9741, 0.9807, 0.9762]
	train_accs: [0.9816167, 0.98071665, 0.97568333, 0.9816667, 0.97715]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.9784201
	best: 0.9807

Starting e_i: 516
Model ind 665 epoch 516 batch: 0 avg loss -2.926671 avg loss no lamb -2.926671 time 2020-06-26 07:41:18.430336
Model ind 665 epoch 516 batch: 100 avg loss -2.825341 avg loss no lamb -2.825341 time 2020-06-26 07:41:29.050757
Model ind 665 epoch 516 batch: 200 avg loss -2.847353 avg loss no lamb -2.847353 time 2020-06-26 07:41:39.549132
Model ind 665 epoch 516 batch: 300 avg loss -2.853252 avg loss no lamb -2.853252 time 2020-06-26 07:41:50.261531
Model ind 665 epoch 516 batch: 400 avg loss -2.769552 avg loss no lamb -2.769552 time 2020-06-26 07:42:00.907967
Model ind 665 epoch 516 batch: 500 avg loss -2.797700 avg loss no lamb -2.797700 time 2020-06-26 07:42:11.624728
Model ind 665 epoch 516 batch: 600 avg loss -2.844820 avg loss no lamb -2.844820 time 2020-06-26 07:42:22.229462
Model ind 665 epoch 516 batch: 700 avg loss -2.643941 avg loss no lamb -2.643941 time 2020-06-26 07:42:32.666457
Model ind 665 epoch 516 batch: 800 avg loss -2.847005 avg loss no lamb -2.847005 time 2020-06-26 07:42:43.142410
last batch sz 10
Pre: time 2020-06-26 07:42:56.912196: 
 	std: 0.0028192066
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9798, 0.9731, 0.9798, 0.9757]
	train_accs: [0.98108333, 0.98045, 0.97525, 0.98083335, 0.9762167]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97769994
	best: 0.9801

Starting e_i: 517
Model ind 665 epoch 517 batch: 0 avg loss -2.940013 avg loss no lamb -2.940013 time 2020-06-26 07:42:57.782918
Model ind 665 epoch 517 batch: 100 avg loss -2.826579 avg loss no lamb -2.826579 time 2020-06-26 07:43:08.550627
Model ind 665 epoch 517 batch: 200 avg loss -2.801636 avg loss no lamb -2.801636 time 2020-06-26 07:43:19.254953
Model ind 665 epoch 517 batch: 300 avg loss -2.801972 avg loss no lamb -2.801972 time 2020-06-26 07:43:29.915721
Model ind 665 epoch 517 batch: 400 avg loss -2.808206 avg loss no lamb -2.808206 time 2020-06-26 07:43:40.637605
Model ind 665 epoch 517 batch: 500 avg loss -2.767298 avg loss no lamb -2.767298 time 2020-06-26 07:43:51.270356
Model ind 665 epoch 517 batch: 600 avg loss -2.856126 avg loss no lamb -2.856126 time 2020-06-26 07:44:02.127980
Model ind 665 epoch 517 batch: 700 avg loss -2.771638 avg loss no lamb -2.771638 time 2020-06-26 07:44:12.832340
Model ind 665 epoch 517 batch: 800 avg loss -2.843636 avg loss no lamb -2.843636 time 2020-06-26 07:44:23.407886
last batch sz 10
Pre: time 2020-06-26 07:44:37.222328: 
 	std: 0.0031410689
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9787, 0.9724, 0.9798, 0.9742]
	train_accs: [0.98071665, 0.9792167, 0.9748, 0.98048335, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97704
	best: 0.9801

Starting e_i: 518
Model ind 665 epoch 518 batch: 0 avg loss -2.950229 avg loss no lamb -2.950229 time 2020-06-26 07:44:38.092973
Model ind 665 epoch 518 batch: 100 avg loss -2.793982 avg loss no lamb -2.793982 time 2020-06-26 07:44:49.097132
Model ind 665 epoch 518 batch: 200 avg loss -2.846991 avg loss no lamb -2.846991 time 2020-06-26 07:44:59.676515
Model ind 665 epoch 518 batch: 300 avg loss -2.800678 avg loss no lamb -2.800678 time 2020-06-26 07:45:10.423198
Model ind 665 epoch 518 batch: 400 avg loss -2.790994 avg loss no lamb -2.790994 time 2020-06-26 07:45:20.958454
Model ind 665 epoch 518 batch: 500 avg loss -2.763293 avg loss no lamb -2.763293 time 2020-06-26 07:45:31.793075
Model ind 665 epoch 518 batch: 600 avg loss -2.833410 avg loss no lamb -2.833410 time 2020-06-26 07:45:42.473754
Model ind 665 epoch 518 batch: 700 avg loss -2.707760 avg loss no lamb -2.707760 time 2020-06-26 07:45:53.429781
Model ind 665 epoch 518 batch: 800 avg loss -2.764958 avg loss no lamb -2.764958 time 2020-06-26 07:46:04.104605
last batch sz 10
Pre: time 2020-06-26 07:46:17.972161: 
 	std: 0.002868731
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9792, 0.9727, 0.9796, 0.9751]
	train_accs: [0.9806167, 0.9802833, 0.97473335, 0.9805667, 0.97611666]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97727996
	best: 0.9798

Starting e_i: 519
Model ind 665 epoch 519 batch: 0 avg loss -2.927554 avg loss no lamb -2.927554 time 2020-06-26 07:46:18.905349
Model ind 665 epoch 519 batch: 100 avg loss -2.800741 avg loss no lamb -2.800741 time 2020-06-26 07:46:29.917461
Model ind 665 epoch 519 batch: 200 avg loss -2.830153 avg loss no lamb -2.830153 time 2020-06-26 07:46:40.657597
Model ind 665 epoch 519 batch: 300 avg loss -2.852486 avg loss no lamb -2.852486 time 2020-06-26 07:46:51.520439
Model ind 665 epoch 519 batch: 400 avg loss -2.671438 avg loss no lamb -2.671438 time 2020-06-26 07:47:02.388879
Model ind 665 epoch 519 batch: 500 avg loss -2.782707 avg loss no lamb -2.782707 time 2020-06-26 07:47:13.509341
Model ind 665 epoch 519 batch: 600 avg loss -2.884699 avg loss no lamb -2.884699 time 2020-06-26 07:47:24.286472
Model ind 665 epoch 519 batch: 700 avg loss -2.771222 avg loss no lamb -2.771222 time 2020-06-26 07:47:35.058365
Model ind 665 epoch 519 batch: 800 avg loss -2.802531 avg loss no lamb -2.802531 time 2020-06-26 07:47:45.876868
last batch sz 10
Pre: time 2020-06-26 07:47:59.580741: 
 	std: 0.0027578317
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9793, 0.9737, 0.9806, 0.9758]
	train_accs: [0.9812, 0.98025, 0.97536665, 0.98065, 0.9766]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97798
	best: 0.9805

Starting e_i: 520
Model ind 665 epoch 520 batch: 0 avg loss -2.890616 avg loss no lamb -2.890616 time 2020-06-26 07:48:00.489124
Model ind 665 epoch 520 batch: 100 avg loss -2.907037 avg loss no lamb -2.907037 time 2020-06-26 07:48:11.181969
Model ind 665 epoch 520 batch: 200 avg loss -2.861911 avg loss no lamb -2.861911 time 2020-06-26 07:48:21.765037
Model ind 665 epoch 520 batch: 300 avg loss -2.727682 avg loss no lamb -2.727682 time 2020-06-26 07:48:32.308403
Model ind 665 epoch 520 batch: 400 avg loss -2.727098 avg loss no lamb -2.727098 time 2020-06-26 07:48:42.968974
Model ind 665 epoch 520 batch: 500 avg loss -2.802729 avg loss no lamb -2.802729 time 2020-06-26 07:48:53.622088
Model ind 665 epoch 520 batch: 600 avg loss -2.879765 avg loss no lamb -2.879765 time 2020-06-26 07:49:04.249800
Model ind 665 epoch 520 batch: 700 avg loss -2.718858 avg loss no lamb -2.718858 time 2020-06-26 07:49:14.840028
Model ind 665 epoch 520 batch: 800 avg loss -2.805734 avg loss no lamb -2.805734 time 2020-06-26 07:49:25.667953
last batch sz 10
Pre: time 2020-06-26 07:49:39.657623: 
 	std: 0.00286006
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9802, 0.9743, 0.981, 0.9756]
	train_accs: [0.98158336, 0.981, 0.9757, 0.98146665, 0.97616667]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97840005
	best: 0.9809

Starting e_i: 521
Model ind 665 epoch 521 batch: 0 avg loss -2.919662 avg loss no lamb -2.919662 time 2020-06-26 07:49:41.626850
Model ind 665 epoch 521 batch: 100 avg loss -2.858561 avg loss no lamb -2.858561 time 2020-06-26 07:49:52.604186
Model ind 665 epoch 521 batch: 200 avg loss -2.855227 avg loss no lamb -2.855227 time 2020-06-26 07:50:03.337435
Model ind 665 epoch 521 batch: 300 avg loss -2.845475 avg loss no lamb -2.845475 time 2020-06-26 07:50:13.995889
Model ind 665 epoch 521 batch: 400 avg loss -2.773043 avg loss no lamb -2.773043 time 2020-06-26 07:50:24.485636
Model ind 665 epoch 521 batch: 500 avg loss -2.754680 avg loss no lamb -2.754680 time 2020-06-26 07:50:35.271287
Model ind 665 epoch 521 batch: 600 avg loss -2.836376 avg loss no lamb -2.836376 time 2020-06-26 07:50:45.790760
Model ind 665 epoch 521 batch: 700 avg loss -2.680686 avg loss no lamb -2.680686 time 2020-06-26 07:50:56.586154
Model ind 665 epoch 521 batch: 800 avg loss -2.849400 avg loss no lamb -2.849400 time 2020-06-26 07:51:07.377753
last batch sz 10
Pre: time 2020-06-26 07:51:21.034780: 
 	std: 0.0029017231
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9808, 0.9749, 0.981, 0.9752]
	train_accs: [0.98145, 0.98045, 0.9756333, 0.98121667, 0.9766833]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.9786
	best: 0.9811

Starting e_i: 522
Model ind 665 epoch 522 batch: 0 avg loss -2.966438 avg loss no lamb -2.966438 time 2020-06-26 07:51:21.889362
Model ind 665 epoch 522 batch: 100 avg loss -2.800519 avg loss no lamb -2.800519 time 2020-06-26 07:51:32.708027
Model ind 665 epoch 522 batch: 200 avg loss -2.790516 avg loss no lamb -2.790516 time 2020-06-26 07:51:43.371173
Model ind 665 epoch 522 batch: 300 avg loss -2.805014 avg loss no lamb -2.805014 time 2020-06-26 07:51:54.286678
Model ind 665 epoch 522 batch: 400 avg loss -2.719069 avg loss no lamb -2.719069 time 2020-06-26 07:52:05.066334
Model ind 665 epoch 522 batch: 500 avg loss -2.819769 avg loss no lamb -2.819769 time 2020-06-26 07:52:15.694271
Model ind 665 epoch 522 batch: 600 avg loss -2.793573 avg loss no lamb -2.793573 time 2020-06-26 07:52:26.436238
Model ind 665 epoch 522 batch: 700 avg loss -2.659006 avg loss no lamb -2.659006 time 2020-06-26 07:52:37.161638
Model ind 665 epoch 522 batch: 800 avg loss -2.829121 avg loss no lamb -2.829121 time 2020-06-26 07:52:47.864378
last batch sz 10
Pre: time 2020-06-26 07:53:01.682475: 
 	std: 0.0026597755
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9814, 0.9748, 0.9804, 0.9761]
	train_accs: [0.9811, 0.9809833, 0.9763833, 0.9813, 0.97723335]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97863996
	best: 0.9804

Starting e_i: 523
Model ind 665 epoch 523 batch: 0 avg loss -2.861636 avg loss no lamb -2.861636 time 2020-06-26 07:53:02.501460
Model ind 665 epoch 523 batch: 100 avg loss -2.829484 avg loss no lamb -2.829484 time 2020-06-26 07:53:13.040072
Model ind 665 epoch 523 batch: 200 avg loss -2.841602 avg loss no lamb -2.841602 time 2020-06-26 07:53:23.755108
Model ind 665 epoch 523 batch: 300 avg loss -2.837051 avg loss no lamb -2.837051 time 2020-06-26 07:53:34.445144
Model ind 665 epoch 523 batch: 400 avg loss -2.731030 avg loss no lamb -2.731030 time 2020-06-26 07:53:45.237976
Model ind 665 epoch 523 batch: 500 avg loss -2.740135 avg loss no lamb -2.740135 time 2020-06-26 07:53:55.783506
Model ind 665 epoch 523 batch: 600 avg loss -2.855045 avg loss no lamb -2.855045 time 2020-06-26 07:54:06.383810
Model ind 665 epoch 523 batch: 700 avg loss -2.714397 avg loss no lamb -2.714397 time 2020-06-26 07:54:17.423261
Model ind 665 epoch 523 batch: 800 avg loss -2.817708 avg loss no lamb -2.817708 time 2020-06-26 07:54:28.181493
last batch sz 10
Pre: time 2020-06-26 07:54:42.077270: 
 	std: 0.002208173
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9807, 0.9759, 0.9812, 0.9772]
	train_accs: [0.982, 0.9815, 0.9774, 0.98185, 0.978]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.9792
	best: 0.981

Starting e_i: 524
Model ind 665 epoch 524 batch: 0 avg loss -2.892555 avg loss no lamb -2.892555 time 2020-06-26 07:54:43.040960
Model ind 665 epoch 524 batch: 100 avg loss -2.869480 avg loss no lamb -2.869480 time 2020-06-26 07:54:53.595109
Model ind 665 epoch 524 batch: 200 avg loss -2.808633 avg loss no lamb -2.808633 time 2020-06-26 07:55:04.293354
Model ind 665 epoch 524 batch: 300 avg loss -2.815298 avg loss no lamb -2.815298 time 2020-06-26 07:55:15.122433
Model ind 665 epoch 524 batch: 400 avg loss -2.734467 avg loss no lamb -2.734467 time 2020-06-26 07:55:25.992621
Model ind 665 epoch 524 batch: 500 avg loss -2.810451 avg loss no lamb -2.810451 time 2020-06-26 07:55:36.743016
Model ind 665 epoch 524 batch: 600 avg loss -2.775724 avg loss no lamb -2.775724 time 2020-06-26 07:55:47.609287
Model ind 665 epoch 524 batch: 700 avg loss -2.686113 avg loss no lamb -2.686113 time 2020-06-26 07:55:58.340975
Model ind 665 epoch 524 batch: 800 avg loss -2.782132 avg loss no lamb -2.782132 time 2020-06-26 07:56:09.287343
last batch sz 10
Pre: time 2020-06-26 07:56:23.252402: 
 	std: 0.0024709443
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9805, 0.9753, 0.9813, 0.9771]
	train_accs: [0.98195, 0.98135, 0.97641665, 0.9821333, 0.9775]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97911996
	best: 0.9813

Starting e_i: 525
Model ind 665 epoch 525 batch: 0 avg loss -2.914218 avg loss no lamb -2.914218 time 2020-06-26 07:56:24.163898
Model ind 665 epoch 525 batch: 100 avg loss -2.899906 avg loss no lamb -2.899906 time 2020-06-26 07:56:35.245506
Model ind 665 epoch 525 batch: 200 avg loss -2.840412 avg loss no lamb -2.840412 time 2020-06-26 07:56:46.033958
Model ind 665 epoch 525 batch: 300 avg loss -2.868871 avg loss no lamb -2.868871 time 2020-06-26 07:56:56.704053
Model ind 665 epoch 525 batch: 400 avg loss -2.701636 avg loss no lamb -2.701636 time 2020-06-26 07:57:07.573335
Model ind 665 epoch 525 batch: 500 avg loss -2.835825 avg loss no lamb -2.835825 time 2020-06-26 07:57:18.174699
Model ind 665 epoch 525 batch: 600 avg loss -2.825303 avg loss no lamb -2.825303 time 2020-06-26 07:57:29.185626
Model ind 665 epoch 525 batch: 700 avg loss -2.690571 avg loss no lamb -2.690571 time 2020-06-26 07:57:39.941945
Model ind 665 epoch 525 batch: 800 avg loss -2.841794 avg loss no lamb -2.841794 time 2020-06-26 07:57:50.863250
last batch sz 10
Pre: time 2020-06-26 07:58:04.965669: 
 	std: 0.003096846
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9817, 0.9751, 0.9821, 0.9763]
	train_accs: [0.98245, 0.9817167, 0.97615, 0.98253334, 0.9766833]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97945994
	best: 0.9821

Starting e_i: 526
Model ind 665 epoch 526 batch: 0 avg loss -2.937970 avg loss no lamb -2.937970 time 2020-06-26 07:58:05.846765
Model ind 665 epoch 526 batch: 100 avg loss -2.845601 avg loss no lamb -2.845601 time 2020-06-26 07:58:16.563466
Model ind 665 epoch 526 batch: 200 avg loss -2.870638 avg loss no lamb -2.870638 time 2020-06-26 07:58:27.523859
Model ind 665 epoch 526 batch: 300 avg loss -2.820184 avg loss no lamb -2.820184 time 2020-06-26 07:58:38.497428
Model ind 665 epoch 526 batch: 400 avg loss -2.776381 avg loss no lamb -2.776381 time 2020-06-26 07:58:49.181726
Model ind 665 epoch 526 batch: 500 avg loss -2.739994 avg loss no lamb -2.739994 time 2020-06-26 07:58:59.902318
Model ind 665 epoch 526 batch: 600 avg loss -2.841342 avg loss no lamb -2.841342 time 2020-06-26 07:59:10.618169
Model ind 665 epoch 526 batch: 700 avg loss -2.704325 avg loss no lamb -2.704325 time 2020-06-26 07:59:21.587767
Model ind 665 epoch 526 batch: 800 avg loss -2.752391 avg loss no lamb -2.752391 time 2020-06-26 07:59:32.146662
last batch sz 10
Pre: time 2020-06-26 07:59:46.221453: 
 	std: 0.003163928
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9814, 0.9738, 0.9811, 0.9762]
	train_accs: [0.98186666, 0.98155, 0.97571665, 0.98175, 0.97655]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97876006
	best: 0.9813

Starting e_i: 527
Model ind 665 epoch 527 batch: 0 avg loss -2.892511 avg loss no lamb -2.892511 time 2020-06-26 07:59:47.092743
Model ind 665 epoch 527 batch: 100 avg loss -2.820496 avg loss no lamb -2.820496 time 2020-06-26 07:59:57.869043
Model ind 665 epoch 527 batch: 200 avg loss -2.853157 avg loss no lamb -2.853157 time 2020-06-26 08:00:08.844979
Model ind 665 epoch 527 batch: 300 avg loss -2.866360 avg loss no lamb -2.866360 time 2020-06-26 08:00:19.568007
Model ind 665 epoch 527 batch: 400 avg loss -2.717106 avg loss no lamb -2.717106 time 2020-06-26 08:00:30.291589
Model ind 665 epoch 527 batch: 500 avg loss -2.808279 avg loss no lamb -2.808279 time 2020-06-26 08:00:41.067518
Model ind 665 epoch 527 batch: 600 avg loss -2.799462 avg loss no lamb -2.799462 time 2020-06-26 08:00:51.888072
Model ind 665 epoch 527 batch: 700 avg loss -2.659167 avg loss no lamb -2.659167 time 2020-06-26 08:01:02.672697
Model ind 665 epoch 527 batch: 800 avg loss -2.794147 avg loss no lamb -2.794147 time 2020-06-26 08:01:13.350780
last batch sz 10
Pre: time 2020-06-26 08:01:27.329949: 
 	std: 0.0021553766
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9805, 0.9765, 0.981, 0.9762]
	train_accs: [0.98141664, 0.9809, 0.97651666, 0.98125, 0.97653335]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.97897995
	best: 0.9807

Starting e_i: 528
Model ind 665 epoch 528 batch: 0 avg loss -2.897640 avg loss no lamb -2.897640 time 2020-06-26 08:01:28.194949
Model ind 665 epoch 528 batch: 100 avg loss -2.879849 avg loss no lamb -2.879849 time 2020-06-26 08:01:38.954143
Model ind 665 epoch 528 batch: 200 avg loss -2.831559 avg loss no lamb -2.831559 time 2020-06-26 08:01:49.586730
Model ind 665 epoch 528 batch: 300 avg loss -2.832114 avg loss no lamb -2.832114 time 2020-06-26 08:02:00.399789
Model ind 665 epoch 528 batch: 400 avg loss -2.710920 avg loss no lamb -2.710920 time 2020-06-26 08:02:11.009253
Model ind 665 epoch 528 batch: 500 avg loss -2.826970 avg loss no lamb -2.826970 time 2020-06-26 08:02:21.542608
Model ind 665 epoch 528 batch: 600 avg loss -2.876384 avg loss no lamb -2.876384 time 2020-06-26 08:02:32.155172
Model ind 665 epoch 528 batch: 700 avg loss -2.689223 avg loss no lamb -2.689223 time 2020-06-26 08:02:42.874029
Model ind 665 epoch 528 batch: 800 avg loss -2.831849 avg loss no lamb -2.831849 time 2020-06-26 08:02:53.709674
last batch sz 10
Pre: time 2020-06-26 08:03:07.883865: 
 	std: 0.0029895867
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9811, 0.9751, 0.9817, 0.9761]
	train_accs: [0.9824833, 0.98155, 0.97676665, 0.9823167, 0.97715]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97922003
	best: 0.9821

Starting e_i: 529
Model ind 665 epoch 529 batch: 0 avg loss -2.940901 avg loss no lamb -2.940901 time 2020-06-26 08:03:08.832723
Model ind 665 epoch 529 batch: 100 avg loss -2.810985 avg loss no lamb -2.810985 time 2020-06-26 08:03:19.674834
Model ind 665 epoch 529 batch: 200 avg loss -2.912850 avg loss no lamb -2.912850 time 2020-06-26 08:03:30.641344
Model ind 665 epoch 529 batch: 300 avg loss -2.818573 avg loss no lamb -2.818573 time 2020-06-26 08:03:41.588483
Model ind 665 epoch 529 batch: 400 avg loss -2.714422 avg loss no lamb -2.714422 time 2020-06-26 08:03:52.591955
Model ind 665 epoch 529 batch: 500 avg loss -2.794674 avg loss no lamb -2.794674 time 2020-06-26 08:04:03.761531
Model ind 665 epoch 529 batch: 600 avg loss -2.868221 avg loss no lamb -2.868221 time 2020-06-26 08:04:14.644572
Model ind 665 epoch 529 batch: 700 avg loss -2.749579 avg loss no lamb -2.749579 time 2020-06-26 08:04:25.720982
Model ind 665 epoch 529 batch: 800 avg loss -2.886774 avg loss no lamb -2.886774 time 2020-06-26 08:04:36.533858
last batch sz 10
Pre: time 2020-06-26 08:04:50.512701: 
 	std: 0.0019784768
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9796, 0.9757, 0.9802, 0.9767]
	train_accs: [0.9816833, 0.9808667, 0.9770167, 0.98151666, 0.9770167]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.9785601
	best: 0.9806

Starting e_i: 530
Model ind 665 epoch 530 batch: 0 avg loss -2.928378 avg loss no lamb -2.928378 time 2020-06-26 08:04:51.403665
Model ind 665 epoch 530 batch: 100 avg loss -2.873238 avg loss no lamb -2.873238 time 2020-06-26 08:05:02.226852
Model ind 665 epoch 530 batch: 200 avg loss -2.799310 avg loss no lamb -2.799310 time 2020-06-26 08:05:13.106587
Model ind 665 epoch 530 batch: 300 avg loss -2.784234 avg loss no lamb -2.784234 time 2020-06-26 08:05:24.005077
Model ind 665 epoch 530 batch: 400 avg loss -2.706192 avg loss no lamb -2.706192 time 2020-06-26 08:05:34.791948
Model ind 665 epoch 530 batch: 500 avg loss -2.853647 avg loss no lamb -2.853647 time 2020-06-26 08:05:45.355787
Model ind 665 epoch 530 batch: 600 avg loss -2.873457 avg loss no lamb -2.873457 time 2020-06-26 08:05:56.087706
Model ind 665 epoch 530 batch: 700 avg loss -2.784005 avg loss no lamb -2.784005 time 2020-06-26 08:06:07.061092
Model ind 665 epoch 530 batch: 800 avg loss -2.792986 avg loss no lamb -2.792986 time 2020-06-26 08:06:17.657767
last batch sz 10
Pre: time 2020-06-26 08:06:31.773575: 
 	std: 0.0029803424
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9782, 0.9726, 0.9795, 0.9731]
	train_accs: [0.98085, 0.9802333, 0.97463334, 0.9812833, 0.9752667]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97646
	best: 0.9795

Starting e_i: 531
Model ind 665 epoch 531 batch: 0 avg loss -2.949016 avg loss no lamb -2.949016 time 2020-06-26 08:06:33.847302
Model ind 665 epoch 531 batch: 100 avg loss -2.916656 avg loss no lamb -2.916656 time 2020-06-26 08:06:44.423936
Model ind 665 epoch 531 batch: 200 avg loss -2.840398 avg loss no lamb -2.840398 time 2020-06-26 08:06:55.167426
Model ind 665 epoch 531 batch: 300 avg loss -2.873194 avg loss no lamb -2.873194 time 2020-06-26 08:07:05.678639
Model ind 665 epoch 531 batch: 400 avg loss -2.607896 avg loss no lamb -2.607896 time 2020-06-26 08:07:16.221216
Model ind 665 epoch 531 batch: 500 avg loss -2.818305 avg loss no lamb -2.818305 time 2020-06-26 08:07:27.054554
Model ind 665 epoch 531 batch: 600 avg loss -2.853043 avg loss no lamb -2.853043 time 2020-06-26 08:07:37.918546
Model ind 665 epoch 531 batch: 700 avg loss -2.717586 avg loss no lamb -2.717586 time 2020-06-26 08:07:48.681398
Model ind 665 epoch 531 batch: 800 avg loss -2.820571 avg loss no lamb -2.820571 time 2020-06-26 08:07:59.337897
last batch sz 10
Pre: time 2020-06-26 08:08:13.204406: 
 	std: 0.0033256565
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9806, 0.9746, 0.9816, 0.9741]
	train_accs: [0.98121667, 0.98088336, 0.9761, 0.98178333, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97840005
	best: 0.9816

Starting e_i: 532
Model ind 665 epoch 532 batch: 0 avg loss -2.911067 avg loss no lamb -2.911067 time 2020-06-26 08:08:14.090830
Model ind 665 epoch 532 batch: 100 avg loss -2.833448 avg loss no lamb -2.833448 time 2020-06-26 08:08:24.722219
Model ind 665 epoch 532 batch: 200 avg loss -2.825192 avg loss no lamb -2.825192 time 2020-06-26 08:08:35.310209
Model ind 665 epoch 532 batch: 300 avg loss -2.870517 avg loss no lamb -2.870517 time 2020-06-26 08:08:46.211581
Model ind 665 epoch 532 batch: 400 avg loss -2.670083 avg loss no lamb -2.670083 time 2020-06-26 08:08:56.571422
Model ind 665 epoch 532 batch: 500 avg loss -2.787691 avg loss no lamb -2.787691 time 2020-06-26 08:09:07.290885
Model ind 665 epoch 532 batch: 600 avg loss -2.836435 avg loss no lamb -2.836435 time 2020-06-26 08:09:17.647742
Model ind 665 epoch 532 batch: 700 avg loss -2.783924 avg loss no lamb -2.783924 time 2020-06-26 08:09:28.359998
Model ind 665 epoch 532 batch: 800 avg loss -2.777642 avg loss no lamb -2.777642 time 2020-06-26 08:09:39.188007
last batch sz 10
Pre: time 2020-06-26 08:09:53.197657: 
 	std: 0.0029403358
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9823, 0.9756, 0.9827, 0.9773]
	train_accs: [0.9819833, 0.9819, 0.97615, 0.98216665, 0.9773]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97998
	best: 0.9827

Starting e_i: 533
Model ind 665 epoch 533 batch: 0 avg loss -2.922310 avg loss no lamb -2.922310 time 2020-06-26 08:09:54.064292
Model ind 665 epoch 533 batch: 100 avg loss -2.849715 avg loss no lamb -2.849715 time 2020-06-26 08:10:04.871275
Model ind 665 epoch 533 batch: 200 avg loss -2.829318 avg loss no lamb -2.829318 time 2020-06-26 08:10:15.437492
Model ind 665 epoch 533 batch: 300 avg loss -2.828909 avg loss no lamb -2.828909 time 2020-06-26 08:10:26.145237
Model ind 665 epoch 533 batch: 400 avg loss -2.734073 avg loss no lamb -2.734073 time 2020-06-26 08:10:36.917834
Model ind 665 epoch 533 batch: 500 avg loss -2.776422 avg loss no lamb -2.776422 time 2020-06-26 08:10:47.503315
Model ind 665 epoch 533 batch: 600 avg loss -2.819062 avg loss no lamb -2.819062 time 2020-06-26 08:10:58.205941
Model ind 665 epoch 533 batch: 700 avg loss -2.733972 avg loss no lamb -2.733972 time 2020-06-26 08:11:08.753465
Model ind 665 epoch 533 batch: 800 avg loss -2.830257 avg loss no lamb -2.830257 time 2020-06-26 08:11:19.310366
last batch sz 10
Pre: time 2020-06-26 08:11:33.274142: 
 	std: 0.0024751504
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9819, 0.9768, 0.9828, 0.9784]
	train_accs: [0.98228335, 0.9816167, 0.9776, 0.9824, 0.97798336]
	best_train_sub_head: 3
	worst: 0.9768
	avg: 0.98054
	best: 0.9828

Starting e_i: 534
Model ind 665 epoch 534 batch: 0 avg loss -2.951458 avg loss no lamb -2.951458 time 2020-06-26 08:11:34.309392
Model ind 665 epoch 534 batch: 100 avg loss -2.829969 avg loss no lamb -2.829969 time 2020-06-26 08:11:45.198204
Model ind 665 epoch 534 batch: 200 avg loss -2.803188 avg loss no lamb -2.803188 time 2020-06-26 08:11:55.991594
Model ind 665 epoch 534 batch: 300 avg loss -2.842818 avg loss no lamb -2.842818 time 2020-06-26 08:12:06.879434
Model ind 665 epoch 534 batch: 400 avg loss -2.774695 avg loss no lamb -2.774695 time 2020-06-26 08:12:17.608496
Model ind 665 epoch 534 batch: 500 avg loss -2.850557 avg loss no lamb -2.850557 time 2020-06-26 08:12:28.604260
Model ind 665 epoch 534 batch: 600 avg loss -2.849237 avg loss no lamb -2.849237 time 2020-06-26 08:12:39.469614
Model ind 665 epoch 534 batch: 700 avg loss -2.690741 avg loss no lamb -2.690741 time 2020-06-26 08:12:50.343966
Model ind 665 epoch 534 batch: 800 avg loss -2.797375 avg loss no lamb -2.797375 time 2020-06-26 08:13:01.115958
last batch sz 10
Pre: time 2020-06-26 08:13:15.222371: 
 	std: 0.003062294
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.981, 0.9751, 0.9822, 0.9764]
	train_accs: [0.9817167, 0.98111665, 0.97645, 0.98156667, 0.97713333]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97942
	best: 0.9824

Starting e_i: 535
Model ind 665 epoch 535 batch: 0 avg loss -2.933617 avg loss no lamb -2.933617 time 2020-06-26 08:13:16.080736
Model ind 665 epoch 535 batch: 100 avg loss -2.826336 avg loss no lamb -2.826336 time 2020-06-26 08:13:26.870206
Model ind 665 epoch 535 batch: 200 avg loss -2.778748 avg loss no lamb -2.778748 time 2020-06-26 08:13:37.345683
Model ind 665 epoch 535 batch: 300 avg loss -2.784366 avg loss no lamb -2.784366 time 2020-06-26 08:13:48.126432
Model ind 665 epoch 535 batch: 400 avg loss -2.690651 avg loss no lamb -2.690651 time 2020-06-26 08:13:58.962214
Model ind 665 epoch 535 batch: 500 avg loss -2.835075 avg loss no lamb -2.835075 time 2020-06-26 08:14:10.028969
Model ind 665 epoch 535 batch: 600 avg loss -2.808596 avg loss no lamb -2.808596 time 2020-06-26 08:14:20.686461
Model ind 665 epoch 535 batch: 700 avg loss -2.724906 avg loss no lamb -2.724906 time 2020-06-26 08:14:31.079319
Model ind 665 epoch 535 batch: 800 avg loss -2.824222 avg loss no lamb -2.824222 time 2020-06-26 08:14:41.782852
last batch sz 10
Pre: time 2020-06-26 08:14:55.718233: 
 	std: 0.002736851
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9807, 0.975, 0.9814, 0.9774]
	train_accs: [0.98153335, 0.98076665, 0.97573334, 0.98155, 0.9769667]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97936
	best: 0.9814

Starting e_i: 536
Model ind 665 epoch 536 batch: 0 avg loss -2.983736 avg loss no lamb -2.983736 time 2020-06-26 08:14:56.562188
Model ind 665 epoch 536 batch: 100 avg loss -2.863534 avg loss no lamb -2.863534 time 2020-06-26 08:15:07.563337
Model ind 665 epoch 536 batch: 200 avg loss -2.798651 avg loss no lamb -2.798651 time 2020-06-26 08:15:18.042093
Model ind 665 epoch 536 batch: 300 avg loss -2.794010 avg loss no lamb -2.794010 time 2020-06-26 08:15:28.597216
Model ind 665 epoch 536 batch: 400 avg loss -2.682105 avg loss no lamb -2.682105 time 2020-06-26 08:15:39.123378
Model ind 665 epoch 536 batch: 500 avg loss -2.805350 avg loss no lamb -2.805350 time 2020-06-26 08:15:50.075457
Model ind 665 epoch 536 batch: 600 avg loss -2.790800 avg loss no lamb -2.790800 time 2020-06-26 08:16:00.939000
Model ind 665 epoch 536 batch: 700 avg loss -2.625764 avg loss no lamb -2.625764 time 2020-06-26 08:16:11.592596
Model ind 665 epoch 536 batch: 800 avg loss -2.807477 avg loss no lamb -2.807477 time 2020-06-26 08:16:22.253879
last batch sz 10
Pre: time 2020-06-26 08:16:36.007519: 
 	std: 0.0032270222
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9814, 0.9747, 0.982, 0.9763]
	train_accs: [0.98223335, 0.9813667, 0.97566664, 0.9820667, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97938
	best: 0.9825

Starting e_i: 537
Model ind 665 epoch 537 batch: 0 avg loss -2.912896 avg loss no lamb -2.912896 time 2020-06-26 08:16:36.922916
Model ind 665 epoch 537 batch: 100 avg loss -2.850873 avg loss no lamb -2.850873 time 2020-06-26 08:16:47.428111
Model ind 665 epoch 537 batch: 200 avg loss -2.789392 avg loss no lamb -2.789392 time 2020-06-26 08:16:58.120836
Model ind 665 epoch 537 batch: 300 avg loss -2.839792 avg loss no lamb -2.839792 time 2020-06-26 08:17:08.949808
Model ind 665 epoch 537 batch: 400 avg loss -2.719412 avg loss no lamb -2.719412 time 2020-06-26 08:17:19.583065
Model ind 665 epoch 537 batch: 500 avg loss -2.799097 avg loss no lamb -2.799097 time 2020-06-26 08:17:30.169282
Model ind 665 epoch 537 batch: 600 avg loss -2.843770 avg loss no lamb -2.843770 time 2020-06-26 08:17:40.706003
Model ind 665 epoch 537 batch: 700 avg loss -2.705675 avg loss no lamb -2.705675 time 2020-06-26 08:17:51.489957
Model ind 665 epoch 537 batch: 800 avg loss -2.781799 avg loss no lamb -2.781799 time 2020-06-26 08:18:02.250240
last batch sz 10
Pre: time 2020-06-26 08:18:16.462261: 
 	std: 0.0028768128
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9807, 0.9747, 0.982, 0.9767]
	train_accs: [0.98183334, 0.98145, 0.97641665, 0.98195, 0.97726667]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.9790999
	best: 0.982

Starting e_i: 538
Model ind 665 epoch 538 batch: 0 avg loss -2.935014 avg loss no lamb -2.935014 time 2020-06-26 08:18:17.382962
Model ind 665 epoch 538 batch: 100 avg loss -2.751004 avg loss no lamb -2.751004 time 2020-06-26 08:18:28.334917
Model ind 665 epoch 538 batch: 200 avg loss -2.799757 avg loss no lamb -2.799757 time 2020-06-26 08:18:39.053330
Model ind 665 epoch 538 batch: 300 avg loss -2.833079 avg loss no lamb -2.833079 time 2020-06-26 08:18:49.705536
Model ind 665 epoch 538 batch: 400 avg loss -2.773754 avg loss no lamb -2.773754 time 2020-06-26 08:19:00.410839
Model ind 665 epoch 538 batch: 500 avg loss -2.799236 avg loss no lamb -2.799236 time 2020-06-26 08:19:11.262378
Model ind 665 epoch 538 batch: 600 avg loss -2.832881 avg loss no lamb -2.832881 time 2020-06-26 08:19:21.996700
Model ind 665 epoch 538 batch: 700 avg loss -2.669288 avg loss no lamb -2.669288 time 2020-06-26 08:19:32.787742
Model ind 665 epoch 538 batch: 800 avg loss -2.790958 avg loss no lamb -2.790958 time 2020-06-26 08:19:43.282106
last batch sz 10
Pre: time 2020-06-26 08:19:57.102049: 
 	std: 0.0032539316
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9798, 0.9737, 0.9817, 0.9754]
	train_accs: [0.9820333, 0.98108333, 0.97566664, 0.98205, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97840005
	best: 0.9817

Starting e_i: 539
Model ind 665 epoch 539 batch: 0 avg loss -2.924083 avg loss no lamb -2.924083 time 2020-06-26 08:19:58.094863
Model ind 665 epoch 539 batch: 100 avg loss -2.864926 avg loss no lamb -2.864926 time 2020-06-26 08:20:09.069026
Model ind 665 epoch 539 batch: 200 avg loss -2.849655 avg loss no lamb -2.849655 time 2020-06-26 08:20:19.712199
Model ind 665 epoch 539 batch: 300 avg loss -2.835604 avg loss no lamb -2.835604 time 2020-06-26 08:20:30.160721
Model ind 665 epoch 539 batch: 400 avg loss -2.771415 avg loss no lamb -2.771415 time 2020-06-26 08:20:41.223984
Model ind 665 epoch 539 batch: 500 avg loss -2.737405 avg loss no lamb -2.737405 time 2020-06-26 08:20:51.977736
Model ind 665 epoch 539 batch: 600 avg loss -2.875735 avg loss no lamb -2.875735 time 2020-06-26 08:21:02.962423
Model ind 665 epoch 539 batch: 700 avg loss -2.656398 avg loss no lamb -2.656398 time 2020-06-26 08:21:13.745781
Model ind 665 epoch 539 batch: 800 avg loss -2.788877 avg loss no lamb -2.788877 time 2020-06-26 08:21:24.315851
last batch sz 10
Pre: time 2020-06-26 08:21:38.106135: 
 	std: 0.0028175237
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9802, 0.9749, 0.9813, 0.9758]
	train_accs: [0.98175, 0.98118335, 0.9763167, 0.9820167, 0.97665]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97874004
	best: 0.9813

Starting e_i: 540
Model ind 665 epoch 540 batch: 0 avg loss -2.963057 avg loss no lamb -2.963057 time 2020-06-26 08:21:38.945099
Model ind 665 epoch 540 batch: 100 avg loss -2.837353 avg loss no lamb -2.837353 time 2020-06-26 08:21:49.640703
Model ind 665 epoch 540 batch: 200 avg loss -2.836109 avg loss no lamb -2.836109 time 2020-06-26 08:22:00.403373
Model ind 665 epoch 540 batch: 300 avg loss -2.876667 avg loss no lamb -2.876667 time 2020-06-26 08:22:10.980828
Model ind 665 epoch 540 batch: 400 avg loss -2.715899 avg loss no lamb -2.715899 time 2020-06-26 08:22:21.844052
Model ind 665 epoch 540 batch: 500 avg loss -2.736245 avg loss no lamb -2.736245 time 2020-06-26 08:22:32.357270
Model ind 665 epoch 540 batch: 600 avg loss -2.814875 avg loss no lamb -2.814875 time 2020-06-26 08:22:43.177344
Model ind 665 epoch 540 batch: 700 avg loss -2.730317 avg loss no lamb -2.730317 time 2020-06-26 08:22:53.586011
Model ind 665 epoch 540 batch: 800 avg loss -2.820855 avg loss no lamb -2.820855 time 2020-06-26 08:23:04.437580
last batch sz 10
Pre: time 2020-06-26 08:23:18.328561: 
 	std: 0.002579912
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9795, 0.9743, 0.9805, 0.976]
	train_accs: [0.98141664, 0.98013335, 0.97478336, 0.9814, 0.97693336]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.9782001
	best: 0.9807

Starting e_i: 541
Model ind 665 epoch 541 batch: 0 avg loss -2.925257 avg loss no lamb -2.925257 time 2020-06-26 08:23:20.455128
Model ind 665 epoch 541 batch: 100 avg loss -2.774975 avg loss no lamb -2.774975 time 2020-06-26 08:23:31.103607
Model ind 665 epoch 541 batch: 200 avg loss -2.838095 avg loss no lamb -2.838095 time 2020-06-26 08:23:41.676113
Model ind 665 epoch 541 batch: 300 avg loss -2.790304 avg loss no lamb -2.790304 time 2020-06-26 08:23:52.360106
Model ind 665 epoch 541 batch: 400 avg loss -2.751881 avg loss no lamb -2.751881 time 2020-06-26 08:24:03.065874
Model ind 665 epoch 541 batch: 500 avg loss -2.740555 avg loss no lamb -2.740555 time 2020-06-26 08:24:13.564410
Model ind 665 epoch 541 batch: 600 avg loss -2.836446 avg loss no lamb -2.836446 time 2020-06-26 08:24:24.139163
Model ind 665 epoch 541 batch: 700 avg loss -2.725489 avg loss no lamb -2.725489 time 2020-06-26 08:24:34.839496
Model ind 665 epoch 541 batch: 800 avg loss -2.852380 avg loss no lamb -2.852380 time 2020-06-26 08:24:45.500285
last batch sz 10
Pre: time 2020-06-26 08:24:59.340421: 
 	std: 0.0025474634
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9813, 0.9761, 0.9818, 0.9771]
	train_accs: [0.982, 0.9813, 0.9767, 0.98205, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.97968006
	best: 0.9818

Starting e_i: 542
Model ind 665 epoch 542 batch: 0 avg loss -2.884630 avg loss no lamb -2.884630 time 2020-06-26 08:25:00.231478
Model ind 665 epoch 542 batch: 100 avg loss -2.920631 avg loss no lamb -2.920631 time 2020-06-26 08:25:11.093728
Model ind 665 epoch 542 batch: 200 avg loss -2.826460 avg loss no lamb -2.826460 time 2020-06-26 08:25:21.689951
Model ind 665 epoch 542 batch: 300 avg loss -2.811414 avg loss no lamb -2.811414 time 2020-06-26 08:25:32.495486
Model ind 665 epoch 542 batch: 400 avg loss -2.738547 avg loss no lamb -2.738547 time 2020-06-26 08:25:43.302363
Model ind 665 epoch 542 batch: 500 avg loss -2.807839 avg loss no lamb -2.807839 time 2020-06-26 08:25:53.870031
Model ind 665 epoch 542 batch: 600 avg loss -2.847720 avg loss no lamb -2.847720 time 2020-06-26 08:26:04.467242
Model ind 665 epoch 542 batch: 700 avg loss -2.722242 avg loss no lamb -2.722242 time 2020-06-26 08:26:14.859753
Model ind 665 epoch 542 batch: 800 avg loss -2.803142 avg loss no lamb -2.803142 time 2020-06-26 08:26:25.600877
last batch sz 10
Pre: time 2020-06-26 08:26:39.494984: 
 	std: 0.0027679612
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9798, 0.9741, 0.9803, 0.9754]
	train_accs: [0.9816667, 0.9809333, 0.97578335, 0.98155, 0.97681665]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97808
	best: 0.9808

Starting e_i: 543
Model ind 665 epoch 543 batch: 0 avg loss -2.931548 avg loss no lamb -2.931548 time 2020-06-26 08:26:40.322285
Model ind 665 epoch 543 batch: 100 avg loss -2.862689 avg loss no lamb -2.862689 time 2020-06-26 08:26:51.144568
Model ind 665 epoch 543 batch: 200 avg loss -2.795530 avg loss no lamb -2.795530 time 2020-06-26 08:27:01.472286
Model ind 665 epoch 543 batch: 300 avg loss -2.747680 avg loss no lamb -2.747680 time 2020-06-26 08:27:11.938820
Model ind 665 epoch 543 batch: 400 avg loss -2.760601 avg loss no lamb -2.760601 time 2020-06-26 08:27:22.805022
Model ind 665 epoch 543 batch: 500 avg loss -2.847899 avg loss no lamb -2.847899 time 2020-06-26 08:27:33.438440
Model ind 665 epoch 543 batch: 600 avg loss -2.845973 avg loss no lamb -2.845973 time 2020-06-26 08:27:44.138417
Model ind 665 epoch 543 batch: 700 avg loss -2.614105 avg loss no lamb -2.614105 time 2020-06-26 08:27:54.691120
Model ind 665 epoch 543 batch: 800 avg loss -2.812415 avg loss no lamb -2.812415 time 2020-06-26 08:28:05.493426
last batch sz 10
Pre: time 2020-06-26 08:28:19.453321: 
 	std: 0.0027563693
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9806, 0.9742, 0.981, 0.9767]
	train_accs: [0.9816333, 0.9810167, 0.97636664, 0.9816833, 0.9775]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97867996
	best: 0.981

Starting e_i: 544
Model ind 665 epoch 544 batch: 0 avg loss -2.942585 avg loss no lamb -2.942585 time 2020-06-26 08:28:20.343897
Model ind 665 epoch 544 batch: 100 avg loss -2.875875 avg loss no lamb -2.875875 time 2020-06-26 08:28:31.152188
Model ind 665 epoch 544 batch: 200 avg loss -2.831871 avg loss no lamb -2.831871 time 2020-06-26 08:28:41.554288
Model ind 665 epoch 544 batch: 300 avg loss -2.882204 avg loss no lamb -2.882204 time 2020-06-26 08:28:52.030579
Model ind 665 epoch 544 batch: 400 avg loss -2.770469 avg loss no lamb -2.770469 time 2020-06-26 08:29:02.904731
Model ind 665 epoch 544 batch: 500 avg loss -2.818132 avg loss no lamb -2.818132 time 2020-06-26 08:29:13.564104
Model ind 665 epoch 544 batch: 600 avg loss -2.830222 avg loss no lamb -2.830222 time 2020-06-26 08:29:24.357215
Model ind 665 epoch 544 batch: 700 avg loss -2.686964 avg loss no lamb -2.686964 time 2020-06-26 08:29:35.128473
Model ind 665 epoch 544 batch: 800 avg loss -2.811890 avg loss no lamb -2.811890 time 2020-06-26 08:29:45.908513
last batch sz 10
Pre: time 2020-06-26 08:29:59.955682: 
 	std: 0.0027931374
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9793, 0.9737, 0.9801, 0.9753]
	train_accs: [0.98146665, 0.98045, 0.97548336, 0.98113334, 0.9769]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97782004
	best: 0.9807

Starting e_i: 545
Model ind 665 epoch 545 batch: 0 avg loss -2.924544 avg loss no lamb -2.924544 time 2020-06-26 08:30:00.863914
Model ind 665 epoch 545 batch: 100 avg loss -2.872260 avg loss no lamb -2.872260 time 2020-06-26 08:30:11.463895
Model ind 665 epoch 545 batch: 200 avg loss -2.849099 avg loss no lamb -2.849099 time 2020-06-26 08:30:22.072407
Model ind 665 epoch 545 batch: 300 avg loss -2.815456 avg loss no lamb -2.815456 time 2020-06-26 08:30:32.844341
Model ind 665 epoch 545 batch: 400 avg loss -2.747734 avg loss no lamb -2.747734 time 2020-06-26 08:30:43.544737
Model ind 665 epoch 545 batch: 500 avg loss -2.833232 avg loss no lamb -2.833232 time 2020-06-26 08:30:54.249971
Model ind 665 epoch 545 batch: 600 avg loss -2.882795 avg loss no lamb -2.882795 time 2020-06-26 08:31:04.962068
Model ind 665 epoch 545 batch: 700 avg loss -2.784734 avg loss no lamb -2.784734 time 2020-06-26 08:31:15.506772
Model ind 665 epoch 545 batch: 800 avg loss -2.821423 avg loss no lamb -2.821423 time 2020-06-26 08:31:26.194898
last batch sz 10
Pre: time 2020-06-26 08:31:40.109919: 
 	std: 0.0023370006
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9794, 0.975, 0.981, 0.9765]
	train_accs: [0.98181665, 0.9806167, 0.9759667, 0.9819833, 0.9769667]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97848
	best: 0.981

Starting e_i: 546
Model ind 665 epoch 546 batch: 0 avg loss -2.917325 avg loss no lamb -2.917325 time 2020-06-26 08:31:41.076239
Model ind 665 epoch 546 batch: 100 avg loss -2.796793 avg loss no lamb -2.796793 time 2020-06-26 08:31:51.734404
Model ind 665 epoch 546 batch: 200 avg loss -2.907258 avg loss no lamb -2.907258 time 2020-06-26 08:32:02.545110
Model ind 665 epoch 546 batch: 300 avg loss -2.781263 avg loss no lamb -2.781263 time 2020-06-26 08:32:13.126948
Model ind 665 epoch 546 batch: 400 avg loss -2.752364 avg loss no lamb -2.752364 time 2020-06-26 08:32:23.959229
Model ind 665 epoch 546 batch: 500 avg loss -2.777215 avg loss no lamb -2.777215 time 2020-06-26 08:32:34.693094
Model ind 665 epoch 546 batch: 600 avg loss -2.837667 avg loss no lamb -2.837667 time 2020-06-26 08:32:45.544680
Model ind 665 epoch 546 batch: 700 avg loss -2.682189 avg loss no lamb -2.682189 time 2020-06-26 08:32:56.225593
Model ind 665 epoch 546 batch: 800 avg loss -2.857982 avg loss no lamb -2.857982 time 2020-06-26 08:33:06.879934
last batch sz 10
Pre: time 2020-06-26 08:33:20.633389: 
 	std: 0.0030929572
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9811, 0.975, 0.9831, 0.9781]
	train_accs: [0.9823667, 0.9816167, 0.9766167, 0.9827, 0.97795]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.98003995
	best: 0.9831

Starting e_i: 547
Model ind 665 epoch 547 batch: 0 avg loss -2.947692 avg loss no lamb -2.947692 time 2020-06-26 08:33:21.512070
Model ind 665 epoch 547 batch: 100 avg loss -2.829763 avg loss no lamb -2.829763 time 2020-06-26 08:33:32.381004
Model ind 665 epoch 547 batch: 200 avg loss -2.827807 avg loss no lamb -2.827807 time 2020-06-26 08:33:43.150302
Model ind 665 epoch 547 batch: 300 avg loss -2.816693 avg loss no lamb -2.816693 time 2020-06-26 08:33:53.901538
Model ind 665 epoch 547 batch: 400 avg loss -2.727070 avg loss no lamb -2.727070 time 2020-06-26 08:34:05.054494
Model ind 665 epoch 547 batch: 500 avg loss -2.755784 avg loss no lamb -2.755784 time 2020-06-26 08:34:15.747046
Model ind 665 epoch 547 batch: 600 avg loss -2.817780 avg loss no lamb -2.817780 time 2020-06-26 08:34:26.504384
Model ind 665 epoch 547 batch: 700 avg loss -2.730097 avg loss no lamb -2.730097 time 2020-06-26 08:34:37.209751
Model ind 665 epoch 547 batch: 800 avg loss -2.823864 avg loss no lamb -2.823864 time 2020-06-26 08:34:47.694663
last batch sz 10
Pre: time 2020-06-26 08:35:01.721668: 
 	std: 0.0032518352
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9791, 0.9732, 0.9815, 0.9759]
	train_accs: [0.9812833, 0.9801667, 0.9742, 0.98135, 0.97616667]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97824
	best: 0.9815

Starting e_i: 548
Model ind 665 epoch 548 batch: 0 avg loss -2.969065 avg loss no lamb -2.969065 time 2020-06-26 08:35:02.607357
Model ind 665 epoch 548 batch: 100 avg loss -2.846657 avg loss no lamb -2.846657 time 2020-06-26 08:35:13.232114
Model ind 665 epoch 548 batch: 200 avg loss -2.810708 avg loss no lamb -2.810708 time 2020-06-26 08:35:23.968793
Model ind 665 epoch 548 batch: 300 avg loss -2.787150 avg loss no lamb -2.787150 time 2020-06-26 08:35:34.805027
Model ind 665 epoch 548 batch: 400 avg loss -2.782413 avg loss no lamb -2.782413 time 2020-06-26 08:35:45.627120
Model ind 665 epoch 548 batch: 500 avg loss -2.776234 avg loss no lamb -2.776234 time 2020-06-26 08:35:56.496784
Model ind 665 epoch 548 batch: 600 avg loss -2.818795 avg loss no lamb -2.818795 time 2020-06-26 08:36:07.562978
Model ind 665 epoch 548 batch: 700 avg loss -2.693349 avg loss no lamb -2.693349 time 2020-06-26 08:36:18.197712
Model ind 665 epoch 548 batch: 800 avg loss -2.801120 avg loss no lamb -2.801120 time 2020-06-26 08:36:28.870889
last batch sz 10
Pre: time 2020-06-26 08:36:42.500632: 
 	std: 0.0026914654
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9808, 0.9754, 0.9819, 0.977]
	train_accs: [0.98185, 0.981, 0.97675, 0.9820667, 0.9774]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.9794
	best: 0.9819

Starting e_i: 549
Model ind 665 epoch 549 batch: 0 avg loss -2.903167 avg loss no lamb -2.903167 time 2020-06-26 08:36:43.350836
Model ind 665 epoch 549 batch: 100 avg loss -2.860947 avg loss no lamb -2.860947 time 2020-06-26 08:36:54.073867
Model ind 665 epoch 549 batch: 200 avg loss -2.791648 avg loss no lamb -2.791648 time 2020-06-26 08:37:04.750178
Model ind 665 epoch 549 batch: 300 avg loss -2.862168 avg loss no lamb -2.862168 time 2020-06-26 08:37:15.408856
Model ind 665 epoch 549 batch: 400 avg loss -2.731920 avg loss no lamb -2.731920 time 2020-06-26 08:37:26.068313
Model ind 665 epoch 549 batch: 500 avg loss -2.731997 avg loss no lamb -2.731997 time 2020-06-26 08:37:36.820817
Model ind 665 epoch 549 batch: 600 avg loss -2.762967 avg loss no lamb -2.762967 time 2020-06-26 08:37:47.564878
Model ind 665 epoch 549 batch: 700 avg loss -2.686504 avg loss no lamb -2.686504 time 2020-06-26 08:37:58.201173
Model ind 665 epoch 549 batch: 800 avg loss -2.851452 avg loss no lamb -2.851452 time 2020-06-26 08:38:09.352181
last batch sz 10
Pre: time 2020-06-26 08:38:23.418815: 
 	std: 0.0024827512
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9796, 0.9741, 0.9807, 0.9766]
	train_accs: [0.9814, 0.98078334, 0.9758833, 0.98146665, 0.97723335]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.9782001
	best: 0.9807

Starting e_i: 550
Model ind 665 epoch 550 batch: 0 avg loss -2.916795 avg loss no lamb -2.916795 time 2020-06-26 08:38:24.279377
Model ind 665 epoch 550 batch: 100 avg loss -2.873050 avg loss no lamb -2.873050 time 2020-06-26 08:38:34.946900
Model ind 665 epoch 550 batch: 200 avg loss -2.831029 avg loss no lamb -2.831029 time 2020-06-26 08:38:45.662397
Model ind 665 epoch 550 batch: 300 avg loss -2.837095 avg loss no lamb -2.837095 time 2020-06-26 08:38:56.380464
Model ind 665 epoch 550 batch: 400 avg loss -2.802685 avg loss no lamb -2.802685 time 2020-06-26 08:39:07.189011
Model ind 665 epoch 550 batch: 500 avg loss -2.771290 avg loss no lamb -2.771290 time 2020-06-26 08:39:17.785424
Model ind 665 epoch 550 batch: 600 avg loss -2.828640 avg loss no lamb -2.828640 time 2020-06-26 08:39:28.551748
Model ind 665 epoch 550 batch: 700 avg loss -2.648364 avg loss no lamb -2.648364 time 2020-06-26 08:39:39.153788
Model ind 665 epoch 550 batch: 800 avg loss -2.791690 avg loss no lamb -2.791690 time 2020-06-26 08:39:49.996492
last batch sz 10
Pre: time 2020-06-26 08:40:03.907295: 
 	std: 0.0025287133
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9777, 0.9738, 0.9799, 0.9752]
	train_accs: [0.98143333, 0.97975, 0.97571665, 0.9813, 0.97721666]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97736007
	best: 0.9802

Starting e_i: 551
Model ind 665 epoch 551 batch: 0 avg loss -2.941671 avg loss no lamb -2.941671 time 2020-06-26 08:40:05.964979
Model ind 665 epoch 551 batch: 100 avg loss -2.808982 avg loss no lamb -2.808982 time 2020-06-26 08:40:16.781428
Model ind 665 epoch 551 batch: 200 avg loss -2.853000 avg loss no lamb -2.853000 time 2020-06-26 08:40:27.457496
Model ind 665 epoch 551 batch: 300 avg loss -2.849921 avg loss no lamb -2.849921 time 2020-06-26 08:40:38.044757
Model ind 665 epoch 551 batch: 400 avg loss -2.795135 avg loss no lamb -2.795135 time 2020-06-26 08:40:49.004476
Model ind 665 epoch 551 batch: 500 avg loss -2.803625 avg loss no lamb -2.803625 time 2020-06-26 08:40:59.874357
Model ind 665 epoch 551 batch: 600 avg loss -2.827463 avg loss no lamb -2.827463 time 2020-06-26 08:41:10.848776
Model ind 665 epoch 551 batch: 700 avg loss -2.703057 avg loss no lamb -2.703057 time 2020-06-26 08:41:21.470303
Model ind 665 epoch 551 batch: 800 avg loss -2.879131 avg loss no lamb -2.879131 time 2020-06-26 08:41:32.446953
last batch sz 10
Pre: time 2020-06-26 08:41:46.690132: 
 	std: 0.0026645926
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.98, 0.9747, 0.9813, 0.9764]
	train_accs: [0.9820667, 0.98123336, 0.9756167, 0.9820167, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.9787
	best: 0.9811

Starting e_i: 552
Model ind 665 epoch 552 batch: 0 avg loss -3.004178 avg loss no lamb -3.004178 time 2020-06-26 08:41:47.543640
Model ind 665 epoch 552 batch: 100 avg loss -2.836496 avg loss no lamb -2.836496 time 2020-06-26 08:41:58.345336
Model ind 665 epoch 552 batch: 200 avg loss -2.836700 avg loss no lamb -2.836700 time 2020-06-26 08:42:09.068270
Model ind 665 epoch 552 batch: 300 avg loss -2.855480 avg loss no lamb -2.855480 time 2020-06-26 08:42:19.958315
Model ind 665 epoch 552 batch: 400 avg loss -2.745991 avg loss no lamb -2.745991 time 2020-06-26 08:42:30.714346
Model ind 665 epoch 552 batch: 500 avg loss -2.836964 avg loss no lamb -2.836964 time 2020-06-26 08:42:41.470968
Model ind 665 epoch 552 batch: 600 avg loss -2.804385 avg loss no lamb -2.804385 time 2020-06-26 08:42:52.431857
Model ind 665 epoch 552 batch: 700 avg loss -2.653882 avg loss no lamb -2.653882 time 2020-06-26 08:43:03.158533
Model ind 665 epoch 552 batch: 800 avg loss -2.795820 avg loss no lamb -2.795820 time 2020-06-26 08:43:13.790164
last batch sz 10
Pre: time 2020-06-26 08:43:27.650450: 
 	std: 0.002637132
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9794, 0.9733, 0.98, 0.9759]
	train_accs: [0.9812667, 0.981, 0.97548336, 0.98153335, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97766
	best: 0.98

Starting e_i: 553
Model ind 665 epoch 553 batch: 0 avg loss -2.938344 avg loss no lamb -2.938344 time 2020-06-26 08:43:28.641960
Model ind 665 epoch 553 batch: 100 avg loss -2.848538 avg loss no lamb -2.848538 time 2020-06-26 08:43:39.281355
Model ind 665 epoch 553 batch: 200 avg loss -2.821111 avg loss no lamb -2.821111 time 2020-06-26 08:43:49.941016
Model ind 665 epoch 553 batch: 300 avg loss -2.849440 avg loss no lamb -2.849440 time 2020-06-26 08:44:00.425254
Model ind 665 epoch 553 batch: 400 avg loss -2.700827 avg loss no lamb -2.700827 time 2020-06-26 08:44:11.254458
Model ind 665 epoch 553 batch: 500 avg loss -2.805277 avg loss no lamb -2.805277 time 2020-06-26 08:44:21.983716
Model ind 665 epoch 553 batch: 600 avg loss -2.837407 avg loss no lamb -2.837407 time 2020-06-26 08:44:32.968937
Model ind 665 epoch 553 batch: 700 avg loss -2.731445 avg loss no lamb -2.731445 time 2020-06-26 08:44:43.640614
Model ind 665 epoch 553 batch: 800 avg loss -2.762414 avg loss no lamb -2.762414 time 2020-06-26 08:44:54.296480
last batch sz 10
Pre: time 2020-06-26 08:45:08.286075: 
 	std: 0.0027488247
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9808, 0.9753, 0.9811, 0.9758]
	train_accs: [0.98221666, 0.9816833, 0.9764, 0.98215, 0.97675]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.9789001
	best: 0.9815

Starting e_i: 554
Model ind 665 epoch 554 batch: 0 avg loss -2.950114 avg loss no lamb -2.950114 time 2020-06-26 08:45:09.129580
Model ind 665 epoch 554 batch: 100 avg loss -2.802078 avg loss no lamb -2.802078 time 2020-06-26 08:45:19.743173
Model ind 665 epoch 554 batch: 200 avg loss -2.818352 avg loss no lamb -2.818352 time 2020-06-26 08:45:30.550891
Model ind 665 epoch 554 batch: 300 avg loss -2.810821 avg loss no lamb -2.810821 time 2020-06-26 08:45:41.444560
Model ind 665 epoch 554 batch: 400 avg loss -2.711543 avg loss no lamb -2.711543 time 2020-06-26 08:45:52.316556
Model ind 665 epoch 554 batch: 500 avg loss -2.728799 avg loss no lamb -2.728799 time 2020-06-26 08:46:03.046047
Model ind 665 epoch 554 batch: 600 avg loss -2.834221 avg loss no lamb -2.834221 time 2020-06-26 08:46:13.786140
Model ind 665 epoch 554 batch: 700 avg loss -2.651070 avg loss no lamb -2.651070 time 2020-06-26 08:46:24.527434
Model ind 665 epoch 554 batch: 800 avg loss -2.820221 avg loss no lamb -2.820221 time 2020-06-26 08:46:35.193574
last batch sz 10
Pre: time 2020-06-26 08:46:49.270468: 
 	std: 0.003040656
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.982, 0.9759, 0.9824, 0.9761]
	train_accs: [0.9819667, 0.98141664, 0.97675, 0.98211664, 0.9771]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97972
	best: 0.9824

Starting e_i: 555
Model ind 665 epoch 555 batch: 0 avg loss -2.923204 avg loss no lamb -2.923204 time 2020-06-26 08:46:50.167445
Model ind 665 epoch 555 batch: 100 avg loss -2.871260 avg loss no lamb -2.871260 time 2020-06-26 08:47:00.772009
Model ind 665 epoch 555 batch: 200 avg loss -2.823909 avg loss no lamb -2.823909 time 2020-06-26 08:47:11.244013
Model ind 665 epoch 555 batch: 300 avg loss -2.864605 avg loss no lamb -2.864605 time 2020-06-26 08:47:22.221024
Model ind 665 epoch 555 batch: 400 avg loss -2.810301 avg loss no lamb -2.810301 time 2020-06-26 08:47:33.046756
Model ind 665 epoch 555 batch: 500 avg loss -2.751941 avg loss no lamb -2.751941 time 2020-06-26 08:47:44.140413
Model ind 665 epoch 555 batch: 600 avg loss -2.815854 avg loss no lamb -2.815854 time 2020-06-26 08:47:54.754254
Model ind 665 epoch 555 batch: 700 avg loss -2.729234 avg loss no lamb -2.729234 time 2020-06-26 08:48:05.193415
Model ind 665 epoch 555 batch: 800 avg loss -2.830378 avg loss no lamb -2.830378 time 2020-06-26 08:48:15.791819
last batch sz 10
Pre: time 2020-06-26 08:48:29.357801: 
 	std: 0.0035414118
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.982, 0.9734, 0.9811, 0.9757]
	train_accs: [0.9819833, 0.98143333, 0.97575, 0.9817167, 0.97665]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97878
	best: 0.9817

Starting e_i: 556
Model ind 665 epoch 556 batch: 0 avg loss -2.913814 avg loss no lamb -2.913814 time 2020-06-26 08:48:30.265702
Model ind 665 epoch 556 batch: 100 avg loss -2.874871 avg loss no lamb -2.874871 time 2020-06-26 08:48:40.740652
Model ind 665 epoch 556 batch: 200 avg loss -2.821014 avg loss no lamb -2.821014 time 2020-06-26 08:48:51.420882
Model ind 665 epoch 556 batch: 300 avg loss -2.875850 avg loss no lamb -2.875850 time 2020-06-26 08:49:02.062812
Model ind 665 epoch 556 batch: 400 avg loss -2.758375 avg loss no lamb -2.758375 time 2020-06-26 08:49:12.746351
Model ind 665 epoch 556 batch: 500 avg loss -2.772609 avg loss no lamb -2.772609 time 2020-06-26 08:49:23.101368
Model ind 665 epoch 556 batch: 600 avg loss -2.819423 avg loss no lamb -2.819423 time 2020-06-26 08:49:33.885321
Model ind 665 epoch 556 batch: 700 avg loss -2.701414 avg loss no lamb -2.701414 time 2020-06-26 08:49:44.682172
Model ind 665 epoch 556 batch: 800 avg loss -2.861197 avg loss no lamb -2.861197 time 2020-06-26 08:49:55.784188
last batch sz 10
Pre: time 2020-06-26 08:50:09.685055: 
 	std: 0.0030785769
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9799, 0.9748, 0.9821, 0.9751]
	train_accs: [0.98146665, 0.98095, 0.9752333, 0.98193336, 0.9762167]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97861993
	best: 0.9821

Starting e_i: 557
Model ind 665 epoch 557 batch: 0 avg loss -2.928319 avg loss no lamb -2.928319 time 2020-06-26 08:50:10.608665
Model ind 665 epoch 557 batch: 100 avg loss -2.862426 avg loss no lamb -2.862426 time 2020-06-26 08:50:21.059913
Model ind 665 epoch 557 batch: 200 avg loss -2.828990 avg loss no lamb -2.828990 time 2020-06-26 08:50:31.872168
Model ind 665 epoch 557 batch: 300 avg loss -2.867202 avg loss no lamb -2.867202 time 2020-06-26 08:50:42.447727
Model ind 665 epoch 557 batch: 400 avg loss -2.712604 avg loss no lamb -2.712604 time 2020-06-26 08:50:53.170073
Model ind 665 epoch 557 batch: 500 avg loss -2.798449 avg loss no lamb -2.798449 time 2020-06-26 08:51:03.771427
Model ind 665 epoch 557 batch: 600 avg loss -2.792811 avg loss no lamb -2.792811 time 2020-06-26 08:51:14.428519
Model ind 665 epoch 557 batch: 700 avg loss -2.663936 avg loss no lamb -2.663936 time 2020-06-26 08:51:25.136874
Model ind 665 epoch 557 batch: 800 avg loss -2.824637 avg loss no lamb -2.824637 time 2020-06-26 08:51:35.769402
last batch sz 10
Pre: time 2020-06-26 08:51:49.588062: 
 	std: 0.0028323752
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9811, 0.9757, 0.9817, 0.9757]
	train_accs: [0.98155, 0.9812833, 0.9761, 0.9817, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97916
	best: 0.9817

Starting e_i: 558
Model ind 665 epoch 558 batch: 0 avg loss -2.913117 avg loss no lamb -2.913117 time 2020-06-26 08:51:50.569960
Model ind 665 epoch 558 batch: 100 avg loss -2.935264 avg loss no lamb -2.935264 time 2020-06-26 08:52:01.263724
Model ind 665 epoch 558 batch: 200 avg loss -2.806238 avg loss no lamb -2.806238 time 2020-06-26 08:52:12.146686
Model ind 665 epoch 558 batch: 300 avg loss -2.812633 avg loss no lamb -2.812633 time 2020-06-26 08:52:22.841392
Model ind 665 epoch 558 batch: 400 avg loss -2.740782 avg loss no lamb -2.740782 time 2020-06-26 08:52:33.592554
Model ind 665 epoch 558 batch: 500 avg loss -2.773778 avg loss no lamb -2.773778 time 2020-06-26 08:52:44.299822
Model ind 665 epoch 558 batch: 600 avg loss -2.857510 avg loss no lamb -2.857510 time 2020-06-26 08:52:55.038023
Model ind 665 epoch 558 batch: 700 avg loss -2.692441 avg loss no lamb -2.692441 time 2020-06-26 08:53:05.578080
Model ind 665 epoch 558 batch: 800 avg loss -2.828721 avg loss no lamb -2.828721 time 2020-06-26 08:53:16.395706
last batch sz 10
Pre: time 2020-06-26 08:53:30.185687: 
 	std: 0.0029068028
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9785, 0.9725, 0.9794, 0.9749]
	train_accs: [0.98035, 0.97966665, 0.9744667, 0.98031664, 0.97585]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97708
	best: 0.9801

Starting e_i: 559
Model ind 665 epoch 559 batch: 0 avg loss -2.927660 avg loss no lamb -2.927660 time 2020-06-26 08:53:31.062217
Model ind 665 epoch 559 batch: 100 avg loss -2.825985 avg loss no lamb -2.825985 time 2020-06-26 08:53:42.028774
Model ind 665 epoch 559 batch: 200 avg loss -2.855315 avg loss no lamb -2.855315 time 2020-06-26 08:53:52.968765
Model ind 665 epoch 559 batch: 300 avg loss -2.882856 avg loss no lamb -2.882856 time 2020-06-26 08:54:03.995000
Model ind 665 epoch 559 batch: 400 avg loss -2.764591 avg loss no lamb -2.764591 time 2020-06-26 08:54:14.594249
Model ind 665 epoch 559 batch: 500 avg loss -2.756096 avg loss no lamb -2.756096 time 2020-06-26 08:54:25.105443
Model ind 665 epoch 559 batch: 600 avg loss -2.832391 avg loss no lamb -2.832391 time 2020-06-26 08:54:35.799891
Model ind 665 epoch 559 batch: 700 avg loss -2.675504 avg loss no lamb -2.675504 time 2020-06-26 08:54:46.620273
Model ind 665 epoch 559 batch: 800 avg loss -2.818800 avg loss no lamb -2.818800 time 2020-06-26 08:54:57.253970
last batch sz 10
Pre: time 2020-06-26 08:55:11.429136: 
 	std: 0.0021378496
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9804, 0.977, 0.9825, 0.9785]
	train_accs: [0.9823167, 0.98105, 0.97693336, 0.98251665, 0.9779]
	best_train_sub_head: 3
	worst: 0.977
	avg: 0.98014003
	best: 0.9825

Starting e_i: 560
Model ind 665 epoch 560 batch: 0 avg loss -2.936163 avg loss no lamb -2.936163 time 2020-06-26 08:55:12.361745
Model ind 665 epoch 560 batch: 100 avg loss -2.810789 avg loss no lamb -2.810789 time 2020-06-26 08:55:22.968352
Model ind 665 epoch 560 batch: 200 avg loss -2.868015 avg loss no lamb -2.868015 time 2020-06-26 08:55:33.657810
Model ind 665 epoch 560 batch: 300 avg loss -2.901858 avg loss no lamb -2.901858 time 2020-06-26 08:55:44.222142
Model ind 665 epoch 560 batch: 400 avg loss -2.740159 avg loss no lamb -2.740159 time 2020-06-26 08:55:54.945074
Model ind 665 epoch 560 batch: 500 avg loss -2.799476 avg loss no lamb -2.799476 time 2020-06-26 08:56:05.449936
Model ind 665 epoch 560 batch: 600 avg loss -2.824769 avg loss no lamb -2.824769 time 2020-06-26 08:56:16.059606
Model ind 665 epoch 560 batch: 700 avg loss -2.774879 avg loss no lamb -2.774879 time 2020-06-26 08:56:26.498909
Model ind 665 epoch 560 batch: 800 avg loss -2.812813 avg loss no lamb -2.812813 time 2020-06-26 08:56:37.085445
last batch sz 10
Pre: time 2020-06-26 08:56:50.944151: 
 	std: 0.002691909
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9803, 0.9759, 0.9816, 0.9757]
	train_accs: [0.9819, 0.9809, 0.97648335, 0.9819, 0.9766833]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97904
	best: 0.9817

Starting e_i: 561
Model ind 665 epoch 561 batch: 0 avg loss -2.920277 avg loss no lamb -2.920277 time 2020-06-26 08:56:52.976802
Model ind 665 epoch 561 batch: 100 avg loss -2.867366 avg loss no lamb -2.867366 time 2020-06-26 08:57:03.747694
Model ind 665 epoch 561 batch: 200 avg loss -2.874514 avg loss no lamb -2.874514 time 2020-06-26 08:57:14.273487
Model ind 665 epoch 561 batch: 300 avg loss -2.841795 avg loss no lamb -2.841795 time 2020-06-26 08:57:25.117977
Model ind 665 epoch 561 batch: 400 avg loss -2.755286 avg loss no lamb -2.755286 time 2020-06-26 08:57:35.826520
Model ind 665 epoch 561 batch: 500 avg loss -2.792212 avg loss no lamb -2.792212 time 2020-06-26 08:57:46.645092
Model ind 665 epoch 561 batch: 600 avg loss -2.809306 avg loss no lamb -2.809306 time 2020-06-26 08:57:57.245492
Model ind 665 epoch 561 batch: 700 avg loss -2.783460 avg loss no lamb -2.783460 time 2020-06-26 08:58:07.945875
Model ind 665 epoch 561 batch: 800 avg loss -2.822278 avg loss no lamb -2.822278 time 2020-06-26 08:58:18.680298
last batch sz 10
Pre: time 2020-06-26 08:58:32.695487: 
 	std: 0.0021228313
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9799, 0.9751, 0.9803, 0.9768]
	train_accs: [0.9816667, 0.9813833, 0.9766, 0.98188335, 0.97735]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97846
	best: 0.9803

Starting e_i: 562
Model ind 665 epoch 562 batch: 0 avg loss -2.895522 avg loss no lamb -2.895522 time 2020-06-26 08:58:33.605096
Model ind 665 epoch 562 batch: 100 avg loss -2.841835 avg loss no lamb -2.841835 time 2020-06-26 08:58:44.270135
Model ind 665 epoch 562 batch: 200 avg loss -2.799090 avg loss no lamb -2.799090 time 2020-06-26 08:58:54.879127
Model ind 665 epoch 562 batch: 300 avg loss -2.745396 avg loss no lamb -2.745396 time 2020-06-26 08:59:05.745181
Model ind 665 epoch 562 batch: 400 avg loss -2.747871 avg loss no lamb -2.747871 time 2020-06-26 08:59:16.321449
Model ind 665 epoch 562 batch: 500 avg loss -2.866770 avg loss no lamb -2.866770 time 2020-06-26 08:59:26.767404
Model ind 665 epoch 562 batch: 600 avg loss -2.811304 avg loss no lamb -2.811304 time 2020-06-26 08:59:37.375076
Model ind 665 epoch 562 batch: 700 avg loss -2.702495 avg loss no lamb -2.702495 time 2020-06-26 08:59:48.252428
Model ind 665 epoch 562 batch: 800 avg loss -2.796470 avg loss no lamb -2.796470 time 2020-06-26 08:59:59.250256
last batch sz 10
Pre: time 2020-06-26 09:00:13.280564: 
 	std: 0.0034833325
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9806, 0.9738, 0.9815, 0.9752]
	train_accs: [0.98226666, 0.98153335, 0.97608334, 0.98215, 0.97688335]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97868
	best: 0.9823

Starting e_i: 563
Model ind 665 epoch 563 batch: 0 avg loss -2.924858 avg loss no lamb -2.924858 time 2020-06-26 09:00:14.157669
Model ind 665 epoch 563 batch: 100 avg loss -2.813394 avg loss no lamb -2.813394 time 2020-06-26 09:00:24.830716
Model ind 665 epoch 563 batch: 200 avg loss -2.841503 avg loss no lamb -2.841503 time 2020-06-26 09:00:35.672218
Model ind 665 epoch 563 batch: 300 avg loss -2.888821 avg loss no lamb -2.888821 time 2020-06-26 09:00:46.557824
Model ind 665 epoch 563 batch: 400 avg loss -2.686894 avg loss no lamb -2.686894 time 2020-06-26 09:00:57.307182
Model ind 665 epoch 563 batch: 500 avg loss -2.814249 avg loss no lamb -2.814249 time 2020-06-26 09:01:08.078205
Model ind 665 epoch 563 batch: 600 avg loss -2.851119 avg loss no lamb -2.851119 time 2020-06-26 09:01:18.794033
Model ind 665 epoch 563 batch: 700 avg loss -2.704452 avg loss no lamb -2.704452 time 2020-06-26 09:01:29.722581
Model ind 665 epoch 563 batch: 800 avg loss -2.817574 avg loss no lamb -2.817574 time 2020-06-26 09:01:40.416644
last batch sz 10
Pre: time 2020-06-26 09:01:54.370913: 
 	std: 0.0031012243
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9792, 0.9735, 0.9812, 0.9758]
	train_accs: [0.98146665, 0.98075, 0.9756167, 0.9815, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97822
	best: 0.9812

Starting e_i: 564
Model ind 665 epoch 564 batch: 0 avg loss -2.904979 avg loss no lamb -2.904979 time 2020-06-26 09:01:55.248496
Model ind 665 epoch 564 batch: 100 avg loss -2.858219 avg loss no lamb -2.858219 time 2020-06-26 09:02:05.918723
Model ind 665 epoch 564 batch: 200 avg loss -2.831531 avg loss no lamb -2.831531 time 2020-06-26 09:02:16.647901
Model ind 665 epoch 564 batch: 300 avg loss -2.849745 avg loss no lamb -2.849745 time 2020-06-26 09:02:27.376360
Model ind 665 epoch 564 batch: 400 avg loss -2.736486 avg loss no lamb -2.736486 time 2020-06-26 09:02:37.883776
Model ind 665 epoch 564 batch: 500 avg loss -2.806261 avg loss no lamb -2.806261 time 2020-06-26 09:02:48.541324
Model ind 665 epoch 564 batch: 600 avg loss -2.847922 avg loss no lamb -2.847922 time 2020-06-26 09:02:59.121196
Model ind 665 epoch 564 batch: 700 avg loss -2.632209 avg loss no lamb -2.632209 time 2020-06-26 09:03:09.959755
Model ind 665 epoch 564 batch: 800 avg loss -2.842180 avg loss no lamb -2.842180 time 2020-06-26 09:03:20.859665
last batch sz 10
Pre: time 2020-06-26 09:03:34.794035: 
 	std: 0.0020895964
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9804, 0.9756, 0.9808, 0.9769]
	train_accs: [0.9813667, 0.9811, 0.97643334, 0.9813333, 0.97675]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97874004
	best: 0.98

Starting e_i: 565
Model ind 665 epoch 565 batch: 0 avg loss -2.879095 avg loss no lamb -2.879095 time 2020-06-26 09:03:35.768717
Model ind 665 epoch 565 batch: 100 avg loss -2.810816 avg loss no lamb -2.810816 time 2020-06-26 09:03:46.508117
Model ind 665 epoch 565 batch: 200 avg loss -2.836022 avg loss no lamb -2.836022 time 2020-06-26 09:03:57.225752
Model ind 665 epoch 565 batch: 300 avg loss -2.842873 avg loss no lamb -2.842873 time 2020-06-26 09:04:08.172956
Model ind 665 epoch 565 batch: 400 avg loss -2.715615 avg loss no lamb -2.715615 time 2020-06-26 09:04:19.146011
Model ind 665 epoch 565 batch: 500 avg loss -2.782263 avg loss no lamb -2.782263 time 2020-06-26 09:04:29.885474
Model ind 665 epoch 565 batch: 600 avg loss -2.856542 avg loss no lamb -2.856542 time 2020-06-26 09:04:40.674007
Model ind 665 epoch 565 batch: 700 avg loss -2.746417 avg loss no lamb -2.746417 time 2020-06-26 09:04:51.307822
Model ind 665 epoch 565 batch: 800 avg loss -2.870165 avg loss no lamb -2.870165 time 2020-06-26 09:05:02.541781
last batch sz 10
Pre: time 2020-06-26 09:05:16.443879: 
 	std: 0.0028135309
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9805, 0.9746, 0.9808, 0.9758]
	train_accs: [0.9814, 0.98066664, 0.97573334, 0.98145, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.9785999
	best: 0.9808

Starting e_i: 566
Model ind 665 epoch 566 batch: 0 avg loss -2.897797 avg loss no lamb -2.897797 time 2020-06-26 09:05:17.361250
Model ind 665 epoch 566 batch: 100 avg loss -2.876901 avg loss no lamb -2.876901 time 2020-06-26 09:05:27.997088
Model ind 665 epoch 566 batch: 200 avg loss -2.812616 avg loss no lamb -2.812616 time 2020-06-26 09:05:38.685167
Model ind 665 epoch 566 batch: 300 avg loss -2.788765 avg loss no lamb -2.788765 time 2020-06-26 09:05:49.167042
Model ind 665 epoch 566 batch: 400 avg loss -2.705924 avg loss no lamb -2.705924 time 2020-06-26 09:05:59.997870
Model ind 665 epoch 566 batch: 500 avg loss -2.761137 avg loss no lamb -2.761137 time 2020-06-26 09:06:10.711780
Model ind 665 epoch 566 batch: 600 avg loss -2.809001 avg loss no lamb -2.809001 time 2020-06-26 09:06:21.469265
Model ind 665 epoch 566 batch: 700 avg loss -2.769645 avg loss no lamb -2.769645 time 2020-06-26 09:06:32.197086
Model ind 665 epoch 566 batch: 800 avg loss -2.818806 avg loss no lamb -2.818806 time 2020-06-26 09:06:42.926993
last batch sz 10
Pre: time 2020-06-26 09:06:56.930280: 
 	std: 0.003192865
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9779, 0.9722, 0.9801, 0.9743]
	train_accs: [0.9808, 0.9795833, 0.9744, 0.9809167, 0.9754]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97694
	best: 0.9801

Starting e_i: 567
Model ind 665 epoch 567 batch: 0 avg loss -2.929620 avg loss no lamb -2.929620 time 2020-06-26 09:06:57.954555
Model ind 665 epoch 567 batch: 100 avg loss -2.907269 avg loss no lamb -2.907269 time 2020-06-26 09:07:08.359032
Model ind 665 epoch 567 batch: 200 avg loss -2.842126 avg loss no lamb -2.842126 time 2020-06-26 09:07:19.024489
Model ind 665 epoch 567 batch: 300 avg loss -2.820295 avg loss no lamb -2.820295 time 2020-06-26 09:07:29.843832
Model ind 665 epoch 567 batch: 400 avg loss -2.765161 avg loss no lamb -2.765161 time 2020-06-26 09:07:40.646688
Model ind 665 epoch 567 batch: 500 avg loss -2.790863 avg loss no lamb -2.790863 time 2020-06-26 09:07:51.376828
Model ind 665 epoch 567 batch: 600 avg loss -2.837991 avg loss no lamb -2.837991 time 2020-06-26 09:08:02.047238
Model ind 665 epoch 567 batch: 700 avg loss -2.659350 avg loss no lamb -2.659350 time 2020-06-26 09:08:12.848346
Model ind 665 epoch 567 batch: 800 avg loss -2.800449 avg loss no lamb -2.800449 time 2020-06-26 09:08:23.871921
last batch sz 10
Pre: time 2020-06-26 09:08:38.057957: 
 	std: 0.0028652456
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9814, 0.9754, 0.9817, 0.9764]
	train_accs: [0.9823667, 0.98165, 0.97693336, 0.9822, 0.97705]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97937995
	best: 0.982

Starting e_i: 568
Model ind 665 epoch 568 batch: 0 avg loss -2.897847 avg loss no lamb -2.897847 time 2020-06-26 09:08:38.983830
Model ind 665 epoch 568 batch: 100 avg loss -2.835420 avg loss no lamb -2.835420 time 2020-06-26 09:08:49.652942
Model ind 665 epoch 568 batch: 200 avg loss -2.862170 avg loss no lamb -2.862170 time 2020-06-26 09:09:00.076566
Model ind 665 epoch 568 batch: 300 avg loss -2.752391 avg loss no lamb -2.752391 time 2020-06-26 09:09:10.769965
Model ind 665 epoch 568 batch: 400 avg loss -2.687360 avg loss no lamb -2.687360 time 2020-06-26 09:09:21.446058
Model ind 665 epoch 568 batch: 500 avg loss -2.809411 avg loss no lamb -2.809411 time 2020-06-26 09:09:32.356668
Model ind 665 epoch 568 batch: 600 avg loss -2.828088 avg loss no lamb -2.828088 time 2020-06-26 09:09:43.243194
Model ind 665 epoch 568 batch: 700 avg loss -2.702930 avg loss no lamb -2.702930 time 2020-06-26 09:09:54.032319
Model ind 665 epoch 568 batch: 800 avg loss -2.791429 avg loss no lamb -2.791429 time 2020-06-26 09:10:04.986906
last batch sz 10
Pre: time 2020-06-26 09:10:18.938148: 
 	std: 0.0033355039
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9833, 0.9822, 0.9753, 0.9825, 0.9766]
	train_accs: [0.98178333, 0.98116666, 0.9759, 0.9817, 0.9766]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97998
	best: 0.9833

Starting e_i: 569
Model ind 665 epoch 569 batch: 0 avg loss -2.943853 avg loss no lamb -2.943853 time 2020-06-26 09:10:19.818407
Model ind 665 epoch 569 batch: 100 avg loss -2.843367 avg loss no lamb -2.843367 time 2020-06-26 09:10:30.484984
Model ind 665 epoch 569 batch: 200 avg loss -2.867440 avg loss no lamb -2.867440 time 2020-06-26 09:10:41.372078
Model ind 665 epoch 569 batch: 300 avg loss -2.840248 avg loss no lamb -2.840248 time 2020-06-26 09:10:52.549757
Model ind 665 epoch 569 batch: 400 avg loss -2.832972 avg loss no lamb -2.832972 time 2020-06-26 09:11:03.549363
Model ind 665 epoch 569 batch: 500 avg loss -2.807830 avg loss no lamb -2.807830 time 2020-06-26 09:11:14.355916
Model ind 665 epoch 569 batch: 600 avg loss -2.856084 avg loss no lamb -2.856084 time 2020-06-26 09:11:25.119798
Model ind 665 epoch 569 batch: 700 avg loss -2.700637 avg loss no lamb -2.700637 time 2020-06-26 09:11:35.791500
Model ind 665 epoch 569 batch: 800 avg loss -2.822435 avg loss no lamb -2.822435 time 2020-06-26 09:11:46.709702
last batch sz 10
Pre: time 2020-06-26 09:12:00.679495: 
 	std: 0.0027982933
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9798, 0.9738, 0.9804, 0.9762]
	train_accs: [0.98125, 0.98001665, 0.9754, 0.98106664, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97826004
	best: 0.9811

Starting e_i: 570
Model ind 665 epoch 570 batch: 0 avg loss -2.949954 avg loss no lamb -2.949954 time 2020-06-26 09:12:01.547777
Model ind 665 epoch 570 batch: 100 avg loss -2.831193 avg loss no lamb -2.831193 time 2020-06-26 09:12:12.374439
Model ind 665 epoch 570 batch: 200 avg loss -2.812833 avg loss no lamb -2.812833 time 2020-06-26 09:12:23.147653
Model ind 665 epoch 570 batch: 300 avg loss -2.787260 avg loss no lamb -2.787260 time 2020-06-26 09:12:33.925437
Model ind 665 epoch 570 batch: 400 avg loss -2.774291 avg loss no lamb -2.774291 time 2020-06-26 09:12:44.755430
Model ind 665 epoch 570 batch: 500 avg loss -2.798003 avg loss no lamb -2.798003 time 2020-06-26 09:12:55.375911
Model ind 665 epoch 570 batch: 600 avg loss -2.825909 avg loss no lamb -2.825909 time 2020-06-26 09:13:06.357135
Model ind 665 epoch 570 batch: 700 avg loss -2.677012 avg loss no lamb -2.677012 time 2020-06-26 09:13:17.230823
Model ind 665 epoch 570 batch: 800 avg loss -2.834931 avg loss no lamb -2.834931 time 2020-06-26 09:13:27.953622
last batch sz 10
Pre: time 2020-06-26 09:13:41.989705: 
 	std: 0.0028322453
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9794, 0.9738, 0.9802, 0.9747]
	train_accs: [0.98125, 0.98035, 0.9755667, 0.9812, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97768
	best: 0.9803

Starting e_i: 571
Model ind 665 epoch 571 batch: 0 avg loss -2.864261 avg loss no lamb -2.864261 time 2020-06-26 09:13:44.009634
Model ind 665 epoch 571 batch: 100 avg loss -2.857974 avg loss no lamb -2.857974 time 2020-06-26 09:13:54.859711
Model ind 665 epoch 571 batch: 200 avg loss -2.856854 avg loss no lamb -2.856854 time 2020-06-26 09:14:05.672367
Model ind 665 epoch 571 batch: 300 avg loss -2.804098 avg loss no lamb -2.804098 time 2020-06-26 09:14:16.343998
Model ind 665 epoch 571 batch: 400 avg loss -2.729231 avg loss no lamb -2.729231 time 2020-06-26 09:14:26.986392
Model ind 665 epoch 571 batch: 500 avg loss -2.825407 avg loss no lamb -2.825407 time 2020-06-26 09:14:37.556381
Model ind 665 epoch 571 batch: 600 avg loss -2.805969 avg loss no lamb -2.805969 time 2020-06-26 09:14:48.337395
Model ind 665 epoch 571 batch: 700 avg loss -2.728374 avg loss no lamb -2.728374 time 2020-06-26 09:14:59.131767
Model ind 665 epoch 571 batch: 800 avg loss -2.789788 avg loss no lamb -2.789788 time 2020-06-26 09:15:09.926164
last batch sz 10
Pre: time 2020-06-26 09:15:23.897291: 
 	std: 0.0028436594
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9807, 0.9743, 0.9808, 0.9765]
	train_accs: [0.98186666, 0.98095, 0.97583336, 0.98151666, 0.97735]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97875994
	best: 0.9815

Starting e_i: 572
Model ind 665 epoch 572 batch: 0 avg loss -2.903663 avg loss no lamb -2.903663 time 2020-06-26 09:15:24.867406
Model ind 665 epoch 572 batch: 100 avg loss -2.864235 avg loss no lamb -2.864235 time 2020-06-26 09:15:35.649760
Model ind 665 epoch 572 batch: 200 avg loss -2.854990 avg loss no lamb -2.854990 time 2020-06-26 09:15:46.538035
Model ind 665 epoch 572 batch: 300 avg loss -2.833109 avg loss no lamb -2.833109 time 2020-06-26 09:15:57.435744
Model ind 665 epoch 572 batch: 400 avg loss -2.755521 avg loss no lamb -2.755521 time 2020-06-26 09:16:08.386642
Model ind 665 epoch 572 batch: 500 avg loss -2.824175 avg loss no lamb -2.824175 time 2020-06-26 09:16:19.159849
Model ind 665 epoch 572 batch: 600 avg loss -2.785888 avg loss no lamb -2.785888 time 2020-06-26 09:16:29.984314
Model ind 665 epoch 572 batch: 700 avg loss -2.714524 avg loss no lamb -2.714524 time 2020-06-26 09:16:40.844998
Model ind 665 epoch 572 batch: 800 avg loss -2.827126 avg loss no lamb -2.827126 time 2020-06-26 09:16:51.667972
last batch sz 10
Pre: time 2020-06-26 09:17:05.953793: 
 	std: 0.0024358942
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9827, 0.9826, 0.9773, 0.9832, 0.9786]
	train_accs: [0.98265, 0.98188335, 0.9773667, 0.9827333, 0.9779]
	best_train_sub_head: 3
	worst: 0.9773
	avg: 0.98087996
	best: 0.9832

Starting e_i: 573
Model ind 665 epoch 573 batch: 0 avg loss -2.953905 avg loss no lamb -2.953905 time 2020-06-26 09:17:06.881171
Model ind 665 epoch 573 batch: 100 avg loss -2.800299 avg loss no lamb -2.800299 time 2020-06-26 09:17:17.626083
Model ind 665 epoch 573 batch: 200 avg loss -2.781652 avg loss no lamb -2.781652 time 2020-06-26 09:17:28.323094
Model ind 665 epoch 573 batch: 300 avg loss -2.831749 avg loss no lamb -2.831749 time 2020-06-26 09:17:39.089567
Model ind 665 epoch 573 batch: 400 avg loss -2.779533 avg loss no lamb -2.779533 time 2020-06-26 09:17:50.094227
Model ind 665 epoch 573 batch: 500 avg loss -2.861261 avg loss no lamb -2.861261 time 2020-06-26 09:18:00.875863
Model ind 665 epoch 573 batch: 600 avg loss -2.860323 avg loss no lamb -2.860323 time 2020-06-26 09:18:11.667778
Model ind 665 epoch 573 batch: 700 avg loss -2.675631 avg loss no lamb -2.675631 time 2020-06-26 09:18:22.194819
Model ind 665 epoch 573 batch: 800 avg loss -2.820148 avg loss no lamb -2.820148 time 2020-06-26 09:18:32.850549
last batch sz 10
Pre: time 2020-06-26 09:18:47.084266: 
 	std: 0.0031167993
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9837, 0.9837, 0.9772, 0.984, 0.9777]
	train_accs: [0.9827833, 0.98216665, 0.9774, 0.98261666, 0.9776833]
	best_train_sub_head: 0
	worst: 0.9772
	avg: 0.98125994
	best: 0.9837

Starting e_i: 574
Model ind 665 epoch 574 batch: 0 avg loss -2.938407 avg loss no lamb -2.938407 time 2020-06-26 09:18:47.931611
Model ind 665 epoch 574 batch: 100 avg loss -2.835321 avg loss no lamb -2.835321 time 2020-06-26 09:18:58.690924
Model ind 665 epoch 574 batch: 200 avg loss -2.790118 avg loss no lamb -2.790118 time 2020-06-26 09:19:09.563710
Model ind 665 epoch 574 batch: 300 avg loss -2.847031 avg loss no lamb -2.847031 time 2020-06-26 09:19:20.329710
Model ind 665 epoch 574 batch: 400 avg loss -2.715706 avg loss no lamb -2.715706 time 2020-06-26 09:19:31.077829
Model ind 665 epoch 574 batch: 500 avg loss -2.704409 avg loss no lamb -2.704409 time 2020-06-26 09:19:41.947463
Model ind 665 epoch 574 batch: 600 avg loss -2.845016 avg loss no lamb -2.845016 time 2020-06-26 09:19:52.807554
Model ind 665 epoch 574 batch: 700 avg loss -2.680337 avg loss no lamb -2.680337 time 2020-06-26 09:20:03.576044
Model ind 665 epoch 574 batch: 800 avg loss -2.839399 avg loss no lamb -2.839399 time 2020-06-26 09:20:14.437252
last batch sz 10
Pre: time 2020-06-26 09:20:28.309064: 
 	std: 0.0027931326
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9809, 0.9749, 0.9821, 0.9771]
	train_accs: [0.9820667, 0.98178333, 0.9765667, 0.98226666, 0.9770333]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97928
	best: 0.9821

Starting e_i: 575
Model ind 665 epoch 575 batch: 0 avg loss -2.947332 avg loss no lamb -2.947332 time 2020-06-26 09:20:29.235452
Model ind 665 epoch 575 batch: 100 avg loss -2.869067 avg loss no lamb -2.869067 time 2020-06-26 09:20:40.055310
Model ind 665 epoch 575 batch: 200 avg loss -2.810921 avg loss no lamb -2.810921 time 2020-06-26 09:20:50.659154
Model ind 665 epoch 575 batch: 300 avg loss -2.822048 avg loss no lamb -2.822048 time 2020-06-26 09:21:01.196165
Model ind 665 epoch 575 batch: 400 avg loss -2.718987 avg loss no lamb -2.718987 time 2020-06-26 09:21:11.808969
Model ind 665 epoch 575 batch: 500 avg loss -2.798036 avg loss no lamb -2.798036 time 2020-06-26 09:21:22.468335
Model ind 665 epoch 575 batch: 600 avg loss -2.830272 avg loss no lamb -2.830272 time 2020-06-26 09:21:33.439669
Model ind 665 epoch 575 batch: 700 avg loss -2.719454 avg loss no lamb -2.719454 time 2020-06-26 09:21:44.064115
Model ind 665 epoch 575 batch: 800 avg loss -2.844768 avg loss no lamb -2.844768 time 2020-06-26 09:21:54.718252
last batch sz 10
Pre: time 2020-06-26 09:22:08.806657: 
 	std: 0.002172917
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9809, 0.9761, 0.9813, 0.9777]
	train_accs: [0.9819667, 0.98178333, 0.9769667, 0.9819, 0.97798336]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.97947997
	best: 0.9814

Starting e_i: 576
Model ind 665 epoch 576 batch: 0 avg loss -2.937940 avg loss no lamb -2.937940 time 2020-06-26 09:22:09.727383
Model ind 665 epoch 576 batch: 100 avg loss -2.907690 avg loss no lamb -2.907690 time 2020-06-26 09:22:20.444956
Model ind 665 epoch 576 batch: 200 avg loss -2.788340 avg loss no lamb -2.788340 time 2020-06-26 09:22:31.078871
Model ind 665 epoch 576 batch: 300 avg loss -2.858781 avg loss no lamb -2.858781 time 2020-06-26 09:22:41.914070
Model ind 665 epoch 576 batch: 400 avg loss -2.744278 avg loss no lamb -2.744278 time 2020-06-26 09:22:52.524279
Model ind 665 epoch 576 batch: 500 avg loss -2.785261 avg loss no lamb -2.785261 time 2020-06-26 09:23:03.252060
Model ind 665 epoch 576 batch: 600 avg loss -2.849987 avg loss no lamb -2.849987 time 2020-06-26 09:23:13.976176
Model ind 665 epoch 576 batch: 700 avg loss -2.691312 avg loss no lamb -2.691312 time 2020-06-26 09:23:24.765517
Model ind 665 epoch 576 batch: 800 avg loss -2.794626 avg loss no lamb -2.794626 time 2020-06-26 09:23:35.787931
last batch sz 10
Pre: time 2020-06-26 09:23:49.894763: 
 	std: 0.0026437808
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9799, 0.975, 0.9815, 0.9763]
	train_accs: [0.9819833, 0.98108333, 0.9762667, 0.9821, 0.9773]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97878
	best: 0.9815

Starting e_i: 577
Model ind 665 epoch 577 batch: 0 avg loss -2.861211 avg loss no lamb -2.861211 time 2020-06-26 09:23:50.913739
Model ind 665 epoch 577 batch: 100 avg loss -2.887853 avg loss no lamb -2.887853 time 2020-06-26 09:24:01.794472
Model ind 665 epoch 577 batch: 200 avg loss -2.917586 avg loss no lamb -2.917586 time 2020-06-26 09:24:12.521827
Model ind 665 epoch 577 batch: 300 avg loss -2.836690 avg loss no lamb -2.836690 time 2020-06-26 09:24:23.454713
Model ind 665 epoch 577 batch: 400 avg loss -2.757849 avg loss no lamb -2.757849 time 2020-06-26 09:24:34.182618
Model ind 665 epoch 577 batch: 500 avg loss -2.778312 avg loss no lamb -2.778312 time 2020-06-26 09:24:45.000629
Model ind 665 epoch 577 batch: 600 avg loss -2.836893 avg loss no lamb -2.836893 time 2020-06-26 09:24:55.580736
Model ind 665 epoch 577 batch: 700 avg loss -2.722706 avg loss no lamb -2.722706 time 2020-06-26 09:25:06.436864
Model ind 665 epoch 577 batch: 800 avg loss -2.803923 avg loss no lamb -2.803923 time 2020-06-26 09:25:16.976779
last batch sz 10
Pre: time 2020-06-26 09:25:30.886912: 
 	std: 0.0029489088
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9811, 0.9754, 0.9816, 0.9756]
	train_accs: [0.98211664, 0.9816833, 0.97576666, 0.98216665, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97910005
	best: 0.9816

Starting e_i: 578
Model ind 665 epoch 578 batch: 0 avg loss -2.930666 avg loss no lamb -2.930666 time 2020-06-26 09:25:31.811390
Model ind 665 epoch 578 batch: 100 avg loss -2.874231 avg loss no lamb -2.874231 time 2020-06-26 09:25:42.476435
Model ind 665 epoch 578 batch: 200 avg loss -2.833069 avg loss no lamb -2.833069 time 2020-06-26 09:25:53.034793
Model ind 665 epoch 578 batch: 300 avg loss -2.786985 avg loss no lamb -2.786985 time 2020-06-26 09:26:03.868330
Model ind 665 epoch 578 batch: 400 avg loss -2.750197 avg loss no lamb -2.750197 time 2020-06-26 09:26:14.634248
Model ind 665 epoch 578 batch: 500 avg loss -2.811991 avg loss no lamb -2.811991 time 2020-06-26 09:26:25.477260
Model ind 665 epoch 578 batch: 600 avg loss -2.801833 avg loss no lamb -2.801833 time 2020-06-26 09:26:36.120491
Model ind 665 epoch 578 batch: 700 avg loss -2.703928 avg loss no lamb -2.703928 time 2020-06-26 09:26:46.746790
Model ind 665 epoch 578 batch: 800 avg loss -2.854457 avg loss no lamb -2.854457 time 2020-06-26 09:26:57.538199
last batch sz 10
Pre: time 2020-06-26 09:27:11.661366: 
 	std: 0.002884025
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9809, 0.9751, 0.9819, 0.9764]
	train_accs: [0.9821, 0.98116666, 0.97643334, 0.98228335, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97922003
	best: 0.9819

Starting e_i: 579
Model ind 665 epoch 579 batch: 0 avg loss -2.946294 avg loss no lamb -2.946294 time 2020-06-26 09:27:12.673326
Model ind 665 epoch 579 batch: 100 avg loss -2.831543 avg loss no lamb -2.831543 time 2020-06-26 09:27:23.383476
Model ind 665 epoch 579 batch: 200 avg loss -2.814951 avg loss no lamb -2.814951 time 2020-06-26 09:27:34.003293
Model ind 665 epoch 579 batch: 300 avg loss -2.836637 avg loss no lamb -2.836637 time 2020-06-26 09:27:45.029600
Model ind 665 epoch 579 batch: 400 avg loss -2.771952 avg loss no lamb -2.771952 time 2020-06-26 09:27:55.673471
Model ind 665 epoch 579 batch: 500 avg loss -2.808264 avg loss no lamb -2.808264 time 2020-06-26 09:28:06.380224
Model ind 665 epoch 579 batch: 600 avg loss -2.833529 avg loss no lamb -2.833529 time 2020-06-26 09:28:16.849119
Model ind 665 epoch 579 batch: 700 avg loss -2.702311 avg loss no lamb -2.702311 time 2020-06-26 09:28:27.572567
Model ind 665 epoch 579 batch: 800 avg loss -2.826874 avg loss no lamb -2.826874 time 2020-06-26 09:28:38.498103
last batch sz 10
Pre: time 2020-06-26 09:28:52.661743: 
 	std: 0.0029953246
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9798, 0.9735, 0.9805, 0.9751]
	train_accs: [0.9816833, 0.98113334, 0.97531664, 0.9816667, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.9778999
	best: 0.9806

Starting e_i: 580
Model ind 665 epoch 580 batch: 0 avg loss -2.901593 avg loss no lamb -2.901593 time 2020-06-26 09:28:53.582347
Model ind 665 epoch 580 batch: 100 avg loss -2.817074 avg loss no lamb -2.817074 time 2020-06-26 09:29:04.187113
Model ind 665 epoch 580 batch: 200 avg loss -2.841070 avg loss no lamb -2.841070 time 2020-06-26 09:29:15.050841
Model ind 665 epoch 580 batch: 300 avg loss -2.854647 avg loss no lamb -2.854647 time 2020-06-26 09:29:25.859134
Model ind 665 epoch 580 batch: 400 avg loss -2.763226 avg loss no lamb -2.763226 time 2020-06-26 09:29:36.251166
Model ind 665 epoch 580 batch: 500 avg loss -2.769187 avg loss no lamb -2.769187 time 2020-06-26 09:29:46.780843
Model ind 665 epoch 580 batch: 600 avg loss -2.835443 avg loss no lamb -2.835443 time 2020-06-26 09:29:57.525698
Model ind 665 epoch 580 batch: 700 avg loss -2.688393 avg loss no lamb -2.688393 time 2020-06-26 09:30:08.349260
Model ind 665 epoch 580 batch: 800 avg loss -2.769295 avg loss no lamb -2.769295 time 2020-06-26 09:30:18.951340
last batch sz 10
Pre: time 2020-06-26 09:30:32.897427: 
 	std: 0.0036416398
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.981, 0.9735, 0.9817, 0.9746]
	train_accs: [0.98188335, 0.9817, 0.9752, 0.98191667, 0.97576666]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97848
	best: 0.9817

Starting e_i: 581
Model ind 665 epoch 581 batch: 0 avg loss -2.946333 avg loss no lamb -2.946333 time 2020-06-26 09:30:34.914346
Model ind 665 epoch 581 batch: 100 avg loss -2.887780 avg loss no lamb -2.887780 time 2020-06-26 09:30:45.813032
Model ind 665 epoch 581 batch: 200 avg loss -2.874660 avg loss no lamb -2.874660 time 2020-06-26 09:30:56.653384
Model ind 665 epoch 581 batch: 300 avg loss -2.799471 avg loss no lamb -2.799471 time 2020-06-26 09:31:08.024372
Model ind 665 epoch 581 batch: 400 avg loss -2.741318 avg loss no lamb -2.741318 time 2020-06-26 09:31:18.589019
Model ind 665 epoch 581 batch: 500 avg loss -2.836161 avg loss no lamb -2.836161 time 2020-06-26 09:31:29.356801
Model ind 665 epoch 581 batch: 600 avg loss -2.836763 avg loss no lamb -2.836763 time 2020-06-26 09:31:39.881776
Model ind 665 epoch 581 batch: 700 avg loss -2.683347 avg loss no lamb -2.683347 time 2020-06-26 09:31:50.624875
Model ind 665 epoch 581 batch: 800 avg loss -2.767257 avg loss no lamb -2.767257 time 2020-06-26 09:32:01.408523
last batch sz 10
Pre: time 2020-06-26 09:32:15.446389: 
 	std: 0.002936401
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9799, 0.9736, 0.9812, 0.9766]
	train_accs: [0.9812, 0.98053336, 0.97533333, 0.9813833, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97846
	best: 0.9812

Starting e_i: 582
Model ind 665 epoch 582 batch: 0 avg loss -2.935015 avg loss no lamb -2.935015 time 2020-06-26 09:32:16.334624
Model ind 665 epoch 582 batch: 100 avg loss -2.882186 avg loss no lamb -2.882186 time 2020-06-26 09:32:27.249037
Model ind 665 epoch 582 batch: 200 avg loss -2.875053 avg loss no lamb -2.875053 time 2020-06-26 09:32:38.180468
Model ind 665 epoch 582 batch: 300 avg loss -2.834851 avg loss no lamb -2.834851 time 2020-06-26 09:32:48.472701
Model ind 665 epoch 582 batch: 400 avg loss -2.809872 avg loss no lamb -2.809872 time 2020-06-26 09:32:59.126321
Model ind 665 epoch 582 batch: 500 avg loss -2.817189 avg loss no lamb -2.817189 time 2020-06-26 09:33:09.897115
Model ind 665 epoch 582 batch: 600 avg loss -2.826531 avg loss no lamb -2.826531 time 2020-06-26 09:33:20.603499
Model ind 665 epoch 582 batch: 700 avg loss -2.722211 avg loss no lamb -2.722211 time 2020-06-26 09:33:31.272432
Model ind 665 epoch 582 batch: 800 avg loss -2.889181 avg loss no lamb -2.889181 time 2020-06-26 09:33:41.878145
last batch sz 10
Pre: time 2020-06-26 09:33:55.891086: 
 	std: 0.0028373296
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9815, 0.9751, 0.9818, 0.9768]
	train_accs: [0.982, 0.9814, 0.9767333, 0.9817333, 0.97748333]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97936
	best: 0.9816

Starting e_i: 583
Model ind 665 epoch 583 batch: 0 avg loss -2.984510 avg loss no lamb -2.984510 time 2020-06-26 09:33:56.765386
Model ind 665 epoch 583 batch: 100 avg loss -2.885768 avg loss no lamb -2.885768 time 2020-06-26 09:34:07.525321
Model ind 665 epoch 583 batch: 200 avg loss -2.815615 avg loss no lamb -2.815615 time 2020-06-26 09:34:18.154299
Model ind 665 epoch 583 batch: 300 avg loss -2.896446 avg loss no lamb -2.896446 time 2020-06-26 09:34:28.972502
Model ind 665 epoch 583 batch: 400 avg loss -2.777538 avg loss no lamb -2.777538 time 2020-06-26 09:34:39.456234
Model ind 665 epoch 583 batch: 500 avg loss -2.806435 avg loss no lamb -2.806435 time 2020-06-26 09:34:50.061741
Model ind 665 epoch 583 batch: 600 avg loss -2.816385 avg loss no lamb -2.816385 time 2020-06-26 09:35:00.852072
Model ind 665 epoch 583 batch: 700 avg loss -2.744889 avg loss no lamb -2.744889 time 2020-06-26 09:35:11.745162
Model ind 665 epoch 583 batch: 800 avg loss -2.852499 avg loss no lamb -2.852499 time 2020-06-26 09:35:22.395658
last batch sz 10
Pre: time 2020-06-26 09:35:36.061780: 
 	std: 0.0027737303
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9796, 0.9743, 0.9804, 0.9757]
	train_accs: [0.9813833, 0.9802333, 0.97515, 0.98088336, 0.97636664]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97827995
	best: 0.9814

Starting e_i: 584
Model ind 665 epoch 584 batch: 0 avg loss -2.941587 avg loss no lamb -2.941587 time 2020-06-26 09:35:37.010461
Model ind 665 epoch 584 batch: 100 avg loss -2.830004 avg loss no lamb -2.830004 time 2020-06-26 09:35:47.598100
Model ind 665 epoch 584 batch: 200 avg loss -2.875415 avg loss no lamb -2.875415 time 2020-06-26 09:35:58.412199
Model ind 665 epoch 584 batch: 300 avg loss -2.854615 avg loss no lamb -2.854615 time 2020-06-26 09:36:09.247983
Model ind 665 epoch 584 batch: 400 avg loss -2.736804 avg loss no lamb -2.736804 time 2020-06-26 09:36:19.716428
Model ind 665 epoch 584 batch: 500 avg loss -2.861449 avg loss no lamb -2.861449 time 2020-06-26 09:36:30.556028
Model ind 665 epoch 584 batch: 600 avg loss -2.844446 avg loss no lamb -2.844446 time 2020-06-26 09:36:41.431839
Model ind 665 epoch 584 batch: 700 avg loss -2.741445 avg loss no lamb -2.741445 time 2020-06-26 09:36:52.276987
Model ind 665 epoch 584 batch: 800 avg loss -2.871939 avg loss no lamb -2.871939 time 2020-06-26 09:37:03.103176
last batch sz 10
Pre: time 2020-06-26 09:37:17.062846: 
 	std: 0.0029193047
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9804, 0.975, 0.9819, 0.9764]
	train_accs: [0.98215, 0.9809833, 0.9765, 0.98226666, 0.9771]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97916
	best: 0.9819

Starting e_i: 585
Model ind 665 epoch 585 batch: 0 avg loss -2.931270 avg loss no lamb -2.931270 time 2020-06-26 09:37:17.921152
Model ind 665 epoch 585 batch: 100 avg loss -2.845993 avg loss no lamb -2.845993 time 2020-06-26 09:37:28.682715
Model ind 665 epoch 585 batch: 200 avg loss -2.849190 avg loss no lamb -2.849190 time 2020-06-26 09:37:39.244450
Model ind 665 epoch 585 batch: 300 avg loss -2.869211 avg loss no lamb -2.869211 time 2020-06-26 09:37:49.919732
Model ind 665 epoch 585 batch: 400 avg loss -2.741121 avg loss no lamb -2.741121 time 2020-06-26 09:38:00.592177
Model ind 665 epoch 585 batch: 500 avg loss -2.795184 avg loss no lamb -2.795184 time 2020-06-26 09:38:11.374179
Model ind 665 epoch 585 batch: 600 avg loss -2.841139 avg loss no lamb -2.841139 time 2020-06-26 09:38:22.172005
Model ind 665 epoch 585 batch: 700 avg loss -2.694080 avg loss no lamb -2.694080 time 2020-06-26 09:38:32.870484
Model ind 665 epoch 585 batch: 800 avg loss -2.871866 avg loss no lamb -2.871866 time 2020-06-26 09:38:43.636909
last batch sz 10
Pre: time 2020-06-26 09:38:57.556492: 
 	std: 0.0027339214
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9799, 0.9743, 0.9806, 0.9756]
	train_accs: [0.98183334, 0.98085, 0.976, 0.98181665, 0.9768]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97824
	best: 0.9808

Starting e_i: 586
Model ind 665 epoch 586 batch: 0 avg loss -2.859290 avg loss no lamb -2.859290 time 2020-06-26 09:38:58.585951
Model ind 665 epoch 586 batch: 100 avg loss -2.807178 avg loss no lamb -2.807178 time 2020-06-26 09:39:09.367176
Model ind 665 epoch 586 batch: 200 avg loss -2.901957 avg loss no lamb -2.901957 time 2020-06-26 09:39:20.225201
Model ind 665 epoch 586 batch: 300 avg loss -2.858640 avg loss no lamb -2.858640 time 2020-06-26 09:39:30.858363
Model ind 665 epoch 586 batch: 400 avg loss -2.714012 avg loss no lamb -2.714012 time 2020-06-26 09:39:41.481655
Model ind 665 epoch 586 batch: 500 avg loss -2.767704 avg loss no lamb -2.767704 time 2020-06-26 09:39:52.002324
Model ind 665 epoch 586 batch: 600 avg loss -2.843235 avg loss no lamb -2.843235 time 2020-06-26 09:40:02.644601
Model ind 665 epoch 586 batch: 700 avg loss -2.706005 avg loss no lamb -2.706005 time 2020-06-26 09:40:13.195104
Model ind 665 epoch 586 batch: 800 avg loss -2.811931 avg loss no lamb -2.811931 time 2020-06-26 09:40:23.911251
last batch sz 10
Pre: time 2020-06-26 09:40:37.705305: 
 	std: 0.0023567704
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.981, 0.9809, 0.9757, 0.981, 0.9767]
	train_accs: [0.9817333, 0.98195, 0.9769833, 0.98181665, 0.97756666]
	best_train_sub_head: 1
	worst: 0.9757
	avg: 0.97906
	best: 0.9809

Starting e_i: 587
Model ind 665 epoch 587 batch: 0 avg loss -2.945205 avg loss no lamb -2.945205 time 2020-06-26 09:40:38.649559
Model ind 665 epoch 587 batch: 100 avg loss -2.927943 avg loss no lamb -2.927943 time 2020-06-26 09:40:49.147617
Model ind 665 epoch 587 batch: 200 avg loss -2.828176 avg loss no lamb -2.828176 time 2020-06-26 09:41:00.106415
Model ind 665 epoch 587 batch: 300 avg loss -2.843370 avg loss no lamb -2.843370 time 2020-06-26 09:41:10.970122
Model ind 665 epoch 587 batch: 400 avg loss -2.779855 avg loss no lamb -2.779855 time 2020-06-26 09:41:21.662005
Model ind 665 epoch 587 batch: 500 avg loss -2.884022 avg loss no lamb -2.884022 time 2020-06-26 09:41:32.286414
Model ind 665 epoch 587 batch: 600 avg loss -2.825341 avg loss no lamb -2.825341 time 2020-06-26 09:41:42.697115
Model ind 665 epoch 587 batch: 700 avg loss -2.677586 avg loss no lamb -2.677586 time 2020-06-26 09:41:53.625160
Model ind 665 epoch 587 batch: 800 avg loss -2.844595 avg loss no lamb -2.844595 time 2020-06-26 09:42:04.446857
last batch sz 10
Pre: time 2020-06-26 09:42:18.622192: 
 	std: 0.003307206
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9811, 0.9735, 0.9814, 0.9761]
	train_accs: [0.98188335, 0.9812833, 0.97581667, 0.9816333, 0.9770333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97872
	best: 0.9815

Starting e_i: 588
Model ind 665 epoch 588 batch: 0 avg loss -2.921546 avg loss no lamb -2.921546 time 2020-06-26 09:42:19.524868
Model ind 665 epoch 588 batch: 100 avg loss -2.816549 avg loss no lamb -2.816549 time 2020-06-26 09:42:30.188688
Model ind 665 epoch 588 batch: 200 avg loss -2.833028 avg loss no lamb -2.833028 time 2020-06-26 09:42:40.960990
Model ind 665 epoch 588 batch: 300 avg loss -2.853359 avg loss no lamb -2.853359 time 2020-06-26 09:42:51.825296
Model ind 665 epoch 588 batch: 400 avg loss -2.764935 avg loss no lamb -2.764935 time 2020-06-26 09:43:02.710914
Model ind 665 epoch 588 batch: 500 avg loss -2.809019 avg loss no lamb -2.809019 time 2020-06-26 09:43:13.311925
Model ind 665 epoch 588 batch: 600 avg loss -2.837326 avg loss no lamb -2.837326 time 2020-06-26 09:43:23.844423
Model ind 665 epoch 588 batch: 700 avg loss -2.745222 avg loss no lamb -2.745222 time 2020-06-26 09:43:34.476343
Model ind 665 epoch 588 batch: 800 avg loss -2.793350 avg loss no lamb -2.793350 time 2020-06-26 09:43:45.435372
last batch sz 10
Pre: time 2020-06-26 09:43:59.242642: 
 	std: 0.0031090744
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9808, 0.9745, 0.9817, 0.9764]
	train_accs: [0.9820833, 0.98113334, 0.97538334, 0.9820167, 0.97748333]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97914
	best: 0.9823

Starting e_i: 589
Model ind 665 epoch 589 batch: 0 avg loss -2.934524 avg loss no lamb -2.934524 time 2020-06-26 09:44:00.137028
Model ind 665 epoch 589 batch: 100 avg loss -2.904004 avg loss no lamb -2.904004 time 2020-06-26 09:44:11.077916
Model ind 665 epoch 589 batch: 200 avg loss -2.798308 avg loss no lamb -2.798308 time 2020-06-26 09:44:21.632900
Model ind 665 epoch 589 batch: 300 avg loss -2.923148 avg loss no lamb -2.923148 time 2020-06-26 09:44:32.552612
Model ind 665 epoch 589 batch: 400 avg loss -2.684165 avg loss no lamb -2.684165 time 2020-06-26 09:44:43.166299
Model ind 665 epoch 589 batch: 500 avg loss -2.812047 avg loss no lamb -2.812047 time 2020-06-26 09:44:53.697162
Model ind 665 epoch 589 batch: 600 avg loss -2.842167 avg loss no lamb -2.842167 time 2020-06-26 09:45:04.481934
Model ind 665 epoch 589 batch: 700 avg loss -2.705457 avg loss no lamb -2.705457 time 2020-06-26 09:45:15.233832
Model ind 665 epoch 589 batch: 800 avg loss -2.844664 avg loss no lamb -2.844664 time 2020-06-26 09:45:25.921503
last batch sz 10
Pre: time 2020-06-26 09:45:39.814447: 
 	std: 0.0033456718
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9813, 0.9739, 0.9819, 0.9763]
	train_accs: [0.98235, 0.98191667, 0.9759333, 0.9824, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97908
	best: 0.9819

Starting e_i: 590
Model ind 665 epoch 590 batch: 0 avg loss -2.932641 avg loss no lamb -2.932641 time 2020-06-26 09:45:40.702688
Model ind 665 epoch 590 batch: 100 avg loss -2.853272 avg loss no lamb -2.853272 time 2020-06-26 09:45:51.971296
Model ind 665 epoch 590 batch: 200 avg loss -2.837065 avg loss no lamb -2.837065 time 2020-06-26 09:46:02.606192
Model ind 665 epoch 590 batch: 300 avg loss -2.822581 avg loss no lamb -2.822581 time 2020-06-26 09:46:13.303835
Model ind 665 epoch 590 batch: 400 avg loss -2.736087 avg loss no lamb -2.736087 time 2020-06-26 09:46:23.627388
Model ind 665 epoch 590 batch: 500 avg loss -2.838305 avg loss no lamb -2.838305 time 2020-06-26 09:46:34.564731
Model ind 665 epoch 590 batch: 600 avg loss -2.863628 avg loss no lamb -2.863628 time 2020-06-26 09:46:45.352185
Model ind 665 epoch 590 batch: 700 avg loss -2.754825 avg loss no lamb -2.754825 time 2020-06-26 09:46:55.985632
Model ind 665 epoch 590 batch: 800 avg loss -2.820855 avg loss no lamb -2.820855 time 2020-06-26 09:47:06.883406
last batch sz 10
Pre: time 2020-06-26 09:47:20.748554: 
 	std: 0.0027462174
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9815, 0.9754, 0.9822, 0.9777]
	train_accs: [0.98235, 0.98165, 0.9767333, 0.9823667, 0.9778]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97978
	best: 0.9822

Starting e_i: 591
Model ind 665 epoch 591 batch: 0 avg loss -2.962225 avg loss no lamb -2.962225 time 2020-06-26 09:47:22.879301
Model ind 665 epoch 591 batch: 100 avg loss -2.852605 avg loss no lamb -2.852605 time 2020-06-26 09:47:33.875338
Model ind 665 epoch 591 batch: 200 avg loss -2.899314 avg loss no lamb -2.899314 time 2020-06-26 09:47:44.556563
Model ind 665 epoch 591 batch: 300 avg loss -2.804616 avg loss no lamb -2.804616 time 2020-06-26 09:47:55.251637
Model ind 665 epoch 591 batch: 400 avg loss -2.762953 avg loss no lamb -2.762953 time 2020-06-26 09:48:05.967477
Model ind 665 epoch 591 batch: 500 avg loss -2.826773 avg loss no lamb -2.826773 time 2020-06-26 09:48:16.728278
Model ind 665 epoch 591 batch: 600 avg loss -2.872728 avg loss no lamb -2.872728 time 2020-06-26 09:48:27.628667
Model ind 665 epoch 591 batch: 700 avg loss -2.727792 avg loss no lamb -2.727792 time 2020-06-26 09:48:38.399995
Model ind 665 epoch 591 batch: 800 avg loss -2.860737 avg loss no lamb -2.860737 time 2020-06-26 09:48:49.069236
last batch sz 10
Pre: time 2020-06-26 09:49:02.956927: 
 	std: 0.0025015282
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9811, 0.9753, 0.9811, 0.9765]
	train_accs: [0.9820667, 0.9816167, 0.9766667, 0.9823667, 0.9772]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97892
	best: 0.9811

Starting e_i: 592
Model ind 665 epoch 592 batch: 0 avg loss -2.931670 avg loss no lamb -2.931670 time 2020-06-26 09:49:03.816209
Model ind 665 epoch 592 batch: 100 avg loss -2.855428 avg loss no lamb -2.855428 time 2020-06-26 09:49:14.458545
Model ind 665 epoch 592 batch: 200 avg loss -2.815419 avg loss no lamb -2.815419 time 2020-06-26 09:49:25.243357
Model ind 665 epoch 592 batch: 300 avg loss -2.843021 avg loss no lamb -2.843021 time 2020-06-26 09:49:35.564978
Model ind 665 epoch 592 batch: 400 avg loss -2.751530 avg loss no lamb -2.751530 time 2020-06-26 09:49:46.269254
Model ind 665 epoch 592 batch: 500 avg loss -2.830862 avg loss no lamb -2.830862 time 2020-06-26 09:49:57.014357
Model ind 665 epoch 592 batch: 600 avg loss -2.820739 avg loss no lamb -2.820739 time 2020-06-26 09:50:07.870305
Model ind 665 epoch 592 batch: 700 avg loss -2.748716 avg loss no lamb -2.748716 time 2020-06-26 09:50:18.437069
Model ind 665 epoch 592 batch: 800 avg loss -2.803514 avg loss no lamb -2.803514 time 2020-06-26 09:50:29.268517
last batch sz 10
Pre: time 2020-06-26 09:50:43.061040: 
 	std: 0.002958384
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9819, 0.9756, 0.9822, 0.9766]
	train_accs: [0.98256665, 0.98183334, 0.9768, 0.98251665, 0.9775]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.9797001
	best: 0.9822

Starting e_i: 593
Model ind 665 epoch 593 batch: 0 avg loss -2.952642 avg loss no lamb -2.952642 time 2020-06-26 09:50:44.083259
Model ind 665 epoch 593 batch: 100 avg loss -2.899978 avg loss no lamb -2.899978 time 2020-06-26 09:50:54.786744
Model ind 665 epoch 593 batch: 200 avg loss -2.883438 avg loss no lamb -2.883438 time 2020-06-26 09:51:05.446519
Model ind 665 epoch 593 batch: 300 avg loss -2.875026 avg loss no lamb -2.875026 time 2020-06-26 09:51:15.953410
Model ind 665 epoch 593 batch: 400 avg loss -2.755678 avg loss no lamb -2.755678 time 2020-06-26 09:51:26.670010
Model ind 665 epoch 593 batch: 500 avg loss -2.801827 avg loss no lamb -2.801827 time 2020-06-26 09:51:37.405061
Model ind 665 epoch 593 batch: 600 avg loss -2.832875 avg loss no lamb -2.832875 time 2020-06-26 09:51:48.123729
Model ind 665 epoch 593 batch: 700 avg loss -2.773632 avg loss no lamb -2.773632 time 2020-06-26 09:51:58.930734
Model ind 665 epoch 593 batch: 800 avg loss -2.805952 avg loss no lamb -2.805952 time 2020-06-26 09:52:09.805268
last batch sz 10
Pre: time 2020-06-26 09:52:23.417534: 
 	std: 0.0027264615
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9802, 0.9741, 0.9806, 0.9762]
	train_accs: [0.9819833, 0.98123336, 0.9755, 0.98191667, 0.97721666]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97838
	best: 0.9808

Starting e_i: 594
Model ind 665 epoch 594 batch: 0 avg loss -2.867406 avg loss no lamb -2.867406 time 2020-06-26 09:52:24.316236
Model ind 665 epoch 594 batch: 100 avg loss -2.906526 avg loss no lamb -2.906526 time 2020-06-26 09:52:35.232826
Model ind 665 epoch 594 batch: 200 avg loss -2.846819 avg loss no lamb -2.846819 time 2020-06-26 09:52:46.036673
Model ind 665 epoch 594 batch: 300 avg loss -2.808632 avg loss no lamb -2.808632 time 2020-06-26 09:52:56.728447
Model ind 665 epoch 594 batch: 400 avg loss -2.728240 avg loss no lamb -2.728240 time 2020-06-26 09:53:07.521141
Model ind 665 epoch 594 batch: 500 avg loss -2.775116 avg loss no lamb -2.775116 time 2020-06-26 09:53:18.186353
Model ind 665 epoch 594 batch: 600 avg loss -2.890729 avg loss no lamb -2.890729 time 2020-06-26 09:53:28.938567
Model ind 665 epoch 594 batch: 700 avg loss -2.713509 avg loss no lamb -2.713509 time 2020-06-26 09:53:39.698901
Model ind 665 epoch 594 batch: 800 avg loss -2.818824 avg loss no lamb -2.818824 time 2020-06-26 09:53:50.588991
last batch sz 10
Pre: time 2020-06-26 09:54:04.618939: 
 	std: 0.0034579828
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9803, 0.9726, 0.9811, 0.975]
	train_accs: [0.9812833, 0.98076665, 0.9745167, 0.98145, 0.97616667]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97792006
	best: 0.9811

Starting e_i: 595
Model ind 665 epoch 595 batch: 0 avg loss -2.912738 avg loss no lamb -2.912738 time 2020-06-26 09:54:05.493884
Model ind 665 epoch 595 batch: 100 avg loss -2.876538 avg loss no lamb -2.876538 time 2020-06-26 09:54:16.176891
Model ind 665 epoch 595 batch: 200 avg loss -2.894955 avg loss no lamb -2.894955 time 2020-06-26 09:54:27.126356
Model ind 665 epoch 595 batch: 300 avg loss -2.886559 avg loss no lamb -2.886559 time 2020-06-26 09:54:37.845933
Model ind 665 epoch 595 batch: 400 avg loss -2.695537 avg loss no lamb -2.695537 time 2020-06-26 09:54:48.572907
Model ind 665 epoch 595 batch: 500 avg loss -2.832033 avg loss no lamb -2.832033 time 2020-06-26 09:54:59.094532
Model ind 665 epoch 595 batch: 600 avg loss -2.870193 avg loss no lamb -2.870193 time 2020-06-26 09:55:10.083675
Model ind 665 epoch 595 batch: 700 avg loss -2.628586 avg loss no lamb -2.628586 time 2020-06-26 09:55:20.991312
Model ind 665 epoch 595 batch: 800 avg loss -2.776910 avg loss no lamb -2.776910 time 2020-06-26 09:55:31.706335
last batch sz 10
Pre: time 2020-06-26 09:55:45.340502: 
 	std: 0.0027441676
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9805, 0.9748, 0.9815, 0.9766]
	train_accs: [0.98246664, 0.98156667, 0.97611666, 0.98233336, 0.9774167]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97896004
	best: 0.9814

Starting e_i: 596
Model ind 665 epoch 596 batch: 0 avg loss -2.917582 avg loss no lamb -2.917582 time 2020-06-26 09:55:46.242478
Model ind 665 epoch 596 batch: 100 avg loss -2.780747 avg loss no lamb -2.780747 time 2020-06-26 09:55:56.804908
Model ind 665 epoch 596 batch: 200 avg loss -2.824825 avg loss no lamb -2.824825 time 2020-06-26 09:56:07.506536
Model ind 665 epoch 596 batch: 300 avg loss -2.902138 avg loss no lamb -2.902138 time 2020-06-26 09:56:18.274671
Model ind 665 epoch 596 batch: 400 avg loss -2.766779 avg loss no lamb -2.766779 time 2020-06-26 09:56:29.190737
Model ind 665 epoch 596 batch: 500 avg loss -2.856049 avg loss no lamb -2.856049 time 2020-06-26 09:56:40.034036
Model ind 665 epoch 596 batch: 600 avg loss -2.864270 avg loss no lamb -2.864270 time 2020-06-26 09:56:50.568498
Model ind 665 epoch 596 batch: 700 avg loss -2.724942 avg loss no lamb -2.724942 time 2020-06-26 09:57:01.188650
Model ind 665 epoch 596 batch: 800 avg loss -2.839434 avg loss no lamb -2.839434 time 2020-06-26 09:57:11.738523
last batch sz 10
Pre: time 2020-06-26 09:57:25.845194: 
 	std: 0.002696959
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9812, 0.9764, 0.982, 0.9762]
	train_accs: [0.98218334, 0.9814, 0.9766833, 0.98215, 0.97711664]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.97958004
	best: 0.9821

Starting e_i: 597
Model ind 665 epoch 597 batch: 0 avg loss -2.911205 avg loss no lamb -2.911205 time 2020-06-26 09:57:26.752921
Model ind 665 epoch 597 batch: 100 avg loss -2.868344 avg loss no lamb -2.868344 time 2020-06-26 09:57:37.328939
Model ind 665 epoch 597 batch: 200 avg loss -2.814748 avg loss no lamb -2.814748 time 2020-06-26 09:57:48.067980
Model ind 665 epoch 597 batch: 300 avg loss -2.856945 avg loss no lamb -2.856945 time 2020-06-26 09:57:58.644064
Model ind 665 epoch 597 batch: 400 avg loss -2.807296 avg loss no lamb -2.807296 time 2020-06-26 09:58:09.295030
Model ind 665 epoch 597 batch: 500 avg loss -2.776209 avg loss no lamb -2.776209 time 2020-06-26 09:58:20.119652
Model ind 665 epoch 597 batch: 600 avg loss -2.900180 avg loss no lamb -2.900180 time 2020-06-26 09:58:30.750362
Model ind 665 epoch 597 batch: 700 avg loss -2.661561 avg loss no lamb -2.661561 time 2020-06-26 09:58:41.292486
Model ind 665 epoch 597 batch: 800 avg loss -2.805416 avg loss no lamb -2.805416 time 2020-06-26 09:58:51.849135
last batch sz 10
Pre: time 2020-06-26 09:59:05.914060: 
 	std: 0.002942398
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.98, 0.974, 0.9807, 0.9748]
	train_accs: [0.98151666, 0.98053336, 0.9759333, 0.9815, 0.9766]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97798
	best: 0.9804

Starting e_i: 598
Model ind 665 epoch 598 batch: 0 avg loss -2.959126 avg loss no lamb -2.959126 time 2020-06-26 09:59:06.945107
Model ind 665 epoch 598 batch: 100 avg loss -2.858131 avg loss no lamb -2.858131 time 2020-06-26 09:59:17.713163
Model ind 665 epoch 598 batch: 200 avg loss -2.834279 avg loss no lamb -2.834279 time 2020-06-26 09:59:28.548132
Model ind 665 epoch 598 batch: 300 avg loss -2.873812 avg loss no lamb -2.873812 time 2020-06-26 09:59:39.272617
Model ind 665 epoch 598 batch: 400 avg loss -2.675269 avg loss no lamb -2.675269 time 2020-06-26 09:59:50.162539
Model ind 665 epoch 598 batch: 500 avg loss -2.767725 avg loss no lamb -2.767725 time 2020-06-26 10:00:01.008358
Model ind 665 epoch 598 batch: 600 avg loss -2.874367 avg loss no lamb -2.874367 time 2020-06-26 10:00:11.977123
Model ind 665 epoch 598 batch: 700 avg loss -2.690642 avg loss no lamb -2.690642 time 2020-06-26 10:00:22.792485
Model ind 665 epoch 598 batch: 800 avg loss -2.849189 avg loss no lamb -2.849189 time 2020-06-26 10:00:33.396298
last batch sz 10
Pre: time 2020-06-26 10:00:47.312399: 
 	std: 0.0029284838
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9828, 0.9763, 0.983, 0.9776]
	train_accs: [0.9830833, 0.98225, 0.9770667, 0.98286664, 0.9779]
	best_train_sub_head: 0
	worst: 0.9763
	avg: 0.98050004
	best: 0.9828

Starting e_i: 599
Model ind 665 epoch 599 batch: 0 avg loss -2.928005 avg loss no lamb -2.928005 time 2020-06-26 10:00:48.216249
Model ind 665 epoch 599 batch: 100 avg loss -2.810423 avg loss no lamb -2.810423 time 2020-06-26 10:00:58.944007
Model ind 665 epoch 599 batch: 200 avg loss -2.790028 avg loss no lamb -2.790028 time 2020-06-26 10:01:09.754858
Model ind 665 epoch 599 batch: 300 avg loss -2.858684 avg loss no lamb -2.858684 time 2020-06-26 10:01:20.373200
Model ind 665 epoch 599 batch: 400 avg loss -2.753330 avg loss no lamb -2.753330 time 2020-06-26 10:01:31.025360
Model ind 665 epoch 599 batch: 500 avg loss -2.815400 avg loss no lamb -2.815400 time 2020-06-26 10:01:41.725278
Model ind 665 epoch 599 batch: 600 avg loss -2.835862 avg loss no lamb -2.835862 time 2020-06-26 10:01:52.440279
Model ind 665 epoch 599 batch: 700 avg loss -2.711140 avg loss no lamb -2.711140 time 2020-06-26 10:02:03.153798
Model ind 665 epoch 599 batch: 800 avg loss -2.766396 avg loss no lamb -2.766396 time 2020-06-26 10:02:13.797778
last batch sz 10
Pre: time 2020-06-26 10:02:27.672905: 
 	std: 0.0030419799
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9833, 0.982, 0.9765, 0.9832, 0.9769]
	train_accs: [0.9823667, 0.9816167, 0.97685, 0.98223335, 0.97688335]
	best_train_sub_head: 0
	worst: 0.9765
	avg: 0.98037994
	best: 0.9833

Starting e_i: 600
Model ind 665 epoch 600 batch: 0 avg loss -2.907323 avg loss no lamb -2.907323 time 2020-06-26 10:02:28.582724
Model ind 665 epoch 600 batch: 100 avg loss -2.946686 avg loss no lamb -2.946686 time 2020-06-26 10:02:39.247919
Model ind 665 epoch 600 batch: 200 avg loss -2.841169 avg loss no lamb -2.841169 time 2020-06-26 10:02:49.857177
Model ind 665 epoch 600 batch: 300 avg loss -2.779282 avg loss no lamb -2.779282 time 2020-06-26 10:03:00.675412
Model ind 665 epoch 600 batch: 400 avg loss -2.775963 avg loss no lamb -2.775963 time 2020-06-26 10:03:11.546057
Model ind 665 epoch 600 batch: 500 avg loss -2.768295 avg loss no lamb -2.768295 time 2020-06-26 10:03:22.511834
Model ind 665 epoch 600 batch: 600 avg loss -2.844460 avg loss no lamb -2.844460 time 2020-06-26 10:03:33.370643
Model ind 665 epoch 600 batch: 700 avg loss -2.691169 avg loss no lamb -2.691169 time 2020-06-26 10:03:44.178844
Model ind 665 epoch 600 batch: 800 avg loss -2.777288 avg loss no lamb -2.777288 time 2020-06-26 10:03:55.070099
last batch sz 10
Pre: time 2020-06-26 10:04:08.872821: 
 	std: 0.0029781933
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9824, 0.9758, 0.9823, 0.9767]
	train_accs: [0.98233336, 0.9816667, 0.9768, 0.98245, 0.97745]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.97988003
	best: 0.9823

Starting e_i: 601
Model ind 665 epoch 601 batch: 0 avg loss -2.949327 avg loss no lamb -2.949327 time 2020-06-26 10:04:10.920060
Model ind 665 epoch 601 batch: 100 avg loss -2.871217 avg loss no lamb -2.871217 time 2020-06-26 10:04:21.572034
Model ind 665 epoch 601 batch: 200 avg loss -2.880970 avg loss no lamb -2.880970 time 2020-06-26 10:04:32.188659
Model ind 665 epoch 601 batch: 300 avg loss -2.894955 avg loss no lamb -2.894955 time 2020-06-26 10:04:42.874391
Model ind 665 epoch 601 batch: 400 avg loss -2.701150 avg loss no lamb -2.701150 time 2020-06-26 10:04:53.200680
Model ind 665 epoch 601 batch: 500 avg loss -2.795243 avg loss no lamb -2.795243 time 2020-06-26 10:05:04.031988
Model ind 665 epoch 601 batch: 600 avg loss -2.848958 avg loss no lamb -2.848958 time 2020-06-26 10:05:14.705954
Model ind 665 epoch 601 batch: 700 avg loss -2.730713 avg loss no lamb -2.730713 time 2020-06-26 10:05:25.235000
Model ind 665 epoch 601 batch: 800 avg loss -2.802431 avg loss no lamb -2.802431 time 2020-06-26 10:05:35.828273
last batch sz 10
Pre: time 2020-06-26 10:05:49.855267: 
 	std: 0.0030766218
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9816, 0.9748, 0.982, 0.9768]
	train_accs: [0.98245, 0.9816333, 0.97646666, 0.98235, 0.9774]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97947997
	best: 0.9822

Starting e_i: 602
Model ind 665 epoch 602 batch: 0 avg loss -2.934349 avg loss no lamb -2.934349 time 2020-06-26 10:05:50.759272
Model ind 665 epoch 602 batch: 100 avg loss -2.926425 avg loss no lamb -2.926425 time 2020-06-26 10:06:01.570604
Model ind 665 epoch 602 batch: 200 avg loss -2.784442 avg loss no lamb -2.784442 time 2020-06-26 10:06:12.362706
Model ind 665 epoch 602 batch: 300 avg loss -2.863924 avg loss no lamb -2.863924 time 2020-06-26 10:06:23.170119
Model ind 665 epoch 602 batch: 400 avg loss -2.734257 avg loss no lamb -2.734257 time 2020-06-26 10:06:33.851694
Model ind 665 epoch 602 batch: 500 avg loss -2.802984 avg loss no lamb -2.802984 time 2020-06-26 10:06:44.293622
Model ind 665 epoch 602 batch: 600 avg loss -2.877867 avg loss no lamb -2.877867 time 2020-06-26 10:06:55.139000
Model ind 665 epoch 602 batch: 700 avg loss -2.694910 avg loss no lamb -2.694910 time 2020-06-26 10:07:05.951311
Model ind 665 epoch 602 batch: 800 avg loss -2.834399 avg loss no lamb -2.834399 time 2020-06-26 10:07:16.845011
last batch sz 10
Pre: time 2020-06-26 10:07:30.583753: 
 	std: 0.0027694136
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9812, 0.9758, 0.9821, 0.9769]
	train_accs: [0.98216665, 0.9813333, 0.97646666, 0.9820333, 0.9772]
	best_train_sub_head: 0
	worst: 0.9758
	avg: 0.97967994
	best: 0.9824

Starting e_i: 603
Model ind 665 epoch 603 batch: 0 avg loss -2.975325 avg loss no lamb -2.975325 time 2020-06-26 10:07:31.480325
Model ind 665 epoch 603 batch: 100 avg loss -2.877975 avg loss no lamb -2.877975 time 2020-06-26 10:07:42.071764
Model ind 665 epoch 603 batch: 200 avg loss -2.780972 avg loss no lamb -2.780972 time 2020-06-26 10:07:52.857790
Model ind 665 epoch 603 batch: 300 avg loss -2.868241 avg loss no lamb -2.868241 time 2020-06-26 10:08:03.692851
Model ind 665 epoch 603 batch: 400 avg loss -2.772539 avg loss no lamb -2.772539 time 2020-06-26 10:08:14.551132
Model ind 665 epoch 603 batch: 500 avg loss -2.823695 avg loss no lamb -2.823695 time 2020-06-26 10:08:24.949819
Model ind 665 epoch 603 batch: 600 avg loss -2.830611 avg loss no lamb -2.830611 time 2020-06-26 10:08:35.711765
Model ind 665 epoch 603 batch: 700 avg loss -2.707512 avg loss no lamb -2.707512 time 2020-06-26 10:08:46.694161
Model ind 665 epoch 603 batch: 800 avg loss -2.840612 avg loss no lamb -2.840612 time 2020-06-26 10:08:57.361398
last batch sz 10
Pre: time 2020-06-26 10:09:11.557942: 
 	std: 0.0034735554
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9803, 0.9735, 0.9814, 0.9743]
	train_accs: [0.98183334, 0.9815, 0.97548336, 0.9820833, 0.9757]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97812
	best: 0.9814

Starting e_i: 604
Model ind 665 epoch 604 batch: 0 avg loss -2.924979 avg loss no lamb -2.924979 time 2020-06-26 10:09:12.465292
Model ind 665 epoch 604 batch: 100 avg loss -2.937019 avg loss no lamb -2.937019 time 2020-06-26 10:09:23.276668
Model ind 665 epoch 604 batch: 200 avg loss -2.836705 avg loss no lamb -2.836705 time 2020-06-26 10:09:33.962170
Model ind 665 epoch 604 batch: 300 avg loss -2.890204 avg loss no lamb -2.890204 time 2020-06-26 10:09:44.953525
Model ind 665 epoch 604 batch: 400 avg loss -2.742657 avg loss no lamb -2.742657 time 2020-06-26 10:09:55.962781
Model ind 665 epoch 604 batch: 500 avg loss -2.819381 avg loss no lamb -2.819381 time 2020-06-26 10:10:06.454007
Model ind 665 epoch 604 batch: 600 avg loss -2.809655 avg loss no lamb -2.809655 time 2020-06-26 10:10:16.947153
Model ind 665 epoch 604 batch: 700 avg loss -2.735378 avg loss no lamb -2.735378 time 2020-06-26 10:10:27.642689
Model ind 665 epoch 604 batch: 800 avg loss -2.816349 avg loss no lamb -2.816349 time 2020-06-26 10:10:38.677479
last batch sz 10
Pre: time 2020-06-26 10:10:52.750708: 
 	std: 0.0033018629
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9794, 0.9728, 0.9806, 0.9746]
	train_accs: [0.98151666, 0.9805167, 0.9748333, 0.9812667, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97764
	best: 0.9808

Starting e_i: 605
Model ind 665 epoch 605 batch: 0 avg loss -2.925042 avg loss no lamb -2.925042 time 2020-06-26 10:10:53.744725
Model ind 665 epoch 605 batch: 100 avg loss -2.934977 avg loss no lamb -2.934977 time 2020-06-26 10:11:04.430136
Model ind 665 epoch 605 batch: 200 avg loss -2.833304 avg loss no lamb -2.833304 time 2020-06-26 10:11:15.197500
Model ind 665 epoch 605 batch: 300 avg loss -2.837148 avg loss no lamb -2.837148 time 2020-06-26 10:11:26.110539
Model ind 665 epoch 605 batch: 400 avg loss -2.774267 avg loss no lamb -2.774267 time 2020-06-26 10:11:36.973580
Model ind 665 epoch 605 batch: 500 avg loss -2.783668 avg loss no lamb -2.783668 time 2020-06-26 10:11:47.593206
Model ind 665 epoch 605 batch: 600 avg loss -2.886973 avg loss no lamb -2.886973 time 2020-06-26 10:11:58.242252
Model ind 665 epoch 605 batch: 700 avg loss -2.715332 avg loss no lamb -2.715332 time 2020-06-26 10:12:09.188305
Model ind 665 epoch 605 batch: 800 avg loss -2.859813 avg loss no lamb -2.859813 time 2020-06-26 10:12:19.789143
last batch sz 10
Pre: time 2020-06-26 10:12:33.789638: 
 	std: 0.0029979877
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9819, 0.9753, 0.982, 0.976]
	train_accs: [0.98216665, 0.98211664, 0.97653335, 0.98225, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.9792999
	best: 0.982

Starting e_i: 606
Model ind 665 epoch 606 batch: 0 avg loss -2.911229 avg loss no lamb -2.911229 time 2020-06-26 10:12:34.680122
Model ind 665 epoch 606 batch: 100 avg loss -2.866605 avg loss no lamb -2.866605 time 2020-06-26 10:12:45.254443
Model ind 665 epoch 606 batch: 200 avg loss -2.815395 avg loss no lamb -2.815395 time 2020-06-26 10:12:56.193947
Model ind 665 epoch 606 batch: 300 avg loss -2.829822 avg loss no lamb -2.829822 time 2020-06-26 10:13:07.083268
Model ind 665 epoch 606 batch: 400 avg loss -2.735582 avg loss no lamb -2.735582 time 2020-06-26 10:13:17.670451
Model ind 665 epoch 606 batch: 500 avg loss -2.832891 avg loss no lamb -2.832891 time 2020-06-26 10:13:28.269276
Model ind 665 epoch 606 batch: 600 avg loss -2.786847 avg loss no lamb -2.786847 time 2020-06-26 10:13:38.878697
Model ind 665 epoch 606 batch: 700 avg loss -2.781195 avg loss no lamb -2.781195 time 2020-06-26 10:13:49.768023
Model ind 665 epoch 606 batch: 800 avg loss -2.856845 avg loss no lamb -2.856845 time 2020-06-26 10:14:00.481431
last batch sz 10
Pre: time 2020-06-26 10:14:14.236086: 
 	std: 0.0026102925
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9812, 0.9759, 0.9824, 0.9777]
	train_accs: [0.98233336, 0.9816, 0.9769833, 0.9823667, 0.97756666]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.97988
	best: 0.9824

Starting e_i: 607
Model ind 665 epoch 607 batch: 0 avg loss -2.925262 avg loss no lamb -2.925262 time 2020-06-26 10:14:15.250815
Model ind 665 epoch 607 batch: 100 avg loss -2.807431 avg loss no lamb -2.807431 time 2020-06-26 10:14:26.045646
Model ind 665 epoch 607 batch: 200 avg loss -2.830096 avg loss no lamb -2.830096 time 2020-06-26 10:14:36.947958
Model ind 665 epoch 607 batch: 300 avg loss -2.869310 avg loss no lamb -2.869310 time 2020-06-26 10:14:47.474237
Model ind 665 epoch 607 batch: 400 avg loss -2.693519 avg loss no lamb -2.693519 time 2020-06-26 10:14:58.117033
Model ind 665 epoch 607 batch: 500 avg loss -2.836504 avg loss no lamb -2.836504 time 2020-06-26 10:15:08.909472
Model ind 665 epoch 607 batch: 600 avg loss -2.865982 avg loss no lamb -2.865982 time 2020-06-26 10:15:19.852503
Model ind 665 epoch 607 batch: 700 avg loss -2.759121 avg loss no lamb -2.759121 time 2020-06-26 10:15:30.678142
Model ind 665 epoch 607 batch: 800 avg loss -2.843538 avg loss no lamb -2.843538 time 2020-06-26 10:15:41.375914
last batch sz 10
Pre: time 2020-06-26 10:15:54.860915: 
 	std: 0.0026498158
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9812, 0.9761, 0.9818, 0.9764]
	train_accs: [0.9823167, 0.9815, 0.977, 0.9822, 0.9773333]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.97947997
	best: 0.9819

Starting e_i: 608
Model ind 665 epoch 608 batch: 0 avg loss -2.937942 avg loss no lamb -2.937942 time 2020-06-26 10:15:55.792231
Model ind 665 epoch 608 batch: 100 avg loss -2.825668 avg loss no lamb -2.825668 time 2020-06-26 10:16:06.380686
Model ind 665 epoch 608 batch: 200 avg loss -2.900421 avg loss no lamb -2.900421 time 2020-06-26 10:16:16.886993
Model ind 665 epoch 608 batch: 300 avg loss -2.854059 avg loss no lamb -2.854059 time 2020-06-26 10:16:27.592120
Model ind 665 epoch 608 batch: 400 avg loss -2.707725 avg loss no lamb -2.707725 time 2020-06-26 10:16:38.438257
Model ind 665 epoch 608 batch: 500 avg loss -2.846596 avg loss no lamb -2.846596 time 2020-06-26 10:16:49.058655
Model ind 665 epoch 608 batch: 600 avg loss -2.865384 avg loss no lamb -2.865384 time 2020-06-26 10:16:59.707096
Model ind 665 epoch 608 batch: 700 avg loss -2.759282 avg loss no lamb -2.759282 time 2020-06-26 10:17:10.654299
Model ind 665 epoch 608 batch: 800 avg loss -2.870691 avg loss no lamb -2.870691 time 2020-06-26 10:17:21.219731
last batch sz 10
Pre: time 2020-06-26 10:17:35.225340: 
 	std: 0.0031714947
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9841, 0.9834, 0.9762, 0.9835, 0.9786]
	train_accs: [0.983, 0.9823, 0.977, 0.98263335, 0.97783333]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.98116
	best: 0.9841

Starting e_i: 609
Model ind 665 epoch 609 batch: 0 avg loss -2.915018 avg loss no lamb -2.915018 time 2020-06-26 10:17:36.085063
Model ind 665 epoch 609 batch: 100 avg loss -2.869441 avg loss no lamb -2.869441 time 2020-06-26 10:17:46.786931
Model ind 665 epoch 609 batch: 200 avg loss -2.854055 avg loss no lamb -2.854055 time 2020-06-26 10:17:57.505631
Model ind 665 epoch 609 batch: 300 avg loss -2.841915 avg loss no lamb -2.841915 time 2020-06-26 10:18:08.197674
Model ind 665 epoch 609 batch: 400 avg loss -2.704925 avg loss no lamb -2.704925 time 2020-06-26 10:18:18.874382
Model ind 665 epoch 609 batch: 500 avg loss -2.856464 avg loss no lamb -2.856464 time 2020-06-26 10:18:29.403861
Model ind 665 epoch 609 batch: 600 avg loss -2.871662 avg loss no lamb -2.871662 time 2020-06-26 10:18:40.148946
Model ind 665 epoch 609 batch: 700 avg loss -2.698624 avg loss no lamb -2.698624 time 2020-06-26 10:18:50.986519
Model ind 665 epoch 609 batch: 800 avg loss -2.896734 avg loss no lamb -2.896734 time 2020-06-26 10:19:01.698854
last batch sz 10
Pre: time 2020-06-26 10:19:15.574150: 
 	std: 0.0030653016
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.98, 0.9738, 0.9815, 0.9755]
	train_accs: [0.98185, 0.98078334, 0.97513336, 0.9822, 0.9765]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.9783
	best: 0.9815

Starting e_i: 610
Model ind 665 epoch 610 batch: 0 avg loss -2.906059 avg loss no lamb -2.906059 time 2020-06-26 10:19:16.501329
Model ind 665 epoch 610 batch: 100 avg loss -2.860112 avg loss no lamb -2.860112 time 2020-06-26 10:19:27.392515
Model ind 665 epoch 610 batch: 200 avg loss -2.793080 avg loss no lamb -2.793080 time 2020-06-26 10:19:38.343321
Model ind 665 epoch 610 batch: 300 avg loss -2.811947 avg loss no lamb -2.811947 time 2020-06-26 10:19:49.117851
Model ind 665 epoch 610 batch: 400 avg loss -2.767011 avg loss no lamb -2.767011 time 2020-06-26 10:19:59.794386
Model ind 665 epoch 610 batch: 500 avg loss -2.806585 avg loss no lamb -2.806585 time 2020-06-26 10:20:10.925054
Model ind 665 epoch 610 batch: 600 avg loss -2.780908 avg loss no lamb -2.780908 time 2020-06-26 10:20:21.624667
Model ind 665 epoch 610 batch: 700 avg loss -2.756591 avg loss no lamb -2.756591 time 2020-06-26 10:20:32.468271
Model ind 665 epoch 610 batch: 800 avg loss -2.777919 avg loss no lamb -2.777919 time 2020-06-26 10:20:43.349433
last batch sz 10
Pre: time 2020-06-26 10:20:57.574002: 
 	std: 0.0033326214
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9833, 0.9819, 0.9754, 0.983, 0.9767]
	train_accs: [0.9827167, 0.9817333, 0.9763, 0.9827167, 0.9770833]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.98006
	best: 0.9833

Starting e_i: 611
Model ind 665 epoch 611 batch: 0 avg loss -2.927562 avg loss no lamb -2.927562 time 2020-06-26 10:20:59.626361
Model ind 665 epoch 611 batch: 100 avg loss -2.900808 avg loss no lamb -2.900808 time 2020-06-26 10:21:10.401644
Model ind 665 epoch 611 batch: 200 avg loss -2.786120 avg loss no lamb -2.786120 time 2020-06-26 10:21:21.266534
Model ind 665 epoch 611 batch: 300 avg loss -2.789399 avg loss no lamb -2.789399 time 2020-06-26 10:21:32.077269
Model ind 665 epoch 611 batch: 400 avg loss -2.760403 avg loss no lamb -2.760403 time 2020-06-26 10:21:42.880497
Model ind 665 epoch 611 batch: 500 avg loss -2.826797 avg loss no lamb -2.826797 time 2020-06-26 10:21:53.767828
Model ind 665 epoch 611 batch: 600 avg loss -2.915958 avg loss no lamb -2.915958 time 2020-06-26 10:22:04.682938
Model ind 665 epoch 611 batch: 700 avg loss -2.663637 avg loss no lamb -2.663637 time 2020-06-26 10:22:15.605930
Model ind 665 epoch 611 batch: 800 avg loss -2.829515 avg loss no lamb -2.829515 time 2020-06-26 10:22:26.349503
last batch sz 10
Pre: time 2020-06-26 10:22:40.305924: 
 	std: 0.0032021273
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.981, 0.9734, 0.9805, 0.9756]
	train_accs: [0.9815, 0.98121667, 0.97536665, 0.98105, 0.97595]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97832
	best: 0.9811

Starting e_i: 612
Model ind 665 epoch 612 batch: 0 avg loss -2.927050 avg loss no lamb -2.927050 time 2020-06-26 10:22:41.300591
Model ind 665 epoch 612 batch: 100 avg loss -2.813305 avg loss no lamb -2.813305 time 2020-06-26 10:22:51.839211
Model ind 665 epoch 612 batch: 200 avg loss -2.898664 avg loss no lamb -2.898664 time 2020-06-26 10:23:02.703252
Model ind 665 epoch 612 batch: 300 avg loss -2.822284 avg loss no lamb -2.822284 time 2020-06-26 10:23:13.787662
Model ind 665 epoch 612 batch: 400 avg loss -2.736871 avg loss no lamb -2.736871 time 2020-06-26 10:23:24.602921
Model ind 665 epoch 612 batch: 500 avg loss -2.836500 avg loss no lamb -2.836500 time 2020-06-26 10:23:35.375770
Model ind 665 epoch 612 batch: 600 avg loss -2.833962 avg loss no lamb -2.833962 time 2020-06-26 10:23:46.161301
Model ind 665 epoch 612 batch: 700 avg loss -2.743234 avg loss no lamb -2.743234 time 2020-06-26 10:23:56.808065
Model ind 665 epoch 612 batch: 800 avg loss -2.823701 avg loss no lamb -2.823701 time 2020-06-26 10:24:07.685569
last batch sz 10
Pre: time 2020-06-26 10:24:21.538025: 
 	std: 0.003002394
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9821, 0.9757, 0.9827, 0.9769]
	train_accs: [0.98228335, 0.9819667, 0.9763, 0.98265, 0.97728336]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97994006
	best: 0.9827

Starting e_i: 613
Model ind 665 epoch 613 batch: 0 avg loss -2.926533 avg loss no lamb -2.926533 time 2020-06-26 10:24:22.403354
Model ind 665 epoch 613 batch: 100 avg loss -2.847952 avg loss no lamb -2.847952 time 2020-06-26 10:24:33.145921
Model ind 665 epoch 613 batch: 200 avg loss -2.806705 avg loss no lamb -2.806705 time 2020-06-26 10:24:44.056078
Model ind 665 epoch 613 batch: 300 avg loss -2.868783 avg loss no lamb -2.868783 time 2020-06-26 10:24:54.712126
Model ind 665 epoch 613 batch: 400 avg loss -2.712510 avg loss no lamb -2.712510 time 2020-06-26 10:25:05.411919
Model ind 665 epoch 613 batch: 500 avg loss -2.774237 avg loss no lamb -2.774237 time 2020-06-26 10:25:16.070974
Model ind 665 epoch 613 batch: 600 avg loss -2.801083 avg loss no lamb -2.801083 time 2020-06-26 10:25:27.034034
Model ind 665 epoch 613 batch: 700 avg loss -2.684072 avg loss no lamb -2.684072 time 2020-06-26 10:25:37.945191
Model ind 665 epoch 613 batch: 800 avg loss -2.782488 avg loss no lamb -2.782488 time 2020-06-26 10:25:48.712426
last batch sz 10
Pre: time 2020-06-26 10:26:02.713049: 
 	std: 0.0025490432
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9812, 0.9758, 0.9811, 0.976]
	train_accs: [0.98185, 0.98146665, 0.9766333, 0.98211664, 0.97693336]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.97902
	best: 0.9811

Starting e_i: 614
Model ind 665 epoch 614 batch: 0 avg loss -2.943587 avg loss no lamb -2.943587 time 2020-06-26 10:26:03.696074
Model ind 665 epoch 614 batch: 100 avg loss -2.883393 avg loss no lamb -2.883393 time 2020-06-26 10:26:14.497561
Model ind 665 epoch 614 batch: 200 avg loss -2.860262 avg loss no lamb -2.860262 time 2020-06-26 10:26:25.382938
Model ind 665 epoch 614 batch: 300 avg loss -2.839764 avg loss no lamb -2.839764 time 2020-06-26 10:26:36.118527
Model ind 665 epoch 614 batch: 400 avg loss -2.791442 avg loss no lamb -2.791442 time 2020-06-26 10:26:46.824595
Model ind 665 epoch 614 batch: 500 avg loss -2.792010 avg loss no lamb -2.792010 time 2020-06-26 10:26:57.572348
Model ind 665 epoch 614 batch: 600 avg loss -2.881006 avg loss no lamb -2.881006 time 2020-06-26 10:27:08.287290
Model ind 665 epoch 614 batch: 700 avg loss -2.649217 avg loss no lamb -2.649217 time 2020-06-26 10:27:18.994630
Model ind 665 epoch 614 batch: 800 avg loss -2.845730 avg loss no lamb -2.845730 time 2020-06-26 10:27:29.742011
last batch sz 10
Pre: time 2020-06-26 10:27:43.728594: 
 	std: 0.0030309183
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9814, 0.9754, 0.9817, 0.9753]
	train_accs: [0.98183334, 0.98148334, 0.97651666, 0.98195, 0.97615]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97906
	best: 0.9817

Starting e_i: 615
Model ind 665 epoch 615 batch: 0 avg loss -2.909035 avg loss no lamb -2.909035 time 2020-06-26 10:27:44.644755
Model ind 665 epoch 615 batch: 100 avg loss -2.875011 avg loss no lamb -2.875011 time 2020-06-26 10:27:55.678793
Model ind 665 epoch 615 batch: 200 avg loss -2.889405 avg loss no lamb -2.889405 time 2020-06-26 10:28:06.443727
Model ind 665 epoch 615 batch: 300 avg loss -2.839022 avg loss no lamb -2.839022 time 2020-06-26 10:28:17.042745
Model ind 665 epoch 615 batch: 400 avg loss -2.755817 avg loss no lamb -2.755817 time 2020-06-26 10:28:27.755285
Model ind 665 epoch 615 batch: 500 avg loss -2.799619 avg loss no lamb -2.799619 time 2020-06-26 10:28:38.441054
Model ind 665 epoch 615 batch: 600 avg loss -2.827891 avg loss no lamb -2.827891 time 2020-06-26 10:28:49.388214
Model ind 665 epoch 615 batch: 700 avg loss -2.723937 avg loss no lamb -2.723937 time 2020-06-26 10:29:00.074100
Model ind 665 epoch 615 batch: 800 avg loss -2.852591 avg loss no lamb -2.852591 time 2020-06-26 10:29:10.584126
last batch sz 10
Pre: time 2020-06-26 10:29:24.277500: 
 	std: 0.0027688346
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9809, 0.9754, 0.9816, 0.9762]
	train_accs: [0.98191667, 0.9812667, 0.9764, 0.98178333, 0.9769]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97916
	best: 0.9817

Starting e_i: 616
Model ind 665 epoch 616 batch: 0 avg loss -2.929842 avg loss no lamb -2.929842 time 2020-06-26 10:29:25.274523
Model ind 665 epoch 616 batch: 100 avg loss -2.934751 avg loss no lamb -2.934751 time 2020-06-26 10:29:36.027003
Model ind 665 epoch 616 batch: 200 avg loss -2.754825 avg loss no lamb -2.754825 time 2020-06-26 10:29:46.784856
Model ind 665 epoch 616 batch: 300 avg loss -2.857559 avg loss no lamb -2.857559 time 2020-06-26 10:29:57.643074
Model ind 665 epoch 616 batch: 400 avg loss -2.730423 avg loss no lamb -2.730423 time 2020-06-26 10:30:08.343316
Model ind 665 epoch 616 batch: 500 avg loss -2.795687 avg loss no lamb -2.795687 time 2020-06-26 10:30:18.997410
Model ind 665 epoch 616 batch: 600 avg loss -2.784290 avg loss no lamb -2.784290 time 2020-06-26 10:30:29.690773
Model ind 665 epoch 616 batch: 700 avg loss -2.712289 avg loss no lamb -2.712289 time 2020-06-26 10:30:40.521993
Model ind 665 epoch 616 batch: 800 avg loss -2.854337 avg loss no lamb -2.854337 time 2020-06-26 10:30:51.293419
last batch sz 10
Pre: time 2020-06-26 10:31:05.072306: 
 	std: 0.0028646782
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9809, 0.9745, 0.9813, 0.9757]
	train_accs: [0.98145, 0.9808, 0.9755833, 0.98188335, 0.97635]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97856
	best: 0.9813

Starting e_i: 617
Model ind 665 epoch 617 batch: 0 avg loss -2.951743 avg loss no lamb -2.951743 time 2020-06-26 10:31:05.977404
Model ind 665 epoch 617 batch: 100 avg loss -2.857032 avg loss no lamb -2.857032 time 2020-06-26 10:31:16.606875
Model ind 665 epoch 617 batch: 200 avg loss -2.866327 avg loss no lamb -2.866327 time 2020-06-26 10:31:27.293666
Model ind 665 epoch 617 batch: 300 avg loss -2.782270 avg loss no lamb -2.782270 time 2020-06-26 10:31:38.040223
Model ind 665 epoch 617 batch: 400 avg loss -2.697255 avg loss no lamb -2.697255 time 2020-06-26 10:31:48.993526
Model ind 665 epoch 617 batch: 500 avg loss -2.817512 avg loss no lamb -2.817512 time 2020-06-26 10:31:59.579343
Model ind 665 epoch 617 batch: 600 avg loss -2.817438 avg loss no lamb -2.817438 time 2020-06-26 10:32:10.303223
Model ind 665 epoch 617 batch: 700 avg loss -2.697657 avg loss no lamb -2.697657 time 2020-06-26 10:32:21.088145
Model ind 665 epoch 617 batch: 800 avg loss -2.831992 avg loss no lamb -2.831992 time 2020-06-26 10:32:31.970592
last batch sz 10
Pre: time 2020-06-26 10:32:46.087566: 
 	std: 0.0031593605
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.981, 0.9748, 0.9819, 0.9757]
	train_accs: [0.9816833, 0.98075, 0.9751833, 0.98183334, 0.9762]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97908
	best: 0.9819

Starting e_i: 618
Model ind 665 epoch 618 batch: 0 avg loss -2.939600 avg loss no lamb -2.939600 time 2020-06-26 10:32:47.100354
Model ind 665 epoch 618 batch: 100 avg loss -2.849238 avg loss no lamb -2.849238 time 2020-06-26 10:32:57.948744
Model ind 665 epoch 618 batch: 200 avg loss -2.865621 avg loss no lamb -2.865621 time 2020-06-26 10:33:08.720436
Model ind 665 epoch 618 batch: 300 avg loss -2.924453 avg loss no lamb -2.924453 time 2020-06-26 10:33:19.320367
Model ind 665 epoch 618 batch: 400 avg loss -2.696342 avg loss no lamb -2.696342 time 2020-06-26 10:33:30.211050
Model ind 665 epoch 618 batch: 500 avg loss -2.788641 avg loss no lamb -2.788641 time 2020-06-26 10:33:40.868569
Model ind 665 epoch 618 batch: 600 avg loss -2.826004 avg loss no lamb -2.826004 time 2020-06-26 10:33:51.577558
Model ind 665 epoch 618 batch: 700 avg loss -2.751468 avg loss no lamb -2.751468 time 2020-06-26 10:34:02.202843
Model ind 665 epoch 618 batch: 800 avg loss -2.883688 avg loss no lamb -2.883688 time 2020-06-26 10:34:12.950326
last batch sz 10
Pre: time 2020-06-26 10:34:26.680405: 
 	std: 0.002994398
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9812, 0.9754, 0.9821, 0.976]
	train_accs: [0.98233336, 0.98105, 0.97615, 0.98233336, 0.9769]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97933996
	best: 0.982

Starting e_i: 619
Model ind 665 epoch 619 batch: 0 avg loss -2.912095 avg loss no lamb -2.912095 time 2020-06-26 10:34:27.611115
Model ind 665 epoch 619 batch: 100 avg loss -2.777102 avg loss no lamb -2.777102 time 2020-06-26 10:34:38.404000
Model ind 665 epoch 619 batch: 200 avg loss -2.839855 avg loss no lamb -2.839855 time 2020-06-26 10:34:49.107687
Model ind 665 epoch 619 batch: 300 avg loss -2.803887 avg loss no lamb -2.803887 time 2020-06-26 10:34:59.756852
Model ind 665 epoch 619 batch: 400 avg loss -2.725109 avg loss no lamb -2.725109 time 2020-06-26 10:35:10.480755
Model ind 665 epoch 619 batch: 500 avg loss -2.700999 avg loss no lamb -2.700999 time 2020-06-26 10:35:20.980753
Model ind 665 epoch 619 batch: 600 avg loss -2.762689 avg loss no lamb -2.762689 time 2020-06-26 10:35:31.633000
Model ind 665 epoch 619 batch: 700 avg loss -2.726556 avg loss no lamb -2.726556 time 2020-06-26 10:35:42.370586
Model ind 665 epoch 619 batch: 800 avg loss -2.778789 avg loss no lamb -2.778789 time 2020-06-26 10:35:53.245283
last batch sz 10
Pre: time 2020-06-26 10:36:07.157908: 
 	std: 0.0028274416
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9809, 0.9755, 0.9818, 0.9759]
	train_accs: [0.98211664, 0.9816, 0.97678334, 0.9819833, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97914
	best: 0.9816

Starting e_i: 620
Model ind 665 epoch 620 batch: 0 avg loss -2.961543 avg loss no lamb -2.961543 time 2020-06-26 10:36:08.177376
Model ind 665 epoch 620 batch: 100 avg loss -2.910733 avg loss no lamb -2.910733 time 2020-06-26 10:36:18.461605
Model ind 665 epoch 620 batch: 200 avg loss -2.886693 avg loss no lamb -2.886693 time 2020-06-26 10:36:29.123584
Model ind 665 epoch 620 batch: 300 avg loss -2.825602 avg loss no lamb -2.825602 time 2020-06-26 10:36:39.863790
Model ind 665 epoch 620 batch: 400 avg loss -2.716982 avg loss no lamb -2.716982 time 2020-06-26 10:36:50.665861
Model ind 665 epoch 620 batch: 500 avg loss -2.778540 avg loss no lamb -2.778540 time 2020-06-26 10:37:01.238190
Model ind 665 epoch 620 batch: 600 avg loss -2.812352 avg loss no lamb -2.812352 time 2020-06-26 10:37:11.845104
Model ind 665 epoch 620 batch: 700 avg loss -2.708975 avg loss no lamb -2.708975 time 2020-06-26 10:37:22.643205
Model ind 665 epoch 620 batch: 800 avg loss -2.922583 avg loss no lamb -2.922583 time 2020-06-26 10:37:33.548105
last batch sz 10
Pre: time 2020-06-26 10:37:47.483239: 
 	std: 0.0028823642
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9803, 0.9748, 0.982, 0.9769]
	train_accs: [0.98216665, 0.9809667, 0.9763, 0.98218334, 0.9774]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.9792
	best: 0.982

Starting e_i: 621
Model ind 665 epoch 621 batch: 0 avg loss -2.953578 avg loss no lamb -2.953578 time 2020-06-26 10:37:49.531122
Model ind 665 epoch 621 batch: 100 avg loss -2.820800 avg loss no lamb -2.820800 time 2020-06-26 10:38:00.071245
Model ind 665 epoch 621 batch: 200 avg loss -2.814421 avg loss no lamb -2.814421 time 2020-06-26 10:38:10.747831
Model ind 665 epoch 621 batch: 300 avg loss -2.817050 avg loss no lamb -2.817050 time 2020-06-26 10:38:21.406200
Model ind 665 epoch 621 batch: 400 avg loss -2.786616 avg loss no lamb -2.786616 time 2020-06-26 10:38:32.250827
Model ind 665 epoch 621 batch: 500 avg loss -2.791908 avg loss no lamb -2.791908 time 2020-06-26 10:38:43.005091
Model ind 665 epoch 621 batch: 600 avg loss -2.806522 avg loss no lamb -2.806522 time 2020-06-26 10:38:53.577427
Model ind 665 epoch 621 batch: 700 avg loss -2.719785 avg loss no lamb -2.719785 time 2020-06-26 10:39:04.267395
Model ind 665 epoch 621 batch: 800 avg loss -2.777942 avg loss no lamb -2.777942 time 2020-06-26 10:39:14.983760
last batch sz 10
Pre: time 2020-06-26 10:39:28.866382: 
 	std: 0.003198124
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9786, 0.9724, 0.9799, 0.9745]
	train_accs: [0.98143333, 0.9799333, 0.97425, 0.98141664, 0.97615]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97720003
	best: 0.9806

Starting e_i: 622
Model ind 665 epoch 622 batch: 0 avg loss -2.931183 avg loss no lamb -2.931183 time 2020-06-26 10:39:29.792885
Model ind 665 epoch 622 batch: 100 avg loss -2.839097 avg loss no lamb -2.839097 time 2020-06-26 10:39:40.592898
Model ind 665 epoch 622 batch: 200 avg loss -2.821720 avg loss no lamb -2.821720 time 2020-06-26 10:39:51.176745
Model ind 665 epoch 622 batch: 300 avg loss -2.834911 avg loss no lamb -2.834911 time 2020-06-26 10:40:01.904639
Model ind 665 epoch 622 batch: 400 avg loss -2.698652 avg loss no lamb -2.698652 time 2020-06-26 10:40:12.645641
Model ind 665 epoch 622 batch: 500 avg loss -2.806569 avg loss no lamb -2.806569 time 2020-06-26 10:40:23.592056
Model ind 665 epoch 622 batch: 600 avg loss -2.807158 avg loss no lamb -2.807158 time 2020-06-26 10:40:34.275366
Model ind 665 epoch 622 batch: 700 avg loss -2.722440 avg loss no lamb -2.722440 time 2020-06-26 10:40:44.950369
Model ind 665 epoch 622 batch: 800 avg loss -2.832653 avg loss no lamb -2.832653 time 2020-06-26 10:40:55.780048
last batch sz 10
Pre: time 2020-06-26 10:41:09.813927: 
 	std: 0.0029897166
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9803, 0.9748, 0.9816, 0.976]
	train_accs: [0.9819833, 0.9808, 0.97575, 0.9818, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97896004
	best: 0.9821

Starting e_i: 623
Model ind 665 epoch 623 batch: 0 avg loss -2.920298 avg loss no lamb -2.920298 time 2020-06-26 10:41:10.732133
Model ind 665 epoch 623 batch: 100 avg loss -2.853449 avg loss no lamb -2.853449 time 2020-06-26 10:41:21.293208
Model ind 665 epoch 623 batch: 200 avg loss -2.798838 avg loss no lamb -2.798838 time 2020-06-26 10:41:32.075662
Model ind 665 epoch 623 batch: 300 avg loss -2.860690 avg loss no lamb -2.860690 time 2020-06-26 10:41:42.754423
Model ind 665 epoch 623 batch: 400 avg loss -2.786166 avg loss no lamb -2.786166 time 2020-06-26 10:41:53.424052
Model ind 665 epoch 623 batch: 500 avg loss -2.809163 avg loss no lamb -2.809163 time 2020-06-26 10:42:04.222537
Model ind 665 epoch 623 batch: 600 avg loss -2.878574 avg loss no lamb -2.878574 time 2020-06-26 10:42:14.971521
Model ind 665 epoch 623 batch: 700 avg loss -2.770302 avg loss no lamb -2.770302 time 2020-06-26 10:42:25.808512
Model ind 665 epoch 623 batch: 800 avg loss -2.826668 avg loss no lamb -2.826668 time 2020-06-26 10:42:36.722082
last batch sz 10
Pre: time 2020-06-26 10:42:50.871219: 
 	std: 0.002692214
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9784, 0.9734, 0.98, 0.9746]
	train_accs: [0.98115, 0.97966665, 0.97565, 0.98123336, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.9771999
	best: 0.98

Starting e_i: 624
Model ind 665 epoch 624 batch: 0 avg loss -2.955346 avg loss no lamb -2.955346 time 2020-06-26 10:42:51.835448
Model ind 665 epoch 624 batch: 100 avg loss -2.897475 avg loss no lamb -2.897475 time 2020-06-26 10:43:02.456484
Model ind 665 epoch 624 batch: 200 avg loss -2.867217 avg loss no lamb -2.867217 time 2020-06-26 10:43:12.996283
Model ind 665 epoch 624 batch: 300 avg loss -2.827901 avg loss no lamb -2.827901 time 2020-06-26 10:43:23.871140
Model ind 665 epoch 624 batch: 400 avg loss -2.753770 avg loss no lamb -2.753770 time 2020-06-26 10:43:34.570683
Model ind 665 epoch 624 batch: 500 avg loss -2.765139 avg loss no lamb -2.765139 time 2020-06-26 10:43:45.480107
Model ind 665 epoch 624 batch: 600 avg loss -2.865127 avg loss no lamb -2.865127 time 2020-06-26 10:43:56.087528
Model ind 665 epoch 624 batch: 700 avg loss -2.751862 avg loss no lamb -2.751862 time 2020-06-26 10:44:06.677189
Model ind 665 epoch 624 batch: 800 avg loss -2.815177 avg loss no lamb -2.815177 time 2020-06-26 10:44:17.551500
last batch sz 10
Pre: time 2020-06-26 10:44:31.486346: 
 	std: 0.0035010963
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.981, 0.9738, 0.9818, 0.9754]
	train_accs: [0.98216665, 0.98073334, 0.975, 0.98211664, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97882
	best: 0.9821

Starting e_i: 625
Model ind 665 epoch 625 batch: 0 avg loss -2.920321 avg loss no lamb -2.920321 time 2020-06-26 10:44:32.434095
Model ind 665 epoch 625 batch: 100 avg loss -2.866360 avg loss no lamb -2.866360 time 2020-06-26 10:44:43.175507
Model ind 665 epoch 625 batch: 200 avg loss -2.852709 avg loss no lamb -2.852709 time 2020-06-26 10:44:53.688842
Model ind 665 epoch 625 batch: 300 avg loss -2.801933 avg loss no lamb -2.801933 time 2020-06-26 10:45:04.351977
Model ind 665 epoch 625 batch: 400 avg loss -2.722381 avg loss no lamb -2.722381 time 2020-06-26 10:45:15.148856
Model ind 665 epoch 625 batch: 500 avg loss -2.842859 avg loss no lamb -2.842859 time 2020-06-26 10:45:25.708365
Model ind 665 epoch 625 batch: 600 avg loss -2.855834 avg loss no lamb -2.855834 time 2020-06-26 10:45:35.947704
Model ind 665 epoch 625 batch: 700 avg loss -2.754362 avg loss no lamb -2.754362 time 2020-06-26 10:45:46.668647
Model ind 665 epoch 625 batch: 800 avg loss -2.824225 avg loss no lamb -2.824225 time 2020-06-26 10:45:57.361467
last batch sz 10
Pre: time 2020-06-26 10:46:11.480744: 
 	std: 0.0023854575
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9794, 0.9752, 0.98, 0.9759]
	train_accs: [0.9816667, 0.98075, 0.9757, 0.98121667, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97836
	best: 0.9813

Starting e_i: 626
Model ind 665 epoch 626 batch: 0 avg loss -2.953820 avg loss no lamb -2.953820 time 2020-06-26 10:46:12.393293
Model ind 665 epoch 626 batch: 100 avg loss -2.880836 avg loss no lamb -2.880836 time 2020-06-26 10:46:23.255421
Model ind 665 epoch 626 batch: 200 avg loss -2.857739 avg loss no lamb -2.857739 time 2020-06-26 10:46:33.695584
Model ind 665 epoch 626 batch: 300 avg loss -2.823264 avg loss no lamb -2.823264 time 2020-06-26 10:46:44.414871
Model ind 665 epoch 626 batch: 400 avg loss -2.680191 avg loss no lamb -2.680191 time 2020-06-26 10:46:55.155726
Model ind 665 epoch 626 batch: 500 avg loss -2.807098 avg loss no lamb -2.807098 time 2020-06-26 10:47:05.976714
Model ind 665 epoch 626 batch: 600 avg loss -2.832879 avg loss no lamb -2.832879 time 2020-06-26 10:47:16.718849
Model ind 665 epoch 626 batch: 700 avg loss -2.754760 avg loss no lamb -2.754760 time 2020-06-26 10:47:27.389521
Model ind 665 epoch 626 batch: 800 avg loss -2.857315 avg loss no lamb -2.857315 time 2020-06-26 10:47:38.234877
last batch sz 10
Pre: time 2020-06-26 10:47:52.395906: 
 	std: 0.003833751
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9804, 0.972, 0.9816, 0.9758]
	train_accs: [0.9824167, 0.9813, 0.9755333, 0.9823167, 0.9771]
	best_train_sub_head: 0
	worst: 0.972
	avg: 0.97832
	best: 0.9818

Starting e_i: 627
Model ind 665 epoch 627 batch: 0 avg loss -2.928102 avg loss no lamb -2.928102 time 2020-06-26 10:47:53.428169
Model ind 665 epoch 627 batch: 100 avg loss -2.842752 avg loss no lamb -2.842752 time 2020-06-26 10:48:04.374628
Model ind 665 epoch 627 batch: 200 avg loss -2.869149 avg loss no lamb -2.869149 time 2020-06-26 10:48:15.245246
Model ind 665 epoch 627 batch: 300 avg loss -2.826302 avg loss no lamb -2.826302 time 2020-06-26 10:48:25.881922
Model ind 665 epoch 627 batch: 400 avg loss -2.723682 avg loss no lamb -2.723682 time 2020-06-26 10:48:36.436171
Model ind 665 epoch 627 batch: 500 avg loss -2.850823 avg loss no lamb -2.850823 time 2020-06-26 10:48:47.171964
Model ind 665 epoch 627 batch: 600 avg loss -2.850078 avg loss no lamb -2.850078 time 2020-06-26 10:48:57.903054
Model ind 665 epoch 627 batch: 700 avg loss -2.738544 avg loss no lamb -2.738544 time 2020-06-26 10:49:08.456507
Model ind 665 epoch 627 batch: 800 avg loss -2.773778 avg loss no lamb -2.773778 time 2020-06-26 10:49:19.351702
last batch sz 10
Pre: time 2020-06-26 10:49:33.213907: 
 	std: 0.003507949
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9813, 0.9733, 0.9815, 0.9763]
	train_accs: [0.98228335, 0.9816333, 0.97595, 0.982, 0.9769667]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97892
	best: 0.9822

Starting e_i: 628
Model ind 665 epoch 628 batch: 0 avg loss -2.920465 avg loss no lamb -2.920465 time 2020-06-26 10:49:34.134667
Model ind 665 epoch 628 batch: 100 avg loss -2.830612 avg loss no lamb -2.830612 time 2020-06-26 10:49:45.074372
Model ind 665 epoch 628 batch: 200 avg loss -2.873722 avg loss no lamb -2.873722 time 2020-06-26 10:49:55.727916
Model ind 665 epoch 628 batch: 300 avg loss -2.821201 avg loss no lamb -2.821201 time 2020-06-26 10:50:06.291837
Model ind 665 epoch 628 batch: 400 avg loss -2.718896 avg loss no lamb -2.718896 time 2020-06-26 10:50:16.834031
Model ind 665 epoch 628 batch: 500 avg loss -2.846493 avg loss no lamb -2.846493 time 2020-06-26 10:50:27.639611
Model ind 665 epoch 628 batch: 600 avg loss -2.848745 avg loss no lamb -2.848745 time 2020-06-26 10:50:38.321663
Model ind 665 epoch 628 batch: 700 avg loss -2.704708 avg loss no lamb -2.704708 time 2020-06-26 10:50:48.862911
Model ind 665 epoch 628 batch: 800 avg loss -2.870023 avg loss no lamb -2.870023 time 2020-06-26 10:50:59.336740
last batch sz 10
Pre: time 2020-06-26 10:51:13.282070: 
 	std: 0.0028309776
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.98, 0.9745, 0.9807, 0.9759]
	train_accs: [0.98141664, 0.9805167, 0.9759167, 0.98141664, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97856
	best: 0.9817

Starting e_i: 629
Model ind 665 epoch 629 batch: 0 avg loss -2.930022 avg loss no lamb -2.930022 time 2020-06-26 10:51:14.309985
Model ind 665 epoch 629 batch: 100 avg loss -2.891442 avg loss no lamb -2.891442 time 2020-06-26 10:51:24.989301
Model ind 665 epoch 629 batch: 200 avg loss -2.851223 avg loss no lamb -2.851223 time 2020-06-26 10:51:35.678091
Model ind 665 epoch 629 batch: 300 avg loss -2.798506 avg loss no lamb -2.798506 time 2020-06-26 10:51:46.646864
Model ind 665 epoch 629 batch: 400 avg loss -2.755834 avg loss no lamb -2.755834 time 2020-06-26 10:51:57.373753
Model ind 665 epoch 629 batch: 500 avg loss -2.854547 avg loss no lamb -2.854547 time 2020-06-26 10:52:08.117486
Model ind 665 epoch 629 batch: 600 avg loss -2.861589 avg loss no lamb -2.861589 time 2020-06-26 10:52:18.637370
Model ind 665 epoch 629 batch: 700 avg loss -2.706387 avg loss no lamb -2.706387 time 2020-06-26 10:52:29.585345
Model ind 665 epoch 629 batch: 800 avg loss -2.798957 avg loss no lamb -2.798957 time 2020-06-26 10:52:40.264359
last batch sz 10
Pre: time 2020-06-26 10:52:54.352091: 
 	std: 0.0025253925
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9797, 0.9748, 0.9805, 0.9753]
	train_accs: [0.98145, 0.98025, 0.97601664, 0.98148334, 0.97648335]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97812
	best: 0.9805

Starting e_i: 630
Model ind 665 epoch 630 batch: 0 avg loss -2.954706 avg loss no lamb -2.954706 time 2020-06-26 10:52:55.306717
Model ind 665 epoch 630 batch: 100 avg loss -2.827817 avg loss no lamb -2.827817 time 2020-06-26 10:53:06.103656
Model ind 665 epoch 630 batch: 200 avg loss -2.866890 avg loss no lamb -2.866890 time 2020-06-26 10:53:16.726101
Model ind 665 epoch 630 batch: 300 avg loss -2.885171 avg loss no lamb -2.885171 time 2020-06-26 10:53:27.428015
Model ind 665 epoch 630 batch: 400 avg loss -2.759205 avg loss no lamb -2.759205 time 2020-06-26 10:53:38.025021
Model ind 665 epoch 630 batch: 500 avg loss -2.825896 avg loss no lamb -2.825896 time 2020-06-26 10:53:48.661565
Model ind 665 epoch 630 batch: 600 avg loss -2.863707 avg loss no lamb -2.863707 time 2020-06-26 10:53:59.303477
Model ind 665 epoch 630 batch: 700 avg loss -2.736050 avg loss no lamb -2.736050 time 2020-06-26 10:54:10.253886
Model ind 665 epoch 630 batch: 800 avg loss -2.801018 avg loss no lamb -2.801018 time 2020-06-26 10:54:21.062285
last batch sz 10
Pre: time 2020-06-26 10:54:34.969605: 
 	std: 0.0032066207
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9802, 0.974, 0.9815, 0.9757]
	train_accs: [0.98181665, 0.9805833, 0.9756167, 0.9816833, 0.97705]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97866
	best: 0.9819

Starting e_i: 631
Model ind 665 epoch 631 batch: 0 avg loss -2.885232 avg loss no lamb -2.885232 time 2020-06-26 10:54:36.951192
Model ind 665 epoch 631 batch: 100 avg loss -2.869526 avg loss no lamb -2.869526 time 2020-06-26 10:54:47.277949
Model ind 665 epoch 631 batch: 200 avg loss -2.849441 avg loss no lamb -2.849441 time 2020-06-26 10:54:57.796118
Model ind 665 epoch 631 batch: 300 avg loss -2.891921 avg loss no lamb -2.891921 time 2020-06-26 10:55:08.499701
Model ind 665 epoch 631 batch: 400 avg loss -2.754940 avg loss no lamb -2.754940 time 2020-06-26 10:55:19.391542
Model ind 665 epoch 631 batch: 500 avg loss -2.828496 avg loss no lamb -2.828496 time 2020-06-26 10:55:30.113143
Model ind 665 epoch 631 batch: 600 avg loss -2.860821 avg loss no lamb -2.860821 time 2020-06-26 10:55:40.688950
Model ind 665 epoch 631 batch: 700 avg loss -2.678663 avg loss no lamb -2.678663 time 2020-06-26 10:55:51.341157
Model ind 665 epoch 631 batch: 800 avg loss -2.840117 avg loss no lamb -2.840117 time 2020-06-26 10:56:02.264247
last batch sz 10
Pre: time 2020-06-26 10:56:16.153092: 
 	std: 0.0028715017
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9819, 0.976, 0.9819, 0.9768]
	train_accs: [0.98256665, 0.98178333, 0.97688335, 0.98205, 0.97723335]
	best_train_sub_head: 0
	worst: 0.976
	avg: 0.97988003
	best: 0.9828

Starting e_i: 632
Model ind 665 epoch 632 batch: 0 avg loss -2.957186 avg loss no lamb -2.957186 time 2020-06-26 10:56:17.053848
Model ind 665 epoch 632 batch: 100 avg loss -2.888601 avg loss no lamb -2.888601 time 2020-06-26 10:56:27.541560
Model ind 665 epoch 632 batch: 200 avg loss -2.888214 avg loss no lamb -2.888214 time 2020-06-26 10:56:38.552976
Model ind 665 epoch 632 batch: 300 avg loss -2.856137 avg loss no lamb -2.856137 time 2020-06-26 10:56:49.358059
Model ind 665 epoch 632 batch: 400 avg loss -2.771813 avg loss no lamb -2.771813 time 2020-06-26 10:56:59.958816
Model ind 665 epoch 632 batch: 500 avg loss -2.838538 avg loss no lamb -2.838538 time 2020-06-26 10:57:10.578534
Model ind 665 epoch 632 batch: 600 avg loss -2.750909 avg loss no lamb -2.750909 time 2020-06-26 10:57:21.204652
Model ind 665 epoch 632 batch: 700 avg loss -2.756799 avg loss no lamb -2.756799 time 2020-06-26 10:57:32.086652
Model ind 665 epoch 632 batch: 800 avg loss -2.767138 avg loss no lamb -2.767138 time 2020-06-26 10:57:42.696971
last batch sz 10
Pre: time 2020-06-26 10:57:56.518743: 
 	std: 0.003129602
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9838, 0.9819, 0.9756, 0.9832, 0.9783]
	train_accs: [0.9827, 0.9816, 0.9762667, 0.98233336, 0.9775]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.98056
	best: 0.9838

Starting e_i: 633
Model ind 665 epoch 633 batch: 0 avg loss -2.882135 avg loss no lamb -2.882135 time 2020-06-26 10:57:57.445555
Model ind 665 epoch 633 batch: 100 avg loss -2.853660 avg loss no lamb -2.853660 time 2020-06-26 10:58:08.137802
Model ind 665 epoch 633 batch: 200 avg loss -2.828979 avg loss no lamb -2.828979 time 2020-06-26 10:58:18.811646
Model ind 665 epoch 633 batch: 300 avg loss -2.847858 avg loss no lamb -2.847858 time 2020-06-26 10:58:29.378055
Model ind 665 epoch 633 batch: 400 avg loss -2.707278 avg loss no lamb -2.707278 time 2020-06-26 10:58:39.990127
Model ind 665 epoch 633 batch: 500 avg loss -2.783299 avg loss no lamb -2.783299 time 2020-06-26 10:58:50.479423
Model ind 665 epoch 633 batch: 600 avg loss -2.836278 avg loss no lamb -2.836278 time 2020-06-26 10:59:01.153630
Model ind 665 epoch 633 batch: 700 avg loss -2.738176 avg loss no lamb -2.738176 time 2020-06-26 10:59:11.782495
Model ind 665 epoch 633 batch: 800 avg loss -2.861162 avg loss no lamb -2.861162 time 2020-06-26 10:59:22.307454
last batch sz 10
Pre: time 2020-06-26 10:59:36.019370: 
 	std: 0.0027978634
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9804, 0.9746, 0.9805, 0.9754]
	train_accs: [0.98191667, 0.98135, 0.97585, 0.9819, 0.97625]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97840005
	best: 0.9811

Starting e_i: 634
Model ind 665 epoch 634 batch: 0 avg loss -2.889420 avg loss no lamb -2.889420 time 2020-06-26 10:59:36.927896
Model ind 665 epoch 634 batch: 100 avg loss -2.877010 avg loss no lamb -2.877010 time 2020-06-26 10:59:47.286562
Model ind 665 epoch 634 batch: 200 avg loss -2.822641 avg loss no lamb -2.822641 time 2020-06-26 10:59:57.923405
Model ind 665 epoch 634 batch: 300 avg loss -2.847321 avg loss no lamb -2.847321 time 2020-06-26 11:00:08.967850
Model ind 665 epoch 634 batch: 400 avg loss -2.752442 avg loss no lamb -2.752442 time 2020-06-26 11:00:19.645213
Model ind 665 epoch 634 batch: 500 avg loss -2.785803 avg loss no lamb -2.785803 time 2020-06-26 11:00:30.360860
Model ind 665 epoch 634 batch: 600 avg loss -2.820853 avg loss no lamb -2.820853 time 2020-06-26 11:00:41.106860
Model ind 665 epoch 634 batch: 700 avg loss -2.688247 avg loss no lamb -2.688247 time 2020-06-26 11:00:51.696156
Model ind 665 epoch 634 batch: 800 avg loss -2.841067 avg loss no lamb -2.841067 time 2020-06-26 11:01:02.573170
last batch sz 10
Pre: time 2020-06-26 11:01:16.506252: 
 	std: 0.002693408
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9814, 0.976, 0.9822, 0.9772]
	train_accs: [0.98246664, 0.98165, 0.9770167, 0.9820167, 0.9776667]
	best_train_sub_head: 0
	worst: 0.976
	avg: 0.97984
	best: 0.9824

Starting e_i: 635
Model ind 665 epoch 635 batch: 0 avg loss -2.921313 avg loss no lamb -2.921313 time 2020-06-26 11:01:17.413849
Model ind 665 epoch 635 batch: 100 avg loss -2.884619 avg loss no lamb -2.884619 time 2020-06-26 11:01:28.281585
Model ind 665 epoch 635 batch: 200 avg loss -2.845040 avg loss no lamb -2.845040 time 2020-06-26 11:01:39.151807
Model ind 665 epoch 635 batch: 300 avg loss -2.834101 avg loss no lamb -2.834101 time 2020-06-26 11:01:49.697760
Model ind 665 epoch 635 batch: 400 avg loss -2.830773 avg loss no lamb -2.830773 time 2020-06-26 11:02:00.348171
Model ind 665 epoch 635 batch: 500 avg loss -2.809557 avg loss no lamb -2.809557 time 2020-06-26 11:02:11.099874
Model ind 665 epoch 635 batch: 600 avg loss -2.883968 avg loss no lamb -2.883968 time 2020-06-26 11:02:21.550538
Model ind 665 epoch 635 batch: 700 avg loss -2.741171 avg loss no lamb -2.741171 time 2020-06-26 11:02:32.063413
Model ind 665 epoch 635 batch: 800 avg loss -2.786258 avg loss no lamb -2.786258 time 2020-06-26 11:02:42.720171
last batch sz 10
Pre: time 2020-06-26 11:02:56.702282: 
 	std: 0.0024285018
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9802, 0.9758, 0.9813, 0.9768]
	train_accs: [0.9818, 0.9806833, 0.97615, 0.98155, 0.97716665]
	best_train_sub_head: 0
	worst: 0.9758
	avg: 0.97918
	best: 0.9818

Starting e_i: 636
Model ind 665 epoch 636 batch: 0 avg loss -2.938819 avg loss no lamb -2.938819 time 2020-06-26 11:02:57.664602
Model ind 665 epoch 636 batch: 100 avg loss -2.826057 avg loss no lamb -2.826057 time 2020-06-26 11:03:08.372682
Model ind 665 epoch 636 batch: 200 avg loss -2.783011 avg loss no lamb -2.783011 time 2020-06-26 11:03:19.071245
Model ind 665 epoch 636 batch: 300 avg loss -2.859303 avg loss no lamb -2.859303 time 2020-06-26 11:03:29.775027
Model ind 665 epoch 636 batch: 400 avg loss -2.765871 avg loss no lamb -2.765871 time 2020-06-26 11:03:40.339500
Model ind 665 epoch 636 batch: 500 avg loss -2.832340 avg loss no lamb -2.832340 time 2020-06-26 11:03:51.038611
Model ind 665 epoch 636 batch: 600 avg loss -2.874207 avg loss no lamb -2.874207 time 2020-06-26 11:04:02.037190
Model ind 665 epoch 636 batch: 700 avg loss -2.741138 avg loss no lamb -2.741138 time 2020-06-26 11:04:12.761048
Model ind 665 epoch 636 batch: 800 avg loss -2.819529 avg loss no lamb -2.819529 time 2020-06-26 11:04:23.521259
last batch sz 10
Pre: time 2020-06-26 11:04:37.422435: 
 	std: 0.0025716985
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9801, 0.9765, 0.9816, 0.9766]
	train_accs: [0.98225, 0.98083335, 0.97645, 0.98183334, 0.9773]
	best_train_sub_head: 0
	worst: 0.9765
	avg: 0.97952
	best: 0.9828

Starting e_i: 637
Model ind 665 epoch 637 batch: 0 avg loss -2.926623 avg loss no lamb -2.926623 time 2020-06-26 11:04:38.327482
Model ind 665 epoch 637 batch: 100 avg loss -2.894665 avg loss no lamb -2.894665 time 2020-06-26 11:04:49.048382
Model ind 665 epoch 637 batch: 200 avg loss -2.832523 avg loss no lamb -2.832523 time 2020-06-26 11:04:59.673898
Model ind 665 epoch 637 batch: 300 avg loss -2.877676 avg loss no lamb -2.877676 time 2020-06-26 11:05:10.274831
Model ind 665 epoch 637 batch: 400 avg loss -2.769318 avg loss no lamb -2.769318 time 2020-06-26 11:05:20.609812
Model ind 665 epoch 637 batch: 500 avg loss -2.795166 avg loss no lamb -2.795166 time 2020-06-26 11:05:31.380485
Model ind 665 epoch 637 batch: 600 avg loss -2.852266 avg loss no lamb -2.852266 time 2020-06-26 11:05:42.276001
Model ind 665 epoch 637 batch: 700 avg loss -2.669154 avg loss no lamb -2.669154 time 2020-06-26 11:05:52.994563
Model ind 665 epoch 637 batch: 800 avg loss -2.829124 avg loss no lamb -2.829124 time 2020-06-26 11:06:03.589768
last batch sz 10
Pre: time 2020-06-26 11:06:17.527154: 
 	std: 0.0027746053
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9815, 0.9756, 0.9811, 0.9759]
	train_accs: [0.9817, 0.98118335, 0.9759833, 0.98155, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97914
	best: 0.9816

Starting e_i: 638
Model ind 665 epoch 638 batch: 0 avg loss -2.972499 avg loss no lamb -2.972499 time 2020-06-26 11:06:18.583370
Model ind 665 epoch 638 batch: 100 avg loss -2.913403 avg loss no lamb -2.913403 time 2020-06-26 11:06:29.426873
Model ind 665 epoch 638 batch: 200 avg loss -2.881905 avg loss no lamb -2.881905 time 2020-06-26 11:06:40.084303
Model ind 665 epoch 638 batch: 300 avg loss -2.827130 avg loss no lamb -2.827130 time 2020-06-26 11:06:50.779702
Model ind 665 epoch 638 batch: 400 avg loss -2.789405 avg loss no lamb -2.789405 time 2020-06-26 11:07:01.479645
Model ind 665 epoch 638 batch: 500 avg loss -2.765335 avg loss no lamb -2.765335 time 2020-06-26 11:07:12.349969
Model ind 665 epoch 638 batch: 600 avg loss -2.882551 avg loss no lamb -2.882551 time 2020-06-26 11:07:23.074387
Model ind 665 epoch 638 batch: 700 avg loss -2.727363 avg loss no lamb -2.727363 time 2020-06-26 11:07:33.752699
Model ind 665 epoch 638 batch: 800 avg loss -2.817782 avg loss no lamb -2.817782 time 2020-06-26 11:07:44.492125
last batch sz 10
Pre: time 2020-06-26 11:07:58.450520: 
 	std: 0.0031890997
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9802, 0.9743, 0.9822, 0.9769]
	train_accs: [0.98221666, 0.98113334, 0.97543335, 0.9820833, 0.97711664]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97924006
	best: 0.9826

Starting e_i: 639
Model ind 665 epoch 639 batch: 0 avg loss -2.900244 avg loss no lamb -2.900244 time 2020-06-26 11:07:59.385412
Model ind 665 epoch 639 batch: 100 avg loss -2.795427 avg loss no lamb -2.795427 time 2020-06-26 11:08:10.173821
Model ind 665 epoch 639 batch: 200 avg loss -2.880813 avg loss no lamb -2.880813 time 2020-06-26 11:08:21.083257
Model ind 665 epoch 639 batch: 300 avg loss -2.821398 avg loss no lamb -2.821398 time 2020-06-26 11:08:31.643442
Model ind 665 epoch 639 batch: 400 avg loss -2.787182 avg loss no lamb -2.787182 time 2020-06-26 11:08:42.585119
Model ind 665 epoch 639 batch: 500 avg loss -2.836934 avg loss no lamb -2.836934 time 2020-06-26 11:08:53.454404
Model ind 665 epoch 639 batch: 600 avg loss -2.858180 avg loss no lamb -2.858180 time 2020-06-26 11:09:04.488408
Model ind 665 epoch 639 batch: 700 avg loss -2.790115 avg loss no lamb -2.790115 time 2020-06-26 11:09:15.075994
Model ind 665 epoch 639 batch: 800 avg loss -2.829419 avg loss no lamb -2.829419 time 2020-06-26 11:09:25.936966
last batch sz 10
Pre: time 2020-06-26 11:09:39.956207: 
 	std: 0.003091655
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9826, 0.976, 0.9831, 0.977]
	train_accs: [0.9824167, 0.9820833, 0.9767333, 0.98246664, 0.9766]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.98026
	best: 0.9831

Starting e_i: 640
Model ind 665 epoch 640 batch: 0 avg loss -2.921671 avg loss no lamb -2.921671 time 2020-06-26 11:09:40.894243
Model ind 665 epoch 640 batch: 100 avg loss -2.815267 avg loss no lamb -2.815267 time 2020-06-26 11:09:51.707692
Model ind 665 epoch 640 batch: 200 avg loss -2.872035 avg loss no lamb -2.872035 time 2020-06-26 11:10:02.397616
Model ind 665 epoch 640 batch: 300 avg loss -2.822155 avg loss no lamb -2.822155 time 2020-06-26 11:10:13.092649
Model ind 665 epoch 640 batch: 400 avg loss -2.742146 avg loss no lamb -2.742146 time 2020-06-26 11:10:23.687938
Model ind 665 epoch 640 batch: 500 avg loss -2.839461 avg loss no lamb -2.839461 time 2020-06-26 11:10:34.424753
Model ind 665 epoch 640 batch: 600 avg loss -2.793575 avg loss no lamb -2.793575 time 2020-06-26 11:10:45.231614
Model ind 665 epoch 640 batch: 700 avg loss -2.696135 avg loss no lamb -2.696135 time 2020-06-26 11:10:55.747308
Model ind 665 epoch 640 batch: 800 avg loss -2.822746 avg loss no lamb -2.822746 time 2020-06-26 11:11:06.292663
last batch sz 10
Pre: time 2020-06-26 11:11:20.321980: 
 	std: 0.0034078716
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9799, 0.9744, 0.9825, 0.9757]
	train_accs: [0.98218334, 0.9809833, 0.97583336, 0.98186666, 0.97635]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97902
	best: 0.9826

Starting e_i: 641
Model ind 665 epoch 641 batch: 0 avg loss -2.924859 avg loss no lamb -2.924859 time 2020-06-26 11:11:22.353237
Model ind 665 epoch 641 batch: 100 avg loss -2.839687 avg loss no lamb -2.839687 time 2020-06-26 11:11:32.944152
Model ind 665 epoch 641 batch: 200 avg loss -2.801251 avg loss no lamb -2.801251 time 2020-06-26 11:11:43.505468
Model ind 665 epoch 641 batch: 300 avg loss -2.839789 avg loss no lamb -2.839789 time 2020-06-26 11:11:54.204694
Model ind 665 epoch 641 batch: 400 avg loss -2.689507 avg loss no lamb -2.689507 time 2020-06-26 11:12:04.791143
Model ind 665 epoch 641 batch: 500 avg loss -2.813145 avg loss no lamb -2.813145 time 2020-06-26 11:12:15.491078
Model ind 665 epoch 641 batch: 600 avg loss -2.834697 avg loss no lamb -2.834697 time 2020-06-26 11:12:26.224884
Model ind 665 epoch 641 batch: 700 avg loss -2.709793 avg loss no lamb -2.709793 time 2020-06-26 11:12:36.640843
Model ind 665 epoch 641 batch: 800 avg loss -2.796794 avg loss no lamb -2.796794 time 2020-06-26 11:12:47.094392
last batch sz 10
Pre: time 2020-06-26 11:13:01.390609: 
 	std: 0.0031123112
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9804, 0.9744, 0.981, 0.9752]
	train_accs: [0.9817167, 0.9808, 0.9757, 0.9816667, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97856
	best: 0.9818

Starting e_i: 642
Model ind 665 epoch 642 batch: 0 avg loss -2.944088 avg loss no lamb -2.944088 time 2020-06-26 11:13:02.322067
Model ind 665 epoch 642 batch: 100 avg loss -2.861670 avg loss no lamb -2.861670 time 2020-06-26 11:13:13.138740
Model ind 665 epoch 642 batch: 200 avg loss -2.841106 avg loss no lamb -2.841106 time 2020-06-26 11:13:23.921089
Model ind 665 epoch 642 batch: 300 avg loss -2.788319 avg loss no lamb -2.788319 time 2020-06-26 11:13:34.571028
Model ind 665 epoch 642 batch: 400 avg loss -2.752964 avg loss no lamb -2.752964 time 2020-06-26 11:13:45.264044
Model ind 665 epoch 642 batch: 500 avg loss -2.773911 avg loss no lamb -2.773911 time 2020-06-26 11:13:55.871769
Model ind 665 epoch 642 batch: 600 avg loss -2.773888 avg loss no lamb -2.773888 time 2020-06-26 11:14:06.771397
Model ind 665 epoch 642 batch: 700 avg loss -2.714121 avg loss no lamb -2.714121 time 2020-06-26 11:14:17.278697
Model ind 665 epoch 642 batch: 800 avg loss -2.806515 avg loss no lamb -2.806515 time 2020-06-26 11:14:27.676690
last batch sz 10
Pre: time 2020-06-26 11:14:41.584059: 
 	std: 0.0041758413
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9809, 0.9722, 0.9816, 0.9737]
	train_accs: [0.98216665, 0.98176664, 0.9748667, 0.9821, 0.9755]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97802
	best: 0.9817

Starting e_i: 643
Model ind 665 epoch 643 batch: 0 avg loss -2.957252 avg loss no lamb -2.957252 time 2020-06-26 11:14:42.566374
Model ind 665 epoch 643 batch: 100 avg loss -2.899935 avg loss no lamb -2.899935 time 2020-06-26 11:14:53.331090
Model ind 665 epoch 643 batch: 200 avg loss -2.812747 avg loss no lamb -2.812747 time 2020-06-26 11:15:04.004901
Model ind 665 epoch 643 batch: 300 avg loss -2.869291 avg loss no lamb -2.869291 time 2020-06-26 11:15:14.317201
Model ind 665 epoch 643 batch: 400 avg loss -2.776778 avg loss no lamb -2.776778 time 2020-06-26 11:15:25.294211
Model ind 665 epoch 643 batch: 500 avg loss -2.753991 avg loss no lamb -2.753991 time 2020-06-26 11:15:36.164803
Model ind 665 epoch 643 batch: 600 avg loss -2.856685 avg loss no lamb -2.856685 time 2020-06-26 11:15:47.072925
Model ind 665 epoch 643 batch: 700 avg loss -2.682391 avg loss no lamb -2.682391 time 2020-06-26 11:15:57.917984
Model ind 665 epoch 643 batch: 800 avg loss -2.799147 avg loss no lamb -2.799147 time 2020-06-26 11:16:08.519708
last batch sz 10
Pre: time 2020-06-26 11:16:22.663917: 
 	std: 0.0029266945
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9817, 0.9757, 0.9826, 0.977]
	train_accs: [0.9819833, 0.98178333, 0.9769833, 0.9821333, 0.97728336]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97988
	best: 0.9826

Starting e_i: 644
Model ind 665 epoch 644 batch: 0 avg loss -2.935122 avg loss no lamb -2.935122 time 2020-06-26 11:16:23.599201
Model ind 665 epoch 644 batch: 100 avg loss -2.844016 avg loss no lamb -2.844016 time 2020-06-26 11:16:34.428255
Model ind 665 epoch 644 batch: 200 avg loss -2.837640 avg loss no lamb -2.837640 time 2020-06-26 11:16:45.084189
Model ind 665 epoch 644 batch: 300 avg loss -2.883727 avg loss no lamb -2.883727 time 2020-06-26 11:16:55.762964
Model ind 665 epoch 644 batch: 400 avg loss -2.752363 avg loss no lamb -2.752363 time 2020-06-26 11:17:06.471370
Model ind 665 epoch 644 batch: 500 avg loss -2.863687 avg loss no lamb -2.863687 time 2020-06-26 11:17:17.152736
Model ind 665 epoch 644 batch: 600 avg loss -2.795312 avg loss no lamb -2.795312 time 2020-06-26 11:17:27.855389
Model ind 665 epoch 644 batch: 700 avg loss -2.748966 avg loss no lamb -2.748966 time 2020-06-26 11:17:38.427450
Model ind 665 epoch 644 batch: 800 avg loss -2.887662 avg loss no lamb -2.887662 time 2020-06-26 11:17:48.963212
last batch sz 10
Pre: time 2020-06-26 11:18:02.684412: 
 	std: 0.0032517116
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9799, 0.9733, 0.9814, 0.9756]
	train_accs: [0.98183334, 0.98116666, 0.9761, 0.9820333, 0.97675]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97828007
	best: 0.9814

Starting e_i: 645
Model ind 665 epoch 645 batch: 0 avg loss -2.949250 avg loss no lamb -2.949250 time 2020-06-26 11:18:03.711306
Model ind 665 epoch 645 batch: 100 avg loss -2.788733 avg loss no lamb -2.788733 time 2020-06-26 11:18:14.540069
Model ind 665 epoch 645 batch: 200 avg loss -2.846639 avg loss no lamb -2.846639 time 2020-06-26 11:18:25.247306
Model ind 665 epoch 645 batch: 300 avg loss -2.826942 avg loss no lamb -2.826942 time 2020-06-26 11:18:36.093576
Model ind 665 epoch 645 batch: 400 avg loss -2.664100 avg loss no lamb -2.664100 time 2020-06-26 11:18:46.746430
Model ind 665 epoch 645 batch: 500 avg loss -2.777078 avg loss no lamb -2.777078 time 2020-06-26 11:18:57.406666
Model ind 665 epoch 645 batch: 600 avg loss -2.831219 avg loss no lamb -2.831219 time 2020-06-26 11:19:08.345256
Model ind 665 epoch 645 batch: 700 avg loss -2.721282 avg loss no lamb -2.721282 time 2020-06-26 11:19:19.313323
Model ind 665 epoch 645 batch: 800 avg loss -2.791654 avg loss no lamb -2.791654 time 2020-06-26 11:19:29.990326
last batch sz 10
Pre: time 2020-06-26 11:19:43.590607: 
 	std: 0.0025466888
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9811, 0.9756, 0.9815, 0.9768]
	train_accs: [0.98185, 0.98141664, 0.97641665, 0.9819, 0.9767333]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97928
	best: 0.9815

Starting e_i: 646
Model ind 665 epoch 646 batch: 0 avg loss -2.873631 avg loss no lamb -2.873631 time 2020-06-26 11:19:44.512678
Model ind 665 epoch 646 batch: 100 avg loss -2.887526 avg loss no lamb -2.887526 time 2020-06-26 11:19:55.520690
Model ind 665 epoch 646 batch: 200 avg loss -2.846436 avg loss no lamb -2.846436 time 2020-06-26 11:20:06.343569
Model ind 665 epoch 646 batch: 300 avg loss -2.822227 avg loss no lamb -2.822227 time 2020-06-26 11:20:17.156609
Model ind 665 epoch 646 batch: 400 avg loss -2.739962 avg loss no lamb -2.739962 time 2020-06-26 11:20:28.012029
Model ind 665 epoch 646 batch: 500 avg loss -2.756660 avg loss no lamb -2.756660 time 2020-06-26 11:20:38.759636
Model ind 665 epoch 646 batch: 600 avg loss -2.832982 avg loss no lamb -2.832982 time 2020-06-26 11:20:49.512897
Model ind 665 epoch 646 batch: 700 avg loss -2.703822 avg loss no lamb -2.703822 time 2020-06-26 11:21:00.210850
Model ind 665 epoch 646 batch: 800 avg loss -2.787686 avg loss no lamb -2.787686 time 2020-06-26 11:21:10.752143
last batch sz 10
Pre: time 2020-06-26 11:21:24.509857: 
 	std: 0.0028772096
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9824, 0.9764, 0.9826, 0.9771]
	train_accs: [0.9824167, 0.9816833, 0.9769167, 0.98185, 0.97723335]
	best_train_sub_head: 0
	worst: 0.9764
	avg: 0.98026
	best: 0.9828

Starting e_i: 647
Model ind 665 epoch 647 batch: 0 avg loss -2.943663 avg loss no lamb -2.943663 time 2020-06-26 11:21:25.480027
Model ind 665 epoch 647 batch: 100 avg loss -2.902445 avg loss no lamb -2.902445 time 2020-06-26 11:21:36.300756
Model ind 665 epoch 647 batch: 200 avg loss -2.850878 avg loss no lamb -2.850878 time 2020-06-26 11:21:46.917957
Model ind 665 epoch 647 batch: 300 avg loss -2.824605 avg loss no lamb -2.824605 time 2020-06-26 11:21:57.684100
Model ind 665 epoch 647 batch: 400 avg loss -2.701509 avg loss no lamb -2.701509 time 2020-06-26 11:22:08.625370
Model ind 665 epoch 647 batch: 500 avg loss -2.789568 avg loss no lamb -2.789568 time 2020-06-26 11:22:19.556083
Model ind 665 epoch 647 batch: 600 avg loss -2.842242 avg loss no lamb -2.842242 time 2020-06-26 11:22:30.542655
Model ind 665 epoch 647 batch: 700 avg loss -2.739547 avg loss no lamb -2.739547 time 2020-06-26 11:22:41.377856
Model ind 665 epoch 647 batch: 800 avg loss -2.827710 avg loss no lamb -2.827710 time 2020-06-26 11:22:52.248373
last batch sz 10
Pre: time 2020-06-26 11:23:05.749188: 
 	std: 0.0025623394
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9789, 0.9732, 0.9798, 0.9765]
	train_accs: [0.98113334, 0.9806167, 0.97545, 0.98108333, 0.97665]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97768
	best: 0.98

Starting e_i: 648
Model ind 665 epoch 648 batch: 0 avg loss -2.971255 avg loss no lamb -2.971255 time 2020-06-26 11:23:06.667447
Model ind 665 epoch 648 batch: 100 avg loss -2.867282 avg loss no lamb -2.867282 time 2020-06-26 11:23:17.649788
Model ind 665 epoch 648 batch: 200 avg loss -2.840372 avg loss no lamb -2.840372 time 2020-06-26 11:23:28.280783
Model ind 665 epoch 648 batch: 300 avg loss -2.868518 avg loss no lamb -2.868518 time 2020-06-26 11:23:39.061907
Model ind 665 epoch 648 batch: 400 avg loss -2.768986 avg loss no lamb -2.768986 time 2020-06-26 11:23:49.625841
Model ind 665 epoch 648 batch: 500 avg loss -2.868572 avg loss no lamb -2.868572 time 2020-06-26 11:24:00.435697
Model ind 665 epoch 648 batch: 600 avg loss -2.827797 avg loss no lamb -2.827797 time 2020-06-26 11:24:11.272722
Model ind 665 epoch 648 batch: 700 avg loss -2.761792 avg loss no lamb -2.761792 time 2020-06-26 11:24:22.104022
Model ind 665 epoch 648 batch: 800 avg loss -2.868423 avg loss no lamb -2.868423 time 2020-06-26 11:24:32.820340
last batch sz 10
Pre: time 2020-06-26 11:24:46.421887: 
 	std: 0.0028118256
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9787, 0.9735, 0.9798, 0.9742]
	train_accs: [0.9805667, 0.98011667, 0.97501665, 0.98038334, 0.97585]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97723997
	best: 0.98

Starting e_i: 649
Model ind 665 epoch 649 batch: 0 avg loss -2.945718 avg loss no lamb -2.945718 time 2020-06-26 11:24:47.440037
Model ind 665 epoch 649 batch: 100 avg loss -2.853904 avg loss no lamb -2.853904 time 2020-06-26 11:24:58.231253
Model ind 665 epoch 649 batch: 200 avg loss -2.794077 avg loss no lamb -2.794077 time 2020-06-26 11:25:09.169870
Model ind 665 epoch 649 batch: 300 avg loss -2.870376 avg loss no lamb -2.870376 time 2020-06-26 11:25:19.862479
Model ind 665 epoch 649 batch: 400 avg loss -2.761048 avg loss no lamb -2.761048 time 2020-06-26 11:25:30.465613
Model ind 665 epoch 649 batch: 500 avg loss -2.822963 avg loss no lamb -2.822963 time 2020-06-26 11:25:41.161874
Model ind 665 epoch 649 batch: 600 avg loss -2.852479 avg loss no lamb -2.852479 time 2020-06-26 11:25:52.239872
Model ind 665 epoch 649 batch: 700 avg loss -2.766049 avg loss no lamb -2.766049 time 2020-06-26 11:26:03.301883
Model ind 665 epoch 649 batch: 800 avg loss -2.811234 avg loss no lamb -2.811234 time 2020-06-26 11:26:14.047166
last batch sz 10
Pre: time 2020-06-26 11:26:27.715386: 
 	std: 0.003652076
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9799, 0.972, 0.9806, 0.9736]
	train_accs: [0.98118335, 0.98095, 0.97431666, 0.9813, 0.97505]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97721994
	best: 0.9806

Starting e_i: 650
Model ind 665 epoch 650 batch: 0 avg loss -2.929092 avg loss no lamb -2.929092 time 2020-06-26 11:26:28.651464
Model ind 665 epoch 650 batch: 100 avg loss -2.877251 avg loss no lamb -2.877251 time 2020-06-26 11:26:39.436356
Model ind 665 epoch 650 batch: 200 avg loss -2.820031 avg loss no lamb -2.820031 time 2020-06-26 11:26:49.970775
Model ind 665 epoch 650 batch: 300 avg loss -2.896371 avg loss no lamb -2.896371 time 2020-06-26 11:27:00.829793
Model ind 665 epoch 650 batch: 400 avg loss -2.758806 avg loss no lamb -2.758806 time 2020-06-26 11:27:11.239414
Model ind 665 epoch 650 batch: 500 avg loss -2.831645 avg loss no lamb -2.831645 time 2020-06-26 11:27:21.997125
Model ind 665 epoch 650 batch: 600 avg loss -2.819487 avg loss no lamb -2.819487 time 2020-06-26 11:27:32.893358
Model ind 665 epoch 650 batch: 700 avg loss -2.764559 avg loss no lamb -2.764559 time 2020-06-26 11:27:43.770981
Model ind 665 epoch 650 batch: 800 avg loss -2.811636 avg loss no lamb -2.811636 time 2020-06-26 11:27:54.457526
last batch sz 10
Pre: time 2020-06-26 11:28:08.472008: 
 	std: 0.0028590863
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9806, 0.9745, 0.9808, 0.9752]
	train_accs: [0.9816167, 0.98073334, 0.9762833, 0.9816333, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97833997
	best: 0.9808

Starting e_i: 651
Model ind 665 epoch 651 batch: 0 avg loss -2.942686 avg loss no lamb -2.942686 time 2020-06-26 11:28:10.644887
Model ind 665 epoch 651 batch: 100 avg loss -2.822624 avg loss no lamb -2.822624 time 2020-06-26 11:28:21.375479
Model ind 665 epoch 651 batch: 200 avg loss -2.865057 avg loss no lamb -2.865057 time 2020-06-26 11:28:31.999768
Model ind 665 epoch 651 batch: 300 avg loss -2.844031 avg loss no lamb -2.844031 time 2020-06-26 11:28:42.893913
Model ind 665 epoch 651 batch: 400 avg loss -2.751353 avg loss no lamb -2.751353 time 2020-06-26 11:28:53.659260
Model ind 665 epoch 651 batch: 500 avg loss -2.786380 avg loss no lamb -2.786380 time 2020-06-26 11:29:04.313688
Model ind 665 epoch 651 batch: 600 avg loss -2.822407 avg loss no lamb -2.822407 time 2020-06-26 11:29:15.112914
Model ind 665 epoch 651 batch: 700 avg loss -2.775695 avg loss no lamb -2.775695 time 2020-06-26 11:29:25.739695
Model ind 665 epoch 651 batch: 800 avg loss -2.838462 avg loss no lamb -2.838462 time 2020-06-26 11:29:36.332311
last batch sz 10
Pre: time 2020-06-26 11:29:50.442413: 
 	std: 0.0028407194
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9806, 0.9747, 0.9815, 0.9766]
	train_accs: [0.98218334, 0.9815, 0.9766167, 0.9820833, 0.9775]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97902
	best: 0.9817

Starting e_i: 652
Model ind 665 epoch 652 batch: 0 avg loss -2.967567 avg loss no lamb -2.967567 time 2020-06-26 11:29:51.365182
Model ind 665 epoch 652 batch: 100 avg loss -2.863439 avg loss no lamb -2.863439 time 2020-06-26 11:30:02.056958
Model ind 665 epoch 652 batch: 200 avg loss -2.881976 avg loss no lamb -2.881976 time 2020-06-26 11:30:12.541082
Model ind 665 epoch 652 batch: 300 avg loss -2.804515 avg loss no lamb -2.804515 time 2020-06-26 11:30:23.062213
Model ind 665 epoch 652 batch: 400 avg loss -2.686159 avg loss no lamb -2.686159 time 2020-06-26 11:30:33.722840
Model ind 665 epoch 652 batch: 500 avg loss -2.797423 avg loss no lamb -2.797423 time 2020-06-26 11:30:44.428979
Model ind 665 epoch 652 batch: 600 avg loss -2.854542 avg loss no lamb -2.854542 time 2020-06-26 11:30:55.321509
Model ind 665 epoch 652 batch: 700 avg loss -2.715121 avg loss no lamb -2.715121 time 2020-06-26 11:31:06.119521
Model ind 665 epoch 652 batch: 800 avg loss -2.863795 avg loss no lamb -2.863795 time 2020-06-26 11:31:16.671025
last batch sz 10
Pre: time 2020-06-26 11:31:30.820061: 
 	std: 0.0035583188
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9784, 0.9712, 0.9797, 0.9741]
	train_accs: [0.9812833, 0.9799167, 0.97375, 0.9806167, 0.97513336]
	best_train_sub_head: 0
	worst: 0.9712
	avg: 0.97678006
	best: 0.9805

Starting e_i: 653
Model ind 665 epoch 653 batch: 0 avg loss -2.945766 avg loss no lamb -2.945766 time 2020-06-26 11:31:31.762082
Model ind 665 epoch 653 batch: 100 avg loss -2.911336 avg loss no lamb -2.911336 time 2020-06-26 11:31:42.590825
Model ind 665 epoch 653 batch: 200 avg loss -2.868722 avg loss no lamb -2.868722 time 2020-06-26 11:31:53.350363
Model ind 665 epoch 653 batch: 300 avg loss -2.917237 avg loss no lamb -2.917237 time 2020-06-26 11:32:04.030343
Model ind 665 epoch 653 batch: 400 avg loss -2.767732 avg loss no lamb -2.767732 time 2020-06-26 11:32:14.656609
Model ind 665 epoch 653 batch: 500 avg loss -2.881163 avg loss no lamb -2.881163 time 2020-06-26 11:32:25.283917
Model ind 665 epoch 653 batch: 600 avg loss -2.863008 avg loss no lamb -2.863008 time 2020-06-26 11:32:36.203405
Model ind 665 epoch 653 batch: 700 avg loss -2.695294 avg loss no lamb -2.695294 time 2020-06-26 11:32:47.117851
Model ind 665 epoch 653 batch: 800 avg loss -2.824012 avg loss no lamb -2.824012 time 2020-06-26 11:32:57.825120
last batch sz 10
Pre: time 2020-06-26 11:33:11.651938: 
 	std: 0.0032842662
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9795, 0.9741, 0.9814, 0.9746]
	train_accs: [0.98158336, 0.98053336, 0.9751, 0.9813167, 0.97548336]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.9782599
	best: 0.9817

Starting e_i: 654
Model ind 665 epoch 654 batch: 0 avg loss -2.890211 avg loss no lamb -2.890211 time 2020-06-26 11:33:12.601191
Model ind 665 epoch 654 batch: 100 avg loss -2.928544 avg loss no lamb -2.928544 time 2020-06-26 11:33:23.086297
Model ind 665 epoch 654 batch: 200 avg loss -2.899344 avg loss no lamb -2.899344 time 2020-06-26 11:33:33.901931
Model ind 665 epoch 654 batch: 300 avg loss -2.850227 avg loss no lamb -2.850227 time 2020-06-26 11:33:44.752615
Model ind 665 epoch 654 batch: 400 avg loss -2.761632 avg loss no lamb -2.761632 time 2020-06-26 11:33:55.441014
Model ind 665 epoch 654 batch: 500 avg loss -2.798203 avg loss no lamb -2.798203 time 2020-06-26 11:34:06.170373
Model ind 665 epoch 654 batch: 600 avg loss -2.814637 avg loss no lamb -2.814637 time 2020-06-26 11:34:16.546687
Model ind 665 epoch 654 batch: 700 avg loss -2.702246 avg loss no lamb -2.702246 time 2020-06-26 11:34:27.188993
Model ind 665 epoch 654 batch: 800 avg loss -2.825826 avg loss no lamb -2.825826 time 2020-06-26 11:34:37.919915
last batch sz 10
Pre: time 2020-06-26 11:34:52.146538: 
 	std: 0.0024383494
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9801, 0.9743, 0.9799, 0.9759]
	train_accs: [0.98125, 0.9812, 0.9755333, 0.9813, 0.9770167]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97802
	best: 0.9799

Starting e_i: 655
Model ind 665 epoch 655 batch: 0 avg loss -2.948475 avg loss no lamb -2.948475 time 2020-06-26 11:34:53.088459
Model ind 665 epoch 655 batch: 100 avg loss -2.892261 avg loss no lamb -2.892261 time 2020-06-26 11:35:03.859763
Model ind 665 epoch 655 batch: 200 avg loss -2.792372 avg loss no lamb -2.792372 time 2020-06-26 11:35:14.585053
Model ind 665 epoch 655 batch: 300 avg loss -2.837053 avg loss no lamb -2.837053 time 2020-06-26 11:35:25.288730
Model ind 665 epoch 655 batch: 400 avg loss -2.766227 avg loss no lamb -2.766227 time 2020-06-26 11:35:36.280517
Model ind 665 epoch 655 batch: 500 avg loss -2.809464 avg loss no lamb -2.809464 time 2020-06-26 11:35:46.971297
Model ind 665 epoch 655 batch: 600 avg loss -2.858141 avg loss no lamb -2.858141 time 2020-06-26 11:35:57.578764
Model ind 665 epoch 655 batch: 700 avg loss -2.742964 avg loss no lamb -2.742964 time 2020-06-26 11:36:08.549470
Model ind 665 epoch 655 batch: 800 avg loss -2.794188 avg loss no lamb -2.794188 time 2020-06-26 11:36:19.397188
last batch sz 10
Pre: time 2020-06-26 11:36:33.238232: 
 	std: 0.0026762665
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9818, 0.9756, 0.9817, 0.9775]
	train_accs: [0.9818, 0.98165, 0.9758833, 0.98156667, 0.9770167]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97974
	best: 0.9821

Starting e_i: 656
Model ind 665 epoch 656 batch: 0 avg loss -2.989793 avg loss no lamb -2.989793 time 2020-06-26 11:36:34.264485
Model ind 665 epoch 656 batch: 100 avg loss -2.922537 avg loss no lamb -2.922537 time 2020-06-26 11:36:44.967668
Model ind 665 epoch 656 batch: 200 avg loss -2.841135 avg loss no lamb -2.841135 time 2020-06-26 11:36:55.714229
Model ind 665 epoch 656 batch: 300 avg loss -2.838478 avg loss no lamb -2.838478 time 2020-06-26 11:37:06.586131
Model ind 665 epoch 656 batch: 400 avg loss -2.771543 avg loss no lamb -2.771543 time 2020-06-26 11:37:17.410123
Model ind 665 epoch 656 batch: 500 avg loss -2.805708 avg loss no lamb -2.805708 time 2020-06-26 11:37:28.291191
Model ind 665 epoch 656 batch: 600 avg loss -2.784081 avg loss no lamb -2.784081 time 2020-06-26 11:37:38.882386
Model ind 665 epoch 656 batch: 700 avg loss -2.716605 avg loss no lamb -2.716605 time 2020-06-26 11:37:49.697003
Model ind 665 epoch 656 batch: 800 avg loss -2.798713 avg loss no lamb -2.798713 time 2020-06-26 11:38:00.672087
last batch sz 10
Pre: time 2020-06-26 11:38:14.646954: 
 	std: 0.0028596402
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9791, 0.9732, 0.9802, 0.9748]
	train_accs: [0.98125, 0.9806167, 0.97543335, 0.98143333, 0.97645]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97742003
	best: 0.9802

Starting e_i: 657
Model ind 665 epoch 657 batch: 0 avg loss -2.924799 avg loss no lamb -2.924799 time 2020-06-26 11:38:15.553661
Model ind 665 epoch 657 batch: 100 avg loss -2.860012 avg loss no lamb -2.860012 time 2020-06-26 11:38:26.169536
Model ind 665 epoch 657 batch: 200 avg loss -2.855892 avg loss no lamb -2.855892 time 2020-06-26 11:38:36.879699
Model ind 665 epoch 657 batch: 300 avg loss -2.815641 avg loss no lamb -2.815641 time 2020-06-26 11:38:47.786898
Model ind 665 epoch 657 batch: 400 avg loss -2.780208 avg loss no lamb -2.780208 time 2020-06-26 11:38:58.395031
Model ind 665 epoch 657 batch: 500 avg loss -2.837867 avg loss no lamb -2.837867 time 2020-06-26 11:39:09.200136
Model ind 665 epoch 657 batch: 600 avg loss -2.814834 avg loss no lamb -2.814834 time 2020-06-26 11:39:20.012217
Model ind 665 epoch 657 batch: 700 avg loss -2.694004 avg loss no lamb -2.694004 time 2020-06-26 11:39:30.794711
Model ind 665 epoch 657 batch: 800 avg loss -2.799000 avg loss no lamb -2.799000 time 2020-06-26 11:39:41.583635
last batch sz 10
Pre: time 2020-06-26 11:39:55.625059: 
 	std: 0.0027129499
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9806, 0.9754, 0.9818, 0.977]
	train_accs: [0.9823833, 0.98153335, 0.97651666, 0.9821333, 0.9775]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97940004
	best: 0.9822

Starting e_i: 658
Model ind 665 epoch 658 batch: 0 avg loss -2.919473 avg loss no lamb -2.919473 time 2020-06-26 11:39:56.653236
Model ind 665 epoch 658 batch: 100 avg loss -2.859984 avg loss no lamb -2.859984 time 2020-06-26 11:40:07.527368
Model ind 665 epoch 658 batch: 200 avg loss -2.835578 avg loss no lamb -2.835578 time 2020-06-26 11:40:18.354833
Model ind 665 epoch 658 batch: 300 avg loss -2.826318 avg loss no lamb -2.826318 time 2020-06-26 11:40:29.045674
Model ind 665 epoch 658 batch: 400 avg loss -2.730907 avg loss no lamb -2.730907 time 2020-06-26 11:40:39.588760
Model ind 665 epoch 658 batch: 500 avg loss -2.810451 avg loss no lamb -2.810451 time 2020-06-26 11:40:50.274561
Model ind 665 epoch 658 batch: 600 avg loss -2.854664 avg loss no lamb -2.854664 time 2020-06-26 11:41:01.074135
Model ind 665 epoch 658 batch: 700 avg loss -2.685318 avg loss no lamb -2.685318 time 2020-06-26 11:41:11.796924
Model ind 665 epoch 658 batch: 800 avg loss -2.842774 avg loss no lamb -2.842774 time 2020-06-26 11:41:22.647631
last batch sz 10
Pre: time 2020-06-26 11:41:36.448587: 
 	std: 0.0031050988
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9789, 0.9722, 0.9802, 0.9756]
	train_accs: [0.98106664, 0.98071665, 0.9744667, 0.981, 0.9764]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97742003
	best: 0.9802

Starting e_i: 659
Model ind 665 epoch 659 batch: 0 avg loss -2.931941 avg loss no lamb -2.931941 time 2020-06-26 11:41:37.366474
Model ind 665 epoch 659 batch: 100 avg loss -2.907469 avg loss no lamb -2.907469 time 2020-06-26 11:41:48.152714
Model ind 665 epoch 659 batch: 200 avg loss -2.819013 avg loss no lamb -2.819013 time 2020-06-26 11:41:58.920433
Model ind 665 epoch 659 batch: 300 avg loss -2.796456 avg loss no lamb -2.796456 time 2020-06-26 11:42:09.676407
Model ind 665 epoch 659 batch: 400 avg loss -2.793455 avg loss no lamb -2.793455 time 2020-06-26 11:42:20.443004
Model ind 665 epoch 659 batch: 500 avg loss -2.781228 avg loss no lamb -2.781228 time 2020-06-26 11:42:30.989515
Model ind 665 epoch 659 batch: 600 avg loss -2.839668 avg loss no lamb -2.839668 time 2020-06-26 11:42:41.950245
Model ind 665 epoch 659 batch: 700 avg loss -2.721380 avg loss no lamb -2.721380 time 2020-06-26 11:42:52.803018
Model ind 665 epoch 659 batch: 800 avg loss -2.877819 avg loss no lamb -2.877819 time 2020-06-26 11:43:03.460916
last batch sz 10
Pre: time 2020-06-26 11:43:17.345709: 
 	std: 0.002995063
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9795, 0.9734, 0.9803, 0.9752]
	train_accs: [0.9813667, 0.98045, 0.9752, 0.9810167, 0.9766167]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97786
	best: 0.9809

Starting e_i: 660
Model ind 665 epoch 660 batch: 0 avg loss -2.899091 avg loss no lamb -2.899091 time 2020-06-26 11:43:18.300204
Model ind 665 epoch 660 batch: 100 avg loss -2.772975 avg loss no lamb -2.772975 time 2020-06-26 11:43:29.297113
Model ind 665 epoch 660 batch: 200 avg loss -2.815202 avg loss no lamb -2.815202 time 2020-06-26 11:43:40.141060
Model ind 665 epoch 660 batch: 300 avg loss -2.832751 avg loss no lamb -2.832751 time 2020-06-26 11:43:50.875833
Model ind 665 epoch 660 batch: 400 avg loss -2.741206 avg loss no lamb -2.741206 time 2020-06-26 11:44:01.455585
Model ind 665 epoch 660 batch: 500 avg loss -2.860507 avg loss no lamb -2.860507 time 2020-06-26 11:44:12.188359
Model ind 665 epoch 660 batch: 600 avg loss -2.822513 avg loss no lamb -2.822513 time 2020-06-26 11:44:23.023873
Model ind 665 epoch 660 batch: 700 avg loss -2.695676 avg loss no lamb -2.695676 time 2020-06-26 11:44:33.641828
Model ind 665 epoch 660 batch: 800 avg loss -2.771651 avg loss no lamb -2.771651 time 2020-06-26 11:44:44.497966
last batch sz 10
Pre: time 2020-06-26 11:44:58.163756: 
 	std: 0.0032751139
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9798, 0.9737, 0.9806, 0.9738]
	train_accs: [0.98165, 0.9809833, 0.97573334, 0.98155, 0.9756]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97774
	best: 0.9808

Starting e_i: 661
Model ind 665 epoch 661 batch: 0 avg loss -2.916704 avg loss no lamb -2.916704 time 2020-06-26 11:45:00.254913
Model ind 665 epoch 661 batch: 100 avg loss -2.880322 avg loss no lamb -2.880322 time 2020-06-26 11:45:11.025279
Model ind 665 epoch 661 batch: 200 avg loss -2.866894 avg loss no lamb -2.866894 time 2020-06-26 11:45:21.862601
Model ind 665 epoch 661 batch: 300 avg loss -2.827014 avg loss no lamb -2.827014 time 2020-06-26 11:45:32.639228
Model ind 665 epoch 661 batch: 400 avg loss -2.743060 avg loss no lamb -2.743060 time 2020-06-26 11:45:43.233682
Model ind 665 epoch 661 batch: 500 avg loss -2.828413 avg loss no lamb -2.828413 time 2020-06-26 11:45:53.561904
Model ind 665 epoch 661 batch: 600 avg loss -2.828218 avg loss no lamb -2.828218 time 2020-06-26 11:46:04.192658
Model ind 665 epoch 661 batch: 700 avg loss -2.846388 avg loss no lamb -2.846388 time 2020-06-26 11:46:15.093820
Model ind 665 epoch 661 batch: 800 avg loss -2.859588 avg loss no lamb -2.859588 time 2020-06-26 11:46:26.112740
last batch sz 10
Pre: time 2020-06-26 11:46:40.121357: 
 	std: 0.002750564
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.98, 0.9741, 0.9806, 0.9759]
	train_accs: [0.98176664, 0.98078334, 0.97573334, 0.9817167, 0.97641665]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97827995
	best: 0.9808

Starting e_i: 662
Model ind 665 epoch 662 batch: 0 avg loss -2.949497 avg loss no lamb -2.949497 time 2020-06-26 11:46:41.013505
Model ind 665 epoch 662 batch: 100 avg loss -2.868537 avg loss no lamb -2.868537 time 2020-06-26 11:46:51.544859
Model ind 665 epoch 662 batch: 200 avg loss -2.886041 avg loss no lamb -2.886041 time 2020-06-26 11:47:02.348902
Model ind 665 epoch 662 batch: 300 avg loss -2.898501 avg loss no lamb -2.898501 time 2020-06-26 11:47:13.028116
Model ind 665 epoch 662 batch: 400 avg loss -2.750775 avg loss no lamb -2.750775 time 2020-06-26 11:47:23.831893
Model ind 665 epoch 662 batch: 500 avg loss -2.772410 avg loss no lamb -2.772410 time 2020-06-26 11:47:34.624765
Model ind 665 epoch 662 batch: 600 avg loss -2.807579 avg loss no lamb -2.807579 time 2020-06-26 11:47:45.183652
Model ind 665 epoch 662 batch: 700 avg loss -2.766123 avg loss no lamb -2.766123 time 2020-06-26 11:47:55.912569
Model ind 665 epoch 662 batch: 800 avg loss -2.845161 avg loss no lamb -2.845161 time 2020-06-26 11:48:06.918007
last batch sz 10
Pre: time 2020-06-26 11:48:20.682734: 
 	std: 0.0030017176
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9801, 0.9739, 0.9809, 0.9756]
	train_accs: [0.9820833, 0.98106664, 0.97498333, 0.98191667, 0.9773833]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97833997
	best: 0.9812

Starting e_i: 663
Model ind 665 epoch 663 batch: 0 avg loss -2.909122 avg loss no lamb -2.909122 time 2020-06-26 11:48:21.684757
Model ind 665 epoch 663 batch: 100 avg loss -2.843677 avg loss no lamb -2.843677 time 2020-06-26 11:48:32.272361
Model ind 665 epoch 663 batch: 200 avg loss -2.854020 avg loss no lamb -2.854020 time 2020-06-26 11:48:42.836672
Model ind 665 epoch 663 batch: 300 avg loss -2.851518 avg loss no lamb -2.851518 time 2020-06-26 11:48:53.575183
Model ind 665 epoch 663 batch: 400 avg loss -2.733999 avg loss no lamb -2.733999 time 2020-06-26 11:49:04.489915
Model ind 665 epoch 663 batch: 500 avg loss -2.804948 avg loss no lamb -2.804948 time 2020-06-26 11:49:15.178063
Model ind 665 epoch 663 batch: 600 avg loss -2.782999 avg loss no lamb -2.782999 time 2020-06-26 11:49:25.987177
Model ind 665 epoch 663 batch: 700 avg loss -2.676836 avg loss no lamb -2.676836 time 2020-06-26 11:49:36.638198
Model ind 665 epoch 663 batch: 800 avg loss -2.830021 avg loss no lamb -2.830021 time 2020-06-26 11:49:47.519525
last batch sz 10
Pre: time 2020-06-26 11:50:01.674288: 
 	std: 0.00246138
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9797, 0.9746, 0.9802, 0.976]
	train_accs: [0.98115, 0.9806833, 0.9757, 0.98106664, 0.97675]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97824
	best: 0.9807

Starting e_i: 664
Model ind 665 epoch 664 batch: 0 avg loss -2.883456 avg loss no lamb -2.883456 time 2020-06-26 11:50:02.605982
Model ind 665 epoch 664 batch: 100 avg loss -2.885115 avg loss no lamb -2.885115 time 2020-06-26 11:50:13.342412
Model ind 665 epoch 664 batch: 200 avg loss -2.846905 avg loss no lamb -2.846905 time 2020-06-26 11:50:23.892205
Model ind 665 epoch 664 batch: 300 avg loss -2.840525 avg loss no lamb -2.840525 time 2020-06-26 11:50:34.798482
Model ind 665 epoch 664 batch: 400 avg loss -2.799604 avg loss no lamb -2.799604 time 2020-06-26 11:50:45.615812
Model ind 665 epoch 664 batch: 500 avg loss -2.823165 avg loss no lamb -2.823165 time 2020-06-26 11:50:56.453811
Model ind 665 epoch 664 batch: 600 avg loss -2.872709 avg loss no lamb -2.872709 time 2020-06-26 11:51:07.126642
Model ind 665 epoch 664 batch: 700 avg loss -2.684820 avg loss no lamb -2.684820 time 2020-06-26 11:51:18.002251
Model ind 665 epoch 664 batch: 800 avg loss -2.833802 avg loss no lamb -2.833802 time 2020-06-26 11:51:28.813048
last batch sz 10
Pre: time 2020-06-26 11:51:42.702727: 
 	std: 0.0026978487
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9798, 0.9746, 0.9806, 0.9754]
	train_accs: [0.9813667, 0.9813333, 0.9756333, 0.98158336, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97826004
	best: 0.9806

Starting e_i: 665
Model ind 665 epoch 665 batch: 0 avg loss -2.935835 avg loss no lamb -2.935835 time 2020-06-26 11:51:43.749123
Model ind 665 epoch 665 batch: 100 avg loss -2.886234 avg loss no lamb -2.886234 time 2020-06-26 11:51:54.371978
Model ind 665 epoch 665 batch: 200 avg loss -2.836772 avg loss no lamb -2.836772 time 2020-06-26 11:52:05.103615
Model ind 665 epoch 665 batch: 300 avg loss -2.865057 avg loss no lamb -2.865057 time 2020-06-26 11:52:15.915785
Model ind 665 epoch 665 batch: 400 avg loss -2.727176 avg loss no lamb -2.727176 time 2020-06-26 11:52:26.842694
Model ind 665 epoch 665 batch: 500 avg loss -2.819877 avg loss no lamb -2.819877 time 2020-06-26 11:52:37.596860
Model ind 665 epoch 665 batch: 600 avg loss -2.868012 avg loss no lamb -2.868012 time 2020-06-26 11:52:48.178739
Model ind 665 epoch 665 batch: 700 avg loss -2.790928 avg loss no lamb -2.790928 time 2020-06-26 11:52:58.642593
Model ind 665 epoch 665 batch: 800 avg loss -2.795426 avg loss no lamb -2.795426 time 2020-06-26 11:53:09.522446
last batch sz 10
Pre: time 2020-06-26 11:53:23.515534: 
 	std: 0.00277517
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.982, 0.976, 0.9822, 0.9771]
	train_accs: [0.9823833, 0.9818, 0.97681665, 0.9823833, 0.9776667]
	best_train_sub_head: 0
	worst: 0.976
	avg: 0.97992
	best: 0.9823

Starting e_i: 666
Model ind 665 epoch 666 batch: 0 avg loss -2.974359 avg loss no lamb -2.974359 time 2020-06-26 11:53:24.436683
Model ind 665 epoch 666 batch: 100 avg loss -2.900596 avg loss no lamb -2.900596 time 2020-06-26 11:53:35.149716
Model ind 665 epoch 666 batch: 200 avg loss -2.842426 avg loss no lamb -2.842426 time 2020-06-26 11:53:45.978764
Model ind 665 epoch 666 batch: 300 avg loss -2.832260 avg loss no lamb -2.832260 time 2020-06-26 11:53:56.642257
Model ind 665 epoch 666 batch: 400 avg loss -2.743642 avg loss no lamb -2.743642 time 2020-06-26 11:54:07.477966
Model ind 665 epoch 666 batch: 500 avg loss -2.751772 avg loss no lamb -2.751772 time 2020-06-26 11:54:18.269875
Model ind 665 epoch 666 batch: 600 avg loss -2.787558 avg loss no lamb -2.787558 time 2020-06-26 11:54:29.068964
Model ind 665 epoch 666 batch: 700 avg loss -2.742614 avg loss no lamb -2.742614 time 2020-06-26 11:54:39.589818
Model ind 665 epoch 666 batch: 800 avg loss -2.875285 avg loss no lamb -2.875285 time 2020-06-26 11:54:50.440389
last batch sz 10
Pre: time 2020-06-26 11:55:04.574277: 
 	std: 0.0026627837
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9808, 0.9747, 0.9803, 0.976]
	train_accs: [0.9818, 0.98148334, 0.97616667, 0.9817, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97856
	best: 0.981

Starting e_i: 667
Model ind 665 epoch 667 batch: 0 avg loss -2.885745 avg loss no lamb -2.885745 time 2020-06-26 11:55:05.601484
Model ind 665 epoch 667 batch: 100 avg loss -2.894406 avg loss no lamb -2.894406 time 2020-06-26 11:55:16.372141
Model ind 665 epoch 667 batch: 200 avg loss -2.895690 avg loss no lamb -2.895690 time 2020-06-26 11:55:26.872076
Model ind 665 epoch 667 batch: 300 avg loss -2.786480 avg loss no lamb -2.786480 time 2020-06-26 11:55:37.713047
Model ind 665 epoch 667 batch: 400 avg loss -2.795147 avg loss no lamb -2.795147 time 2020-06-26 11:55:48.591634
Model ind 665 epoch 667 batch: 500 avg loss -2.772070 avg loss no lamb -2.772070 time 2020-06-26 11:55:59.390293
Model ind 665 epoch 667 batch: 600 avg loss -2.883897 avg loss no lamb -2.883897 time 2020-06-26 11:56:10.271942
Model ind 665 epoch 667 batch: 700 avg loss -2.721565 avg loss no lamb -2.721565 time 2020-06-26 11:56:20.959258
Model ind 665 epoch 667 batch: 800 avg loss -2.805329 avg loss no lamb -2.805329 time 2020-06-26 11:56:31.695643
last batch sz 10
Pre: time 2020-06-26 11:56:45.568833: 
 	std: 0.0037695519
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9789, 0.9707, 0.9801, 0.9738]
	train_accs: [0.98095, 0.9798667, 0.9734, 0.9806833, 0.9751]
	best_train_sub_head: 0
	worst: 0.9707
	avg: 0.97668
	best: 0.9799

Starting e_i: 668
Model ind 665 epoch 668 batch: 0 avg loss -2.926301 avg loss no lamb -2.926301 time 2020-06-26 11:56:46.476414
Model ind 665 epoch 668 batch: 100 avg loss -2.922338 avg loss no lamb -2.922338 time 2020-06-26 11:56:57.287333
Model ind 665 epoch 668 batch: 200 avg loss -2.764079 avg loss no lamb -2.764079 time 2020-06-26 11:57:07.994398
Model ind 665 epoch 668 batch: 300 avg loss -2.762226 avg loss no lamb -2.762226 time 2020-06-26 11:57:18.727811
Model ind 665 epoch 668 batch: 400 avg loss -2.783437 avg loss no lamb -2.783437 time 2020-06-26 11:57:29.608700
Model ind 665 epoch 668 batch: 500 avg loss -2.818784 avg loss no lamb -2.818784 time 2020-06-26 11:57:40.285828
Model ind 665 epoch 668 batch: 600 avg loss -2.851311 avg loss no lamb -2.851311 time 2020-06-26 11:57:50.992677
Model ind 665 epoch 668 batch: 700 avg loss -2.705249 avg loss no lamb -2.705249 time 2020-06-26 11:58:01.666352
Model ind 665 epoch 668 batch: 800 avg loss -2.804057 avg loss no lamb -2.804057 time 2020-06-26 11:58:12.269808
last batch sz 10
Pre: time 2020-06-26 11:58:26.437111: 
 	std: 0.002905439
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9794, 0.9742, 0.9805, 0.9744]
	train_accs: [0.98121667, 0.9805, 0.9755833, 0.9813167, 0.976]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97782004
	best: 0.9805

Starting e_i: 669
Model ind 665 epoch 669 batch: 0 avg loss -2.955617 avg loss no lamb -2.955617 time 2020-06-26 11:58:27.393892
Model ind 665 epoch 669 batch: 100 avg loss -2.883455 avg loss no lamb -2.883455 time 2020-06-26 11:58:38.069424
Model ind 665 epoch 669 batch: 200 avg loss -2.814094 avg loss no lamb -2.814094 time 2020-06-26 11:58:48.671525
Model ind 665 epoch 669 batch: 300 avg loss -2.820379 avg loss no lamb -2.820379 time 2020-06-26 11:58:59.441922
Model ind 665 epoch 669 batch: 400 avg loss -2.756162 avg loss no lamb -2.756162 time 2020-06-26 11:59:09.923990
Model ind 665 epoch 669 batch: 500 avg loss -2.783847 avg loss no lamb -2.783847 time 2020-06-26 11:59:20.746555
Model ind 665 epoch 669 batch: 600 avg loss -2.840715 avg loss no lamb -2.840715 time 2020-06-26 11:59:31.425454
Model ind 665 epoch 669 batch: 700 avg loss -2.635381 avg loss no lamb -2.635381 time 2020-06-26 11:59:42.109041
Model ind 665 epoch 669 batch: 800 avg loss -2.824908 avg loss no lamb -2.824908 time 2020-06-26 11:59:52.766427
last batch sz 10
Pre: time 2020-06-26 12:00:06.385426: 
 	std: 0.0022276433
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9797, 0.9749, 0.98, 0.976]
	train_accs: [0.9816167, 0.9805667, 0.9764, 0.9817, 0.9770333]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97814
	best: 0.98

Starting e_i: 670
Model ind 665 epoch 670 batch: 0 avg loss -2.936992 avg loss no lamb -2.936992 time 2020-06-26 12:00:07.321450
Model ind 665 epoch 670 batch: 100 avg loss -2.854953 avg loss no lamb -2.854953 time 2020-06-26 12:00:18.112704
Model ind 665 epoch 670 batch: 200 avg loss -2.863495 avg loss no lamb -2.863495 time 2020-06-26 12:00:29.054866
Model ind 665 epoch 670 batch: 300 avg loss -2.885868 avg loss no lamb -2.885868 time 2020-06-26 12:00:39.643831
Model ind 665 epoch 670 batch: 400 avg loss -2.775883 avg loss no lamb -2.775883 time 2020-06-26 12:00:50.401514
Model ind 665 epoch 670 batch: 500 avg loss -2.807208 avg loss no lamb -2.807208 time 2020-06-26 12:01:01.340036
Model ind 665 epoch 670 batch: 600 avg loss -2.848090 avg loss no lamb -2.848090 time 2020-06-26 12:01:12.209535
Model ind 665 epoch 670 batch: 700 avg loss -2.747368 avg loss no lamb -2.747368 time 2020-06-26 12:01:23.080284
Model ind 665 epoch 670 batch: 800 avg loss -2.831334 avg loss no lamb -2.831334 time 2020-06-26 12:01:34.096476
last batch sz 10
Pre: time 2020-06-26 12:01:48.238809: 
 	std: 0.0031225572
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9807, 0.9739, 0.9813, 0.9765]
	train_accs: [0.98215, 0.98141664, 0.97601664, 0.98186666, 0.97735]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97886
	best: 0.9819

Starting e_i: 671
Model ind 665 epoch 671 batch: 0 avg loss -2.936005 avg loss no lamb -2.936005 time 2020-06-26 12:01:50.294122
Model ind 665 epoch 671 batch: 100 avg loss -2.843589 avg loss no lamb -2.843589 time 2020-06-26 12:02:01.226734
Model ind 665 epoch 671 batch: 200 avg loss -2.873777 avg loss no lamb -2.873777 time 2020-06-26 12:02:12.065146
Model ind 665 epoch 671 batch: 300 avg loss -2.828063 avg loss no lamb -2.828063 time 2020-06-26 12:02:22.779264
Model ind 665 epoch 671 batch: 400 avg loss -2.715684 avg loss no lamb -2.715684 time 2020-06-26 12:02:33.452231
Model ind 665 epoch 671 batch: 500 avg loss -2.773538 avg loss no lamb -2.773538 time 2020-06-26 12:02:44.000425
Model ind 665 epoch 671 batch: 600 avg loss -2.846480 avg loss no lamb -2.846480 time 2020-06-26 12:02:54.941803
Model ind 665 epoch 671 batch: 700 avg loss -2.655233 avg loss no lamb -2.655233 time 2020-06-26 12:03:06.189093
Model ind 665 epoch 671 batch: 800 avg loss -2.847426 avg loss no lamb -2.847426 time 2020-06-26 12:03:16.972666
last batch sz 10
Pre: time 2020-06-26 12:03:31.017400: 
 	std: 0.0027849535
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9807, 0.9749, 0.982, 0.9767]
	train_accs: [0.98146665, 0.98123336, 0.97645, 0.9817333, 0.97716665]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.9790999
	best: 0.982

Starting e_i: 672
Model ind 665 epoch 672 batch: 0 avg loss -2.886955 avg loss no lamb -2.886955 time 2020-06-26 12:03:32.052609
Model ind 665 epoch 672 batch: 100 avg loss -2.874205 avg loss no lamb -2.874205 time 2020-06-26 12:03:42.628741
Model ind 665 epoch 672 batch: 200 avg loss -2.798080 avg loss no lamb -2.798080 time 2020-06-26 12:03:53.450033
Model ind 665 epoch 672 batch: 300 avg loss -2.887452 avg loss no lamb -2.887452 time 2020-06-26 12:04:04.567438
Model ind 665 epoch 672 batch: 400 avg loss -2.776459 avg loss no lamb -2.776459 time 2020-06-26 12:04:15.332762
Model ind 665 epoch 672 batch: 500 avg loss -2.800653 avg loss no lamb -2.800653 time 2020-06-26 12:04:25.909837
Model ind 665 epoch 672 batch: 600 avg loss -2.851645 avg loss no lamb -2.851645 time 2020-06-26 12:04:36.924921
Model ind 665 epoch 672 batch: 700 avg loss -2.740420 avg loss no lamb -2.740420 time 2020-06-26 12:04:47.863532
Model ind 665 epoch 672 batch: 800 avg loss -2.835735 avg loss no lamb -2.835735 time 2020-06-26 12:04:58.770845
last batch sz 10
Pre: time 2020-06-26 12:05:12.636752: 
 	std: 0.0028874904
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9795, 0.9739, 0.981, 0.9759]
	train_accs: [0.9817167, 0.98078334, 0.97613335, 0.9817333, 0.97715]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97827995
	best: 0.981

Starting e_i: 673
Model ind 665 epoch 673 batch: 0 avg loss -2.887202 avg loss no lamb -2.887202 time 2020-06-26 12:05:13.558816
Model ind 665 epoch 673 batch: 100 avg loss -2.880276 avg loss no lamb -2.880276 time 2020-06-26 12:05:24.151086
Model ind 665 epoch 673 batch: 200 avg loss -2.881350 avg loss no lamb -2.881350 time 2020-06-26 12:05:34.918448
Model ind 665 epoch 673 batch: 300 avg loss -2.832556 avg loss no lamb -2.832556 time 2020-06-26 12:05:45.629438
Model ind 665 epoch 673 batch: 400 avg loss -2.777198 avg loss no lamb -2.777198 time 2020-06-26 12:05:56.184421
Model ind 665 epoch 673 batch: 500 avg loss -2.833677 avg loss no lamb -2.833677 time 2020-06-26 12:06:06.777876
Model ind 665 epoch 673 batch: 600 avg loss -2.873221 avg loss no lamb -2.873221 time 2020-06-26 12:06:17.526473
Model ind 665 epoch 673 batch: 700 avg loss -2.756874 avg loss no lamb -2.756874 time 2020-06-26 12:06:28.276186
Model ind 665 epoch 673 batch: 800 avg loss -2.821874 avg loss no lamb -2.821874 time 2020-06-26 12:06:39.203592
last batch sz 10
Pre: time 2020-06-26 12:06:53.375875: 
 	std: 0.0033302219
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.979, 0.9727, 0.98, 0.9727]
	train_accs: [0.98095, 0.9802667, 0.9748, 0.9809167, 0.97536665]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97676
	best: 0.9794

Starting e_i: 674
Model ind 665 epoch 674 batch: 0 avg loss -2.910483 avg loss no lamb -2.910483 time 2020-06-26 12:06:54.390823
Model ind 665 epoch 674 batch: 100 avg loss -2.830400 avg loss no lamb -2.830400 time 2020-06-26 12:07:05.057603
Model ind 665 epoch 674 batch: 200 avg loss -2.867715 avg loss no lamb -2.867715 time 2020-06-26 12:07:15.892217
Model ind 665 epoch 674 batch: 300 avg loss -2.871520 avg loss no lamb -2.871520 time 2020-06-26 12:07:26.622393
Model ind 665 epoch 674 batch: 400 avg loss -2.678806 avg loss no lamb -2.678806 time 2020-06-26 12:07:37.363119
Model ind 665 epoch 674 batch: 500 avg loss -2.818138 avg loss no lamb -2.818138 time 2020-06-26 12:07:48.123426
Model ind 665 epoch 674 batch: 600 avg loss -2.812434 avg loss no lamb -2.812434 time 2020-06-26 12:07:58.711638
Model ind 665 epoch 674 batch: 700 avg loss -2.691319 avg loss no lamb -2.691319 time 2020-06-26 12:08:09.741059
Model ind 665 epoch 674 batch: 800 avg loss -2.891418 avg loss no lamb -2.891418 time 2020-06-26 12:08:20.634030
last batch sz 10
Pre: time 2020-06-26 12:08:34.449400: 
 	std: 0.0029021502
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9811, 0.9747, 0.9809, 0.9755]
	train_accs: [0.98175, 0.98145, 0.97615, 0.9820167, 0.97625]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97863996
	best: 0.9809

Starting e_i: 675
Model ind 665 epoch 675 batch: 0 avg loss -2.930122 avg loss no lamb -2.930122 time 2020-06-26 12:08:35.336773
Model ind 665 epoch 675 batch: 100 avg loss -2.871853 avg loss no lamb -2.871853 time 2020-06-26 12:08:45.917880
Model ind 665 epoch 675 batch: 200 avg loss -2.841380 avg loss no lamb -2.841380 time 2020-06-26 12:08:56.784176
Model ind 665 epoch 675 batch: 300 avg loss -2.853843 avg loss no lamb -2.853843 time 2020-06-26 12:09:07.871105
Model ind 665 epoch 675 batch: 400 avg loss -2.801054 avg loss no lamb -2.801054 time 2020-06-26 12:09:18.664478
Model ind 665 epoch 675 batch: 500 avg loss -2.734899 avg loss no lamb -2.734899 time 2020-06-26 12:09:29.276105
Model ind 665 epoch 675 batch: 600 avg loss -2.859163 avg loss no lamb -2.859163 time 2020-06-26 12:09:39.840624
Model ind 665 epoch 675 batch: 700 avg loss -2.742754 avg loss no lamb -2.742754 time 2020-06-26 12:09:50.515290
Model ind 665 epoch 675 batch: 800 avg loss -2.779485 avg loss no lamb -2.779485 time 2020-06-26 12:10:01.630672
last batch sz 10
Pre: time 2020-06-26 12:10:15.732903: 
 	std: 0.0029253322
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9803, 0.9746, 0.9809, 0.9756]
	train_accs: [0.9819667, 0.98115, 0.97585, 0.98143333, 0.97636664]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97861993
	best: 0.9817

Starting e_i: 676
Model ind 665 epoch 676 batch: 0 avg loss -2.933101 avg loss no lamb -2.933101 time 2020-06-26 12:10:16.795154
Model ind 665 epoch 676 batch: 100 avg loss -2.859802 avg loss no lamb -2.859802 time 2020-06-26 12:10:27.147446
Model ind 665 epoch 676 batch: 200 avg loss -2.861997 avg loss no lamb -2.861997 time 2020-06-26 12:10:37.871352
Model ind 665 epoch 676 batch: 300 avg loss -2.858736 avg loss no lamb -2.858736 time 2020-06-26 12:10:48.736499
Model ind 665 epoch 676 batch: 400 avg loss -2.725402 avg loss no lamb -2.725402 time 2020-06-26 12:10:59.589786
Model ind 665 epoch 676 batch: 500 avg loss -2.791268 avg loss no lamb -2.791268 time 2020-06-26 12:11:10.279946
Model ind 665 epoch 676 batch: 600 avg loss -2.863557 avg loss no lamb -2.863557 time 2020-06-26 12:11:21.000056
Model ind 665 epoch 676 batch: 700 avg loss -2.766040 avg loss no lamb -2.766040 time 2020-06-26 12:11:31.748926
Model ind 665 epoch 676 batch: 800 avg loss -2.856109 avg loss no lamb -2.856109 time 2020-06-26 12:11:42.503411
last batch sz 10
Pre: time 2020-06-26 12:11:56.489670: 
 	std: 0.0027148426
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9801, 0.9745, 0.9808, 0.9759]
	train_accs: [0.98175, 0.98118335, 0.97566664, 0.98183334, 0.9767]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97846
	best: 0.9808

Starting e_i: 677
Model ind 665 epoch 677 batch: 0 avg loss -2.916135 avg loss no lamb -2.916135 time 2020-06-26 12:11:57.452680
Model ind 665 epoch 677 batch: 100 avg loss -2.857519 avg loss no lamb -2.857519 time 2020-06-26 12:12:08.197066
Model ind 665 epoch 677 batch: 200 avg loss -2.893054 avg loss no lamb -2.893054 time 2020-06-26 12:12:18.771881
Model ind 665 epoch 677 batch: 300 avg loss -2.778038 avg loss no lamb -2.778038 time 2020-06-26 12:12:29.428160
Model ind 665 epoch 677 batch: 400 avg loss -2.810551 avg loss no lamb -2.810551 time 2020-06-26 12:12:40.073602
Model ind 665 epoch 677 batch: 500 avg loss -2.828866 avg loss no lamb -2.828866 time 2020-06-26 12:12:50.773235
Model ind 665 epoch 677 batch: 600 avg loss -2.863223 avg loss no lamb -2.863223 time 2020-06-26 12:13:01.735245
Model ind 665 epoch 677 batch: 700 avg loss -2.769403 avg loss no lamb -2.769403 time 2020-06-26 12:13:12.478025
Model ind 665 epoch 677 batch: 800 avg loss -2.834974 avg loss no lamb -2.834974 time 2020-06-26 12:13:23.054876
last batch sz 10
Pre: time 2020-06-26 12:13:37.056272: 
 	std: 0.0029355697
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9813, 0.9745, 0.9819, 0.9773]
	train_accs: [0.9812833, 0.9810333, 0.9762333, 0.98141664, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97931993
	best: 0.9819

Starting e_i: 678
Model ind 665 epoch 678 batch: 0 avg loss -2.930357 avg loss no lamb -2.930357 time 2020-06-26 12:13:38.006709
Model ind 665 epoch 678 batch: 100 avg loss -2.872144 avg loss no lamb -2.872144 time 2020-06-26 12:13:48.758535
Model ind 665 epoch 678 batch: 200 avg loss -2.888319 avg loss no lamb -2.888319 time 2020-06-26 12:13:59.479039
Model ind 665 epoch 678 batch: 300 avg loss -2.871615 avg loss no lamb -2.871615 time 2020-06-26 12:14:10.032384
Model ind 665 epoch 678 batch: 400 avg loss -2.750543 avg loss no lamb -2.750543 time 2020-06-26 12:14:20.881804
Model ind 665 epoch 678 batch: 500 avg loss -2.804968 avg loss no lamb -2.804968 time 2020-06-26 12:14:31.715596
Model ind 665 epoch 678 batch: 600 avg loss -2.837523 avg loss no lamb -2.837523 time 2020-06-26 12:14:42.367796
Model ind 665 epoch 678 batch: 700 avg loss -2.748242 avg loss no lamb -2.748242 time 2020-06-26 12:14:52.782004
Model ind 665 epoch 678 batch: 800 avg loss -2.827471 avg loss no lamb -2.827471 time 2020-06-26 12:15:03.643168
last batch sz 10
Pre: time 2020-06-26 12:15:17.704008: 
 	std: 0.0033623767
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9817, 0.9745, 0.9821, 0.976]
	train_accs: [0.9823667, 0.9818, 0.97611666, 0.9823, 0.97693336]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97931993
	best: 0.9823

Starting e_i: 679
Model ind 665 epoch 679 batch: 0 avg loss -2.910414 avg loss no lamb -2.910414 time 2020-06-26 12:15:18.659009
Model ind 665 epoch 679 batch: 100 avg loss -2.913294 avg loss no lamb -2.913294 time 2020-06-26 12:15:29.259011
Model ind 665 epoch 679 batch: 200 avg loss -2.854943 avg loss no lamb -2.854943 time 2020-06-26 12:15:39.845662
Model ind 665 epoch 679 batch: 300 avg loss -2.882849 avg loss no lamb -2.882849 time 2020-06-26 12:15:50.239490
Model ind 665 epoch 679 batch: 400 avg loss -2.769543 avg loss no lamb -2.769543 time 2020-06-26 12:16:00.919603
Model ind 665 epoch 679 batch: 500 avg loss -2.786981 avg loss no lamb -2.786981 time 2020-06-26 12:16:11.731195
Model ind 665 epoch 679 batch: 600 avg loss -2.779926 avg loss no lamb -2.779926 time 2020-06-26 12:16:22.641997
Model ind 665 epoch 679 batch: 700 avg loss -2.699477 avg loss no lamb -2.699477 time 2020-06-26 12:16:33.175764
Model ind 665 epoch 679 batch: 800 avg loss -2.809664 avg loss no lamb -2.809664 time 2020-06-26 12:16:43.715243
last batch sz 10
Pre: time 2020-06-26 12:16:57.680176: 
 	std: 0.0029159014
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9816, 0.9758, 0.9822, 0.9766]
	train_accs: [0.98185, 0.98143333, 0.9766167, 0.98185, 0.97645]
	best_train_sub_head: 0
	worst: 0.9758
	avg: 0.97974
	best: 0.9825

Starting e_i: 680
Model ind 665 epoch 680 batch: 0 avg loss -2.921916 avg loss no lamb -2.921916 time 2020-06-26 12:16:58.652179
Model ind 665 epoch 680 batch: 100 avg loss -2.898475 avg loss no lamb -2.898475 time 2020-06-26 12:17:09.740197
Model ind 665 epoch 680 batch: 200 avg loss -2.858428 avg loss no lamb -2.858428 time 2020-06-26 12:17:20.560530
Model ind 665 epoch 680 batch: 300 avg loss -2.890536 avg loss no lamb -2.890536 time 2020-06-26 12:17:31.170124
Model ind 665 epoch 680 batch: 400 avg loss -2.858078 avg loss no lamb -2.858078 time 2020-06-26 12:17:41.575514
Model ind 665 epoch 680 batch: 500 avg loss -2.801928 avg loss no lamb -2.801928 time 2020-06-26 12:17:52.451290
Model ind 665 epoch 680 batch: 600 avg loss -2.902550 avg loss no lamb -2.902550 time 2020-06-26 12:18:03.198730
Model ind 665 epoch 680 batch: 700 avg loss -2.710549 avg loss no lamb -2.710549 time 2020-06-26 12:18:14.088496
Model ind 665 epoch 680 batch: 800 avg loss -2.840052 avg loss no lamb -2.840052 time 2020-06-26 12:18:24.695251
last batch sz 10
Pre: time 2020-06-26 12:18:38.506747: 
 	std: 0.0027925593
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9829, 0.9822, 0.9761, 0.9821, 0.9775]
	train_accs: [0.98226666, 0.98186666, 0.9766, 0.98226666, 0.97763336]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.98016006
	best: 0.9829

Starting e_i: 681
Model ind 665 epoch 681 batch: 0 avg loss -2.925700 avg loss no lamb -2.925700 time 2020-06-26 12:18:40.588499
Model ind 665 epoch 681 batch: 100 avg loss -2.875638 avg loss no lamb -2.875638 time 2020-06-26 12:18:51.262589
Model ind 665 epoch 681 batch: 200 avg loss -2.846930 avg loss no lamb -2.846930 time 2020-06-26 12:19:02.343495
Model ind 665 epoch 681 batch: 300 avg loss -2.832209 avg loss no lamb -2.832209 time 2020-06-26 12:19:13.248463
Model ind 665 epoch 681 batch: 400 avg loss -2.745166 avg loss no lamb -2.745166 time 2020-06-26 12:19:23.651996
Model ind 665 epoch 681 batch: 500 avg loss -2.853947 avg loss no lamb -2.853947 time 2020-06-26 12:19:34.403156
Model ind 665 epoch 681 batch: 600 avg loss -2.879636 avg loss no lamb -2.879636 time 2020-06-26 12:19:45.319246
Model ind 665 epoch 681 batch: 700 avg loss -2.706378 avg loss no lamb -2.706378 time 2020-06-26 12:19:56.306755
Model ind 665 epoch 681 batch: 800 avg loss -2.800982 avg loss no lamb -2.800982 time 2020-06-26 12:20:07.070513
last batch sz 10
Pre: time 2020-06-26 12:20:21.056243: 
 	std: 0.0024620278
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9806, 0.9757, 0.9819, 0.9776]
	train_accs: [0.9824333, 0.9816333, 0.9770167, 0.9824167, 0.97798336]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97951996
	best: 0.9818

Starting e_i: 682
Model ind 665 epoch 682 batch: 0 avg loss -2.938671 avg loss no lamb -2.938671 time 2020-06-26 12:20:21.998282
Model ind 665 epoch 682 batch: 100 avg loss -2.850023 avg loss no lamb -2.850023 time 2020-06-26 12:20:32.904463
Model ind 665 epoch 682 batch: 200 avg loss -2.798823 avg loss no lamb -2.798823 time 2020-06-26 12:20:43.863114
Model ind 665 epoch 682 batch: 300 avg loss -2.863623 avg loss no lamb -2.863623 time 2020-06-26 12:20:54.772157
Model ind 665 epoch 682 batch: 400 avg loss -2.747499 avg loss no lamb -2.747499 time 2020-06-26 12:21:05.197555
Model ind 665 epoch 682 batch: 500 avg loss -2.824295 avg loss no lamb -2.824295 time 2020-06-26 12:21:15.984452
Model ind 665 epoch 682 batch: 600 avg loss -2.826413 avg loss no lamb -2.826413 time 2020-06-26 12:21:27.001547
Model ind 665 epoch 682 batch: 700 avg loss -2.760239 avg loss no lamb -2.760239 time 2020-06-26 12:21:37.878847
Model ind 665 epoch 682 batch: 800 avg loss -2.850127 avg loss no lamb -2.850127 time 2020-06-26 12:21:48.662162
last batch sz 10
Pre: time 2020-06-26 12:22:02.425535: 
 	std: 0.002588444
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9803, 0.9752, 0.9811, 0.9764]
	train_accs: [0.98225, 0.98165, 0.97645, 0.98193336, 0.97688335]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.9789001
	best: 0.9815

Starting e_i: 683
Model ind 665 epoch 683 batch: 0 avg loss -2.943250 avg loss no lamb -2.943250 time 2020-06-26 12:22:03.428969
Model ind 665 epoch 683 batch: 100 avg loss -2.819179 avg loss no lamb -2.819179 time 2020-06-26 12:22:14.087247
Model ind 665 epoch 683 batch: 200 avg loss -2.865097 avg loss no lamb -2.865097 time 2020-06-26 12:22:24.836786
Model ind 665 epoch 683 batch: 300 avg loss -2.840143 avg loss no lamb -2.840143 time 2020-06-26 12:22:35.788127
Model ind 665 epoch 683 batch: 400 avg loss -2.731776 avg loss no lamb -2.731776 time 2020-06-26 12:22:46.372100
Model ind 665 epoch 683 batch: 500 avg loss -2.790955 avg loss no lamb -2.790955 time 2020-06-26 12:22:56.943976
Model ind 665 epoch 683 batch: 600 avg loss -2.875170 avg loss no lamb -2.875170 time 2020-06-26 12:23:07.896496
Model ind 665 epoch 683 batch: 700 avg loss -2.679355 avg loss no lamb -2.679355 time 2020-06-26 12:23:18.578391
Model ind 665 epoch 683 batch: 800 avg loss -2.885525 avg loss no lamb -2.885525 time 2020-06-26 12:23:29.270391
last batch sz 10
Pre: time 2020-06-26 12:23:43.387754: 
 	std: 0.0023726947
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9793, 0.9747, 0.98, 0.9758]
	train_accs: [0.9817, 0.9805167, 0.9759, 0.9817, 0.97711664]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97808
	best: 0.9806

Starting e_i: 684
Model ind 665 epoch 684 batch: 0 avg loss -2.910499 avg loss no lamb -2.910499 time 2020-06-26 12:23:44.313338
Model ind 665 epoch 684 batch: 100 avg loss -2.858022 avg loss no lamb -2.858022 time 2020-06-26 12:23:55.105640
Model ind 665 epoch 684 batch: 200 avg loss -2.867831 avg loss no lamb -2.867831 time 2020-06-26 12:24:05.844470
Model ind 665 epoch 684 batch: 300 avg loss -2.859105 avg loss no lamb -2.859105 time 2020-06-26 12:24:16.403192
Model ind 665 epoch 684 batch: 400 avg loss -2.784430 avg loss no lamb -2.784430 time 2020-06-26 12:24:27.171243
Model ind 665 epoch 684 batch: 500 avg loss -2.801302 avg loss no lamb -2.801302 time 2020-06-26 12:24:37.876220
Model ind 665 epoch 684 batch: 600 avg loss -2.871239 avg loss no lamb -2.871239 time 2020-06-26 12:24:48.588491
Model ind 665 epoch 684 batch: 700 avg loss -2.746564 avg loss no lamb -2.746564 time 2020-06-26 12:24:59.246881
Model ind 665 epoch 684 batch: 800 avg loss -2.889477 avg loss no lamb -2.889477 time 2020-06-26 12:25:09.952591
last batch sz 10
Pre: time 2020-06-26 12:25:24.134572: 
 	std: 0.002935992
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9807, 0.9748, 0.9815, 0.9759]
	train_accs: [0.9817, 0.9809667, 0.9755667, 0.9816833, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.9789001
	best: 0.9816

Starting e_i: 685
Model ind 665 epoch 685 batch: 0 avg loss -2.902709 avg loss no lamb -2.902709 time 2020-06-26 12:25:25.176895
Model ind 665 epoch 685 batch: 100 avg loss -2.822788 avg loss no lamb -2.822788 time 2020-06-26 12:25:35.978150
Model ind 665 epoch 685 batch: 200 avg loss -2.777113 avg loss no lamb -2.777113 time 2020-06-26 12:25:46.755092
Model ind 665 epoch 685 batch: 300 avg loss -2.781110 avg loss no lamb -2.781110 time 2020-06-26 12:25:57.556973
Model ind 665 epoch 685 batch: 400 avg loss -2.781542 avg loss no lamb -2.781542 time 2020-06-26 12:26:08.538798
Model ind 665 epoch 685 batch: 500 avg loss -2.769606 avg loss no lamb -2.769606 time 2020-06-26 12:26:19.369479
Model ind 665 epoch 685 batch: 600 avg loss -2.830372 avg loss no lamb -2.830372 time 2020-06-26 12:26:30.132216
Model ind 665 epoch 685 batch: 700 avg loss -2.772684 avg loss no lamb -2.772684 time 2020-06-26 12:26:41.029691
Model ind 665 epoch 685 batch: 800 avg loss -2.834736 avg loss no lamb -2.834736 time 2020-06-26 12:26:51.844172
last batch sz 10
Pre: time 2020-06-26 12:27:06.038111: 
 	std: 0.0033683348
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9807, 0.9741, 0.9818, 0.9751]
	train_accs: [0.98181665, 0.9809667, 0.97525, 0.9821333, 0.9756167]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97868
	best: 0.9818

Starting e_i: 686
Model ind 665 epoch 686 batch: 0 avg loss -2.931464 avg loss no lamb -2.931464 time 2020-06-26 12:27:06.978653
Model ind 665 epoch 686 batch: 100 avg loss -2.890507 avg loss no lamb -2.890507 time 2020-06-26 12:27:18.010808
Model ind 665 epoch 686 batch: 200 avg loss -2.855975 avg loss no lamb -2.855975 time 2020-06-26 12:27:28.600821
Model ind 665 epoch 686 batch: 300 avg loss -2.854532 avg loss no lamb -2.854532 time 2020-06-26 12:27:39.373559
Model ind 665 epoch 686 batch: 400 avg loss -2.741577 avg loss no lamb -2.741577 time 2020-06-26 12:27:50.310697
Model ind 665 epoch 686 batch: 500 avg loss -2.789820 avg loss no lamb -2.789820 time 2020-06-26 12:28:01.071012
Model ind 665 epoch 686 batch: 600 avg loss -2.877845 avg loss no lamb -2.877845 time 2020-06-26 12:28:11.945528
Model ind 665 epoch 686 batch: 700 avg loss -2.718283 avg loss no lamb -2.718283 time 2020-06-26 12:28:22.916488
Model ind 665 epoch 686 batch: 800 avg loss -2.763087 avg loss no lamb -2.763087 time 2020-06-26 12:28:33.948199
last batch sz 10
Pre: time 2020-06-26 12:28:47.903869: 
 	std: 0.0029702515
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9814, 0.9755, 0.9818, 0.9764]
	train_accs: [0.9820667, 0.9813833, 0.97613335, 0.9821, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97954
	best: 0.9818

Starting e_i: 687
Model ind 665 epoch 687 batch: 0 avg loss -2.942805 avg loss no lamb -2.942805 time 2020-06-26 12:28:49.002391
Model ind 665 epoch 687 batch: 100 avg loss -2.821837 avg loss no lamb -2.821837 time 2020-06-26 12:28:59.874575
Model ind 665 epoch 687 batch: 200 avg loss -2.912236 avg loss no lamb -2.912236 time 2020-06-26 12:29:10.559800
Model ind 665 epoch 687 batch: 300 avg loss -2.839324 avg loss no lamb -2.839324 time 2020-06-26 12:29:21.269018
Model ind 665 epoch 687 batch: 400 avg loss -2.800190 avg loss no lamb -2.800190 time 2020-06-26 12:29:32.050827
Model ind 665 epoch 687 batch: 500 avg loss -2.849209 avg loss no lamb -2.849209 time 2020-06-26 12:29:43.379153
Model ind 665 epoch 687 batch: 600 avg loss -2.851467 avg loss no lamb -2.851467 time 2020-06-26 12:29:54.158062
Model ind 665 epoch 687 batch: 700 avg loss -2.686259 avg loss no lamb -2.686259 time 2020-06-26 12:30:04.844917
Model ind 665 epoch 687 batch: 800 avg loss -2.863584 avg loss no lamb -2.863584 time 2020-06-26 12:30:15.708230
last batch sz 10
Pre: time 2020-06-26 12:30:29.557975: 
 	std: 0.002848444
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9798, 0.9742, 0.9811, 0.9754]
	train_accs: [0.98106664, 0.9803333, 0.97508335, 0.98095, 0.97605]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97822
	best: 0.9806

Starting e_i: 688
Model ind 665 epoch 688 batch: 0 avg loss -2.924843 avg loss no lamb -2.924843 time 2020-06-26 12:30:30.479097
Model ind 665 epoch 688 batch: 100 avg loss -2.741792 avg loss no lamb -2.741792 time 2020-06-26 12:30:41.292822
Model ind 665 epoch 688 batch: 200 avg loss -2.780383 avg loss no lamb -2.780383 time 2020-06-26 12:30:51.979238
Model ind 665 epoch 688 batch: 300 avg loss -2.854379 avg loss no lamb -2.854379 time 2020-06-26 12:31:02.649973
Model ind 665 epoch 688 batch: 400 avg loss -2.836403 avg loss no lamb -2.836403 time 2020-06-26 12:31:13.212668
Model ind 665 epoch 688 batch: 500 avg loss -2.816467 avg loss no lamb -2.816467 time 2020-06-26 12:31:23.744160
Model ind 665 epoch 688 batch: 600 avg loss -2.801498 avg loss no lamb -2.801498 time 2020-06-26 12:31:34.641223
Model ind 665 epoch 688 batch: 700 avg loss -2.656134 avg loss no lamb -2.656134 time 2020-06-26 12:31:45.444257
Model ind 665 epoch 688 batch: 800 avg loss -2.828458 avg loss no lamb -2.828458 time 2020-06-26 12:31:56.226784
last batch sz 10
Pre: time 2020-06-26 12:32:10.518694: 
 	std: 0.0025334717
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9789, 0.974, 0.9799, 0.9748]
	train_accs: [0.9806167, 0.9804, 0.9755833, 0.9808667, 0.9766]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97746
	best: 0.9799

Starting e_i: 689
Model ind 665 epoch 689 batch: 0 avg loss -2.949286 avg loss no lamb -2.949286 time 2020-06-26 12:32:11.484971
Model ind 665 epoch 689 batch: 100 avg loss -2.812500 avg loss no lamb -2.812500 time 2020-06-26 12:32:22.187300
Model ind 665 epoch 689 batch: 200 avg loss -2.860199 avg loss no lamb -2.860199 time 2020-06-26 12:32:33.120345
Model ind 665 epoch 689 batch: 300 avg loss -2.786316 avg loss no lamb -2.786316 time 2020-06-26 12:32:44.183174
Model ind 665 epoch 689 batch: 400 avg loss -2.736966 avg loss no lamb -2.736966 time 2020-06-26 12:32:55.197011
Model ind 665 epoch 689 batch: 500 avg loss -2.823113 avg loss no lamb -2.823113 time 2020-06-26 12:33:06.129185
Model ind 665 epoch 689 batch: 600 avg loss -2.846850 avg loss no lamb -2.846850 time 2020-06-26 12:33:17.038357
Model ind 665 epoch 689 batch: 700 avg loss -2.757237 avg loss no lamb -2.757237 time 2020-06-26 12:33:27.862443
Model ind 665 epoch 689 batch: 800 avg loss -2.813256 avg loss no lamb -2.813256 time 2020-06-26 12:33:38.804378
last batch sz 10
Pre: time 2020-06-26 12:33:52.750726: 
 	std: 0.002762171
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9796, 0.9741, 0.981, 0.9757]
	train_accs: [0.98155, 0.98075, 0.9755333, 0.9815, 0.9762]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97817993
	best: 0.9805

Starting e_i: 690
Model ind 665 epoch 690 batch: 0 avg loss -2.893567 avg loss no lamb -2.893567 time 2020-06-26 12:33:53.678645
Model ind 665 epoch 690 batch: 100 avg loss -2.808565 avg loss no lamb -2.808565 time 2020-06-26 12:34:04.383069
Model ind 665 epoch 690 batch: 200 avg loss -2.813447 avg loss no lamb -2.813447 time 2020-06-26 12:34:15.241948
Model ind 665 epoch 690 batch: 300 avg loss -2.851529 avg loss no lamb -2.851529 time 2020-06-26 12:34:26.008821
Model ind 665 epoch 690 batch: 400 avg loss -2.791417 avg loss no lamb -2.791417 time 2020-06-26 12:34:36.800149
Model ind 665 epoch 690 batch: 500 avg loss -2.833053 avg loss no lamb -2.833053 time 2020-06-26 12:34:47.724761
Model ind 665 epoch 690 batch: 600 avg loss -2.843258 avg loss no lamb -2.843258 time 2020-06-26 12:34:58.623710
Model ind 665 epoch 690 batch: 700 avg loss -2.637929 avg loss no lamb -2.637929 time 2020-06-26 12:35:09.578649
Model ind 665 epoch 690 batch: 800 avg loss -2.812196 avg loss no lamb -2.812196 time 2020-06-26 12:35:20.487999
last batch sz 10
Pre: time 2020-06-26 12:35:34.922150: 
 	std: 0.0028645443
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.98, 0.9746, 0.9817, 0.9763]
	train_accs: [0.98135, 0.98038334, 0.97515, 0.98148334, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97882
	best: 0.9817

Starting e_i: 691
Model ind 665 epoch 691 batch: 0 avg loss -2.897538 avg loss no lamb -2.897538 time 2020-06-26 12:35:37.081216
Model ind 665 epoch 691 batch: 100 avg loss -2.852945 avg loss no lamb -2.852945 time 2020-06-26 12:35:47.878655
Model ind 665 epoch 691 batch: 200 avg loss -2.825000 avg loss no lamb -2.825000 time 2020-06-26 12:35:58.737760
Model ind 665 epoch 691 batch: 300 avg loss -2.869837 avg loss no lamb -2.869837 time 2020-06-26 12:36:09.662979
Model ind 665 epoch 691 batch: 400 avg loss -2.727876 avg loss no lamb -2.727876 time 2020-06-26 12:36:20.403871
Model ind 665 epoch 691 batch: 500 avg loss -2.814580 avg loss no lamb -2.814580 time 2020-06-26 12:36:31.106892
Model ind 665 epoch 691 batch: 600 avg loss -2.897006 avg loss no lamb -2.897006 time 2020-06-26 12:36:42.122095
Model ind 665 epoch 691 batch: 700 avg loss -2.695491 avg loss no lamb -2.695491 time 2020-06-26 12:36:52.787701
Model ind 665 epoch 691 batch: 800 avg loss -2.713114 avg loss no lamb -2.713114 time 2020-06-26 12:37:03.546021
last batch sz 10
Pre: time 2020-06-26 12:37:17.267878: 
 	std: 0.0027412262
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9788, 0.9738, 0.9798, 0.9743]
	train_accs: [0.9811, 0.98053336, 0.9758, 0.9806833, 0.97608334]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97735995
	best: 0.9801

Starting e_i: 692
Model ind 665 epoch 692 batch: 0 avg loss -2.942966 avg loss no lamb -2.942966 time 2020-06-26 12:37:18.297875
Model ind 665 epoch 692 batch: 100 avg loss -2.895060 avg loss no lamb -2.895060 time 2020-06-26 12:37:29.272961
Model ind 665 epoch 692 batch: 200 avg loss -2.859599 avg loss no lamb -2.859599 time 2020-06-26 12:37:39.854800
Model ind 665 epoch 692 batch: 300 avg loss -2.894972 avg loss no lamb -2.894972 time 2020-06-26 12:37:50.592189
Model ind 665 epoch 692 batch: 400 avg loss -2.743435 avg loss no lamb -2.743435 time 2020-06-26 12:38:01.363899
Model ind 665 epoch 692 batch: 500 avg loss -2.796733 avg loss no lamb -2.796733 time 2020-06-26 12:38:12.084685
Model ind 665 epoch 692 batch: 600 avg loss -2.875119 avg loss no lamb -2.875119 time 2020-06-26 12:38:22.908539
Model ind 665 epoch 692 batch: 700 avg loss -2.726285 avg loss no lamb -2.726285 time 2020-06-26 12:38:33.674445
Model ind 665 epoch 692 batch: 800 avg loss -2.841259 avg loss no lamb -2.841259 time 2020-06-26 12:38:44.516346
last batch sz 10
Pre: time 2020-06-26 12:38:58.376120: 
 	std: 0.0020722975
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9783, 0.9782, 0.9733, 0.9783, 0.9751]
	train_accs: [0.98081666, 0.9805, 0.9759833, 0.9810333, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97664005
	best: 0.9783

Starting e_i: 693
Model ind 665 epoch 693 batch: 0 avg loss -2.904085 avg loss no lamb -2.904085 time 2020-06-26 12:38:59.330208
Model ind 665 epoch 693 batch: 100 avg loss -2.889396 avg loss no lamb -2.889396 time 2020-06-26 12:39:10.070022
Model ind 665 epoch 693 batch: 200 avg loss -2.902734 avg loss no lamb -2.902734 time 2020-06-26 12:39:20.782737
Model ind 665 epoch 693 batch: 300 avg loss -2.864346 avg loss no lamb -2.864346 time 2020-06-26 12:39:31.300455
Model ind 665 epoch 693 batch: 400 avg loss -2.698528 avg loss no lamb -2.698528 time 2020-06-26 12:39:42.111496
Model ind 665 epoch 693 batch: 500 avg loss -2.799018 avg loss no lamb -2.799018 time 2020-06-26 12:39:52.858431
Model ind 665 epoch 693 batch: 600 avg loss -2.840170 avg loss no lamb -2.840170 time 2020-06-26 12:40:03.681778
Model ind 665 epoch 693 batch: 700 avg loss -2.819831 avg loss no lamb -2.819831 time 2020-06-26 12:40:14.284728
Model ind 665 epoch 693 batch: 800 avg loss -2.814187 avg loss no lamb -2.814187 time 2020-06-26 12:40:24.918328
last batch sz 10
Pre: time 2020-06-26 12:40:38.933579: 
 	std: 0.0030979884
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9806, 0.9741, 0.9819, 0.9761]
	train_accs: [0.9816667, 0.9813667, 0.97583336, 0.98175, 0.97678334]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97878
	best: 0.9819

Starting e_i: 694
Model ind 665 epoch 694 batch: 0 avg loss -2.908766 avg loss no lamb -2.908766 time 2020-06-26 12:40:39.989533
Model ind 665 epoch 694 batch: 100 avg loss -2.846586 avg loss no lamb -2.846586 time 2020-06-26 12:40:50.606338
Model ind 665 epoch 694 batch: 200 avg loss -2.873828 avg loss no lamb -2.873828 time 2020-06-26 12:41:01.441379
Model ind 665 epoch 694 batch: 300 avg loss -2.880073 avg loss no lamb -2.880073 time 2020-06-26 12:41:12.245167
Model ind 665 epoch 694 batch: 400 avg loss -2.754868 avg loss no lamb -2.754868 time 2020-06-26 12:41:23.043927
Model ind 665 epoch 694 batch: 500 avg loss -2.787609 avg loss no lamb -2.787609 time 2020-06-26 12:41:33.674083
Model ind 665 epoch 694 batch: 600 avg loss -2.833115 avg loss no lamb -2.833115 time 2020-06-26 12:41:44.374440
Model ind 665 epoch 694 batch: 700 avg loss -2.726616 avg loss no lamb -2.726616 time 2020-06-26 12:41:55.187113
Model ind 665 epoch 694 batch: 800 avg loss -2.834639 avg loss no lamb -2.834639 time 2020-06-26 12:42:06.025310
last batch sz 10
Pre: time 2020-06-26 12:42:20.063316: 
 	std: 0.002904752
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9804, 0.9738, 0.9801, 0.9749]
	train_accs: [0.98141664, 0.9809167, 0.97576666, 0.98148334, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97788
	best: 0.9801

Starting e_i: 695
Model ind 665 epoch 695 batch: 0 avg loss -2.930515 avg loss no lamb -2.930515 time 2020-06-26 12:42:21.059226
Model ind 665 epoch 695 batch: 100 avg loss -2.889090 avg loss no lamb -2.889090 time 2020-06-26 12:42:31.484919
Model ind 665 epoch 695 batch: 200 avg loss -2.892083 avg loss no lamb -2.892083 time 2020-06-26 12:42:42.301131
Model ind 665 epoch 695 batch: 300 avg loss -2.808309 avg loss no lamb -2.808309 time 2020-06-26 12:42:52.794381
Model ind 665 epoch 695 batch: 400 avg loss -2.805347 avg loss no lamb -2.805347 time 2020-06-26 12:43:03.662134
Model ind 665 epoch 695 batch: 500 avg loss -2.820350 avg loss no lamb -2.820350 time 2020-06-26 12:43:14.520578
Model ind 665 epoch 695 batch: 600 avg loss -2.846408 avg loss no lamb -2.846408 time 2020-06-26 12:43:25.241846
Model ind 665 epoch 695 batch: 700 avg loss -2.781014 avg loss no lamb -2.781014 time 2020-06-26 12:43:35.958282
Model ind 665 epoch 695 batch: 800 avg loss -2.820870 avg loss no lamb -2.820870 time 2020-06-26 12:43:46.747020
last batch sz 10
Pre: time 2020-06-26 12:44:00.810018: 
 	std: 0.003175142
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9796, 0.9732, 0.9802, 0.9739]
	train_accs: [0.98123336, 0.9810167, 0.9755167, 0.98116666, 0.97583336]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97742003
	best: 0.9802

Starting e_i: 696
Model ind 665 epoch 696 batch: 0 avg loss -2.940730 avg loss no lamb -2.940730 time 2020-06-26 12:44:01.873915
Model ind 665 epoch 696 batch: 100 avg loss -2.851628 avg loss no lamb -2.851628 time 2020-06-26 12:44:12.636634
Model ind 665 epoch 696 batch: 200 avg loss -2.862127 avg loss no lamb -2.862127 time 2020-06-26 12:44:23.244676
Model ind 665 epoch 696 batch: 300 avg loss -2.910538 avg loss no lamb -2.910538 time 2020-06-26 12:44:33.875327
Model ind 665 epoch 696 batch: 400 avg loss -2.718549 avg loss no lamb -2.718549 time 2020-06-26 12:44:44.542664
Model ind 665 epoch 696 batch: 500 avg loss -2.800573 avg loss no lamb -2.800573 time 2020-06-26 12:44:55.105256
Model ind 665 epoch 696 batch: 600 avg loss -2.863002 avg loss no lamb -2.863002 time 2020-06-26 12:45:05.978053
Model ind 665 epoch 696 batch: 700 avg loss -2.739228 avg loss no lamb -2.739228 time 2020-06-26 12:45:16.532017
Model ind 665 epoch 696 batch: 800 avg loss -2.810131 avg loss no lamb -2.810131 time 2020-06-26 12:45:27.234329
last batch sz 10
Pre: time 2020-06-26 12:45:40.755059: 
 	std: 0.0036295569
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9811, 0.9818, 0.9737, 0.9811, 0.9742]
	train_accs: [0.98155, 0.98178333, 0.9761, 0.98158336, 0.97576666]
	best_train_sub_head: 1
	worst: 0.9737
	avg: 0.97838
	best: 0.9818

Starting e_i: 697
Model ind 665 epoch 697 batch: 0 avg loss -2.943765 avg loss no lamb -2.943765 time 2020-06-26 12:45:41.685788
Model ind 665 epoch 697 batch: 100 avg loss -2.868133 avg loss no lamb -2.868133 time 2020-06-26 12:45:52.544868
Model ind 665 epoch 697 batch: 200 avg loss -2.831151 avg loss no lamb -2.831151 time 2020-06-26 12:46:03.271861
Model ind 665 epoch 697 batch: 300 avg loss -2.853080 avg loss no lamb -2.853080 time 2020-06-26 12:46:13.992961
Model ind 665 epoch 697 batch: 400 avg loss -2.728032 avg loss no lamb -2.728032 time 2020-06-26 12:46:24.285150
Model ind 665 epoch 697 batch: 500 avg loss -2.806761 avg loss no lamb -2.806761 time 2020-06-26 12:46:35.045836
Model ind 665 epoch 697 batch: 600 avg loss -2.884960 avg loss no lamb -2.884960 time 2020-06-26 12:46:45.856805
Model ind 665 epoch 697 batch: 700 avg loss -2.753410 avg loss no lamb -2.753410 time 2020-06-26 12:46:56.516526
Model ind 665 epoch 697 batch: 800 avg loss -2.852489 avg loss no lamb -2.852489 time 2020-06-26 12:47:07.343758
last batch sz 10
Pre: time 2020-06-26 12:47:21.283160: 
 	std: 0.0031883616
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9786, 0.9729, 0.9808, 0.9741]
	train_accs: [0.98143333, 0.98076665, 0.9756, 0.9816833, 0.97613335]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97727996
	best: 0.9808

Starting e_i: 698
Model ind 665 epoch 698 batch: 0 avg loss -2.934718 avg loss no lamb -2.934718 time 2020-06-26 12:47:22.197657
Model ind 665 epoch 698 batch: 100 avg loss -2.898400 avg loss no lamb -2.898400 time 2020-06-26 12:47:33.107134
Model ind 665 epoch 698 batch: 200 avg loss -2.911990 avg loss no lamb -2.911990 time 2020-06-26 12:47:43.761414
Model ind 665 epoch 698 batch: 300 avg loss -2.840482 avg loss no lamb -2.840482 time 2020-06-26 12:47:54.441796
Model ind 665 epoch 698 batch: 400 avg loss -2.727995 avg loss no lamb -2.727995 time 2020-06-26 12:48:05.234665
Model ind 665 epoch 698 batch: 500 avg loss -2.859987 avg loss no lamb -2.859987 time 2020-06-26 12:48:16.003062
Model ind 665 epoch 698 batch: 600 avg loss -2.876734 avg loss no lamb -2.876734 time 2020-06-26 12:48:26.852796
Model ind 665 epoch 698 batch: 700 avg loss -2.649810 avg loss no lamb -2.649810 time 2020-06-26 12:48:37.637239
Model ind 665 epoch 698 batch: 800 avg loss -2.795930 avg loss no lamb -2.795930 time 2020-06-26 12:48:48.348016
last batch sz 10
Pre: time 2020-06-26 12:49:02.106245: 
 	std: 0.0031352194
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9804, 0.9733, 0.9808, 0.9757]
	train_accs: [0.98176664, 0.98105, 0.97573334, 0.98188335, 0.9770167]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97822
	best: 0.9808

Starting e_i: 699
Model ind 665 epoch 699 batch: 0 avg loss -2.920886 avg loss no lamb -2.920886 time 2020-06-26 12:49:03.057989
Model ind 665 epoch 699 batch: 100 avg loss -2.848543 avg loss no lamb -2.848543 time 2020-06-26 12:49:13.681939
Model ind 665 epoch 699 batch: 200 avg loss -2.830654 avg loss no lamb -2.830654 time 2020-06-26 12:49:24.328483
Model ind 665 epoch 699 batch: 300 avg loss -2.860981 avg loss no lamb -2.860981 time 2020-06-26 12:49:34.948844
Model ind 665 epoch 699 batch: 400 avg loss -2.690636 avg loss no lamb -2.690636 time 2020-06-26 12:49:45.672070
Model ind 665 epoch 699 batch: 500 avg loss -2.848549 avg loss no lamb -2.848549 time 2020-06-26 12:49:56.278299
Model ind 665 epoch 699 batch: 600 avg loss -2.900307 avg loss no lamb -2.900307 time 2020-06-26 12:50:06.926620
Model ind 665 epoch 699 batch: 700 avg loss -2.760757 avg loss no lamb -2.760757 time 2020-06-26 12:50:17.701553
Model ind 665 epoch 699 batch: 800 avg loss -2.770085 avg loss no lamb -2.770085 time 2020-06-26 12:50:28.433331
last batch sz 10
Pre: time 2020-06-26 12:50:42.402608: 
 	std: 0.003387866
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9807, 0.974, 0.982, 0.9755]
	train_accs: [0.98193336, 0.9812667, 0.9756, 0.9819833, 0.97648335]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97882
	best: 0.982

Starting e_i: 700
Model ind 665 epoch 700 batch: 0 avg loss -2.929693 avg loss no lamb -2.929693 time 2020-06-26 12:50:43.352196
Model ind 665 epoch 700 batch: 100 avg loss -2.833460 avg loss no lamb -2.833460 time 2020-06-26 12:50:54.025920
Model ind 665 epoch 700 batch: 200 avg loss -2.865635 avg loss no lamb -2.865635 time 2020-06-26 12:51:04.740133
Model ind 665 epoch 700 batch: 300 avg loss -2.865072 avg loss no lamb -2.865072 time 2020-06-26 12:51:15.449577
Model ind 665 epoch 700 batch: 400 avg loss -2.789390 avg loss no lamb -2.789390 time 2020-06-26 12:51:26.043778
Model ind 665 epoch 700 batch: 500 avg loss -2.848476 avg loss no lamb -2.848476 time 2020-06-26 12:51:36.823184
Model ind 665 epoch 700 batch: 600 avg loss -2.865169 avg loss no lamb -2.865169 time 2020-06-26 12:51:47.746928
Model ind 665 epoch 700 batch: 700 avg loss -2.736200 avg loss no lamb -2.736200 time 2020-06-26 12:51:58.651511
Model ind 665 epoch 700 batch: 800 avg loss -2.893826 avg loss no lamb -2.893826 time 2020-06-26 12:52:09.581439
last batch sz 10
Pre: time 2020-06-26 12:52:23.251634: 
 	std: 0.0036989658
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9816, 0.9735, 0.9816, 0.9746]
	train_accs: [0.98226666, 0.98178333, 0.9755, 0.98225, 0.97573334]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97856
	best: 0.9815

Starting e_i: 701
Model ind 665 epoch 701 batch: 0 avg loss -2.925605 avg loss no lamb -2.925605 time 2020-06-26 12:52:25.279654
Model ind 665 epoch 701 batch: 100 avg loss -2.792689 avg loss no lamb -2.792689 time 2020-06-26 12:52:35.920843
Model ind 665 epoch 701 batch: 200 avg loss -2.893676 avg loss no lamb -2.893676 time 2020-06-26 12:52:46.585658
Model ind 665 epoch 701 batch: 300 avg loss -2.906787 avg loss no lamb -2.906787 time 2020-06-26 12:52:56.880269
Model ind 665 epoch 701 batch: 400 avg loss -2.748847 avg loss no lamb -2.748847 time 2020-06-26 12:53:07.623724
Model ind 665 epoch 701 batch: 500 avg loss -2.865631 avg loss no lamb -2.865631 time 2020-06-26 12:53:18.266839
Model ind 665 epoch 701 batch: 600 avg loss -2.887302 avg loss no lamb -2.887302 time 2020-06-26 12:53:29.026611
Model ind 665 epoch 701 batch: 700 avg loss -2.820919 avg loss no lamb -2.820919 time 2020-06-26 12:53:39.754873
Model ind 665 epoch 701 batch: 800 avg loss -2.855877 avg loss no lamb -2.855877 time 2020-06-26 12:53:50.562740
last batch sz 10
Pre: time 2020-06-26 12:54:04.568519: 
 	std: 0.0029404748
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9791, 0.9733, 0.9801, 0.9745]
	train_accs: [0.98158336, 0.98055, 0.9755, 0.9817333, 0.9762333]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97744
	best: 0.9801

Starting e_i: 702
Model ind 665 epoch 702 batch: 0 avg loss -2.935660 avg loss no lamb -2.935660 time 2020-06-26 12:54:05.486797
Model ind 665 epoch 702 batch: 100 avg loss -2.890878 avg loss no lamb -2.890878 time 2020-06-26 12:54:16.327153
Model ind 665 epoch 702 batch: 200 avg loss -2.796411 avg loss no lamb -2.796411 time 2020-06-26 12:54:27.041220
Model ind 665 epoch 702 batch: 300 avg loss -2.846010 avg loss no lamb -2.846010 time 2020-06-26 12:54:37.575694
Model ind 665 epoch 702 batch: 400 avg loss -2.817693 avg loss no lamb -2.817693 time 2020-06-26 12:54:48.441462
Model ind 665 epoch 702 batch: 500 avg loss -2.776023 avg loss no lamb -2.776023 time 2020-06-26 12:54:58.926790
Model ind 665 epoch 702 batch: 600 avg loss -2.831223 avg loss no lamb -2.831223 time 2020-06-26 12:55:09.785372
Model ind 665 epoch 702 batch: 700 avg loss -2.769531 avg loss no lamb -2.769531 time 2020-06-26 12:55:20.728602
Model ind 665 epoch 702 batch: 800 avg loss -2.813055 avg loss no lamb -2.813055 time 2020-06-26 12:55:31.650003
last batch sz 10
Pre: time 2020-06-26 12:55:45.614741: 
 	std: 0.0028343631
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.9781, 0.9723, 0.9787, 0.9734]
	train_accs: [0.9809833, 0.9802833, 0.9745833, 0.9809667, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97628003
	best: 0.9789

Starting e_i: 703
Model ind 665 epoch 703 batch: 0 avg loss -2.924191 avg loss no lamb -2.924191 time 2020-06-26 12:55:46.658100
Model ind 665 epoch 703 batch: 100 avg loss -2.908251 avg loss no lamb -2.908251 time 2020-06-26 12:55:57.302303
Model ind 665 epoch 703 batch: 200 avg loss -2.918730 avg loss no lamb -2.918730 time 2020-06-26 12:56:08.192215
Model ind 665 epoch 703 batch: 300 avg loss -2.887063 avg loss no lamb -2.887063 time 2020-06-26 12:56:18.881202
Model ind 665 epoch 703 batch: 400 avg loss -2.788568 avg loss no lamb -2.788568 time 2020-06-26 12:56:29.570930
Model ind 665 epoch 703 batch: 500 avg loss -2.776674 avg loss no lamb -2.776674 time 2020-06-26 12:56:40.385953
Model ind 665 epoch 703 batch: 600 avg loss -2.879359 avg loss no lamb -2.879359 time 2020-06-26 12:56:51.283709
Model ind 665 epoch 703 batch: 700 avg loss -2.754066 avg loss no lamb -2.754066 time 2020-06-26 12:57:02.076418
Model ind 665 epoch 703 batch: 800 avg loss -2.848737 avg loss no lamb -2.848737 time 2020-06-26 12:57:12.757661
last batch sz 10
Pre: time 2020-06-26 12:57:26.437687: 
 	std: 0.0030281409
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9806, 0.9741, 0.9821, 0.9767]
	train_accs: [0.98175, 0.9808, 0.9755167, 0.98205, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97892
	best: 0.9821

Starting e_i: 704
Model ind 665 epoch 704 batch: 0 avg loss -2.941971 avg loss no lamb -2.941971 time 2020-06-26 12:57:27.393766
Model ind 665 epoch 704 batch: 100 avg loss -2.837976 avg loss no lamb -2.837976 time 2020-06-26 12:57:37.976897
Model ind 665 epoch 704 batch: 200 avg loss -2.816898 avg loss no lamb -2.816898 time 2020-06-26 12:57:48.772045
Model ind 665 epoch 704 batch: 300 avg loss -2.796109 avg loss no lamb -2.796109 time 2020-06-26 12:57:59.449231
Model ind 665 epoch 704 batch: 400 avg loss -2.759287 avg loss no lamb -2.759287 time 2020-06-26 12:58:10.061831
Model ind 665 epoch 704 batch: 500 avg loss -2.846724 avg loss no lamb -2.846724 time 2020-06-26 12:58:20.597484
Model ind 665 epoch 704 batch: 600 avg loss -2.908519 avg loss no lamb -2.908519 time 2020-06-26 12:58:31.313000
Model ind 665 epoch 704 batch: 700 avg loss -2.679632 avg loss no lamb -2.679632 time 2020-06-26 12:58:42.044097
Model ind 665 epoch 704 batch: 800 avg loss -2.845090 avg loss no lamb -2.845090 time 2020-06-26 12:58:52.776235
last batch sz 10
Pre: time 2020-06-26 12:59:06.649895: 
 	std: 0.0027981307
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9792, 0.9739, 0.9807, 0.9761]
	train_accs: [0.98148334, 0.98031664, 0.97546667, 0.98111665, 0.9770333]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97822
	best: 0.9812

Starting e_i: 705
Model ind 665 epoch 705 batch: 0 avg loss -2.881242 avg loss no lamb -2.881242 time 2020-06-26 12:59:07.694355
Model ind 665 epoch 705 batch: 100 avg loss -2.861653 avg loss no lamb -2.861653 time 2020-06-26 12:59:18.209584
Model ind 665 epoch 705 batch: 200 avg loss -2.900718 avg loss no lamb -2.900718 time 2020-06-26 12:59:29.058282
Model ind 665 epoch 705 batch: 300 avg loss -2.873887 avg loss no lamb -2.873887 time 2020-06-26 12:59:39.809195
Model ind 665 epoch 705 batch: 400 avg loss -2.734112 avg loss no lamb -2.734112 time 2020-06-26 12:59:50.634856
Model ind 665 epoch 705 batch: 500 avg loss -2.765257 avg loss no lamb -2.765257 time 2020-06-26 13:00:01.404064
Model ind 665 epoch 705 batch: 600 avg loss -2.843570 avg loss no lamb -2.843570 time 2020-06-26 13:00:12.010519
Model ind 665 epoch 705 batch: 700 avg loss -2.616532 avg loss no lamb -2.616532 time 2020-06-26 13:00:22.929971
Model ind 665 epoch 705 batch: 800 avg loss -2.845309 avg loss no lamb -2.845309 time 2020-06-26 13:00:33.973255
last batch sz 10
Pre: time 2020-06-26 13:00:48.194160: 
 	std: 0.002858393
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9798, 0.9745, 0.9815, 0.9763]
	train_accs: [0.98105, 0.9805167, 0.97555, 0.9813333, 0.97636664]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.9787399
	best: 0.9815

Starting e_i: 706
Model ind 665 epoch 706 batch: 0 avg loss -2.929067 avg loss no lamb -2.929067 time 2020-06-26 13:00:49.129222
Model ind 665 epoch 706 batch: 100 avg loss -2.894613 avg loss no lamb -2.894613 time 2020-06-26 13:00:59.745347
Model ind 665 epoch 706 batch: 200 avg loss -2.873524 avg loss no lamb -2.873524 time 2020-06-26 13:01:10.570405
Model ind 665 epoch 706 batch: 300 avg loss -2.875933 avg loss no lamb -2.875933 time 2020-06-26 13:01:21.368459
Model ind 665 epoch 706 batch: 400 avg loss -2.774931 avg loss no lamb -2.774931 time 2020-06-26 13:01:32.092526
Model ind 665 epoch 706 batch: 500 avg loss -2.849171 avg loss no lamb -2.849171 time 2020-06-26 13:01:42.692728
Model ind 665 epoch 706 batch: 600 avg loss -2.828685 avg loss no lamb -2.828685 time 2020-06-26 13:01:53.263992
Model ind 665 epoch 706 batch: 700 avg loss -2.658506 avg loss no lamb -2.658506 time 2020-06-26 13:02:04.084156
Model ind 665 epoch 706 batch: 800 avg loss -2.853554 avg loss no lamb -2.853554 time 2020-06-26 13:02:14.935997
last batch sz 10
Pre: time 2020-06-26 13:02:29.152315: 
 	std: 0.0028519381
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9802, 0.9738, 0.9805, 0.9756]
	train_accs: [0.98116666, 0.9810167, 0.97546667, 0.98105, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97812
	best: 0.9805

Starting e_i: 707
Model ind 665 epoch 707 batch: 0 avg loss -2.936502 avg loss no lamb -2.936502 time 2020-06-26 13:02:30.244004
Model ind 665 epoch 707 batch: 100 avg loss -2.891636 avg loss no lamb -2.891636 time 2020-06-26 13:02:40.877239
Model ind 665 epoch 707 batch: 200 avg loss -2.881152 avg loss no lamb -2.881152 time 2020-06-26 13:02:51.779126
Model ind 665 epoch 707 batch: 300 avg loss -2.862340 avg loss no lamb -2.862340 time 2020-06-26 13:03:02.636322
Model ind 665 epoch 707 batch: 400 avg loss -2.772453 avg loss no lamb -2.772453 time 2020-06-26 13:03:13.657267
Model ind 665 epoch 707 batch: 500 avg loss -2.837101 avg loss no lamb -2.837101 time 2020-06-26 13:03:24.511916
Model ind 665 epoch 707 batch: 600 avg loss -2.863132 avg loss no lamb -2.863132 time 2020-06-26 13:03:35.275520
Model ind 665 epoch 707 batch: 700 avg loss -2.706959 avg loss no lamb -2.706959 time 2020-06-26 13:03:46.262090
Model ind 665 epoch 707 batch: 800 avg loss -2.837738 avg loss no lamb -2.837738 time 2020-06-26 13:03:57.205455
last batch sz 10
Pre: time 2020-06-26 13:04:11.071559: 
 	std: 0.002878193
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9805, 0.974, 0.9804, 0.975]
	train_accs: [0.981, 0.98076665, 0.9754, 0.9813, 0.9759667]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.978
	best: 0.9804

Starting e_i: 708
Model ind 665 epoch 708 batch: 0 avg loss -2.950999 avg loss no lamb -2.950999 time 2020-06-26 13:04:12.067674
Model ind 665 epoch 708 batch: 100 avg loss -2.862311 avg loss no lamb -2.862311 time 2020-06-26 13:04:22.689052
Model ind 665 epoch 708 batch: 200 avg loss -2.782476 avg loss no lamb -2.782476 time 2020-06-26 13:04:33.653747
Model ind 665 epoch 708 batch: 300 avg loss -2.853934 avg loss no lamb -2.853934 time 2020-06-26 13:04:44.327686
Model ind 665 epoch 708 batch: 400 avg loss -2.688331 avg loss no lamb -2.688331 time 2020-06-26 13:04:55.256599
Model ind 665 epoch 708 batch: 500 avg loss -2.845773 avg loss no lamb -2.845773 time 2020-06-26 13:05:06.107033
Model ind 665 epoch 708 batch: 600 avg loss -2.879636 avg loss no lamb -2.879636 time 2020-06-26 13:05:16.918991
Model ind 665 epoch 708 batch: 700 avg loss -2.718312 avg loss no lamb -2.718312 time 2020-06-26 13:05:27.435759
Model ind 665 epoch 708 batch: 800 avg loss -2.827039 avg loss no lamb -2.827039 time 2020-06-26 13:05:38.097141
last batch sz 10
Pre: time 2020-06-26 13:05:51.911056: 
 	std: 0.0033908004
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9812, 0.9738, 0.981, 0.975]
	train_accs: [0.98178333, 0.9813833, 0.9756333, 0.98155, 0.97603333]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97852004
	best: 0.9816

Starting e_i: 709
Model ind 665 epoch 709 batch: 0 avg loss -2.918025 avg loss no lamb -2.918025 time 2020-06-26 13:05:52.983975
Model ind 665 epoch 709 batch: 100 avg loss -2.829025 avg loss no lamb -2.829025 time 2020-06-26 13:06:03.952611
Model ind 665 epoch 709 batch: 200 avg loss -2.880281 avg loss no lamb -2.880281 time 2020-06-26 13:06:14.684090
Model ind 665 epoch 709 batch: 300 avg loss -2.812925 avg loss no lamb -2.812925 time 2020-06-26 13:06:25.475596
Model ind 665 epoch 709 batch: 400 avg loss -2.779835 avg loss no lamb -2.779835 time 2020-06-26 13:06:36.091093
Model ind 665 epoch 709 batch: 500 avg loss -2.847607 avg loss no lamb -2.847607 time 2020-06-26 13:06:46.643641
Model ind 665 epoch 709 batch: 600 avg loss -2.853174 avg loss no lamb -2.853174 time 2020-06-26 13:06:57.506794
Model ind 665 epoch 709 batch: 700 avg loss -2.783264 avg loss no lamb -2.783264 time 2020-06-26 13:07:08.457933
Model ind 665 epoch 709 batch: 800 avg loss -2.789825 avg loss no lamb -2.789825 time 2020-06-26 13:07:19.317477
last batch sz 10
Pre: time 2020-06-26 13:07:33.323887: 
 	std: 0.002645456
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9807, 0.9751, 0.9807, 0.976]
	train_accs: [0.98215, 0.98148334, 0.97606665, 0.98143333, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97875994
	best: 0.9813

Starting e_i: 710
Model ind 665 epoch 710 batch: 0 avg loss -2.967857 avg loss no lamb -2.967857 time 2020-06-26 13:07:34.255404
Model ind 665 epoch 710 batch: 100 avg loss -2.904233 avg loss no lamb -2.904233 time 2020-06-26 13:07:45.147739
Model ind 665 epoch 710 batch: 200 avg loss -2.888520 avg loss no lamb -2.888520 time 2020-06-26 13:07:55.788043
Model ind 665 epoch 710 batch: 300 avg loss -2.844785 avg loss no lamb -2.844785 time 2020-06-26 13:08:06.703867
Model ind 665 epoch 710 batch: 400 avg loss -2.733198 avg loss no lamb -2.733198 time 2020-06-26 13:08:17.400093
Model ind 665 epoch 710 batch: 500 avg loss -2.819708 avg loss no lamb -2.819708 time 2020-06-26 13:08:28.107071
Model ind 665 epoch 710 batch: 600 avg loss -2.847759 avg loss no lamb -2.847759 time 2020-06-26 13:08:38.857262
Model ind 665 epoch 710 batch: 700 avg loss -2.787920 avg loss no lamb -2.787920 time 2020-06-26 13:08:49.751864
Model ind 665 epoch 710 batch: 800 avg loss -2.910801 avg loss no lamb -2.910801 time 2020-06-26 13:09:00.313401
last batch sz 10
Pre: time 2020-06-26 13:09:14.394162: 
 	std: 0.0031044637
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9815, 0.9745, 0.9818, 0.9765]
	train_accs: [0.9821, 0.9816, 0.97616667, 0.98205, 0.9770333]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97922003
	best: 0.9818

Starting e_i: 711
Model ind 665 epoch 711 batch: 0 avg loss -2.980498 avg loss no lamb -2.980498 time 2020-06-26 13:09:16.704143
Model ind 665 epoch 711 batch: 100 avg loss -2.902565 avg loss no lamb -2.902565 time 2020-06-26 13:09:27.448109
Model ind 665 epoch 711 batch: 200 avg loss -2.852154 avg loss no lamb -2.852154 time 2020-06-26 13:09:38.198573
Model ind 665 epoch 711 batch: 300 avg loss -2.852014 avg loss no lamb -2.852014 time 2020-06-26 13:09:48.927914
Model ind 665 epoch 711 batch: 400 avg loss -2.758739 avg loss no lamb -2.758739 time 2020-06-26 13:09:59.894166
Model ind 665 epoch 711 batch: 500 avg loss -2.861111 avg loss no lamb -2.861111 time 2020-06-26 13:10:10.749520
Model ind 665 epoch 711 batch: 600 avg loss -2.851612 avg loss no lamb -2.851612 time 2020-06-26 13:10:21.657504
Model ind 665 epoch 711 batch: 700 avg loss -2.609690 avg loss no lamb -2.609690 time 2020-06-26 13:10:32.423473
Model ind 665 epoch 711 batch: 800 avg loss -2.730695 avg loss no lamb -2.730695 time 2020-06-26 13:10:43.240259
last batch sz 10
Pre: time 2020-06-26 13:10:57.123785: 
 	std: 0.0027339386
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9834, 0.9825, 0.9769, 0.9832, 0.9782]
	train_accs: [0.98258334, 0.9818, 0.9774333, 0.98261666, 0.97795]
	best_train_sub_head: 3
	worst: 0.9769
	avg: 0.98084
	best: 0.9832

Starting e_i: 712
Model ind 665 epoch 712 batch: 0 avg loss -2.887960 avg loss no lamb -2.887960 time 2020-06-26 13:10:58.073460
Model ind 665 epoch 712 batch: 100 avg loss -2.915296 avg loss no lamb -2.915296 time 2020-06-26 13:11:08.692620
Model ind 665 epoch 712 batch: 200 avg loss -2.864313 avg loss no lamb -2.864313 time 2020-06-26 13:11:19.227549
Model ind 665 epoch 712 batch: 300 avg loss -2.801349 avg loss no lamb -2.801349 time 2020-06-26 13:11:30.253346
Model ind 665 epoch 712 batch: 400 avg loss -2.774932 avg loss no lamb -2.774932 time 2020-06-26 13:11:41.202960
Model ind 665 epoch 712 batch: 500 avg loss -2.820888 avg loss no lamb -2.820888 time 2020-06-26 13:11:51.963203
Model ind 665 epoch 712 batch: 600 avg loss -2.790028 avg loss no lamb -2.790028 time 2020-06-26 13:12:02.661097
Model ind 665 epoch 712 batch: 700 avg loss -2.710634 avg loss no lamb -2.710634 time 2020-06-26 13:12:13.051770
Model ind 665 epoch 712 batch: 800 avg loss -2.844976 avg loss no lamb -2.844976 time 2020-06-26 13:12:23.550214
last batch sz 10
Pre: time 2020-06-26 13:12:37.591899: 
 	std: 0.0028435425
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.9804, 0.9747, 0.9807, 0.9747]
	train_accs: [0.98175, 0.98088336, 0.9756333, 0.9817333, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97818005
	best: 0.9804

Starting e_i: 713
Model ind 665 epoch 713 batch: 0 avg loss -2.960257 avg loss no lamb -2.960257 time 2020-06-26 13:12:38.520645
Model ind 665 epoch 713 batch: 100 avg loss -2.831180 avg loss no lamb -2.831180 time 2020-06-26 13:12:49.174490
Model ind 665 epoch 713 batch: 200 avg loss -2.853714 avg loss no lamb -2.853714 time 2020-06-26 13:12:59.598428
Model ind 665 epoch 713 batch: 300 avg loss -2.886807 avg loss no lamb -2.886807 time 2020-06-26 13:13:10.388283
Model ind 665 epoch 713 batch: 400 avg loss -2.741995 avg loss no lamb -2.741995 time 2020-06-26 13:13:21.163925
Model ind 665 epoch 713 batch: 500 avg loss -2.848334 avg loss no lamb -2.848334 time 2020-06-26 13:13:32.055167
Model ind 665 epoch 713 batch: 600 avg loss -2.807889 avg loss no lamb -2.807889 time 2020-06-26 13:13:42.488637
Model ind 665 epoch 713 batch: 700 avg loss -2.770026 avg loss no lamb -2.770026 time 2020-06-26 13:13:53.271120
Model ind 665 epoch 713 batch: 800 avg loss -2.855264 avg loss no lamb -2.855264 time 2020-06-26 13:14:04.028194
last batch sz 10
Pre: time 2020-06-26 13:14:17.967283: 
 	std: 0.0026795596
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9795, 0.9744, 0.9806, 0.9747]
	train_accs: [0.98115, 0.98078334, 0.97595, 0.98148334, 0.97605]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.9778
	best: 0.9806

Starting e_i: 714
Model ind 665 epoch 714 batch: 0 avg loss -2.952327 avg loss no lamb -2.952327 time 2020-06-26 13:14:18.923627
Model ind 665 epoch 714 batch: 100 avg loss -2.889951 avg loss no lamb -2.889951 time 2020-06-26 13:14:29.586198
Model ind 665 epoch 714 batch: 200 avg loss -2.810441 avg loss no lamb -2.810441 time 2020-06-26 13:14:40.295712
Model ind 665 epoch 714 batch: 300 avg loss -2.812600 avg loss no lamb -2.812600 time 2020-06-26 13:14:50.920434
Model ind 665 epoch 714 batch: 400 avg loss -2.869505 avg loss no lamb -2.869505 time 2020-06-26 13:15:01.823664
Model ind 665 epoch 714 batch: 500 avg loss -2.871796 avg loss no lamb -2.871796 time 2020-06-26 13:15:12.749045
Model ind 665 epoch 714 batch: 600 avg loss -2.857534 avg loss no lamb -2.857534 time 2020-06-26 13:15:23.442814
Model ind 665 epoch 714 batch: 700 avg loss -2.742145 avg loss no lamb -2.742145 time 2020-06-26 13:15:34.029421
Model ind 665 epoch 714 batch: 800 avg loss -2.873626 avg loss no lamb -2.873626 time 2020-06-26 13:15:44.772992
last batch sz 10
Pre: time 2020-06-26 13:15:58.833954: 
 	std: 0.0026366685
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9799, 0.9748, 0.9806, 0.9754]
	train_accs: [0.9816, 0.9811, 0.9766333, 0.98143333, 0.97715]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.9783
	best: 0.9808

Starting e_i: 715
Model ind 665 epoch 715 batch: 0 avg loss -2.969582 avg loss no lamb -2.969582 time 2020-06-26 13:15:59.789754
Model ind 665 epoch 715 batch: 100 avg loss -2.881622 avg loss no lamb -2.881622 time 2020-06-26 13:16:10.580043
Model ind 665 epoch 715 batch: 200 avg loss -2.881255 avg loss no lamb -2.881255 time 2020-06-26 13:16:21.245029
Model ind 665 epoch 715 batch: 300 avg loss -2.841639 avg loss no lamb -2.841639 time 2020-06-26 13:16:31.841066
Model ind 665 epoch 715 batch: 400 avg loss -2.798658 avg loss no lamb -2.798658 time 2020-06-26 13:16:42.608204
Model ind 665 epoch 715 batch: 500 avg loss -2.810344 avg loss no lamb -2.810344 time 2020-06-26 13:16:53.483366
Model ind 665 epoch 715 batch: 600 avg loss -2.843222 avg loss no lamb -2.843222 time 2020-06-26 13:17:04.207530
Model ind 665 epoch 715 batch: 700 avg loss -2.671955 avg loss no lamb -2.671955 time 2020-06-26 13:17:14.966580
Model ind 665 epoch 715 batch: 800 avg loss -2.873374 avg loss no lamb -2.873374 time 2020-06-26 13:17:25.556525
last batch sz 10
Pre: time 2020-06-26 13:17:39.416865: 
 	std: 0.0025529603
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9807, 0.9756, 0.9815, 0.9771]
	train_accs: [0.98165, 0.9813833, 0.9767333, 0.9817167, 0.9774]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97938
	best: 0.9815

Starting e_i: 716
Model ind 665 epoch 716 batch: 0 avg loss -2.945316 avg loss no lamb -2.945316 time 2020-06-26 13:17:40.399148
Model ind 665 epoch 716 batch: 100 avg loss -2.902467 avg loss no lamb -2.902467 time 2020-06-26 13:17:51.408578
Model ind 665 epoch 716 batch: 200 avg loss -2.881458 avg loss no lamb -2.881458 time 2020-06-26 13:18:02.076553
Model ind 665 epoch 716 batch: 300 avg loss -2.849604 avg loss no lamb -2.849604 time 2020-06-26 13:18:13.058670
Model ind 665 epoch 716 batch: 400 avg loss -2.790658 avg loss no lamb -2.790658 time 2020-06-26 13:18:23.703507
Model ind 665 epoch 716 batch: 500 avg loss -2.777946 avg loss no lamb -2.777946 time 2020-06-26 13:18:34.454919
Model ind 665 epoch 716 batch: 600 avg loss -2.883290 avg loss no lamb -2.883290 time 2020-06-26 13:18:45.206836
Model ind 665 epoch 716 batch: 700 avg loss -2.733680 avg loss no lamb -2.733680 time 2020-06-26 13:18:55.880993
Model ind 665 epoch 716 batch: 800 avg loss -2.866768 avg loss no lamb -2.866768 time 2020-06-26 13:19:06.601158
last batch sz 10
Pre: time 2020-06-26 13:19:20.713437: 
 	std: 0.002706588
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9811, 0.9752, 0.9816, 0.9765]
	train_accs: [0.98148334, 0.98125, 0.9761, 0.9817, 0.97716665]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97911996
	best: 0.9816

Starting e_i: 717
Model ind 665 epoch 717 batch: 0 avg loss -2.943926 avg loss no lamb -2.943926 time 2020-06-26 13:19:21.665175
Model ind 665 epoch 717 batch: 100 avg loss -2.931293 avg loss no lamb -2.931293 time 2020-06-26 13:19:32.594717
Model ind 665 epoch 717 batch: 200 avg loss -2.860635 avg loss no lamb -2.860635 time 2020-06-26 13:19:43.250559
Model ind 665 epoch 717 batch: 300 avg loss -2.820212 avg loss no lamb -2.820212 time 2020-06-26 13:19:53.919563
Model ind 665 epoch 717 batch: 400 avg loss -2.792240 avg loss no lamb -2.792240 time 2020-06-26 13:20:04.345322
Model ind 665 epoch 717 batch: 500 avg loss -2.831386 avg loss no lamb -2.831386 time 2020-06-26 13:20:15.127118
Model ind 665 epoch 717 batch: 600 avg loss -2.835547 avg loss no lamb -2.835547 time 2020-06-26 13:20:25.893050
Model ind 665 epoch 717 batch: 700 avg loss -2.750982 avg loss no lamb -2.750982 time 2020-06-26 13:20:36.563043
Model ind 665 epoch 717 batch: 800 avg loss -2.846247 avg loss no lamb -2.846247 time 2020-06-26 13:20:46.976651
last batch sz 10
Pre: time 2020-06-26 13:21:01.064850: 
 	std: 0.0031193844
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9797, 0.9736, 0.9815, 0.9751]
	train_accs: [0.98148334, 0.9809, 0.97501665, 0.98178333, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97806007
	best: 0.9815

Starting e_i: 718
Model ind 665 epoch 718 batch: 0 avg loss -2.956661 avg loss no lamb -2.956661 time 2020-06-26 13:21:02.107748
Model ind 665 epoch 718 batch: 100 avg loss -2.850782 avg loss no lamb -2.850782 time 2020-06-26 13:21:12.865080
Model ind 665 epoch 718 batch: 200 avg loss -2.827518 avg loss no lamb -2.827518 time 2020-06-26 13:21:23.674441
Model ind 665 epoch 718 batch: 300 avg loss -2.819159 avg loss no lamb -2.819159 time 2020-06-26 13:21:34.273269
Model ind 665 epoch 718 batch: 400 avg loss -2.721373 avg loss no lamb -2.721373 time 2020-06-26 13:21:45.138446
Model ind 665 epoch 718 batch: 500 avg loss -2.826125 avg loss no lamb -2.826125 time 2020-06-26 13:21:56.386009
Model ind 665 epoch 718 batch: 600 avg loss -2.868184 avg loss no lamb -2.868184 time 2020-06-26 13:22:07.301699
Model ind 665 epoch 718 batch: 700 avg loss -2.781848 avg loss no lamb -2.781848 time 2020-06-26 13:22:18.162356
Model ind 665 epoch 718 batch: 800 avg loss -2.866374 avg loss no lamb -2.866374 time 2020-06-26 13:22:28.667918
last batch sz 10
Pre: time 2020-06-26 13:22:42.676393: 
 	std: 0.0025455714
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9794, 0.9746, 0.9812, 0.9768]
	train_accs: [0.9816833, 0.98043334, 0.97583336, 0.9819833, 0.9774167]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.9786
	best: 0.9812

Starting e_i: 719
Model ind 665 epoch 719 batch: 0 avg loss -2.890507 avg loss no lamb -2.890507 time 2020-06-26 13:22:43.617746
Model ind 665 epoch 719 batch: 100 avg loss -2.915427 avg loss no lamb -2.915427 time 2020-06-26 13:22:54.379304
Model ind 665 epoch 719 batch: 200 avg loss -2.851742 avg loss no lamb -2.851742 time 2020-06-26 13:23:05.404524
Model ind 665 epoch 719 batch: 300 avg loss -2.819214 avg loss no lamb -2.819214 time 2020-06-26 13:23:16.245233
Model ind 665 epoch 719 batch: 400 avg loss -2.780951 avg loss no lamb -2.780951 time 2020-06-26 13:23:26.789715
Model ind 665 epoch 719 batch: 500 avg loss -2.894706 avg loss no lamb -2.894706 time 2020-06-26 13:23:37.650586
Model ind 665 epoch 719 batch: 600 avg loss -2.828441 avg loss no lamb -2.828441 time 2020-06-26 13:23:48.664531
Model ind 665 epoch 719 batch: 700 avg loss -2.705600 avg loss no lamb -2.705600 time 2020-06-26 13:23:59.515331
Model ind 665 epoch 719 batch: 800 avg loss -2.820049 avg loss no lamb -2.820049 time 2020-06-26 13:24:10.291572
last batch sz 10
Pre: time 2020-06-26 13:24:24.376551: 
 	std: 0.002858947
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9798, 0.9741, 0.9813, 0.976]
	train_accs: [0.98188335, 0.9812, 0.97571665, 0.98188335, 0.97688335]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97841996
	best: 0.9809

Starting e_i: 720
Model ind 665 epoch 720 batch: 0 avg loss -2.922414 avg loss no lamb -2.922414 time 2020-06-26 13:24:25.465789
Model ind 665 epoch 720 batch: 100 avg loss -2.884459 avg loss no lamb -2.884459 time 2020-06-26 13:24:36.150129
Model ind 665 epoch 720 batch: 200 avg loss -2.842943 avg loss no lamb -2.842943 time 2020-06-26 13:24:46.800798
Model ind 665 epoch 720 batch: 300 avg loss -2.883775 avg loss no lamb -2.883775 time 2020-06-26 13:24:57.310441
Model ind 665 epoch 720 batch: 400 avg loss -2.737129 avg loss no lamb -2.737129 time 2020-06-26 13:25:07.941917
Model ind 665 epoch 720 batch: 500 avg loss -2.806047 avg loss no lamb -2.806047 time 2020-06-26 13:25:18.640188
Model ind 665 epoch 720 batch: 600 avg loss -2.908822 avg loss no lamb -2.908822 time 2020-06-26 13:25:29.450435
Model ind 665 epoch 720 batch: 700 avg loss -2.690843 avg loss no lamb -2.690843 time 2020-06-26 13:25:40.274993
Model ind 665 epoch 720 batch: 800 avg loss -2.812575 avg loss no lamb -2.812575 time 2020-06-26 13:25:51.039373
last batch sz 10
Pre: time 2020-06-26 13:26:04.737549: 
 	std: 0.003374258
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9792, 0.9713, 0.9789, 0.9737]
	train_accs: [0.9808667, 0.98031664, 0.97318333, 0.98046666, 0.97503334]
	best_train_sub_head: 0
	worst: 0.9713
	avg: 0.97651994
	best: 0.9795

Starting e_i: 721
Model ind 665 epoch 721 batch: 0 avg loss -2.917284 avg loss no lamb -2.917284 time 2020-06-26 13:26:06.897779
Model ind 665 epoch 721 batch: 100 avg loss -2.907632 avg loss no lamb -2.907632 time 2020-06-26 13:26:17.230798
Model ind 665 epoch 721 batch: 200 avg loss -2.846565 avg loss no lamb -2.846565 time 2020-06-26 13:26:27.904336
Model ind 665 epoch 721 batch: 300 avg loss -2.822562 avg loss no lamb -2.822562 time 2020-06-26 13:26:38.445137
Model ind 665 epoch 721 batch: 400 avg loss -2.820436 avg loss no lamb -2.820436 time 2020-06-26 13:26:49.032363
Model ind 665 epoch 721 batch: 500 avg loss -2.827084 avg loss no lamb -2.827084 time 2020-06-26 13:26:59.560713
Model ind 665 epoch 721 batch: 600 avg loss -2.863683 avg loss no lamb -2.863683 time 2020-06-26 13:27:10.495084
Model ind 665 epoch 721 batch: 700 avg loss -2.738337 avg loss no lamb -2.738337 time 2020-06-26 13:27:21.389386
Model ind 665 epoch 721 batch: 800 avg loss -2.809143 avg loss no lamb -2.809143 time 2020-06-26 13:27:32.533232
last batch sz 10
Pre: time 2020-06-26 13:27:46.604910: 
 	std: 0.0030811725
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.981, 0.9748, 0.9817, 0.976]
	train_accs: [0.9823, 0.9818, 0.97681665, 0.9821, 0.97685]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97911996
	best: 0.9821

Starting e_i: 722
Model ind 665 epoch 722 batch: 0 avg loss -2.986691 avg loss no lamb -2.986691 time 2020-06-26 13:27:47.546370
Model ind 665 epoch 722 batch: 100 avg loss -2.903681 avg loss no lamb -2.903681 time 2020-06-26 13:27:58.169492
Model ind 665 epoch 722 batch: 200 avg loss -2.874615 avg loss no lamb -2.874615 time 2020-06-26 13:28:08.921976
Model ind 665 epoch 722 batch: 300 avg loss -2.845520 avg loss no lamb -2.845520 time 2020-06-26 13:28:19.599109
Model ind 665 epoch 722 batch: 400 avg loss -2.791688 avg loss no lamb -2.791688 time 2020-06-26 13:28:30.520419
Model ind 665 epoch 722 batch: 500 avg loss -2.804966 avg loss no lamb -2.804966 time 2020-06-26 13:28:41.388249
Model ind 665 epoch 722 batch: 600 avg loss -2.883470 avg loss no lamb -2.883470 time 2020-06-26 13:28:52.354270
Model ind 665 epoch 722 batch: 700 avg loss -2.744195 avg loss no lamb -2.744195 time 2020-06-26 13:29:03.231476
Model ind 665 epoch 722 batch: 800 avg loss -2.834166 avg loss no lamb -2.834166 time 2020-06-26 13:29:14.049890
last batch sz 10
Pre: time 2020-06-26 13:29:28.109696: 
 	std: 0.0027295444
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9803, 0.9752, 0.982, 0.9778]
	train_accs: [0.9827333, 0.9820833, 0.97725, 0.9827, 0.9777833]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97956
	best: 0.9825

Starting e_i: 723
Model ind 665 epoch 723 batch: 0 avg loss -2.955012 avg loss no lamb -2.955012 time 2020-06-26 13:29:29.073904
Model ind 665 epoch 723 batch: 100 avg loss -2.863250 avg loss no lamb -2.863250 time 2020-06-26 13:29:39.611479
Model ind 665 epoch 723 batch: 200 avg loss -2.860157 avg loss no lamb -2.860157 time 2020-06-26 13:29:50.312744
Model ind 665 epoch 723 batch: 300 avg loss -2.826864 avg loss no lamb -2.826864 time 2020-06-26 13:30:01.098059
Model ind 665 epoch 723 batch: 400 avg loss -2.802934 avg loss no lamb -2.802934 time 2020-06-26 13:30:12.027643
Model ind 665 epoch 723 batch: 500 avg loss -2.788755 avg loss no lamb -2.788755 time 2020-06-26 13:30:22.569772
Model ind 665 epoch 723 batch: 600 avg loss -2.802619 avg loss no lamb -2.802619 time 2020-06-26 13:30:33.093531
Model ind 665 epoch 723 batch: 700 avg loss -2.683389 avg loss no lamb -2.683389 time 2020-06-26 13:30:43.801634
Model ind 665 epoch 723 batch: 800 avg loss -2.821608 avg loss no lamb -2.821608 time 2020-06-26 13:30:54.642304
last batch sz 10
Pre: time 2020-06-26 13:31:08.555604: 
 	std: 0.0025570437
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.98, 0.9747, 0.9806, 0.9763]
	train_accs: [0.98175, 0.9812, 0.97646666, 0.98156667, 0.97711664]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97854006
	best: 0.9811

Starting e_i: 724
Model ind 665 epoch 724 batch: 0 avg loss -2.913548 avg loss no lamb -2.913548 time 2020-06-26 13:31:09.491178
Model ind 665 epoch 724 batch: 100 avg loss -2.872478 avg loss no lamb -2.872478 time 2020-06-26 13:31:20.186925
Model ind 665 epoch 724 batch: 200 avg loss -2.833125 avg loss no lamb -2.833125 time 2020-06-26 13:31:30.882814
Model ind 665 epoch 724 batch: 300 avg loss -2.867494 avg loss no lamb -2.867494 time 2020-06-26 13:31:41.407959
Model ind 665 epoch 724 batch: 400 avg loss -2.729964 avg loss no lamb -2.729964 time 2020-06-26 13:31:52.092388
Model ind 665 epoch 724 batch: 500 avg loss -2.795995 avg loss no lamb -2.795995 time 2020-06-26 13:32:02.959150
Model ind 665 epoch 724 batch: 600 avg loss -2.797372 avg loss no lamb -2.797372 time 2020-06-26 13:32:13.717326
Model ind 665 epoch 724 batch: 700 avg loss -2.713532 avg loss no lamb -2.713532 time 2020-06-26 13:32:24.574135
Model ind 665 epoch 724 batch: 800 avg loss -2.818406 avg loss no lamb -2.818406 time 2020-06-26 13:32:35.170592
last batch sz 10
Pre: time 2020-06-26 13:32:49.118817: 
 	std: 0.0029448285
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9795, 0.9734, 0.9805, 0.9747]
	train_accs: [0.9813, 0.9809333, 0.97513336, 0.98155, 0.9765]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.9775999
	best: 0.9805

Starting e_i: 725
Model ind 665 epoch 725 batch: 0 avg loss -2.915516 avg loss no lamb -2.915516 time 2020-06-26 13:32:50.074698
Model ind 665 epoch 725 batch: 100 avg loss -2.883723 avg loss no lamb -2.883723 time 2020-06-26 13:33:00.878870
Model ind 665 epoch 725 batch: 200 avg loss -2.820842 avg loss no lamb -2.820842 time 2020-06-26 13:33:11.662603
Model ind 665 epoch 725 batch: 300 avg loss -2.852488 avg loss no lamb -2.852488 time 2020-06-26 13:33:22.423706
Model ind 665 epoch 725 batch: 400 avg loss -2.724723 avg loss no lamb -2.724723 time 2020-06-26 13:33:33.124111
Model ind 665 epoch 725 batch: 500 avg loss -2.831768 avg loss no lamb -2.831768 time 2020-06-26 13:33:43.975909
Model ind 665 epoch 725 batch: 600 avg loss -2.854644 avg loss no lamb -2.854644 time 2020-06-26 13:33:54.799438
Model ind 665 epoch 725 batch: 700 avg loss -2.712842 avg loss no lamb -2.712842 time 2020-06-26 13:34:05.666077
Model ind 665 epoch 725 batch: 800 avg loss -2.846115 avg loss no lamb -2.846115 time 2020-06-26 13:34:16.394583
last batch sz 10
Pre: time 2020-06-26 13:34:30.612578: 
 	std: 0.0030340687
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9779, 0.9724, 0.98, 0.9743]
	train_accs: [0.98156667, 0.9804, 0.9752833, 0.9813833, 0.97641665]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97687995
	best: 0.9798

Starting e_i: 726
Model ind 665 epoch 726 batch: 0 avg loss -2.921269 avg loss no lamb -2.921269 time 2020-06-26 13:34:31.584820
Model ind 665 epoch 726 batch: 100 avg loss -2.877579 avg loss no lamb -2.877579 time 2020-06-26 13:34:42.530672
Model ind 665 epoch 726 batch: 200 avg loss -2.756715 avg loss no lamb -2.756715 time 2020-06-26 13:34:53.261027
Model ind 665 epoch 726 batch: 300 avg loss -2.812144 avg loss no lamb -2.812144 time 2020-06-26 13:35:03.948315
Model ind 665 epoch 726 batch: 400 avg loss -2.711415 avg loss no lamb -2.711415 time 2020-06-26 13:35:14.656408
Model ind 665 epoch 726 batch: 500 avg loss -2.781938 avg loss no lamb -2.781938 time 2020-06-26 13:35:25.500781
Model ind 665 epoch 726 batch: 600 avg loss -2.884560 avg loss no lamb -2.884560 time 2020-06-26 13:35:36.224405
Model ind 665 epoch 726 batch: 700 avg loss -2.778648 avg loss no lamb -2.778648 time 2020-06-26 13:35:46.807647
Model ind 665 epoch 726 batch: 800 avg loss -2.831800 avg loss no lamb -2.831800 time 2020-06-26 13:35:57.579252
last batch sz 10
Pre: time 2020-06-26 13:36:11.518693: 
 	std: 0.002482408
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9794, 0.9749, 0.9805, 0.975]
	train_accs: [0.9817, 0.98065, 0.9756333, 0.9816833, 0.97653335]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97796
	best: 0.98

Starting e_i: 727
Model ind 665 epoch 727 batch: 0 avg loss -2.926627 avg loss no lamb -2.926627 time 2020-06-26 13:36:12.510139
Model ind 665 epoch 727 batch: 100 avg loss -2.878944 avg loss no lamb -2.878944 time 2020-06-26 13:36:23.368822
Model ind 665 epoch 727 batch: 200 avg loss -2.855673 avg loss no lamb -2.855673 time 2020-06-26 13:36:34.064672
Model ind 665 epoch 727 batch: 300 avg loss -2.921579 avg loss no lamb -2.921579 time 2020-06-26 13:36:44.492515
Model ind 665 epoch 727 batch: 400 avg loss -2.835291 avg loss no lamb -2.835291 time 2020-06-26 13:36:55.280720
Model ind 665 epoch 727 batch: 500 avg loss -2.803686 avg loss no lamb -2.803686 time 2020-06-26 13:37:06.048588
Model ind 665 epoch 727 batch: 600 avg loss -2.837461 avg loss no lamb -2.837461 time 2020-06-26 13:37:16.880151
Model ind 665 epoch 727 batch: 700 avg loss -2.719577 avg loss no lamb -2.719577 time 2020-06-26 13:37:27.578721
Model ind 665 epoch 727 batch: 800 avg loss -2.801157 avg loss no lamb -2.801157 time 2020-06-26 13:37:38.005706
last batch sz 10
Pre: time 2020-06-26 13:37:52.000932: 
 	std: 0.0031581018
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9799, 0.974, 0.9816, 0.9757]
	train_accs: [0.9821333, 0.9812, 0.97603333, 0.98216665, 0.97711664]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97858
	best: 0.9816

Starting e_i: 728
Model ind 665 epoch 728 batch: 0 avg loss -2.847656 avg loss no lamb -2.847656 time 2020-06-26 13:37:52.917163
Model ind 665 epoch 728 batch: 100 avg loss -2.798665 avg loss no lamb -2.798665 time 2020-06-26 13:38:03.685579
Model ind 665 epoch 728 batch: 200 avg loss -2.836198 avg loss no lamb -2.836198 time 2020-06-26 13:38:14.270108
Model ind 665 epoch 728 batch: 300 avg loss -2.842529 avg loss no lamb -2.842529 time 2020-06-26 13:38:25.138174
Model ind 665 epoch 728 batch: 400 avg loss -2.764082 avg loss no lamb -2.764082 time 2020-06-26 13:38:35.755926
Model ind 665 epoch 728 batch: 500 avg loss -2.817604 avg loss no lamb -2.817604 time 2020-06-26 13:38:46.651595
Model ind 665 epoch 728 batch: 600 avg loss -2.845025 avg loss no lamb -2.845025 time 2020-06-26 13:38:57.436918
Model ind 665 epoch 728 batch: 700 avg loss -2.798578 avg loss no lamb -2.798578 time 2020-06-26 13:39:08.191266
Model ind 665 epoch 728 batch: 800 avg loss -2.846959 avg loss no lamb -2.846959 time 2020-06-26 13:39:19.007069
last batch sz 10
Pre: time 2020-06-26 13:39:33.009685: 
 	std: 0.0034591365
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.98, 0.9727, 0.9809, 0.9744]
	train_accs: [0.9816333, 0.9810333, 0.9743, 0.98178333, 0.97605]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97771996
	best: 0.9809

Starting e_i: 729
Model ind 665 epoch 729 batch: 0 avg loss -2.935119 avg loss no lamb -2.935119 time 2020-06-26 13:39:34.087935
Model ind 665 epoch 729 batch: 100 avg loss -2.856471 avg loss no lamb -2.856471 time 2020-06-26 13:39:45.079976
Model ind 665 epoch 729 batch: 200 avg loss -2.854529 avg loss no lamb -2.854529 time 2020-06-26 13:39:55.648495
Model ind 665 epoch 729 batch: 300 avg loss -2.806171 avg loss no lamb -2.806171 time 2020-06-26 13:40:06.798877
Model ind 665 epoch 729 batch: 400 avg loss -2.720745 avg loss no lamb -2.720745 time 2020-06-26 13:40:17.191788
Model ind 665 epoch 729 batch: 500 avg loss -2.793271 avg loss no lamb -2.793271 time 2020-06-26 13:40:27.998962
Model ind 665 epoch 729 batch: 600 avg loss -2.865250 avg loss no lamb -2.865250 time 2020-06-26 13:40:38.739643
Model ind 665 epoch 729 batch: 700 avg loss -2.627595 avg loss no lamb -2.627595 time 2020-06-26 13:40:49.387272
Model ind 665 epoch 729 batch: 800 avg loss -2.835505 avg loss no lamb -2.835505 time 2020-06-26 13:41:00.186622
last batch sz 10
Pre: time 2020-06-26 13:41:14.107998: 
 	std: 0.0031814456
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9806, 0.973, 0.9806, 0.9759]
	train_accs: [0.98175, 0.9817, 0.97543335, 0.98178333, 0.97735]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97818005
	best: 0.9806

Starting e_i: 730
Model ind 665 epoch 730 batch: 0 avg loss -2.948250 avg loss no lamb -2.948250 time 2020-06-26 13:41:15.093407
Model ind 665 epoch 730 batch: 100 avg loss -2.866930 avg loss no lamb -2.866930 time 2020-06-26 13:41:25.778145
Model ind 665 epoch 730 batch: 200 avg loss -2.833647 avg loss no lamb -2.833647 time 2020-06-26 13:41:36.549885
Model ind 665 epoch 730 batch: 300 avg loss -2.845464 avg loss no lamb -2.845464 time 2020-06-26 13:41:47.260592
Model ind 665 epoch 730 batch: 400 avg loss -2.759185 avg loss no lamb -2.759185 time 2020-06-26 13:41:58.305518
Model ind 665 epoch 730 batch: 500 avg loss -2.778347 avg loss no lamb -2.778347 time 2020-06-26 13:42:09.072988
Model ind 665 epoch 730 batch: 600 avg loss -2.846205 avg loss no lamb -2.846205 time 2020-06-26 13:42:19.931801
Model ind 665 epoch 730 batch: 700 avg loss -2.667130 avg loss no lamb -2.667130 time 2020-06-26 13:42:30.665740
Model ind 665 epoch 730 batch: 800 avg loss -2.817879 avg loss no lamb -2.817879 time 2020-06-26 13:42:41.190626
last batch sz 10
Pre: time 2020-06-26 13:42:55.103318: 
 	std: 0.0026461969
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9809, 0.9749, 0.9809, 0.976]
	train_accs: [0.98146665, 0.9816833, 0.976, 0.9819, 0.97725]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97866
	best: 0.9809

Starting e_i: 731
Model ind 665 epoch 731 batch: 0 avg loss -2.928856 avg loss no lamb -2.928856 time 2020-06-26 13:42:57.347370
Model ind 665 epoch 731 batch: 100 avg loss -2.871139 avg loss no lamb -2.871139 time 2020-06-26 13:43:08.073196
Model ind 665 epoch 731 batch: 200 avg loss -2.840098 avg loss no lamb -2.840098 time 2020-06-26 13:43:18.802738
Model ind 665 epoch 731 batch: 300 avg loss -2.777462 avg loss no lamb -2.777462 time 2020-06-26 13:43:29.371881
Model ind 665 epoch 731 batch: 400 avg loss -2.811233 avg loss no lamb -2.811233 time 2020-06-26 13:43:40.047361
Model ind 665 epoch 731 batch: 500 avg loss -2.809067 avg loss no lamb -2.809067 time 2020-06-26 13:43:50.848439
Model ind 665 epoch 731 batch: 600 avg loss -2.889935 avg loss no lamb -2.889935 time 2020-06-26 13:44:01.849937
Model ind 665 epoch 731 batch: 700 avg loss -2.781134 avg loss no lamb -2.781134 time 2020-06-26 13:44:12.584702
Model ind 665 epoch 731 batch: 800 avg loss -2.837124 avg loss no lamb -2.837124 time 2020-06-26 13:44:23.294054
last batch sz 10
Pre: time 2020-06-26 13:44:37.229463: 
 	std: 0.0033919925
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9804, 0.9731, 0.9813, 0.9749]
	train_accs: [0.9816, 0.9816667, 0.9759833, 0.98205, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97808
	best: 0.9813

Starting e_i: 732
Model ind 665 epoch 732 batch: 0 avg loss -2.888466 avg loss no lamb -2.888466 time 2020-06-26 13:44:38.212898
Model ind 665 epoch 732 batch: 100 avg loss -2.838136 avg loss no lamb -2.838136 time 2020-06-26 13:44:49.309444
Model ind 665 epoch 732 batch: 200 avg loss -2.844017 avg loss no lamb -2.844017 time 2020-06-26 13:44:59.893932
Model ind 665 epoch 732 batch: 300 avg loss -2.821517 avg loss no lamb -2.821517 time 2020-06-26 13:45:10.892329
Model ind 665 epoch 732 batch: 400 avg loss -2.765118 avg loss no lamb -2.765118 time 2020-06-26 13:45:21.648878
Model ind 665 epoch 732 batch: 500 avg loss -2.843257 avg loss no lamb -2.843257 time 2020-06-26 13:45:32.459265
Model ind 665 epoch 732 batch: 600 avg loss -2.860744 avg loss no lamb -2.860744 time 2020-06-26 13:45:43.156723
Model ind 665 epoch 732 batch: 700 avg loss -2.752686 avg loss no lamb -2.752686 time 2020-06-26 13:45:53.782247
Model ind 665 epoch 732 batch: 800 avg loss -2.851313 avg loss no lamb -2.851313 time 2020-06-26 13:46:04.596331
last batch sz 10
Pre: time 2020-06-26 13:46:18.436005: 
 	std: 0.003119348
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9816, 0.9743, 0.9816, 0.9763]
	train_accs: [0.9816, 0.9816333, 0.97581667, 0.98178333, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97904
	best: 0.9816

Starting e_i: 733
Model ind 665 epoch 733 batch: 0 avg loss -2.931597 avg loss no lamb -2.931597 time 2020-06-26 13:46:19.437083
Model ind 665 epoch 733 batch: 100 avg loss -2.864136 avg loss no lamb -2.864136 time 2020-06-26 13:46:30.051064
Model ind 665 epoch 733 batch: 200 avg loss -2.817176 avg loss no lamb -2.817176 time 2020-06-26 13:46:40.623807
Model ind 665 epoch 733 batch: 300 avg loss -2.814138 avg loss no lamb -2.814138 time 2020-06-26 13:46:51.263003
Model ind 665 epoch 733 batch: 400 avg loss -2.764073 avg loss no lamb -2.764073 time 2020-06-26 13:47:01.970476
Model ind 665 epoch 733 batch: 500 avg loss -2.862156 avg loss no lamb -2.862156 time 2020-06-26 13:47:12.653327
Model ind 665 epoch 733 batch: 600 avg loss -2.816140 avg loss no lamb -2.816140 time 2020-06-26 13:47:23.491546
Model ind 665 epoch 733 batch: 700 avg loss -2.670575 avg loss no lamb -2.670575 time 2020-06-26 13:47:34.172179
Model ind 665 epoch 733 batch: 800 avg loss -2.807323 avg loss no lamb -2.807323 time 2020-06-26 13:47:44.990977
last batch sz 10
Pre: time 2020-06-26 13:47:58.997244: 
 	std: 0.0032572425
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9806, 0.974, 0.9819, 0.9754]
	train_accs: [0.98218334, 0.98183334, 0.97565, 0.9822, 0.9769667]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97861993
	best: 0.9819

Starting e_i: 734
Model ind 665 epoch 734 batch: 0 avg loss -2.982889 avg loss no lamb -2.982889 time 2020-06-26 13:47:59.989632
Model ind 665 epoch 734 batch: 100 avg loss -2.892491 avg loss no lamb -2.892491 time 2020-06-26 13:48:10.655388
Model ind 665 epoch 734 batch: 200 avg loss -2.882067 avg loss no lamb -2.882067 time 2020-06-26 13:48:21.236514
Model ind 665 epoch 734 batch: 300 avg loss -2.807186 avg loss no lamb -2.807186 time 2020-06-26 13:48:31.938523
Model ind 665 epoch 734 batch: 400 avg loss -2.742498 avg loss no lamb -2.742498 time 2020-06-26 13:48:42.826515
Model ind 665 epoch 734 batch: 500 avg loss -2.810725 avg loss no lamb -2.810725 time 2020-06-26 13:48:53.653348
Model ind 665 epoch 734 batch: 600 avg loss -2.824926 avg loss no lamb -2.824926 time 2020-06-26 13:49:04.462344
Model ind 665 epoch 734 batch: 700 avg loss -2.714055 avg loss no lamb -2.714055 time 2020-06-26 13:49:15.097176
Model ind 665 epoch 734 batch: 800 avg loss -2.869843 avg loss no lamb -2.869843 time 2020-06-26 13:49:25.796880
last batch sz 10
Pre: time 2020-06-26 13:49:40.008292: 
 	std: 0.0028793085
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9811, 0.9747, 0.982, 0.9768]
	train_accs: [0.9823667, 0.9816667, 0.97613335, 0.98218334, 0.9769]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97916
	best: 0.9812

Starting e_i: 735
Model ind 665 epoch 735 batch: 0 avg loss -2.894000 avg loss no lamb -2.894000 time 2020-06-26 13:49:40.978512
Model ind 665 epoch 735 batch: 100 avg loss -2.867788 avg loss no lamb -2.867788 time 2020-06-26 13:49:51.878443
Model ind 665 epoch 735 batch: 200 avg loss -2.794077 avg loss no lamb -2.794077 time 2020-06-26 13:50:02.482366
Model ind 665 epoch 735 batch: 300 avg loss -2.854683 avg loss no lamb -2.854683 time 2020-06-26 13:50:13.505924
Model ind 665 epoch 735 batch: 400 avg loss -2.740608 avg loss no lamb -2.740608 time 2020-06-26 13:50:24.291107
Model ind 665 epoch 735 batch: 500 avg loss -2.793339 avg loss no lamb -2.793339 time 2020-06-26 13:50:34.949557
Model ind 665 epoch 735 batch: 600 avg loss -2.867740 avg loss no lamb -2.867740 time 2020-06-26 13:50:45.863612
Model ind 665 epoch 735 batch: 700 avg loss -2.663867 avg loss no lamb -2.663867 time 2020-06-26 13:50:56.502338
Model ind 665 epoch 735 batch: 800 avg loss -2.855148 avg loss no lamb -2.855148 time 2020-06-26 13:51:07.526721
last batch sz 10
Pre: time 2020-06-26 13:51:21.465999: 
 	std: 0.003254902
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9788, 0.9718, 0.98, 0.9747]
	train_accs: [0.9811, 0.9803333, 0.9748667, 0.98116666, 0.9757]
	best_train_sub_head: 3
	worst: 0.9718
	avg: 0.97704
	best: 0.98

Starting e_i: 736
Model ind 665 epoch 736 batch: 0 avg loss -2.916423 avg loss no lamb -2.916423 time 2020-06-26 13:51:22.480470
Model ind 665 epoch 736 batch: 100 avg loss -2.820024 avg loss no lamb -2.820024 time 2020-06-26 13:51:33.353248
Model ind 665 epoch 736 batch: 200 avg loss -2.905028 avg loss no lamb -2.905028 time 2020-06-26 13:51:43.987373
Model ind 665 epoch 736 batch: 300 avg loss -2.787783 avg loss no lamb -2.787783 time 2020-06-26 13:51:54.524498
Model ind 665 epoch 736 batch: 400 avg loss -2.727408 avg loss no lamb -2.727408 time 2020-06-26 13:52:05.409980
Model ind 665 epoch 736 batch: 500 avg loss -2.905337 avg loss no lamb -2.905337 time 2020-06-26 13:52:15.959020
Model ind 665 epoch 736 batch: 600 avg loss -2.849433 avg loss no lamb -2.849433 time 2020-06-26 13:52:26.872642
Model ind 665 epoch 736 batch: 700 avg loss -2.763368 avg loss no lamb -2.763368 time 2020-06-26 13:52:37.660120
Model ind 665 epoch 736 batch: 800 avg loss -2.848080 avg loss no lamb -2.848080 time 2020-06-26 13:52:48.394219
last batch sz 10
Pre: time 2020-06-26 13:53:02.464962: 
 	std: 0.002535366
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.9786, 0.9734, 0.9785, 0.9736]
	train_accs: [0.98118335, 0.9806333, 0.97578335, 0.98095, 0.97571665]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.9766
	best: 0.9789

Starting e_i: 737
Model ind 665 epoch 737 batch: 0 avg loss -2.920329 avg loss no lamb -2.920329 time 2020-06-26 13:53:03.435957
Model ind 665 epoch 737 batch: 100 avg loss -2.854195 avg loss no lamb -2.854195 time 2020-06-26 13:53:14.147279
Model ind 665 epoch 737 batch: 200 avg loss -2.890798 avg loss no lamb -2.890798 time 2020-06-26 13:53:24.720883
Model ind 665 epoch 737 batch: 300 avg loss -2.870393 avg loss no lamb -2.870393 time 2020-06-26 13:53:35.346551
Model ind 665 epoch 737 batch: 400 avg loss -2.749917 avg loss no lamb -2.749917 time 2020-06-26 13:53:46.045144
Model ind 665 epoch 737 batch: 500 avg loss -2.822758 avg loss no lamb -2.822758 time 2020-06-26 13:53:56.763722
Model ind 665 epoch 737 batch: 600 avg loss -2.884433 avg loss no lamb -2.884433 time 2020-06-26 13:54:07.758115
Model ind 665 epoch 737 batch: 700 avg loss -2.728723 avg loss no lamb -2.728723 time 2020-06-26 13:54:18.483818
Model ind 665 epoch 737 batch: 800 avg loss -2.830191 avg loss no lamb -2.830191 time 2020-06-26 13:54:29.142330
last batch sz 10
Pre: time 2020-06-26 13:54:42.905711: 
 	std: 0.0029418438
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.978, 0.9782, 0.9711, 0.978, 0.9734]
	train_accs: [0.98006666, 0.97978336, 0.97438335, 0.98018336, 0.9758667]
	best_train_sub_head: 3
	worst: 0.9711
	avg: 0.97573996
	best: 0.978

Starting e_i: 738
Model ind 665 epoch 738 batch: 0 avg loss -2.945910 avg loss no lamb -2.945910 time 2020-06-26 13:54:43.960482
Model ind 665 epoch 738 batch: 100 avg loss -2.917540 avg loss no lamb -2.917540 time 2020-06-26 13:54:54.845281
Model ind 665 epoch 738 batch: 200 avg loss -2.863335 avg loss no lamb -2.863335 time 2020-06-26 13:55:05.459982
Model ind 665 epoch 738 batch: 300 avg loss -2.840462 avg loss no lamb -2.840462 time 2020-06-26 13:55:16.398545
Model ind 665 epoch 738 batch: 400 avg loss -2.784616 avg loss no lamb -2.784616 time 2020-06-26 13:55:27.078599
Model ind 665 epoch 738 batch: 500 avg loss -2.819083 avg loss no lamb -2.819083 time 2020-06-26 13:55:37.963798
Model ind 665 epoch 738 batch: 600 avg loss -2.846661 avg loss no lamb -2.846661 time 2020-06-26 13:55:48.569891
Model ind 665 epoch 738 batch: 700 avg loss -2.770885 avg loss no lamb -2.770885 time 2020-06-26 13:55:59.280465
Model ind 665 epoch 738 batch: 800 avg loss -2.780069 avg loss no lamb -2.780069 time 2020-06-26 13:56:10.026476
last batch sz 10
Pre: time 2020-06-26 13:56:23.910417: 
 	std: 0.003074401
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9805, 0.9735, 0.9806, 0.9753]
	train_accs: [0.9813833, 0.9812833, 0.97565, 0.9813333, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97809994
	best: 0.9806

Starting e_i: 739
Model ind 665 epoch 739 batch: 0 avg loss -2.942123 avg loss no lamb -2.942123 time 2020-06-26 13:56:24.850730
Model ind 665 epoch 739 batch: 100 avg loss -2.878164 avg loss no lamb -2.878164 time 2020-06-26 13:56:36.023316
Model ind 665 epoch 739 batch: 200 avg loss -2.846799 avg loss no lamb -2.846799 time 2020-06-26 13:56:46.607398
Model ind 665 epoch 739 batch: 300 avg loss -2.886593 avg loss no lamb -2.886593 time 2020-06-26 13:56:57.235117
Model ind 665 epoch 739 batch: 400 avg loss -2.765014 avg loss no lamb -2.765014 time 2020-06-26 13:57:07.679771
Model ind 665 epoch 739 batch: 500 avg loss -2.779862 avg loss no lamb -2.779862 time 2020-06-26 13:57:18.327780
Model ind 665 epoch 739 batch: 600 avg loss -2.863761 avg loss no lamb -2.863761 time 2020-06-26 13:57:29.244595
Model ind 665 epoch 739 batch: 700 avg loss -2.771836 avg loss no lamb -2.771836 time 2020-06-26 13:57:39.811340
Model ind 665 epoch 739 batch: 800 avg loss -2.869958 avg loss no lamb -2.869958 time 2020-06-26 13:57:50.376167
last batch sz 10
Pre: time 2020-06-26 13:58:04.132431: 
 	std: 0.003407869
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9801, 0.9732, 0.9813, 0.9751]
	train_accs: [0.9821, 0.98165, 0.9756333, 0.98195, 0.9765]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97822
	best: 0.9814

Starting e_i: 740
Model ind 665 epoch 740 batch: 0 avg loss -2.964384 avg loss no lamb -2.964384 time 2020-06-26 13:58:05.211841
Model ind 665 epoch 740 batch: 100 avg loss -2.863972 avg loss no lamb -2.863972 time 2020-06-26 13:58:16.028196
Model ind 665 epoch 740 batch: 200 avg loss -2.873469 avg loss no lamb -2.873469 time 2020-06-26 13:58:26.783341
Model ind 665 epoch 740 batch: 300 avg loss -2.873899 avg loss no lamb -2.873899 time 2020-06-26 13:58:37.673348
Model ind 665 epoch 740 batch: 400 avg loss -2.787607 avg loss no lamb -2.787607 time 2020-06-26 13:58:48.258287
Model ind 665 epoch 740 batch: 500 avg loss -2.874489 avg loss no lamb -2.874489 time 2020-06-26 13:58:58.843302
Model ind 665 epoch 740 batch: 600 avg loss -2.894315 avg loss no lamb -2.894315 time 2020-06-26 13:59:09.592453
Model ind 665 epoch 740 batch: 700 avg loss -2.755417 avg loss no lamb -2.755417 time 2020-06-26 13:59:20.440813
Model ind 665 epoch 740 batch: 800 avg loss -2.773842 avg loss no lamb -2.773842 time 2020-06-26 13:59:31.181188
last batch sz 10
Pre: time 2020-06-26 13:59:45.453261: 
 	std: 0.0027418248
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9825, 0.9761, 0.9822, 0.9773]
	train_accs: [0.9823, 0.98191667, 0.9767, 0.98228335, 0.9778]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.98002005
	best: 0.982

Starting e_i: 741
Model ind 665 epoch 741 batch: 0 avg loss -2.881997 avg loss no lamb -2.881997 time 2020-06-26 13:59:47.568208
Model ind 665 epoch 741 batch: 100 avg loss -2.857275 avg loss no lamb -2.857275 time 2020-06-26 13:59:58.425629
Model ind 665 epoch 741 batch: 200 avg loss -2.866218 avg loss no lamb -2.866218 time 2020-06-26 14:00:09.178426
Model ind 665 epoch 741 batch: 300 avg loss -2.883126 avg loss no lamb -2.883126 time 2020-06-26 14:00:19.767423
Model ind 665 epoch 741 batch: 400 avg loss -2.813368 avg loss no lamb -2.813368 time 2020-06-26 14:00:30.484953
Model ind 665 epoch 741 batch: 500 avg loss -2.855524 avg loss no lamb -2.855524 time 2020-06-26 14:00:41.083521
Model ind 665 epoch 741 batch: 600 avg loss -2.872958 avg loss no lamb -2.872958 time 2020-06-26 14:00:51.634419
Model ind 665 epoch 741 batch: 700 avg loss -2.817510 avg loss no lamb -2.817510 time 2020-06-26 14:01:02.387648
Model ind 665 epoch 741 batch: 800 avg loss -2.884207 avg loss no lamb -2.884207 time 2020-06-26 14:01:12.979611
last batch sz 10
Pre: time 2020-06-26 14:01:26.936832: 
 	std: 0.0028057117
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9819, 0.9752, 0.9816, 0.9776]
	train_accs: [0.9820167, 0.9817333, 0.9766333, 0.9821, 0.9777167]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97969997
	best: 0.9816

Starting e_i: 742
Model ind 665 epoch 742 batch: 0 avg loss -2.966895 avg loss no lamb -2.966895 time 2020-06-26 14:01:28.000189
Model ind 665 epoch 742 batch: 100 avg loss -2.864429 avg loss no lamb -2.864429 time 2020-06-26 14:01:38.680315
Model ind 665 epoch 742 batch: 200 avg loss -2.859571 avg loss no lamb -2.859571 time 2020-06-26 14:01:49.567968
Model ind 665 epoch 742 batch: 300 avg loss -2.862829 avg loss no lamb -2.862829 time 2020-06-26 14:02:00.516434
Model ind 665 epoch 742 batch: 400 avg loss -2.813985 avg loss no lamb -2.813985 time 2020-06-26 14:02:11.079501
Model ind 665 epoch 742 batch: 500 avg loss -2.822558 avg loss no lamb -2.822558 time 2020-06-26 14:02:21.801966
Model ind 665 epoch 742 batch: 600 avg loss -2.774306 avg loss no lamb -2.774306 time 2020-06-26 14:02:32.742718
Model ind 665 epoch 742 batch: 700 avg loss -2.762780 avg loss no lamb -2.762780 time 2020-06-26 14:02:43.466896
Model ind 665 epoch 742 batch: 800 avg loss -2.863946 avg loss no lamb -2.863946 time 2020-06-26 14:02:54.245221
last batch sz 10
Pre: time 2020-06-26 14:03:07.881275: 
 	std: 0.0028652484
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9806, 0.9748, 0.9815, 0.9764]
	train_accs: [0.9817333, 0.98141664, 0.97576666, 0.9817, 0.97653335]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97902
	best: 0.9818

Starting e_i: 743
Model ind 665 epoch 743 batch: 0 avg loss -2.895267 avg loss no lamb -2.895267 time 2020-06-26 14:03:08.780562
Model ind 665 epoch 743 batch: 100 avg loss -2.876842 avg loss no lamb -2.876842 time 2020-06-26 14:03:19.384209
Model ind 665 epoch 743 batch: 200 avg loss -2.818431 avg loss no lamb -2.818431 time 2020-06-26 14:03:30.152170
Model ind 665 epoch 743 batch: 300 avg loss -2.765069 avg loss no lamb -2.765069 time 2020-06-26 14:03:40.813242
Model ind 665 epoch 743 batch: 400 avg loss -2.752774 avg loss no lamb -2.752774 time 2020-06-26 14:03:51.310405
Model ind 665 epoch 743 batch: 500 avg loss -2.782471 avg loss no lamb -2.782471 time 2020-06-26 14:04:01.964117
Model ind 665 epoch 743 batch: 600 avg loss -2.863708 avg loss no lamb -2.863708 time 2020-06-26 14:04:12.572087
Model ind 665 epoch 743 batch: 700 avg loss -2.755991 avg loss no lamb -2.755991 time 2020-06-26 14:04:22.881025
Model ind 665 epoch 743 batch: 800 avg loss -2.867462 avg loss no lamb -2.867462 time 2020-06-26 14:04:33.645486
last batch sz 10
Pre: time 2020-06-26 14:04:47.387342: 
 	std: 0.003335614
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9813, 0.9745, 0.9823, 0.9757]
	train_accs: [0.98195, 0.98155, 0.9762, 0.98215, 0.97678334]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97914
	best: 0.9823

Starting e_i: 744
Model ind 665 epoch 744 batch: 0 avg loss -2.919445 avg loss no lamb -2.919445 time 2020-06-26 14:04:48.453972
Model ind 665 epoch 744 batch: 100 avg loss -2.851689 avg loss no lamb -2.851689 time 2020-06-26 14:04:59.199320
Model ind 665 epoch 744 batch: 200 avg loss -2.851705 avg loss no lamb -2.851705 time 2020-06-26 14:05:09.900138
Model ind 665 epoch 744 batch: 300 avg loss -2.838557 avg loss no lamb -2.838557 time 2020-06-26 14:05:20.736866
Model ind 665 epoch 744 batch: 400 avg loss -2.768427 avg loss no lamb -2.768427 time 2020-06-26 14:05:31.653481
Model ind 665 epoch 744 batch: 500 avg loss -2.842888 avg loss no lamb -2.842888 time 2020-06-26 14:05:42.378067
Model ind 665 epoch 744 batch: 600 avg loss -2.846569 avg loss no lamb -2.846569 time 2020-06-26 14:05:53.121470
Model ind 665 epoch 744 batch: 700 avg loss -2.741543 avg loss no lamb -2.741543 time 2020-06-26 14:06:03.938174
Model ind 665 epoch 744 batch: 800 avg loss -2.788442 avg loss no lamb -2.788442 time 2020-06-26 14:06:14.682680
last batch sz 10
Pre: time 2020-06-26 14:06:28.632525: 
 	std: 0.0030702536
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9806, 0.9736, 0.9809, 0.9758]
	train_accs: [0.9813833, 0.98095, 0.9759833, 0.9814, 0.97685]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97836
	best: 0.9809

Starting e_i: 745
Model ind 665 epoch 745 batch: 0 avg loss -2.985432 avg loss no lamb -2.985432 time 2020-06-26 14:06:29.573822
Model ind 665 epoch 745 batch: 100 avg loss -2.890647 avg loss no lamb -2.890647 time 2020-06-26 14:06:40.457903
Model ind 665 epoch 745 batch: 200 avg loss -2.896007 avg loss no lamb -2.896007 time 2020-06-26 14:06:51.246295
Model ind 665 epoch 745 batch: 300 avg loss -2.825457 avg loss no lamb -2.825457 time 2020-06-26 14:07:02.047783
Model ind 665 epoch 745 batch: 400 avg loss -2.758785 avg loss no lamb -2.758785 time 2020-06-26 14:07:12.551935
Model ind 665 epoch 745 batch: 500 avg loss -2.811260 avg loss no lamb -2.811260 time 2020-06-26 14:07:23.545399
Model ind 665 epoch 745 batch: 600 avg loss -2.840671 avg loss no lamb -2.840671 time 2020-06-26 14:07:34.605034
Model ind 665 epoch 745 batch: 700 avg loss -2.747780 avg loss no lamb -2.747780 time 2020-06-26 14:07:45.677533
Model ind 665 epoch 745 batch: 800 avg loss -2.819520 avg loss no lamb -2.819520 time 2020-06-26 14:07:56.593039
last batch sz 10
Pre: time 2020-06-26 14:08:10.337506: 
 	std: 0.0029312049
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9799, 0.9732, 0.9799, 0.9745]
	train_accs: [0.9807, 0.9805833, 0.9753, 0.9807, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.9774
	best: 0.9795

Starting e_i: 746
Model ind 665 epoch 746 batch: 0 avg loss -2.879448 avg loss no lamb -2.879448 time 2020-06-26 14:08:11.412883
Model ind 665 epoch 746 batch: 100 avg loss -2.846837 avg loss no lamb -2.846837 time 2020-06-26 14:08:22.013667
Model ind 665 epoch 746 batch: 200 avg loss -2.864187 avg loss no lamb -2.864187 time 2020-06-26 14:08:32.944910
Model ind 665 epoch 746 batch: 300 avg loss -2.865798 avg loss no lamb -2.865798 time 2020-06-26 14:08:43.700672
Model ind 665 epoch 746 batch: 400 avg loss -2.819014 avg loss no lamb -2.819014 time 2020-06-26 14:08:54.234578
Model ind 665 epoch 746 batch: 500 avg loss -2.817975 avg loss no lamb -2.817975 time 2020-06-26 14:09:05.064231
Model ind 665 epoch 746 batch: 600 avg loss -2.851161 avg loss no lamb -2.851161 time 2020-06-26 14:09:15.780991
Model ind 665 epoch 746 batch: 700 avg loss -2.801049 avg loss no lamb -2.801049 time 2020-06-26 14:09:26.582692
Model ind 665 epoch 746 batch: 800 avg loss -2.821218 avg loss no lamb -2.821218 time 2020-06-26 14:09:37.232489
last batch sz 10
Pre: time 2020-06-26 14:09:51.128516: 
 	std: 0.0031301125
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9804, 0.9741, 0.9817, 0.9764]
	train_accs: [0.98215, 0.98125, 0.9761, 0.982, 0.9774167]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97892
	best: 0.982

Starting e_i: 747
Model ind 665 epoch 747 batch: 0 avg loss -2.959429 avg loss no lamb -2.959429 time 2020-06-26 14:09:52.091206
Model ind 665 epoch 747 batch: 100 avg loss -2.928844 avg loss no lamb -2.928844 time 2020-06-26 14:10:02.952386
Model ind 665 epoch 747 batch: 200 avg loss -2.867395 avg loss no lamb -2.867395 time 2020-06-26 14:10:13.978202
Model ind 665 epoch 747 batch: 300 avg loss -2.788298 avg loss no lamb -2.788298 time 2020-06-26 14:10:24.738193
Model ind 665 epoch 747 batch: 400 avg loss -2.783901 avg loss no lamb -2.783901 time 2020-06-26 14:10:35.397593
Model ind 665 epoch 747 batch: 500 avg loss -2.805913 avg loss no lamb -2.805913 time 2020-06-26 14:10:46.190324
Model ind 665 epoch 747 batch: 600 avg loss -2.847609 avg loss no lamb -2.847609 time 2020-06-26 14:10:56.807921
Model ind 665 epoch 747 batch: 700 avg loss -2.802798 avg loss no lamb -2.802798 time 2020-06-26 14:11:07.913109
Model ind 665 epoch 747 batch: 800 avg loss -2.784395 avg loss no lamb -2.784395 time 2020-06-26 14:11:18.691754
last batch sz 10
Pre: time 2020-06-26 14:11:32.702684: 
 	std: 0.0026229671
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9805, 0.9751, 0.9808, 0.9757]
	train_accs: [0.9817333, 0.9810333, 0.97595, 0.98156667, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.9786
	best: 0.9809

Starting e_i: 748
Model ind 665 epoch 748 batch: 0 avg loss -2.918454 avg loss no lamb -2.918454 time 2020-06-26 14:11:33.787377
Model ind 665 epoch 748 batch: 100 avg loss -2.923512 avg loss no lamb -2.923512 time 2020-06-26 14:11:44.633267
Model ind 665 epoch 748 batch: 200 avg loss -2.864351 avg loss no lamb -2.864351 time 2020-06-26 14:11:55.447254
Model ind 665 epoch 748 batch: 300 avg loss -2.867619 avg loss no lamb -2.867619 time 2020-06-26 14:12:06.252887
Model ind 665 epoch 748 batch: 400 avg loss -2.809537 avg loss no lamb -2.809537 time 2020-06-26 14:12:16.828550
Model ind 665 epoch 748 batch: 500 avg loss -2.824068 avg loss no lamb -2.824068 time 2020-06-26 14:12:27.306652
Model ind 665 epoch 748 batch: 600 avg loss -2.814717 avg loss no lamb -2.814717 time 2020-06-26 14:12:37.920013
Model ind 665 epoch 748 batch: 700 avg loss -2.713772 avg loss no lamb -2.713772 time 2020-06-26 14:12:48.671578
Model ind 665 epoch 748 batch: 800 avg loss -2.839296 avg loss no lamb -2.839296 time 2020-06-26 14:12:59.495474
last batch sz 10
Pre: time 2020-06-26 14:13:13.588240: 
 	std: 0.0027856776
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9823, 0.9759, 0.9825, 0.9781]
	train_accs: [0.98221666, 0.98195, 0.9768, 0.9819833, 0.9776833]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.98029995
	best: 0.9827

Starting e_i: 749
Model ind 665 epoch 749 batch: 0 avg loss -2.917649 avg loss no lamb -2.917649 time 2020-06-26 14:13:14.571561
Model ind 665 epoch 749 batch: 100 avg loss -2.857063 avg loss no lamb -2.857063 time 2020-06-26 14:13:25.310770
Model ind 665 epoch 749 batch: 200 avg loss -2.869778 avg loss no lamb -2.869778 time 2020-06-26 14:13:36.093399
Model ind 665 epoch 749 batch: 300 avg loss -2.799773 avg loss no lamb -2.799773 time 2020-06-26 14:13:46.883955
Model ind 665 epoch 749 batch: 400 avg loss -2.724908 avg loss no lamb -2.724908 time 2020-06-26 14:13:57.644247
Model ind 665 epoch 749 batch: 500 avg loss -2.807612 avg loss no lamb -2.807612 time 2020-06-26 14:14:08.305870
Model ind 665 epoch 749 batch: 600 avg loss -2.852794 avg loss no lamb -2.852794 time 2020-06-26 14:14:18.673900
Model ind 665 epoch 749 batch: 700 avg loss -2.685164 avg loss no lamb -2.685164 time 2020-06-26 14:14:29.319700
Model ind 665 epoch 749 batch: 800 avg loss -2.906934 avg loss no lamb -2.906934 time 2020-06-26 14:14:40.144249
last batch sz 10
Pre: time 2020-06-26 14:14:54.250099: 
 	std: 0.0028021568
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9802, 0.9747, 0.9813, 0.9765]
	train_accs: [0.9819, 0.98118335, 0.9766, 0.9819667, 0.97758335]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97889996
	best: 0.9813

Starting e_i: 750
Model ind 665 epoch 750 batch: 0 avg loss -2.933490 avg loss no lamb -2.933490 time 2020-06-26 14:14:55.388479
Model ind 665 epoch 750 batch: 100 avg loss -2.843710 avg loss no lamb -2.843710 time 2020-06-26 14:15:06.210644
Model ind 665 epoch 750 batch: 200 avg loss -2.888554 avg loss no lamb -2.888554 time 2020-06-26 14:15:17.004946
Model ind 665 epoch 750 batch: 300 avg loss -2.909192 avg loss no lamb -2.909192 time 2020-06-26 14:15:27.803945
Model ind 665 epoch 750 batch: 400 avg loss -2.760427 avg loss no lamb -2.760427 time 2020-06-26 14:15:38.566518
Model ind 665 epoch 750 batch: 500 avg loss -2.806974 avg loss no lamb -2.806974 time 2020-06-26 14:15:49.374946
Model ind 665 epoch 750 batch: 600 avg loss -2.880990 avg loss no lamb -2.880990 time 2020-06-26 14:16:00.093719
Model ind 665 epoch 750 batch: 700 avg loss -2.775267 avg loss no lamb -2.775267 time 2020-06-26 14:16:10.906697
Model ind 665 epoch 750 batch: 800 avg loss -2.863802 avg loss no lamb -2.863802 time 2020-06-26 14:16:21.591983
last batch sz 10
Pre: time 2020-06-26 14:16:35.596353: 
 	std: 0.0031153911
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9797, 0.9741, 0.9818, 0.9755]
	train_accs: [0.9816, 0.98066664, 0.97585, 0.9817, 0.9765]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97848004
	best: 0.9818

Starting e_i: 751
Model ind 665 epoch 751 batch: 0 avg loss -2.911299 avg loss no lamb -2.911299 time 2020-06-26 14:16:37.815540
Model ind 665 epoch 751 batch: 100 avg loss -2.934361 avg loss no lamb -2.934361 time 2020-06-26 14:16:48.496126
Model ind 665 epoch 751 batch: 200 avg loss -2.803054 avg loss no lamb -2.803054 time 2020-06-26 14:16:59.207670
Model ind 665 epoch 751 batch: 300 avg loss -2.835970 avg loss no lamb -2.835970 time 2020-06-26 14:17:10.033421
Model ind 665 epoch 751 batch: 400 avg loss -2.735718 avg loss no lamb -2.735718 time 2020-06-26 14:17:20.905498
Model ind 665 epoch 751 batch: 500 avg loss -2.815368 avg loss no lamb -2.815368 time 2020-06-26 14:17:31.726816
Model ind 665 epoch 751 batch: 600 avg loss -2.873968 avg loss no lamb -2.873968 time 2020-06-26 14:17:42.267093
Model ind 665 epoch 751 batch: 700 avg loss -2.719112 avg loss no lamb -2.719112 time 2020-06-26 14:17:52.967941
Model ind 665 epoch 751 batch: 800 avg loss -2.818365 avg loss no lamb -2.818365 time 2020-06-26 14:18:03.673251
last batch sz 10
Pre: time 2020-06-26 14:18:17.643057: 
 	std: 0.0032159542
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9813, 0.9749, 0.9817, 0.9756]
	train_accs: [0.98215, 0.9817167, 0.97641665, 0.9820667, 0.97705]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97915995
	best: 0.9823

Starting e_i: 752
Model ind 665 epoch 752 batch: 0 avg loss -2.930860 avg loss no lamb -2.930860 time 2020-06-26 14:18:18.682146
Model ind 665 epoch 752 batch: 100 avg loss -2.832195 avg loss no lamb -2.832195 time 2020-06-26 14:18:29.553816
Model ind 665 epoch 752 batch: 200 avg loss -2.876183 avg loss no lamb -2.876183 time 2020-06-26 14:18:40.136764
Model ind 665 epoch 752 batch: 300 avg loss -2.870047 avg loss no lamb -2.870047 time 2020-06-26 14:18:51.047063
Model ind 665 epoch 752 batch: 400 avg loss -2.802047 avg loss no lamb -2.802047 time 2020-06-26 14:19:01.811722
Model ind 665 epoch 752 batch: 500 avg loss -2.854743 avg loss no lamb -2.854743 time 2020-06-26 14:19:12.490167
Model ind 665 epoch 752 batch: 600 avg loss -2.878496 avg loss no lamb -2.878496 time 2020-06-26 14:19:23.247054
Model ind 665 epoch 752 batch: 700 avg loss -2.729706 avg loss no lamb -2.729706 time 2020-06-26 14:19:33.895936
Model ind 665 epoch 752 batch: 800 avg loss -2.781117 avg loss no lamb -2.781117 time 2020-06-26 14:19:44.726654
last batch sz 10
Pre: time 2020-06-26 14:19:58.587495: 
 	std: 0.00301767
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9809, 0.9741, 0.9812, 0.9757]
	train_accs: [0.98158336, 0.9813667, 0.9752333, 0.98181665, 0.97675]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97854006
	best: 0.9812

Starting e_i: 753
Model ind 665 epoch 753 batch: 0 avg loss -2.948539 avg loss no lamb -2.948539 time 2020-06-26 14:19:59.560139
Model ind 665 epoch 753 batch: 100 avg loss -2.900404 avg loss no lamb -2.900404 time 2020-06-26 14:20:10.372353
Model ind 665 epoch 753 batch: 200 avg loss -2.838231 avg loss no lamb -2.838231 time 2020-06-26 14:20:20.855928
Model ind 665 epoch 753 batch: 300 avg loss -2.811283 avg loss no lamb -2.811283 time 2020-06-26 14:20:31.609736
Model ind 665 epoch 753 batch: 400 avg loss -2.782380 avg loss no lamb -2.782380 time 2020-06-26 14:20:42.442559
Model ind 665 epoch 753 batch: 500 avg loss -2.829340 avg loss no lamb -2.829340 time 2020-06-26 14:20:53.099384
Model ind 665 epoch 753 batch: 600 avg loss -2.872414 avg loss no lamb -2.872414 time 2020-06-26 14:21:03.978093
Model ind 665 epoch 753 batch: 700 avg loss -2.757064 avg loss no lamb -2.757064 time 2020-06-26 14:21:14.879548
Model ind 665 epoch 753 batch: 800 avg loss -2.754217 avg loss no lamb -2.754217 time 2020-06-26 14:21:25.445227
last batch sz 10
Pre: time 2020-06-26 14:21:39.444931: 
 	std: 0.0026310468
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9788, 0.9726, 0.9785, 0.9742]
	train_accs: [0.98055, 0.98053336, 0.97501665, 0.98065, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97656
	best: 0.9785

Starting e_i: 754
Model ind 665 epoch 754 batch: 0 avg loss -2.947147 avg loss no lamb -2.947147 time 2020-06-26 14:21:40.372550
Model ind 665 epoch 754 batch: 100 avg loss -2.887933 avg loss no lamb -2.887933 time 2020-06-26 14:21:51.071351
Model ind 665 epoch 754 batch: 200 avg loss -2.834625 avg loss no lamb -2.834625 time 2020-06-26 14:22:01.998519
Model ind 665 epoch 754 batch: 300 avg loss -2.860374 avg loss no lamb -2.860374 time 2020-06-26 14:22:12.775652
Model ind 665 epoch 754 batch: 400 avg loss -2.791533 avg loss no lamb -2.791533 time 2020-06-26 14:22:23.473103
Model ind 665 epoch 754 batch: 500 avg loss -2.769805 avg loss no lamb -2.769805 time 2020-06-26 14:22:33.997675
Model ind 665 epoch 754 batch: 600 avg loss -2.894823 avg loss no lamb -2.894823 time 2020-06-26 14:22:44.823674
Model ind 665 epoch 754 batch: 700 avg loss -2.777295 avg loss no lamb -2.777295 time 2020-06-26 14:22:55.640671
Model ind 665 epoch 754 batch: 800 avg loss -2.889570 avg loss no lamb -2.889570 time 2020-06-26 14:23:06.447920
last batch sz 10
Pre: time 2020-06-26 14:23:20.429953: 
 	std: 0.0031782999
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9811, 0.9748, 0.9826, 0.9767]
	train_accs: [0.9819833, 0.98151666, 0.9763167, 0.9821, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97952
	best: 0.9826

Starting e_i: 755
Model ind 665 epoch 755 batch: 0 avg loss -2.962848 avg loss no lamb -2.962848 time 2020-06-26 14:23:21.412654
Model ind 665 epoch 755 batch: 100 avg loss -2.862265 avg loss no lamb -2.862265 time 2020-06-26 14:23:32.327046
Model ind 665 epoch 755 batch: 200 avg loss -2.845801 avg loss no lamb -2.845801 time 2020-06-26 14:23:43.131856
Model ind 665 epoch 755 batch: 300 avg loss -2.811458 avg loss no lamb -2.811458 time 2020-06-26 14:23:53.899706
Model ind 665 epoch 755 batch: 400 avg loss -2.769197 avg loss no lamb -2.769197 time 2020-06-26 14:24:04.694754
Model ind 665 epoch 755 batch: 500 avg loss -2.831485 avg loss no lamb -2.831485 time 2020-06-26 14:24:15.481482
Model ind 665 epoch 755 batch: 600 avg loss -2.854478 avg loss no lamb -2.854478 time 2020-06-26 14:24:26.301046
Model ind 665 epoch 755 batch: 700 avg loss -2.787409 avg loss no lamb -2.787409 time 2020-06-26 14:24:37.073475
Model ind 665 epoch 755 batch: 800 avg loss -2.839376 avg loss no lamb -2.839376 time 2020-06-26 14:24:47.761215
last batch sz 10
Pre: time 2020-06-26 14:25:01.871524: 
 	std: 0.003015027
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9805, 0.9733, 0.98, 0.9757]
	train_accs: [0.9817333, 0.98113334, 0.97576666, 0.9817, 0.97676665]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97805995
	best: 0.9808

Starting e_i: 756
Model ind 665 epoch 756 batch: 0 avg loss -2.952562 avg loss no lamb -2.952562 time 2020-06-26 14:25:02.933698
Model ind 665 epoch 756 batch: 100 avg loss -2.876597 avg loss no lamb -2.876597 time 2020-06-26 14:25:13.748269
Model ind 665 epoch 756 batch: 200 avg loss -2.838612 avg loss no lamb -2.838612 time 2020-06-26 14:25:24.525089
Model ind 665 epoch 756 batch: 300 avg loss -2.861786 avg loss no lamb -2.861786 time 2020-06-26 14:25:35.447334
Model ind 665 epoch 756 batch: 400 avg loss -2.790776 avg loss no lamb -2.790776 time 2020-06-26 14:25:46.287347
Model ind 665 epoch 756 batch: 500 avg loss -2.820944 avg loss no lamb -2.820944 time 2020-06-26 14:25:57.057063
Model ind 665 epoch 756 batch: 600 avg loss -2.817905 avg loss no lamb -2.817905 time 2020-06-26 14:26:07.813720
Model ind 665 epoch 756 batch: 700 avg loss -2.662590 avg loss no lamb -2.662590 time 2020-06-26 14:26:18.659325
Model ind 665 epoch 756 batch: 800 avg loss -2.870121 avg loss no lamb -2.870121 time 2020-06-26 14:26:29.644031
last batch sz 10
Pre: time 2020-06-26 14:26:43.730604: 
 	std: 0.003206626
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9805, 0.9736, 0.981, 0.9749]
	train_accs: [0.98186666, 0.98146665, 0.97555, 0.98193336, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97814
	best: 0.981

Starting e_i: 757
Model ind 665 epoch 757 batch: 0 avg loss -2.912895 avg loss no lamb -2.912895 time 2020-06-26 14:26:44.714397
Model ind 665 epoch 757 batch: 100 avg loss -2.833541 avg loss no lamb -2.833541 time 2020-06-26 14:26:55.393535
Model ind 665 epoch 757 batch: 200 avg loss -2.862094 avg loss no lamb -2.862094 time 2020-06-26 14:27:06.286067
Model ind 665 epoch 757 batch: 300 avg loss -2.821611 avg loss no lamb -2.821611 time 2020-06-26 14:27:17.193752
Model ind 665 epoch 757 batch: 400 avg loss -2.763342 avg loss no lamb -2.763342 time 2020-06-26 14:27:27.757969
Model ind 665 epoch 757 batch: 500 avg loss -2.844332 avg loss no lamb -2.844332 time 2020-06-26 14:27:38.266271
Model ind 665 epoch 757 batch: 600 avg loss -2.874305 avg loss no lamb -2.874305 time 2020-06-26 14:27:49.224286
Model ind 665 epoch 757 batch: 700 avg loss -2.677192 avg loss no lamb -2.677192 time 2020-06-26 14:28:00.072790
Model ind 665 epoch 757 batch: 800 avg loss -2.885120 avg loss no lamb -2.885120 time 2020-06-26 14:28:10.962158
last batch sz 10
Pre: time 2020-06-26 14:28:25.415353: 
 	std: 0.00355111
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9791, 0.9727, 0.9813, 0.9742]
	train_accs: [0.98205, 0.98078334, 0.97566664, 0.98191667, 0.97646666]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97766
	best: 0.981

Starting e_i: 758
Model ind 665 epoch 758 batch: 0 avg loss -2.943909 avg loss no lamb -2.943909 time 2020-06-26 14:28:26.430762
Model ind 665 epoch 758 batch: 100 avg loss -2.934193 avg loss no lamb -2.934193 time 2020-06-26 14:28:37.523855
Model ind 665 epoch 758 batch: 200 avg loss -2.860648 avg loss no lamb -2.860648 time 2020-06-26 14:28:48.748037
Model ind 665 epoch 758 batch: 300 avg loss -2.798095 avg loss no lamb -2.798095 time 2020-06-26 14:28:59.838261
Model ind 665 epoch 758 batch: 400 avg loss -2.677778 avg loss no lamb -2.677778 time 2020-06-26 14:29:10.604816
Model ind 665 epoch 758 batch: 500 avg loss -2.817654 avg loss no lamb -2.817654 time 2020-06-26 14:29:21.348484
Model ind 665 epoch 758 batch: 600 avg loss -2.869849 avg loss no lamb -2.869849 time 2020-06-26 14:29:32.140626
Model ind 665 epoch 758 batch: 700 avg loss -2.729584 avg loss no lamb -2.729584 time 2020-06-26 14:29:43.254667
Model ind 665 epoch 758 batch: 800 avg loss -2.777181 avg loss no lamb -2.777181 time 2020-06-26 14:29:54.276011
last batch sz 10
Pre: time 2020-06-26 14:30:08.235373: 
 	std: 0.003276219
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9782, 0.972, 0.9794, 0.9727]
	train_accs: [0.98118335, 0.97995, 0.9744167, 0.98125, 0.9752167]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97631997
	best: 0.9794

Starting e_i: 759
Model ind 665 epoch 759 batch: 0 avg loss -2.955774 avg loss no lamb -2.955774 time 2020-06-26 14:30:09.183980
Model ind 665 epoch 759 batch: 100 avg loss -2.901120 avg loss no lamb -2.901120 time 2020-06-26 14:30:19.889089
Model ind 665 epoch 759 batch: 200 avg loss -2.838474 avg loss no lamb -2.838474 time 2020-06-26 14:30:30.707799
Model ind 665 epoch 759 batch: 300 avg loss -2.895999 avg loss no lamb -2.895999 time 2020-06-26 14:30:41.493803
Model ind 665 epoch 759 batch: 400 avg loss -2.744153 avg loss no lamb -2.744153 time 2020-06-26 14:30:52.185342
Model ind 665 epoch 759 batch: 500 avg loss -2.842893 avg loss no lamb -2.842893 time 2020-06-26 14:31:02.893224
Model ind 665 epoch 759 batch: 600 avg loss -2.832674 avg loss no lamb -2.832674 time 2020-06-26 14:31:13.523583
Model ind 665 epoch 759 batch: 700 avg loss -2.768786 avg loss no lamb -2.768786 time 2020-06-26 14:31:24.503283
Model ind 665 epoch 759 batch: 800 avg loss -2.775875 avg loss no lamb -2.775875 time 2020-06-26 14:31:35.159931
last batch sz 10
Pre: time 2020-06-26 14:31:49.417270: 
 	std: 0.0032165982
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9796, 0.9733, 0.9807, 0.9747]
	train_accs: [0.98141664, 0.9805833, 0.97491664, 0.9815, 0.97543335]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97786
	best: 0.9807

Starting e_i: 760
Model ind 665 epoch 760 batch: 0 avg loss -2.914226 avg loss no lamb -2.914226 time 2020-06-26 14:31:50.376643
Model ind 665 epoch 760 batch: 100 avg loss -2.827893 avg loss no lamb -2.827893 time 2020-06-26 14:32:00.979361
Model ind 665 epoch 760 batch: 200 avg loss -2.765889 avg loss no lamb -2.765889 time 2020-06-26 14:32:11.595455
Model ind 665 epoch 760 batch: 300 avg loss -2.783976 avg loss no lamb -2.783976 time 2020-06-26 14:32:22.476455
Model ind 665 epoch 760 batch: 400 avg loss -2.774668 avg loss no lamb -2.774668 time 2020-06-26 14:32:33.350849
Model ind 665 epoch 760 batch: 500 avg loss -2.823955 avg loss no lamb -2.823955 time 2020-06-26 14:32:44.097836
Model ind 665 epoch 760 batch: 600 avg loss -2.881960 avg loss no lamb -2.881960 time 2020-06-26 14:32:54.786764
Model ind 665 epoch 760 batch: 700 avg loss -2.770616 avg loss no lamb -2.770616 time 2020-06-26 14:33:05.747563
Model ind 665 epoch 760 batch: 800 avg loss -2.821335 avg loss no lamb -2.821335 time 2020-06-26 14:33:16.587440
last batch sz 10
Pre: time 2020-06-26 14:33:30.442954: 
 	std: 0.0029486206
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.98, 0.9746, 0.9812, 0.9752]
	train_accs: [0.98146665, 0.98075, 0.9759, 0.98106664, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97846
	best: 0.9813

Starting e_i: 761
Model ind 665 epoch 761 batch: 0 avg loss -2.952766 avg loss no lamb -2.952766 time 2020-06-26 14:33:32.716604
Model ind 665 epoch 761 batch: 100 avg loss -2.872730 avg loss no lamb -2.872730 time 2020-06-26 14:33:43.463494
Model ind 665 epoch 761 batch: 200 avg loss -2.810567 avg loss no lamb -2.810567 time 2020-06-26 14:33:54.246829
Model ind 665 epoch 761 batch: 300 avg loss -2.876485 avg loss no lamb -2.876485 time 2020-06-26 14:34:05.182489
Model ind 665 epoch 761 batch: 400 avg loss -2.817202 avg loss no lamb -2.817202 time 2020-06-26 14:34:15.943887
Model ind 665 epoch 761 batch: 500 avg loss -2.806914 avg loss no lamb -2.806914 time 2020-06-26 14:34:26.759631
Model ind 665 epoch 761 batch: 600 avg loss -2.887370 avg loss no lamb -2.887370 time 2020-06-26 14:34:37.443533
Model ind 665 epoch 761 batch: 700 avg loss -2.715538 avg loss no lamb -2.715538 time 2020-06-26 14:34:48.203323
Model ind 665 epoch 761 batch: 800 avg loss -2.811151 avg loss no lamb -2.811151 time 2020-06-26 14:34:59.128148
last batch sz 10
Pre: time 2020-06-26 14:35:13.359286: 
 	std: 0.0028040076
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9796, 0.9744, 0.9811, 0.9756]
	train_accs: [0.98145, 0.98078334, 0.97533333, 0.98145, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97833997
	best: 0.981

Starting e_i: 762
Model ind 665 epoch 762 batch: 0 avg loss -2.989263 avg loss no lamb -2.989263 time 2020-06-26 14:35:14.336046
Model ind 665 epoch 762 batch: 100 avg loss -2.874993 avg loss no lamb -2.874993 time 2020-06-26 14:35:24.960274
Model ind 665 epoch 762 batch: 200 avg loss -2.868076 avg loss no lamb -2.868076 time 2020-06-26 14:35:35.550687
Model ind 665 epoch 762 batch: 300 avg loss -2.838481 avg loss no lamb -2.838481 time 2020-06-26 14:35:46.245453
Model ind 665 epoch 762 batch: 400 avg loss -2.800209 avg loss no lamb -2.800209 time 2020-06-26 14:35:57.064234
Model ind 665 epoch 762 batch: 500 avg loss -2.803631 avg loss no lamb -2.803631 time 2020-06-26 14:36:07.851845
Model ind 665 epoch 762 batch: 600 avg loss -2.855592 avg loss no lamb -2.855592 time 2020-06-26 14:36:18.477756
Model ind 665 epoch 762 batch: 700 avg loss -2.723106 avg loss no lamb -2.723106 time 2020-06-26 14:36:29.288216
Model ind 665 epoch 762 batch: 800 avg loss -2.821748 avg loss no lamb -2.821748 time 2020-06-26 14:36:39.859142
last batch sz 10
Pre: time 2020-06-26 14:36:53.785360: 
 	std: 0.0029668931
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9804, 0.9747, 0.9816, 0.9759]
	train_accs: [0.98158336, 0.98071665, 0.9755333, 0.9817167, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97886
	best: 0.9816

Starting e_i: 763
Model ind 665 epoch 763 batch: 0 avg loss -2.964757 avg loss no lamb -2.964757 time 2020-06-26 14:36:54.880888
Model ind 665 epoch 763 batch: 100 avg loss -2.879069 avg loss no lamb -2.879069 time 2020-06-26 14:37:05.670001
Model ind 665 epoch 763 batch: 200 avg loss -2.813359 avg loss no lamb -2.813359 time 2020-06-26 14:37:16.552584
Model ind 665 epoch 763 batch: 300 avg loss -2.845644 avg loss no lamb -2.845644 time 2020-06-26 14:37:27.541043
Model ind 665 epoch 763 batch: 400 avg loss -2.755605 avg loss no lamb -2.755605 time 2020-06-26 14:37:38.169006
Model ind 665 epoch 763 batch: 500 avg loss -2.867598 avg loss no lamb -2.867598 time 2020-06-26 14:37:48.938728
Model ind 665 epoch 763 batch: 600 avg loss -2.800807 avg loss no lamb -2.800807 time 2020-06-26 14:37:59.767753
Model ind 665 epoch 763 batch: 700 avg loss -2.713503 avg loss no lamb -2.713503 time 2020-06-26 14:38:10.760098
Model ind 665 epoch 763 batch: 800 avg loss -2.852982 avg loss no lamb -2.852982 time 2020-06-26 14:38:21.551844
last batch sz 10
Pre: time 2020-06-26 14:38:35.377689: 
 	std: 0.0029116466
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9807, 0.9747, 0.982, 0.9769]
	train_accs: [0.98158336, 0.9809833, 0.9755167, 0.98176664, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97922003
	best: 0.982

Starting e_i: 764
Model ind 665 epoch 764 batch: 0 avg loss -2.932115 avg loss no lamb -2.932115 time 2020-06-26 14:38:36.354941
Model ind 665 epoch 764 batch: 100 avg loss -2.913463 avg loss no lamb -2.913463 time 2020-06-26 14:38:47.001515
Model ind 665 epoch 764 batch: 200 avg loss -2.842153 avg loss no lamb -2.842153 time 2020-06-26 14:38:57.992029
Model ind 665 epoch 764 batch: 300 avg loss -2.788126 avg loss no lamb -2.788126 time 2020-06-26 14:39:08.863333
Model ind 665 epoch 764 batch: 400 avg loss -2.788183 avg loss no lamb -2.788183 time 2020-06-26 14:39:19.587350
Model ind 665 epoch 764 batch: 500 avg loss -2.738361 avg loss no lamb -2.738361 time 2020-06-26 14:39:30.445545
Model ind 665 epoch 764 batch: 600 avg loss -2.870203 avg loss no lamb -2.870203 time 2020-06-26 14:39:41.347882
Model ind 665 epoch 764 batch: 700 avg loss -2.740954 avg loss no lamb -2.740954 time 2020-06-26 14:39:52.168898
Model ind 665 epoch 764 batch: 800 avg loss -2.789022 avg loss no lamb -2.789022 time 2020-06-26 14:40:03.043468
last batch sz 10
Pre: time 2020-06-26 14:40:16.926783: 
 	std: 0.002318104
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9805, 0.9759, 0.9816, 0.9769]
	train_accs: [0.98205, 0.98146665, 0.97651666, 0.9820333, 0.97755]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.97918
	best: 0.981

Starting e_i: 765
Model ind 665 epoch 765 batch: 0 avg loss -2.966565 avg loss no lamb -2.966565 time 2020-06-26 14:40:18.003976
Model ind 665 epoch 765 batch: 100 avg loss -2.849170 avg loss no lamb -2.849170 time 2020-06-26 14:40:28.781191
Model ind 665 epoch 765 batch: 200 avg loss -2.897009 avg loss no lamb -2.897009 time 2020-06-26 14:40:39.626222
Model ind 665 epoch 765 batch: 300 avg loss -2.824843 avg loss no lamb -2.824843 time 2020-06-26 14:40:50.296334
Model ind 665 epoch 765 batch: 400 avg loss -2.758046 avg loss no lamb -2.758046 time 2020-06-26 14:41:01.162715
Model ind 665 epoch 765 batch: 500 avg loss -2.763784 avg loss no lamb -2.763784 time 2020-06-26 14:41:11.887022
Model ind 665 epoch 765 batch: 600 avg loss -2.875596 avg loss no lamb -2.875596 time 2020-06-26 14:41:22.606898
Model ind 665 epoch 765 batch: 700 avg loss -2.749218 avg loss no lamb -2.749218 time 2020-06-26 14:41:33.274152
Model ind 665 epoch 765 batch: 800 avg loss -2.873497 avg loss no lamb -2.873497 time 2020-06-26 14:41:44.071132
last batch sz 10
Pre: time 2020-06-26 14:41:57.946929: 
 	std: 0.0035103147
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9806, 0.9732, 0.9808, 0.9739]
	train_accs: [0.98143333, 0.98146665, 0.97491664, 0.9816667, 0.9756]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97783995
	best: 0.9808

Starting e_i: 766
Model ind 665 epoch 766 batch: 0 avg loss -2.958740 avg loss no lamb -2.958740 time 2020-06-26 14:41:58.966419
Model ind 665 epoch 766 batch: 100 avg loss -2.869064 avg loss no lamb -2.869064 time 2020-06-26 14:42:09.679996
Model ind 665 epoch 766 batch: 200 avg loss -2.834399 avg loss no lamb -2.834399 time 2020-06-26 14:42:20.487506
Model ind 665 epoch 766 batch: 300 avg loss -2.792311 avg loss no lamb -2.792311 time 2020-06-26 14:42:31.343610
Model ind 665 epoch 766 batch: 400 avg loss -2.747649 avg loss no lamb -2.747649 time 2020-06-26 14:42:42.106051
Model ind 665 epoch 766 batch: 500 avg loss -2.867248 avg loss no lamb -2.867248 time 2020-06-26 14:42:52.968439
Model ind 665 epoch 766 batch: 600 avg loss -2.886089 avg loss no lamb -2.886089 time 2020-06-26 14:43:03.639441
Model ind 665 epoch 766 batch: 700 avg loss -2.778965 avg loss no lamb -2.778965 time 2020-06-26 14:43:14.646223
Model ind 665 epoch 766 batch: 800 avg loss -2.869606 avg loss no lamb -2.869606 time 2020-06-26 14:43:25.680688
last batch sz 10
Pre: time 2020-06-26 14:43:39.812909: 
 	std: 0.002450639
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9793, 0.974, 0.9801, 0.9756]
	train_accs: [0.98148334, 0.98076665, 0.97555, 0.98155, 0.9769833]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97771996
	best: 0.9801

Starting e_i: 767
Model ind 665 epoch 767 batch: 0 avg loss -2.959292 avg loss no lamb -2.959292 time 2020-06-26 14:43:40.928063
Model ind 665 epoch 767 batch: 100 avg loss -2.829374 avg loss no lamb -2.829374 time 2020-06-26 14:43:51.716299
Model ind 665 epoch 767 batch: 200 avg loss -2.858753 avg loss no lamb -2.858753 time 2020-06-26 14:44:02.550957
Model ind 665 epoch 767 batch: 300 avg loss -2.856600 avg loss no lamb -2.856600 time 2020-06-26 14:44:13.523210
Model ind 665 epoch 767 batch: 400 avg loss -2.747983 avg loss no lamb -2.747983 time 2020-06-26 14:44:24.276369
Model ind 665 epoch 767 batch: 500 avg loss -2.852691 avg loss no lamb -2.852691 time 2020-06-26 14:44:34.993630
Model ind 665 epoch 767 batch: 600 avg loss -2.875933 avg loss no lamb -2.875933 time 2020-06-26 14:44:45.527139
Model ind 665 epoch 767 batch: 700 avg loss -2.778775 avg loss no lamb -2.778775 time 2020-06-26 14:44:56.551827
Model ind 665 epoch 767 batch: 800 avg loss -2.803268 avg loss no lamb -2.803268 time 2020-06-26 14:45:07.279344
last batch sz 10
Pre: time 2020-06-26 14:45:21.297723: 
 	std: 0.0029624281
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9811, 0.9746, 0.9819, 0.9763]
	train_accs: [0.98153335, 0.98153335, 0.9755333, 0.98183334, 0.97678334]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.979
	best: 0.9819

Starting e_i: 768
Model ind 665 epoch 768 batch: 0 avg loss -2.940967 avg loss no lamb -2.940967 time 2020-06-26 14:45:22.257975
Model ind 665 epoch 768 batch: 100 avg loss -2.943142 avg loss no lamb -2.943142 time 2020-06-26 14:45:33.052411
Model ind 665 epoch 768 batch: 200 avg loss -2.734238 avg loss no lamb -2.734238 time 2020-06-26 14:45:43.637343
Model ind 665 epoch 768 batch: 300 avg loss -2.915758 avg loss no lamb -2.915758 time 2020-06-26 14:45:54.466292
Model ind 665 epoch 768 batch: 400 avg loss -2.739563 avg loss no lamb -2.739563 time 2020-06-26 14:46:05.460359
Model ind 665 epoch 768 batch: 500 avg loss -2.815926 avg loss no lamb -2.815926 time 2020-06-26 14:46:16.243238
Model ind 665 epoch 768 batch: 600 avg loss -2.845099 avg loss no lamb -2.845099 time 2020-06-26 14:46:27.023530
Model ind 665 epoch 768 batch: 700 avg loss -2.672104 avg loss no lamb -2.672104 time 2020-06-26 14:46:37.711663
Model ind 665 epoch 768 batch: 800 avg loss -2.882126 avg loss no lamb -2.882126 time 2020-06-26 14:46:48.429950
last batch sz 10
Pre: time 2020-06-26 14:47:02.806181: 
 	std: 0.0036851037
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9807, 0.972, 0.9809, 0.9751]
	train_accs: [0.98146665, 0.98111665, 0.97478336, 0.98146665, 0.97618335]
	best_train_sub_head: 0
	worst: 0.972
	avg: 0.9778999
	best: 0.9808

Starting e_i: 769
Model ind 665 epoch 769 batch: 0 avg loss -2.942509 avg loss no lamb -2.942509 time 2020-06-26 14:47:03.942100
Model ind 665 epoch 769 batch: 100 avg loss -2.848040 avg loss no lamb -2.848040 time 2020-06-26 14:47:14.670969
Model ind 665 epoch 769 batch: 200 avg loss -2.803161 avg loss no lamb -2.803161 time 2020-06-26 14:47:25.065291
Model ind 665 epoch 769 batch: 300 avg loss -2.898330 avg loss no lamb -2.898330 time 2020-06-26 14:47:35.646516
Model ind 665 epoch 769 batch: 400 avg loss -2.797140 avg loss no lamb -2.797140 time 2020-06-26 14:47:46.639878
Model ind 665 epoch 769 batch: 500 avg loss -2.793689 avg loss no lamb -2.793689 time 2020-06-26 14:47:57.436104
Model ind 665 epoch 769 batch: 600 avg loss -2.903080 avg loss no lamb -2.903080 time 2020-06-26 14:48:08.178673
Model ind 665 epoch 769 batch: 700 avg loss -2.752357 avg loss no lamb -2.752357 time 2020-06-26 14:48:18.683190
Model ind 665 epoch 769 batch: 800 avg loss -2.891201 avg loss no lamb -2.891201 time 2020-06-26 14:48:29.265565
last batch sz 10
Pre: time 2020-06-26 14:48:43.181772: 
 	std: 0.0032671704
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.98, 0.9734, 0.9806, 0.9739]
	train_accs: [0.9813167, 0.98055, 0.9752333, 0.98125, 0.97573334]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97764
	best: 0.9803

Starting e_i: 770
Model ind 665 epoch 770 batch: 0 avg loss -2.934101 avg loss no lamb -2.934101 time 2020-06-26 14:48:44.178025
Model ind 665 epoch 770 batch: 100 avg loss -2.899151 avg loss no lamb -2.899151 time 2020-06-26 14:48:54.790995
Model ind 665 epoch 770 batch: 200 avg loss -2.882161 avg loss no lamb -2.882161 time 2020-06-26 14:49:05.355417
Model ind 665 epoch 770 batch: 300 avg loss -2.881725 avg loss no lamb -2.881725 time 2020-06-26 14:49:16.088633
Model ind 665 epoch 770 batch: 400 avg loss -2.740441 avg loss no lamb -2.740441 time 2020-06-26 14:49:26.879207
Model ind 665 epoch 770 batch: 500 avg loss -2.847401 avg loss no lamb -2.847401 time 2020-06-26 14:49:37.677628
Model ind 665 epoch 770 batch: 600 avg loss -2.851625 avg loss no lamb -2.851625 time 2020-06-26 14:49:48.343158
Model ind 665 epoch 770 batch: 700 avg loss -2.771136 avg loss no lamb -2.771136 time 2020-06-26 14:49:59.057530
Model ind 665 epoch 770 batch: 800 avg loss -2.841233 avg loss no lamb -2.841233 time 2020-06-26 14:50:09.826971
last batch sz 10
Pre: time 2020-06-26 14:50:23.887664: 
 	std: 0.002480652
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9794, 0.9747, 0.9801, 0.9755]
	train_accs: [0.9813167, 0.98035, 0.97578335, 0.98135, 0.97648335]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97808
	best: 0.9801

Starting e_i: 771
Model ind 665 epoch 771 batch: 0 avg loss -2.922444 avg loss no lamb -2.922444 time 2020-06-26 14:50:26.162944
Model ind 665 epoch 771 batch: 100 avg loss -2.862168 avg loss no lamb -2.862168 time 2020-06-26 14:50:36.899242
Model ind 665 epoch 771 batch: 200 avg loss -2.881279 avg loss no lamb -2.881279 time 2020-06-26 14:50:47.787802
Model ind 665 epoch 771 batch: 300 avg loss -2.881989 avg loss no lamb -2.881989 time 2020-06-26 14:50:58.439747
Model ind 665 epoch 771 batch: 400 avg loss -2.742793 avg loss no lamb -2.742793 time 2020-06-26 14:51:09.379717
Model ind 665 epoch 771 batch: 500 avg loss -2.850490 avg loss no lamb -2.850490 time 2020-06-26 14:51:20.245510
Model ind 665 epoch 771 batch: 600 avg loss -2.836704 avg loss no lamb -2.836704 time 2020-06-26 14:51:31.111585
Model ind 665 epoch 771 batch: 700 avg loss -2.777374 avg loss no lamb -2.777374 time 2020-06-26 14:51:41.692841
Model ind 665 epoch 771 batch: 800 avg loss -2.809237 avg loss no lamb -2.809237 time 2020-06-26 14:51:52.192274
last batch sz 10
Pre: time 2020-06-26 14:52:06.270282: 
 	std: 0.002448353
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.98, 0.975, 0.9807, 0.9765]
	train_accs: [0.9813333, 0.9809167, 0.9756, 0.98121667, 0.97685]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97866
	best: 0.9811

Starting e_i: 772
Model ind 665 epoch 772 batch: 0 avg loss -2.941925 avg loss no lamb -2.941925 time 2020-06-26 14:52:07.274204
Model ind 665 epoch 772 batch: 100 avg loss -2.885750 avg loss no lamb -2.885750 time 2020-06-26 14:52:17.914944
Model ind 665 epoch 772 batch: 200 avg loss -2.834349 avg loss no lamb -2.834349 time 2020-06-26 14:52:28.457481
Model ind 665 epoch 772 batch: 300 avg loss -2.818628 avg loss no lamb -2.818628 time 2020-06-26 14:52:39.201660
Model ind 665 epoch 772 batch: 400 avg loss -2.754119 avg loss no lamb -2.754119 time 2020-06-26 14:52:49.940450
Model ind 665 epoch 772 batch: 500 avg loss -2.848627 avg loss no lamb -2.848627 time 2020-06-26 14:53:00.867271
Model ind 665 epoch 772 batch: 600 avg loss -2.834417 avg loss no lamb -2.834417 time 2020-06-26 14:53:11.670074
Model ind 665 epoch 772 batch: 700 avg loss -2.719454 avg loss no lamb -2.719454 time 2020-06-26 14:53:22.481890
Model ind 665 epoch 772 batch: 800 avg loss -2.842499 avg loss no lamb -2.842499 time 2020-06-26 14:53:33.444872
last batch sz 10
Pre: time 2020-06-26 14:53:47.444596: 
 	std: 0.0029302468
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9801, 0.9742, 0.9811, 0.9757]
	train_accs: [0.9816667, 0.981, 0.97583336, 0.9817167, 0.9769]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97846
	best: 0.9811

Starting e_i: 773
Model ind 665 epoch 773 batch: 0 avg loss -2.960498 avg loss no lamb -2.960498 time 2020-06-26 14:53:48.534439
Model ind 665 epoch 773 batch: 100 avg loss -2.882257 avg loss no lamb -2.882257 time 2020-06-26 14:53:59.520489
Model ind 665 epoch 773 batch: 200 avg loss -2.854597 avg loss no lamb -2.854597 time 2020-06-26 14:54:10.098849
Model ind 665 epoch 773 batch: 300 avg loss -2.838073 avg loss no lamb -2.838073 time 2020-06-26 14:54:20.829407
Model ind 665 epoch 773 batch: 400 avg loss -2.809637 avg loss no lamb -2.809637 time 2020-06-26 14:54:31.606925
Model ind 665 epoch 773 batch: 500 avg loss -2.804516 avg loss no lamb -2.804516 time 2020-06-26 14:54:42.437382
Model ind 665 epoch 773 batch: 600 avg loss -2.856955 avg loss no lamb -2.856955 time 2020-06-26 14:54:53.251265
Model ind 665 epoch 773 batch: 700 avg loss -2.766388 avg loss no lamb -2.766388 time 2020-06-26 14:55:04.036216
Model ind 665 epoch 773 batch: 800 avg loss -2.832460 avg loss no lamb -2.832460 time 2020-06-26 14:55:14.789180
last batch sz 10
Pre: time 2020-06-26 14:55:28.755235: 
 	std: 0.0033831457
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9808, 0.974, 0.9815, 0.9745]
	train_accs: [0.98186666, 0.9813667, 0.97566664, 0.98178333, 0.9765667]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97838
	best: 0.9811

Starting e_i: 774
Model ind 665 epoch 774 batch: 0 avg loss -2.917300 avg loss no lamb -2.917300 time 2020-06-26 14:55:29.682918
Model ind 665 epoch 774 batch: 100 avg loss -2.792307 avg loss no lamb -2.792307 time 2020-06-26 14:55:40.369427
Model ind 665 epoch 774 batch: 200 avg loss -2.841182 avg loss no lamb -2.841182 time 2020-06-26 14:55:51.379311
Model ind 665 epoch 774 batch: 300 avg loss -2.883463 avg loss no lamb -2.883463 time 2020-06-26 14:56:02.250826
Model ind 665 epoch 774 batch: 400 avg loss -2.749277 avg loss no lamb -2.749277 time 2020-06-26 14:56:12.891490
Model ind 665 epoch 774 batch: 500 avg loss -2.793396 avg loss no lamb -2.793396 time 2020-06-26 14:56:23.630914
Model ind 665 epoch 774 batch: 600 avg loss -2.889145 avg loss no lamb -2.889145 time 2020-06-26 14:56:34.092075
Model ind 665 epoch 774 batch: 700 avg loss -2.760186 avg loss no lamb -2.760186 time 2020-06-26 14:56:44.930916
Model ind 665 epoch 774 batch: 800 avg loss -2.858087 avg loss no lamb -2.858087 time 2020-06-26 14:56:55.753158
last batch sz 10
Pre: time 2020-06-26 14:57:09.875420: 
 	std: 0.002992931
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.981, 0.9741, 0.9812, 0.9766]
	train_accs: [0.9819833, 0.98113334, 0.97566664, 0.98185, 0.97723335]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97888005
	best: 0.9815

Starting e_i: 775
Model ind 665 epoch 775 batch: 0 avg loss -2.947504 avg loss no lamb -2.947504 time 2020-06-26 14:57:10.923871
Model ind 665 epoch 775 batch: 100 avg loss -2.885485 avg loss no lamb -2.885485 time 2020-06-26 14:57:21.673782
Model ind 665 epoch 775 batch: 200 avg loss -2.803093 avg loss no lamb -2.803093 time 2020-06-26 14:57:32.680825
Model ind 665 epoch 775 batch: 300 avg loss -2.833483 avg loss no lamb -2.833483 time 2020-06-26 14:57:43.660871
Model ind 665 epoch 775 batch: 400 avg loss -2.772300 avg loss no lamb -2.772300 time 2020-06-26 14:57:54.434084
Model ind 665 epoch 775 batch: 500 avg loss -2.752596 avg loss no lamb -2.752596 time 2020-06-26 14:58:05.287111
Model ind 665 epoch 775 batch: 600 avg loss -2.896173 avg loss no lamb -2.896173 time 2020-06-26 14:58:15.769492
Model ind 665 epoch 775 batch: 700 avg loss -2.720081 avg loss no lamb -2.720081 time 2020-06-26 14:58:26.704678
Model ind 665 epoch 775 batch: 800 avg loss -2.853927 avg loss no lamb -2.853927 time 2020-06-26 14:58:37.506830
last batch sz 10
Pre: time 2020-06-26 14:58:51.584431: 
 	std: 0.0023718348
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.981, 0.9764, 0.9818, 0.9771]
	train_accs: [0.98235, 0.98158336, 0.97678334, 0.9823167, 0.97785]
	best_train_sub_head: 0
	worst: 0.9764
	avg: 0.97962
	best: 0.9818

Starting e_i: 776
Model ind 665 epoch 776 batch: 0 avg loss -2.897545 avg loss no lamb -2.897545 time 2020-06-26 14:58:52.606266
Model ind 665 epoch 776 batch: 100 avg loss -2.872301 avg loss no lamb -2.872301 time 2020-06-26 14:59:03.354821
Model ind 665 epoch 776 batch: 200 avg loss -2.833367 avg loss no lamb -2.833367 time 2020-06-26 14:59:14.004384
Model ind 665 epoch 776 batch: 300 avg loss -2.887559 avg loss no lamb -2.887559 time 2020-06-26 14:59:24.694399
Model ind 665 epoch 776 batch: 400 avg loss -2.753775 avg loss no lamb -2.753775 time 2020-06-26 14:59:35.585468
Model ind 665 epoch 776 batch: 500 avg loss -2.773770 avg loss no lamb -2.773770 time 2020-06-26 14:59:46.468456
Model ind 665 epoch 776 batch: 600 avg loss -2.848206 avg loss no lamb -2.848206 time 2020-06-26 14:59:57.180684
Model ind 665 epoch 776 batch: 700 avg loss -2.736941 avg loss no lamb -2.736941 time 2020-06-26 15:00:08.138742
Model ind 665 epoch 776 batch: 800 avg loss -2.884660 avg loss no lamb -2.884660 time 2020-06-26 15:00:19.001795
last batch sz 10
Pre: time 2020-06-26 15:00:33.073176: 
 	std: 0.0030130402
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9803, 0.9741, 0.9807, 0.9753]
	train_accs: [0.9819, 0.98108333, 0.9755167, 0.98183334, 0.97635]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97833997
	best: 0.9813

Starting e_i: 777
Model ind 665 epoch 777 batch: 0 avg loss -2.919301 avg loss no lamb -2.919301 time 2020-06-26 15:00:34.049713
Model ind 665 epoch 777 batch: 100 avg loss -2.929240 avg loss no lamb -2.929240 time 2020-06-26 15:00:44.642784
Model ind 665 epoch 777 batch: 200 avg loss -2.797610 avg loss no lamb -2.797610 time 2020-06-26 15:00:55.336067
Model ind 665 epoch 777 batch: 300 avg loss -2.837940 avg loss no lamb -2.837940 time 2020-06-26 15:01:06.154040
Model ind 665 epoch 777 batch: 400 avg loss -2.759166 avg loss no lamb -2.759166 time 2020-06-26 15:01:16.877192
Model ind 665 epoch 777 batch: 500 avg loss -2.811890 avg loss no lamb -2.811890 time 2020-06-26 15:01:27.566687
Model ind 665 epoch 777 batch: 600 avg loss -2.842679 avg loss no lamb -2.842679 time 2020-06-26 15:01:38.225623
Model ind 665 epoch 777 batch: 700 avg loss -2.769656 avg loss no lamb -2.769656 time 2020-06-26 15:01:49.106704
Model ind 665 epoch 777 batch: 800 avg loss -2.877390 avg loss no lamb -2.877390 time 2020-06-26 15:01:59.575442
last batch sz 10
Pre: time 2020-06-26 15:02:13.594929: 
 	std: 0.002307904
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9806, 0.9756, 0.9809, 0.9772]
	train_accs: [0.9819, 0.98116666, 0.97645, 0.98205, 0.97723335]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97914
	best: 0.9809

Starting e_i: 778
Model ind 665 epoch 778 batch: 0 avg loss -2.942684 avg loss no lamb -2.942684 time 2020-06-26 15:02:14.573533
Model ind 665 epoch 778 batch: 100 avg loss -2.869287 avg loss no lamb -2.869287 time 2020-06-26 15:02:25.343214
Model ind 665 epoch 778 batch: 200 avg loss -2.858748 avg loss no lamb -2.858748 time 2020-06-26 15:02:35.997675
Model ind 665 epoch 778 batch: 300 avg loss -2.851135 avg loss no lamb -2.851135 time 2020-06-26 15:02:46.524757
Model ind 665 epoch 778 batch: 400 avg loss -2.739041 avg loss no lamb -2.739041 time 2020-06-26 15:02:57.406450
Model ind 665 epoch 778 batch: 500 avg loss -2.825096 avg loss no lamb -2.825096 time 2020-06-26 15:03:08.126478
Model ind 665 epoch 778 batch: 600 avg loss -2.839553 avg loss no lamb -2.839553 time 2020-06-26 15:03:18.860756
Model ind 665 epoch 778 batch: 700 avg loss -2.778341 avg loss no lamb -2.778341 time 2020-06-26 15:03:29.477928
Model ind 665 epoch 778 batch: 800 avg loss -2.838330 avg loss no lamb -2.838330 time 2020-06-26 15:03:40.392101
last batch sz 10
Pre: time 2020-06-26 15:03:54.643750: 
 	std: 0.0034884906
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9801, 0.9732, 0.9803, 0.9729]
	train_accs: [0.9812833, 0.98048335, 0.9751667, 0.9813, 0.9756]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97732
	best: 0.9803

Starting e_i: 779
Model ind 665 epoch 779 batch: 0 avg loss -2.906975 avg loss no lamb -2.906975 time 2020-06-26 15:03:55.649332
Model ind 665 epoch 779 batch: 100 avg loss -2.890700 avg loss no lamb -2.890700 time 2020-06-26 15:04:06.366859
Model ind 665 epoch 779 batch: 200 avg loss -2.928310 avg loss no lamb -2.928310 time 2020-06-26 15:04:16.871497
Model ind 665 epoch 779 batch: 300 avg loss -2.836827 avg loss no lamb -2.836827 time 2020-06-26 15:04:27.759926
Model ind 665 epoch 779 batch: 400 avg loss -2.721453 avg loss no lamb -2.721453 time 2020-06-26 15:04:38.439814
Model ind 665 epoch 779 batch: 500 avg loss -2.860665 avg loss no lamb -2.860665 time 2020-06-26 15:04:49.247519
Model ind 665 epoch 779 batch: 600 avg loss -2.785678 avg loss no lamb -2.785678 time 2020-06-26 15:04:59.780035
Model ind 665 epoch 779 batch: 700 avg loss -2.786535 avg loss no lamb -2.786535 time 2020-06-26 15:05:10.670280
Model ind 665 epoch 779 batch: 800 avg loss -2.814533 avg loss no lamb -2.814533 time 2020-06-26 15:05:21.325730
last batch sz 10
Pre: time 2020-06-26 15:05:35.466278: 
 	std: 0.0035858643
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9806, 0.9726, 0.9801, 0.9738]
	train_accs: [0.98141664, 0.9811, 0.97485, 0.98116666, 0.9756167]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.9775599
	best: 0.9807

Starting e_i: 780
Model ind 665 epoch 780 batch: 0 avg loss -2.982840 avg loss no lamb -2.982840 time 2020-06-26 15:05:36.500593
Model ind 665 epoch 780 batch: 100 avg loss -2.881851 avg loss no lamb -2.881851 time 2020-06-26 15:05:47.197730
Model ind 665 epoch 780 batch: 200 avg loss -2.872323 avg loss no lamb -2.872323 time 2020-06-26 15:05:57.901782
Model ind 665 epoch 780 batch: 300 avg loss -2.854754 avg loss no lamb -2.854754 time 2020-06-26 15:06:08.644336
Model ind 665 epoch 780 batch: 400 avg loss -2.746727 avg loss no lamb -2.746727 time 2020-06-26 15:06:19.365998
Model ind 665 epoch 780 batch: 500 avg loss -2.855446 avg loss no lamb -2.855446 time 2020-06-26 15:06:30.000839
Model ind 665 epoch 780 batch: 600 avg loss -2.873865 avg loss no lamb -2.873865 time 2020-06-26 15:06:40.749073
Model ind 665 epoch 780 batch: 700 avg loss -2.742232 avg loss no lamb -2.742232 time 2020-06-26 15:06:51.588361
Model ind 665 epoch 780 batch: 800 avg loss -2.881750 avg loss no lamb -2.881750 time 2020-06-26 15:07:02.331791
last batch sz 10
Pre: time 2020-06-26 15:07:16.344589: 
 	std: 0.0031189842
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9796, 0.974, 0.981, 0.9747]
	train_accs: [0.9817333, 0.9808667, 0.97566664, 0.98175, 0.97636664]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.9781
	best: 0.981

Starting e_i: 781
Model ind 665 epoch 781 batch: 0 avg loss -2.947744 avg loss no lamb -2.947744 time 2020-06-26 15:07:18.533576
Model ind 665 epoch 781 batch: 100 avg loss -2.932360 avg loss no lamb -2.932360 time 2020-06-26 15:07:29.297092
Model ind 665 epoch 781 batch: 200 avg loss -2.826052 avg loss no lamb -2.826052 time 2020-06-26 15:07:40.110653
Model ind 665 epoch 781 batch: 300 avg loss -2.792413 avg loss no lamb -2.792413 time 2020-06-26 15:07:50.779745
Model ind 665 epoch 781 batch: 400 avg loss -2.853099 avg loss no lamb -2.853099 time 2020-06-26 15:08:01.525480
Model ind 665 epoch 781 batch: 500 avg loss -2.816578 avg loss no lamb -2.816578 time 2020-06-26 15:08:12.310992
Model ind 665 epoch 781 batch: 600 avg loss -2.843364 avg loss no lamb -2.843364 time 2020-06-26 15:08:23.193817
Model ind 665 epoch 781 batch: 700 avg loss -2.740189 avg loss no lamb -2.740189 time 2020-06-26 15:08:34.096261
Model ind 665 epoch 781 batch: 800 avg loss -2.821182 avg loss no lamb -2.821182 time 2020-06-26 15:08:44.792781
last batch sz 10
Pre: time 2020-06-26 15:08:58.339068: 
 	std: 0.0029464415
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9791, 0.9728, 0.9798, 0.975]
	train_accs: [0.98146665, 0.98046666, 0.9751833, 0.98143333, 0.97685]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97738
	best: 0.9802

Starting e_i: 782
Model ind 665 epoch 782 batch: 0 avg loss -2.921080 avg loss no lamb -2.921080 time 2020-06-26 15:08:59.303323
Model ind 665 epoch 782 batch: 100 avg loss -2.880565 avg loss no lamb -2.880565 time 2020-06-26 15:09:10.320339
Model ind 665 epoch 782 batch: 200 avg loss -2.836569 avg loss no lamb -2.836569 time 2020-06-26 15:09:21.154598
Model ind 665 epoch 782 batch: 300 avg loss -2.895917 avg loss no lamb -2.895917 time 2020-06-26 15:09:31.783995
Model ind 665 epoch 782 batch: 400 avg loss -2.767230 avg loss no lamb -2.767230 time 2020-06-26 15:09:42.163067
Model ind 665 epoch 782 batch: 500 avg loss -2.817237 avg loss no lamb -2.817237 time 2020-06-26 15:09:53.200948
Model ind 665 epoch 782 batch: 600 avg loss -2.914599 avg loss no lamb -2.914599 time 2020-06-26 15:10:04.246392
Model ind 665 epoch 782 batch: 700 avg loss -2.797964 avg loss no lamb -2.797964 time 2020-06-26 15:10:14.940264
Model ind 665 epoch 782 batch: 800 avg loss -2.809222 avg loss no lamb -2.809222 time 2020-06-26 15:10:25.839281
last batch sz 10
Pre: time 2020-06-26 15:10:39.801678: 
 	std: 0.002930531
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9785, 0.973, 0.9802, 0.9747]
	train_accs: [0.98141664, 0.9806333, 0.97566664, 0.9812833, 0.9761]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97730005
	best: 0.9801

Starting e_i: 783
Model ind 665 epoch 783 batch: 0 avg loss -2.972886 avg loss no lamb -2.972886 time 2020-06-26 15:10:40.795602
Model ind 665 epoch 783 batch: 100 avg loss -2.916239 avg loss no lamb -2.916239 time 2020-06-26 15:10:51.494094
Model ind 665 epoch 783 batch: 200 avg loss -2.872619 avg loss no lamb -2.872619 time 2020-06-26 15:11:02.302391
Model ind 665 epoch 783 batch: 300 avg loss -2.895190 avg loss no lamb -2.895190 time 2020-06-26 15:11:13.009827
Model ind 665 epoch 783 batch: 400 avg loss -2.775857 avg loss no lamb -2.775857 time 2020-06-26 15:11:23.374508
Model ind 665 epoch 783 batch: 500 avg loss -2.813064 avg loss no lamb -2.813064 time 2020-06-26 15:11:34.122565
Model ind 665 epoch 783 batch: 600 avg loss -2.859165 avg loss no lamb -2.859165 time 2020-06-26 15:11:45.078408
Model ind 665 epoch 783 batch: 700 avg loss -2.717422 avg loss no lamb -2.717422 time 2020-06-26 15:11:55.857055
Model ind 665 epoch 783 batch: 800 avg loss -2.818035 avg loss no lamb -2.818035 time 2020-06-26 15:12:06.786774
last batch sz 10
Pre: time 2020-06-26 15:12:20.695010: 
 	std: 0.0034273
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9793, 0.9727, 0.9812, 0.9752]
	train_accs: [0.9814, 0.98095, 0.97485, 0.9816167, 0.9762167]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97793996
	best: 0.9812

Starting e_i: 784
Model ind 665 epoch 784 batch: 0 avg loss -2.936942 avg loss no lamb -2.936942 time 2020-06-26 15:12:21.789358
Model ind 665 epoch 784 batch: 100 avg loss -2.888645 avg loss no lamb -2.888645 time 2020-06-26 15:12:32.583277
Model ind 665 epoch 784 batch: 200 avg loss -2.832881 avg loss no lamb -2.832881 time 2020-06-26 15:12:43.453032
Model ind 665 epoch 784 batch: 300 avg loss -2.871203 avg loss no lamb -2.871203 time 2020-06-26 15:12:54.229663
Model ind 665 epoch 784 batch: 400 avg loss -2.764528 avg loss no lamb -2.764528 time 2020-06-26 15:13:05.087558
Model ind 665 epoch 784 batch: 500 avg loss -2.806473 avg loss no lamb -2.806473 time 2020-06-26 15:13:15.722989
Model ind 665 epoch 784 batch: 600 avg loss -2.850730 avg loss no lamb -2.850730 time 2020-06-26 15:13:26.626412
Model ind 665 epoch 784 batch: 700 avg loss -2.804475 avg loss no lamb -2.804475 time 2020-06-26 15:13:37.509614
Model ind 665 epoch 784 batch: 800 avg loss -2.852266 avg loss no lamb -2.852266 time 2020-06-26 15:13:48.324884
last batch sz 10
Pre: time 2020-06-26 15:14:02.430138: 
 	std: 0.0024449206
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9796, 0.974, 0.9796, 0.976]
	train_accs: [0.98076665, 0.98018336, 0.9756333, 0.9809833, 0.9767]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97788
	best: 0.9796

Starting e_i: 785
Model ind 665 epoch 785 batch: 0 avg loss -2.918501 avg loss no lamb -2.918501 time 2020-06-26 15:14:03.418703
Model ind 665 epoch 785 batch: 100 avg loss -2.955560 avg loss no lamb -2.955560 time 2020-06-26 15:14:14.318347
Model ind 665 epoch 785 batch: 200 avg loss -2.866712 avg loss no lamb -2.866712 time 2020-06-26 15:14:25.183262
Model ind 665 epoch 785 batch: 300 avg loss -2.879228 avg loss no lamb -2.879228 time 2020-06-26 15:14:36.181826
Model ind 665 epoch 785 batch: 400 avg loss -2.772017 avg loss no lamb -2.772017 time 2020-06-26 15:14:47.061967
Model ind 665 epoch 785 batch: 500 avg loss -2.820511 avg loss no lamb -2.820511 time 2020-06-26 15:14:57.660115
Model ind 665 epoch 785 batch: 600 avg loss -2.888377 avg loss no lamb -2.888377 time 2020-06-26 15:15:08.444170
Model ind 665 epoch 785 batch: 700 avg loss -2.709948 avg loss no lamb -2.709948 time 2020-06-26 15:15:19.168968
Model ind 665 epoch 785 batch: 800 avg loss -2.772241 avg loss no lamb -2.772241 time 2020-06-26 15:15:29.907164
last batch sz 10
Pre: time 2020-06-26 15:15:43.720373: 
 	std: 0.0031701014
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9808, 0.9746, 0.982, 0.9758]
	train_accs: [0.98188335, 0.98125, 0.97585, 0.98176664, 0.97653335]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97902
	best: 0.9819

Starting e_i: 786
Model ind 665 epoch 786 batch: 0 avg loss -2.923671 avg loss no lamb -2.923671 time 2020-06-26 15:15:44.858445
Model ind 665 epoch 786 batch: 100 avg loss -2.878077 avg loss no lamb -2.878077 time 2020-06-26 15:15:55.609166
Model ind 665 epoch 786 batch: 200 avg loss -2.892292 avg loss no lamb -2.892292 time 2020-06-26 15:16:06.291088
Model ind 665 epoch 786 batch: 300 avg loss -2.848349 avg loss no lamb -2.848349 time 2020-06-26 15:16:17.128166
Model ind 665 epoch 786 batch: 400 avg loss -2.821190 avg loss no lamb -2.821190 time 2020-06-26 15:16:27.870813
Model ind 665 epoch 786 batch: 500 avg loss -2.795926 avg loss no lamb -2.795926 time 2020-06-26 15:16:38.474459
Model ind 665 epoch 786 batch: 600 avg loss -2.860580 avg loss no lamb -2.860580 time 2020-06-26 15:16:49.600236
Model ind 665 epoch 786 batch: 700 avg loss -2.733446 avg loss no lamb -2.733446 time 2020-06-26 15:17:00.748574
Model ind 665 epoch 786 batch: 800 avg loss -2.860051 avg loss no lamb -2.860051 time 2020-06-26 15:17:11.655493
last batch sz 10
Pre: time 2020-06-26 15:17:25.848991: 
 	std: 0.003229496
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9797, 0.9723, 0.9802, 0.9749]
	train_accs: [0.9808667, 0.98001665, 0.97445, 0.98073334, 0.9758]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97742003
	best: 0.98

Starting e_i: 787
Model ind 665 epoch 787 batch: 0 avg loss -2.992741 avg loss no lamb -2.992741 time 2020-06-26 15:17:26.852714
Model ind 665 epoch 787 batch: 100 avg loss -2.927354 avg loss no lamb -2.927354 time 2020-06-26 15:17:37.661911
Model ind 665 epoch 787 batch: 200 avg loss -2.835141 avg loss no lamb -2.835141 time 2020-06-26 15:17:48.440103
Model ind 665 epoch 787 batch: 300 avg loss -2.868084 avg loss no lamb -2.868084 time 2020-06-26 15:17:59.539838
Model ind 665 epoch 787 batch: 400 avg loss -2.736161 avg loss no lamb -2.736161 time 2020-06-26 15:18:10.386596
Model ind 665 epoch 787 batch: 500 avg loss -2.805507 avg loss no lamb -2.805507 time 2020-06-26 15:18:20.775464
Model ind 665 epoch 787 batch: 600 avg loss -2.875482 avg loss no lamb -2.875482 time 2020-06-26 15:18:31.235174
Model ind 665 epoch 787 batch: 700 avg loss -2.704853 avg loss no lamb -2.704853 time 2020-06-26 15:18:41.909470
Model ind 665 epoch 787 batch: 800 avg loss -2.831497 avg loss no lamb -2.831497 time 2020-06-26 15:18:52.776846
last batch sz 10
Pre: time 2020-06-26 15:19:06.669146: 
 	std: 0.002002411
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.982, 0.9783, 0.9827, 0.9787]
	train_accs: [0.9823167, 0.98191667, 0.9776667, 0.9824167, 0.9782]
	best_train_sub_head: 3
	worst: 0.9783
	avg: 0.98092
	best: 0.9827

Starting e_i: 788
Model ind 665 epoch 788 batch: 0 avg loss -2.946793 avg loss no lamb -2.946793 time 2020-06-26 15:19:07.798670
Model ind 665 epoch 788 batch: 100 avg loss -2.839009 avg loss no lamb -2.839009 time 2020-06-26 15:19:18.873157
Model ind 665 epoch 788 batch: 200 avg loss -2.872614 avg loss no lamb -2.872614 time 2020-06-26 15:19:29.597429
Model ind 665 epoch 788 batch: 300 avg loss -2.878312 avg loss no lamb -2.878312 time 2020-06-26 15:19:40.249328
Model ind 665 epoch 788 batch: 400 avg loss -2.773911 avg loss no lamb -2.773911 time 2020-06-26 15:19:50.989364
Model ind 665 epoch 788 batch: 500 avg loss -2.821168 avg loss no lamb -2.821168 time 2020-06-26 15:20:01.785601
Model ind 665 epoch 788 batch: 600 avg loss -2.847432 avg loss no lamb -2.847432 time 2020-06-26 15:20:12.320344
Model ind 665 epoch 788 batch: 700 avg loss -2.746651 avg loss no lamb -2.746651 time 2020-06-26 15:20:23.231259
Model ind 665 epoch 788 batch: 800 avg loss -2.833878 avg loss no lamb -2.833878 time 2020-06-26 15:20:33.938349
last batch sz 10
Pre: time 2020-06-26 15:20:48.030252: 
 	std: 0.0029741556
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9801, 0.9738, 0.9808, 0.9762]
	train_accs: [0.98191667, 0.98153335, 0.97601664, 0.98178333, 0.97726667]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97848
	best: 0.9815

Starting e_i: 789
Model ind 665 epoch 789 batch: 0 avg loss -2.962930 avg loss no lamb -2.962930 time 2020-06-26 15:20:49.045894
Model ind 665 epoch 789 batch: 100 avg loss -2.910125 avg loss no lamb -2.910125 time 2020-06-26 15:20:59.571295
Model ind 665 epoch 789 batch: 200 avg loss -2.892319 avg loss no lamb -2.892319 time 2020-06-26 15:21:10.335275
Model ind 665 epoch 789 batch: 300 avg loss -2.857125 avg loss no lamb -2.857125 time 2020-06-26 15:21:21.053417
Model ind 665 epoch 789 batch: 400 avg loss -2.795383 avg loss no lamb -2.795383 time 2020-06-26 15:21:31.991864
Model ind 665 epoch 789 batch: 500 avg loss -2.804143 avg loss no lamb -2.804143 time 2020-06-26 15:21:42.673954
Model ind 665 epoch 789 batch: 600 avg loss -2.860360 avg loss no lamb -2.860360 time 2020-06-26 15:21:53.354623
Model ind 665 epoch 789 batch: 700 avg loss -2.729627 avg loss no lamb -2.729627 time 2020-06-26 15:22:04.149968
Model ind 665 epoch 789 batch: 800 avg loss -2.800962 avg loss no lamb -2.800962 time 2020-06-26 15:22:15.024140
last batch sz 10
Pre: time 2020-06-26 15:22:29.075190: 
 	std: 0.0028042854
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9798, 0.9738, 0.9803, 0.9744]
	train_accs: [0.9805833, 0.9802833, 0.97415, 0.98075, 0.97505]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97749996
	best: 0.9803

Starting e_i: 790
Model ind 665 epoch 790 batch: 0 avg loss -2.959150 avg loss no lamb -2.959150 time 2020-06-26 15:22:30.196335
Model ind 665 epoch 790 batch: 100 avg loss -2.871041 avg loss no lamb -2.871041 time 2020-06-26 15:22:41.003441
Model ind 665 epoch 790 batch: 200 avg loss -2.844459 avg loss no lamb -2.844459 time 2020-06-26 15:22:51.630599
Model ind 665 epoch 790 batch: 300 avg loss -2.883831 avg loss no lamb -2.883831 time 2020-06-26 15:23:02.376414
Model ind 665 epoch 790 batch: 400 avg loss -2.770134 avg loss no lamb -2.770134 time 2020-06-26 15:23:13.204992
Model ind 665 epoch 790 batch: 500 avg loss -2.806223 avg loss no lamb -2.806223 time 2020-06-26 15:23:23.969933
Model ind 665 epoch 790 batch: 600 avg loss -2.870050 avg loss no lamb -2.870050 time 2020-06-26 15:23:34.654598
Model ind 665 epoch 790 batch: 700 avg loss -2.740839 avg loss no lamb -2.740839 time 2020-06-26 15:23:45.534162
Model ind 665 epoch 790 batch: 800 avg loss -2.867869 avg loss no lamb -2.867869 time 2020-06-26 15:23:56.294483
last batch sz 10
Pre: time 2020-06-26 15:24:10.257630: 
 	std: 0.002698587
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9813, 0.9749, 0.9808, 0.9765]
	train_accs: [0.98146665, 0.98143333, 0.97573334, 0.98105, 0.97705]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.9789399
	best: 0.9812

Starting e_i: 791
Model ind 665 epoch 791 batch: 0 avg loss -2.909564 avg loss no lamb -2.909564 time 2020-06-26 15:24:12.389592
Model ind 665 epoch 791 batch: 100 avg loss -2.879889 avg loss no lamb -2.879889 time 2020-06-26 15:24:23.228933
Model ind 665 epoch 791 batch: 200 avg loss -2.895062 avg loss no lamb -2.895062 time 2020-06-26 15:24:33.808760
Model ind 665 epoch 791 batch: 300 avg loss -2.872313 avg loss no lamb -2.872313 time 2020-06-26 15:24:44.304883
Model ind 665 epoch 791 batch: 400 avg loss -2.723172 avg loss no lamb -2.723172 time 2020-06-26 15:24:54.840752
Model ind 665 epoch 791 batch: 500 avg loss -2.779060 avg loss no lamb -2.779060 time 2020-06-26 15:25:05.723483
Model ind 665 epoch 791 batch: 600 avg loss -2.868052 avg loss no lamb -2.868052 time 2020-06-26 15:25:16.651634
Model ind 665 epoch 791 batch: 700 avg loss -2.738184 avg loss no lamb -2.738184 time 2020-06-26 15:25:27.371533
Model ind 665 epoch 791 batch: 800 avg loss -2.815473 avg loss no lamb -2.815473 time 2020-06-26 15:25:38.061770
last batch sz 10
Pre: time 2020-06-26 15:25:52.276022: 
 	std: 0.0035454764
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.979, 0.9717, 0.9803, 0.974]
	train_accs: [0.98106664, 0.98046666, 0.97425, 0.9813667, 0.97565]
	best_train_sub_head: 3
	worst: 0.9717
	avg: 0.97705996
	best: 0.9803

Starting e_i: 792
Model ind 665 epoch 792 batch: 0 avg loss -2.911004 avg loss no lamb -2.911004 time 2020-06-26 15:25:53.277613
Model ind 665 epoch 792 batch: 100 avg loss -2.926847 avg loss no lamb -2.926847 time 2020-06-26 15:26:04.024034
Model ind 665 epoch 792 batch: 200 avg loss -2.889917 avg loss no lamb -2.889917 time 2020-06-26 15:26:14.926679
Model ind 665 epoch 792 batch: 300 avg loss -2.841357 avg loss no lamb -2.841357 time 2020-06-26 15:26:25.645607
Model ind 665 epoch 792 batch: 400 avg loss -2.789017 avg loss no lamb -2.789017 time 2020-06-26 15:26:36.622694
Model ind 665 epoch 792 batch: 500 avg loss -2.843999 avg loss no lamb -2.843999 time 2020-06-26 15:26:47.356774
Model ind 665 epoch 792 batch: 600 avg loss -2.853252 avg loss no lamb -2.853252 time 2020-06-26 15:26:58.198532
Model ind 665 epoch 792 batch: 700 avg loss -2.770242 avg loss no lamb -2.770242 time 2020-06-26 15:27:09.104896
Model ind 665 epoch 792 batch: 800 avg loss -2.865635 avg loss no lamb -2.865635 time 2020-06-26 15:27:19.787867
last batch sz 10
Pre: time 2020-06-26 15:27:33.619649: 
 	std: 0.0031504906
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9798, 0.974, 0.9813, 0.975]
	train_accs: [0.98183334, 0.98118335, 0.97585, 0.98191667, 0.9766167]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97827995
	best: 0.9813

Starting e_i: 793
Model ind 665 epoch 793 batch: 0 avg loss -2.937258 avg loss no lamb -2.937258 time 2020-06-26 15:27:34.604533
Model ind 665 epoch 793 batch: 100 avg loss -2.898675 avg loss no lamb -2.898675 time 2020-06-26 15:27:45.540319
Model ind 665 epoch 793 batch: 200 avg loss -2.840740 avg loss no lamb -2.840740 time 2020-06-26 15:27:56.151634
Model ind 665 epoch 793 batch: 300 avg loss -2.820739 avg loss no lamb -2.820739 time 2020-06-26 15:28:06.889784
Model ind 665 epoch 793 batch: 400 avg loss -2.820729 avg loss no lamb -2.820729 time 2020-06-26 15:28:17.523685
Model ind 665 epoch 793 batch: 500 avg loss -2.807079 avg loss no lamb -2.807079 time 2020-06-26 15:28:27.999394
Model ind 665 epoch 793 batch: 600 avg loss -2.849620 avg loss no lamb -2.849620 time 2020-06-26 15:28:38.741737
Model ind 665 epoch 793 batch: 700 avg loss -2.776877 avg loss no lamb -2.776877 time 2020-06-26 15:28:49.553076
Model ind 665 epoch 793 batch: 800 avg loss -2.859550 avg loss no lamb -2.859550 time 2020-06-26 15:29:00.310491
last batch sz 10
Pre: time 2020-06-26 15:29:14.490313: 
 	std: 0.00297093
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9807, 0.9748, 0.9819, 0.9767]
	train_accs: [0.9816333, 0.98081666, 0.9759333, 0.98158336, 0.97688335]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97925997
	best: 0.9822

Starting e_i: 794
Model ind 665 epoch 794 batch: 0 avg loss -2.900226 avg loss no lamb -2.900226 time 2020-06-26 15:29:15.506913
Model ind 665 epoch 794 batch: 100 avg loss -2.832270 avg loss no lamb -2.832270 time 2020-06-26 15:29:25.988421
Model ind 665 epoch 794 batch: 200 avg loss -2.876204 avg loss no lamb -2.876204 time 2020-06-26 15:29:36.836028
Model ind 665 epoch 794 batch: 300 avg loss -2.847634 avg loss no lamb -2.847634 time 2020-06-26 15:29:47.594263
Model ind 665 epoch 794 batch: 400 avg loss -2.778584 avg loss no lamb -2.778584 time 2020-06-26 15:29:58.367437
Model ind 665 epoch 794 batch: 500 avg loss -2.751164 avg loss no lamb -2.751164 time 2020-06-26 15:30:09.155204
Model ind 665 epoch 794 batch: 600 avg loss -2.881717 avg loss no lamb -2.881717 time 2020-06-26 15:30:19.782551
Model ind 665 epoch 794 batch: 700 avg loss -2.788350 avg loss no lamb -2.788350 time 2020-06-26 15:30:30.311424
Model ind 665 epoch 794 batch: 800 avg loss -2.837665 avg loss no lamb -2.837665 time 2020-06-26 15:30:41.137970
last batch sz 10
Pre: time 2020-06-26 15:30:55.270754: 
 	std: 0.003134695
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9808, 0.9742, 0.9813, 0.9753]
	train_accs: [0.98155, 0.9810333, 0.97545, 0.9816667, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97856
	best: 0.9813

Starting e_i: 795
Model ind 665 epoch 795 batch: 0 avg loss -2.896112 avg loss no lamb -2.896112 time 2020-06-26 15:30:56.340945
Model ind 665 epoch 795 batch: 100 avg loss -2.866602 avg loss no lamb -2.866602 time 2020-06-26 15:31:07.453750
Model ind 665 epoch 795 batch: 200 avg loss -2.823951 avg loss no lamb -2.823951 time 2020-06-26 15:31:18.033695
Model ind 665 epoch 795 batch: 300 avg loss -2.854813 avg loss no lamb -2.854813 time 2020-06-26 15:31:28.784921
Model ind 665 epoch 795 batch: 400 avg loss -2.845775 avg loss no lamb -2.845775 time 2020-06-26 15:31:39.626215
Model ind 665 epoch 795 batch: 500 avg loss -2.858676 avg loss no lamb -2.858676 time 2020-06-26 15:31:50.402416
Model ind 665 epoch 795 batch: 600 avg loss -2.874244 avg loss no lamb -2.874244 time 2020-06-26 15:32:00.998092
Model ind 665 epoch 795 batch: 700 avg loss -2.713336 avg loss no lamb -2.713336 time 2020-06-26 15:32:11.722154
Model ind 665 epoch 795 batch: 800 avg loss -2.834979 avg loss no lamb -2.834979 time 2020-06-26 15:32:22.480090
last batch sz 10
Pre: time 2020-06-26 15:32:36.679314: 
 	std: 0.0027846643
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9808, 0.9753, 0.9825, 0.9778]
	train_accs: [0.9819, 0.98115, 0.97636664, 0.98195, 0.97715]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97974
	best: 0.9825

Starting e_i: 796
Model ind 665 epoch 796 batch: 0 avg loss -2.945648 avg loss no lamb -2.945648 time 2020-06-26 15:32:37.662017
Model ind 665 epoch 796 batch: 100 avg loss -2.890130 avg loss no lamb -2.890130 time 2020-06-26 15:32:48.195087
Model ind 665 epoch 796 batch: 200 avg loss -2.852623 avg loss no lamb -2.852623 time 2020-06-26 15:32:58.937541
Model ind 665 epoch 796 batch: 300 avg loss -2.832130 avg loss no lamb -2.832130 time 2020-06-26 15:33:09.506192
Model ind 665 epoch 796 batch: 400 avg loss -2.817381 avg loss no lamb -2.817381 time 2020-06-26 15:33:20.347985
Model ind 665 epoch 796 batch: 500 avg loss -2.838686 avg loss no lamb -2.838686 time 2020-06-26 15:33:31.129540
Model ind 665 epoch 796 batch: 600 avg loss -2.838506 avg loss no lamb -2.838506 time 2020-06-26 15:33:41.568202
Model ind 665 epoch 796 batch: 700 avg loss -2.731066 avg loss no lamb -2.731066 time 2020-06-26 15:33:52.586224
Model ind 665 epoch 796 batch: 800 avg loss -2.826931 avg loss no lamb -2.826931 time 2020-06-26 15:34:03.399067
last batch sz 10
Pre: time 2020-06-26 15:34:17.303763: 
 	std: 0.002660841
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9811, 0.9763, 0.9822, 0.9769]
	train_accs: [0.98191667, 0.98108333, 0.9768, 0.9817333, 0.9770833]
	best_train_sub_head: 0
	worst: 0.9763
	avg: 0.97980005
	best: 0.9825

Starting e_i: 797
Model ind 665 epoch 797 batch: 0 avg loss -2.898987 avg loss no lamb -2.898987 time 2020-06-26 15:34:18.289733
Model ind 665 epoch 797 batch: 100 avg loss -2.846158 avg loss no lamb -2.846158 time 2020-06-26 15:34:28.917112
Model ind 665 epoch 797 batch: 200 avg loss -2.813897 avg loss no lamb -2.813897 time 2020-06-26 15:34:39.523052
Model ind 665 epoch 797 batch: 300 avg loss -2.832951 avg loss no lamb -2.832951 time 2020-06-26 15:34:50.094756
Model ind 665 epoch 797 batch: 400 avg loss -2.734852 avg loss no lamb -2.734852 time 2020-06-26 15:35:00.846907
Model ind 665 epoch 797 batch: 500 avg loss -2.835176 avg loss no lamb -2.835176 time 2020-06-26 15:35:11.777763
Model ind 665 epoch 797 batch: 600 avg loss -2.848454 avg loss no lamb -2.848454 time 2020-06-26 15:35:22.546420
Model ind 665 epoch 797 batch: 700 avg loss -2.737581 avg loss no lamb -2.737581 time 2020-06-26 15:35:33.224299
Model ind 665 epoch 797 batch: 800 avg loss -2.857731 avg loss no lamb -2.857731 time 2020-06-26 15:35:44.120456
last batch sz 10
Pre: time 2020-06-26 15:35:58.412925: 
 	std: 0.0028687266
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9811, 0.9751, 0.9819, 0.9768]
	train_accs: [0.98145, 0.98113334, 0.9759333, 0.9816333, 0.9768]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97938
	best: 0.9819

Starting e_i: 798
Model ind 665 epoch 798 batch: 0 avg loss -2.936724 avg loss no lamb -2.936724 time 2020-06-26 15:35:59.443893
Model ind 665 epoch 798 batch: 100 avg loss -2.880971 avg loss no lamb -2.880971 time 2020-06-26 15:36:10.105599
Model ind 665 epoch 798 batch: 200 avg loss -2.923191 avg loss no lamb -2.923191 time 2020-06-26 15:36:20.757472
Model ind 665 epoch 798 batch: 300 avg loss -2.811969 avg loss no lamb -2.811969 time 2020-06-26 15:36:31.269970
Model ind 665 epoch 798 batch: 400 avg loss -2.739645 avg loss no lamb -2.739645 time 2020-06-26 15:36:42.133512
Model ind 665 epoch 798 batch: 500 avg loss -2.801381 avg loss no lamb -2.801381 time 2020-06-26 15:36:53.055661
Model ind 665 epoch 798 batch: 600 avg loss -2.840964 avg loss no lamb -2.840964 time 2020-06-26 15:37:03.987362
Model ind 665 epoch 798 batch: 700 avg loss -2.758428 avg loss no lamb -2.758428 time 2020-06-26 15:37:14.458173
Model ind 665 epoch 798 batch: 800 avg loss -2.820268 avg loss no lamb -2.820268 time 2020-06-26 15:37:25.246870
last batch sz 10
Pre: time 2020-06-26 15:37:39.159234: 
 	std: 0.002207635
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9799, 0.9763, 0.9817, 0.9772]
	train_accs: [0.98135, 0.98106664, 0.97606665, 0.98176664, 0.97665]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.97931993
	best: 0.9817

Starting e_i: 799
Model ind 665 epoch 799 batch: 0 avg loss -2.948134 avg loss no lamb -2.948134 time 2020-06-26 15:37:40.267648
Model ind 665 epoch 799 batch: 100 avg loss -2.868329 avg loss no lamb -2.868329 time 2020-06-26 15:37:51.224931
Model ind 665 epoch 799 batch: 200 avg loss -2.872085 avg loss no lamb -2.872085 time 2020-06-26 15:38:02.024973
Model ind 665 epoch 799 batch: 300 avg loss -2.841906 avg loss no lamb -2.841906 time 2020-06-26 15:38:12.970791
Model ind 665 epoch 799 batch: 400 avg loss -2.674568 avg loss no lamb -2.674568 time 2020-06-26 15:38:23.761034
Model ind 665 epoch 799 batch: 500 avg loss -2.869402 avg loss no lamb -2.869402 time 2020-06-26 15:38:34.543812
Model ind 665 epoch 799 batch: 600 avg loss -2.786258 avg loss no lamb -2.786258 time 2020-06-26 15:38:45.313874
Model ind 665 epoch 799 batch: 700 avg loss -2.847926 avg loss no lamb -2.847926 time 2020-06-26 15:38:56.152046
Model ind 665 epoch 799 batch: 800 avg loss -2.845535 avg loss no lamb -2.845535 time 2020-06-26 15:39:07.071166
last batch sz 10
Pre: time 2020-06-26 15:39:20.892316: 
 	std: 0.0025381977
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9804, 0.976, 0.9817, 0.9762]
	train_accs: [0.9813333, 0.98116666, 0.97601664, 0.98145, 0.97645]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.97916
	best: 0.9817

Starting e_i: 800
Model ind 665 epoch 800 batch: 0 avg loss -2.955911 avg loss no lamb -2.955911 time 2020-06-26 15:39:21.856023
Model ind 665 epoch 800 batch: 100 avg loss -2.821376 avg loss no lamb -2.821376 time 2020-06-26 15:39:32.410784
Model ind 665 epoch 800 batch: 200 avg loss -2.815131 avg loss no lamb -2.815131 time 2020-06-26 15:39:43.238006
Model ind 665 epoch 800 batch: 300 avg loss -2.844995 avg loss no lamb -2.844995 time 2020-06-26 15:39:54.145652
Model ind 665 epoch 800 batch: 400 avg loss -2.758624 avg loss no lamb -2.758624 time 2020-06-26 15:40:04.899329
Model ind 665 epoch 800 batch: 500 avg loss -2.867008 avg loss no lamb -2.867008 time 2020-06-26 15:40:15.646350
Model ind 665 epoch 800 batch: 600 avg loss -2.913745 avg loss no lamb -2.913745 time 2020-06-26 15:40:26.428660
Model ind 665 epoch 800 batch: 700 avg loss -2.767148 avg loss no lamb -2.767148 time 2020-06-26 15:40:37.156464
Model ind 665 epoch 800 batch: 800 avg loss -2.789480 avg loss no lamb -2.789480 time 2020-06-26 15:40:47.907329
last batch sz 10
Pre: time 2020-06-26 15:41:02.120936: 
 	std: 0.0031870983
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9799, 0.9737, 0.9812, 0.975]
	train_accs: [0.98146665, 0.98083335, 0.97455, 0.9816167, 0.97463334]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97818005
	best: 0.9812

Starting e_i: 801
Model ind 665 epoch 801 batch: 0 avg loss -2.891969 avg loss no lamb -2.891969 time 2020-06-26 15:41:04.388605
Model ind 665 epoch 801 batch: 100 avg loss -2.876305 avg loss no lamb -2.876305 time 2020-06-26 15:41:15.030586
Model ind 665 epoch 801 batch: 200 avg loss -2.821677 avg loss no lamb -2.821677 time 2020-06-26 15:41:25.722927
Model ind 665 epoch 801 batch: 300 avg loss -2.885840 avg loss no lamb -2.885840 time 2020-06-26 15:41:36.554025
Model ind 665 epoch 801 batch: 400 avg loss -2.803118 avg loss no lamb -2.803118 time 2020-06-26 15:41:47.460267
Model ind 665 epoch 801 batch: 500 avg loss -2.815294 avg loss no lamb -2.815294 time 2020-06-26 15:41:58.061150
Model ind 665 epoch 801 batch: 600 avg loss -2.883581 avg loss no lamb -2.883581 time 2020-06-26 15:42:09.042917
Model ind 665 epoch 801 batch: 700 avg loss -2.719264 avg loss no lamb -2.719264 time 2020-06-26 15:42:19.895926
Model ind 665 epoch 801 batch: 800 avg loss -2.832917 avg loss no lamb -2.832917 time 2020-06-26 15:42:30.774947
last batch sz 10
Pre: time 2020-06-26 15:42:44.722449: 
 	std: 0.0023694676
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9805, 0.9763, 0.9819, 0.9774]
	train_accs: [0.9819, 0.9812, 0.97641665, 0.9821333, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.97964
	best: 0.9819

Starting e_i: 802
Model ind 665 epoch 802 batch: 0 avg loss -2.987647 avg loss no lamb -2.987647 time 2020-06-26 15:42:45.725764
Model ind 665 epoch 802 batch: 100 avg loss -2.914720 avg loss no lamb -2.914720 time 2020-06-26 15:42:56.488895
Model ind 665 epoch 802 batch: 200 avg loss -2.888393 avg loss no lamb -2.888393 time 2020-06-26 15:43:07.441768
Model ind 665 epoch 802 batch: 300 avg loss -2.869507 avg loss no lamb -2.869507 time 2020-06-26 15:43:18.168007
Model ind 665 epoch 802 batch: 400 avg loss -2.823079 avg loss no lamb -2.823079 time 2020-06-26 15:43:28.859625
Model ind 665 epoch 802 batch: 500 avg loss -2.830071 avg loss no lamb -2.830071 time 2020-06-26 15:43:39.631051
Model ind 665 epoch 802 batch: 600 avg loss -2.820597 avg loss no lamb -2.820597 time 2020-06-26 15:43:50.384761
Model ind 665 epoch 802 batch: 700 avg loss -2.822583 avg loss no lamb -2.822583 time 2020-06-26 15:44:01.289569
Model ind 665 epoch 802 batch: 800 avg loss -2.833192 avg loss no lamb -2.833192 time 2020-06-26 15:44:11.848905
last batch sz 10
Pre: time 2020-06-26 15:44:25.907211: 
 	std: 0.0028071273
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9805, 0.9753, 0.9821, 0.9764]
	train_accs: [0.98193336, 0.9810333, 0.9759167, 0.9821, 0.97716665]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.9792
	best: 0.9821

Starting e_i: 803
Model ind 665 epoch 803 batch: 0 avg loss -2.964186 avg loss no lamb -2.964186 time 2020-06-26 15:44:27.028729
Model ind 665 epoch 803 batch: 100 avg loss -2.898687 avg loss no lamb -2.898687 time 2020-06-26 15:44:37.702193
Model ind 665 epoch 803 batch: 200 avg loss -2.854713 avg loss no lamb -2.854713 time 2020-06-26 15:44:48.397702
Model ind 665 epoch 803 batch: 300 avg loss -2.807873 avg loss no lamb -2.807873 time 2020-06-26 15:44:59.091839
Model ind 665 epoch 803 batch: 400 avg loss -2.779871 avg loss no lamb -2.779871 time 2020-06-26 15:45:09.986981
Model ind 665 epoch 803 batch: 500 avg loss -2.852355 avg loss no lamb -2.852355 time 2020-06-26 15:45:20.696278
Model ind 665 epoch 803 batch: 600 avg loss -2.841845 avg loss no lamb -2.841845 time 2020-06-26 15:45:31.576100
Model ind 665 epoch 803 batch: 700 avg loss -2.769418 avg loss no lamb -2.769418 time 2020-06-26 15:45:42.273773
Model ind 665 epoch 803 batch: 800 avg loss -2.855907 avg loss no lamb -2.855907 time 2020-06-26 15:45:53.208222
last batch sz 10
Pre: time 2020-06-26 15:46:07.125507: 
 	std: 0.002787396
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9804, 0.9752, 0.9816, 0.9764]
	train_accs: [0.98211664, 0.9810167, 0.9762833, 0.98191667, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97911996
	best: 0.982

Starting e_i: 804
Model ind 665 epoch 804 batch: 0 avg loss -2.934014 avg loss no lamb -2.934014 time 2020-06-26 15:46:08.105141
Model ind 665 epoch 804 batch: 100 avg loss -2.897536 avg loss no lamb -2.897536 time 2020-06-26 15:46:18.922406
Model ind 665 epoch 804 batch: 200 avg loss -2.891012 avg loss no lamb -2.891012 time 2020-06-26 15:46:29.624305
Model ind 665 epoch 804 batch: 300 avg loss -2.907299 avg loss no lamb -2.907299 time 2020-06-26 15:46:40.179477
Model ind 665 epoch 804 batch: 400 avg loss -2.789834 avg loss no lamb -2.789834 time 2020-06-26 15:46:50.842827
Model ind 665 epoch 804 batch: 500 avg loss -2.835920 avg loss no lamb -2.835920 time 2020-06-26 15:47:01.739757
Model ind 665 epoch 804 batch: 600 avg loss -2.821923 avg loss no lamb -2.821923 time 2020-06-26 15:47:12.438314
Model ind 665 epoch 804 batch: 700 avg loss -2.709887 avg loss no lamb -2.709887 time 2020-06-26 15:47:23.096100
Model ind 665 epoch 804 batch: 800 avg loss -2.860210 avg loss no lamb -2.860210 time 2020-06-26 15:47:33.742336
last batch sz 10
Pre: time 2020-06-26 15:47:47.602518: 
 	std: 0.00224357
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9796, 0.975, 0.98, 0.9758]
	train_accs: [0.9818, 0.98085, 0.97616667, 0.98165, 0.9765667]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97812
	best: 0.9802

Starting e_i: 805
Model ind 665 epoch 805 batch: 0 avg loss -2.988319 avg loss no lamb -2.988319 time 2020-06-26 15:47:48.800399
Model ind 665 epoch 805 batch: 100 avg loss -2.867013 avg loss no lamb -2.867013 time 2020-06-26 15:47:59.559690
Model ind 665 epoch 805 batch: 200 avg loss -2.861376 avg loss no lamb -2.861376 time 2020-06-26 15:48:10.364879
Model ind 665 epoch 805 batch: 300 avg loss -2.914215 avg loss no lamb -2.914215 time 2020-06-26 15:48:21.256775
Model ind 665 epoch 805 batch: 400 avg loss -2.772576 avg loss no lamb -2.772576 time 2020-06-26 15:48:32.081689
Model ind 665 epoch 805 batch: 500 avg loss -2.824860 avg loss no lamb -2.824860 time 2020-06-26 15:48:42.775618
Model ind 665 epoch 805 batch: 600 avg loss -2.877548 avg loss no lamb -2.877548 time 2020-06-26 15:48:53.463165
Model ind 665 epoch 805 batch: 700 avg loss -2.725099 avg loss no lamb -2.725099 time 2020-06-26 15:49:04.470824
Model ind 665 epoch 805 batch: 800 avg loss -2.832586 avg loss no lamb -2.832586 time 2020-06-26 15:49:15.240239
last batch sz 10
Pre: time 2020-06-26 15:49:29.048372: 
 	std: 0.0031243614
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9801, 0.9745, 0.9813, 0.9754]
	train_accs: [0.98188335, 0.98118335, 0.97606665, 0.98188335, 0.9769]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97867996
	best: 0.9821

Starting e_i: 806
Model ind 665 epoch 806 batch: 0 avg loss -2.930468 avg loss no lamb -2.930468 time 2020-06-26 15:49:30.040096
Model ind 665 epoch 806 batch: 100 avg loss -2.893962 avg loss no lamb -2.893962 time 2020-06-26 15:49:40.642557
Model ind 665 epoch 806 batch: 200 avg loss -2.880639 avg loss no lamb -2.880639 time 2020-06-26 15:49:51.351455
Model ind 665 epoch 806 batch: 300 avg loss -2.883938 avg loss no lamb -2.883938 time 2020-06-26 15:50:02.180067
Model ind 665 epoch 806 batch: 400 avg loss -2.729522 avg loss no lamb -2.729522 time 2020-06-26 15:50:12.964480
Model ind 665 epoch 806 batch: 500 avg loss -2.834565 avg loss no lamb -2.834565 time 2020-06-26 15:50:23.776206
Model ind 665 epoch 806 batch: 600 avg loss -2.824722 avg loss no lamb -2.824722 time 2020-06-26 15:50:34.678111
Model ind 665 epoch 806 batch: 700 avg loss -2.753787 avg loss no lamb -2.753787 time 2020-06-26 15:50:45.490960
Model ind 665 epoch 806 batch: 800 avg loss -2.838871 avg loss no lamb -2.838871 time 2020-06-26 15:50:56.410165
last batch sz 10
Pre: time 2020-06-26 15:51:10.535048: 
 	std: 0.0031625319
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9803, 0.9738, 0.9815, 0.9757]
	train_accs: [0.98151666, 0.9806833, 0.9755167, 0.9816167, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97852004
	best: 0.9815

Starting e_i: 807
Model ind 665 epoch 807 batch: 0 avg loss -2.944913 avg loss no lamb -2.944913 time 2020-06-26 15:51:11.610307
Model ind 665 epoch 807 batch: 100 avg loss -2.839296 avg loss no lamb -2.839296 time 2020-06-26 15:51:22.156728
Model ind 665 epoch 807 batch: 200 avg loss -2.883728 avg loss no lamb -2.883728 time 2020-06-26 15:51:32.790183
Model ind 665 epoch 807 batch: 300 avg loss -2.791554 avg loss no lamb -2.791554 time 2020-06-26 15:51:43.455141
Model ind 665 epoch 807 batch: 400 avg loss -2.714363 avg loss no lamb -2.714363 time 2020-06-26 15:51:54.080174
Model ind 665 epoch 807 batch: 500 avg loss -2.801550 avg loss no lamb -2.801550 time 2020-06-26 15:52:05.122094
Model ind 665 epoch 807 batch: 600 avg loss -2.868350 avg loss no lamb -2.868350 time 2020-06-26 15:52:16.185885
Model ind 665 epoch 807 batch: 700 avg loss -2.742881 avg loss no lamb -2.742881 time 2020-06-26 15:52:26.908407
Model ind 665 epoch 807 batch: 800 avg loss -2.863151 avg loss no lamb -2.863151 time 2020-06-26 15:52:37.627560
last batch sz 10
Pre: time 2020-06-26 15:52:51.855215: 
 	std: 0.0026401465
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9802, 0.9753, 0.9822, 0.9782]
	train_accs: [0.9816833, 0.98043334, 0.97545, 0.9816167, 0.97711664]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97964
	best: 0.9823

Starting e_i: 808
Model ind 665 epoch 808 batch: 0 avg loss -2.908630 avg loss no lamb -2.908630 time 2020-06-26 15:52:52.878016
Model ind 665 epoch 808 batch: 100 avg loss -2.838999 avg loss no lamb -2.838999 time 2020-06-26 15:53:03.730589
Model ind 665 epoch 808 batch: 200 avg loss -2.866643 avg loss no lamb -2.866643 time 2020-06-26 15:53:14.458827
Model ind 665 epoch 808 batch: 300 avg loss -2.788519 avg loss no lamb -2.788519 time 2020-06-26 15:53:25.177478
Model ind 665 epoch 808 batch: 400 avg loss -2.757841 avg loss no lamb -2.757841 time 2020-06-26 15:53:35.885780
Model ind 665 epoch 808 batch: 500 avg loss -2.805570 avg loss no lamb -2.805570 time 2020-06-26 15:53:46.759725
Model ind 665 epoch 808 batch: 600 avg loss -2.836208 avg loss no lamb -2.836208 time 2020-06-26 15:53:57.553402
Model ind 665 epoch 808 batch: 700 avg loss -2.753388 avg loss no lamb -2.753388 time 2020-06-26 15:54:08.229171
Model ind 665 epoch 808 batch: 800 avg loss -2.861643 avg loss no lamb -2.861643 time 2020-06-26 15:54:18.948143
last batch sz 10
Pre: time 2020-06-26 15:54:32.951178: 
 	std: 0.002705241
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9793, 0.9742, 0.9812, 0.9768]
	train_accs: [0.9811, 0.97985, 0.97475, 0.9813, 0.9763833]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97854006
	best: 0.9812

Starting e_i: 809
Model ind 665 epoch 809 batch: 0 avg loss -2.955880 avg loss no lamb -2.955880 time 2020-06-26 15:54:34.089002
Model ind 665 epoch 809 batch: 100 avg loss -2.929598 avg loss no lamb -2.929598 time 2020-06-26 15:54:44.761961
Model ind 665 epoch 809 batch: 200 avg loss -2.883569 avg loss no lamb -2.883569 time 2020-06-26 15:54:55.282861
Model ind 665 epoch 809 batch: 300 avg loss -2.857837 avg loss no lamb -2.857837 time 2020-06-26 15:55:05.796560
Model ind 665 epoch 809 batch: 400 avg loss -2.745853 avg loss no lamb -2.745853 time 2020-06-26 15:55:16.630490
Model ind 665 epoch 809 batch: 500 avg loss -2.842403 avg loss no lamb -2.842403 time 2020-06-26 15:55:27.584983
Model ind 665 epoch 809 batch: 600 avg loss -2.816361 avg loss no lamb -2.816361 time 2020-06-26 15:55:38.425200
Model ind 665 epoch 809 batch: 700 avg loss -2.740976 avg loss no lamb -2.740976 time 2020-06-26 15:55:49.176502
Model ind 665 epoch 809 batch: 800 avg loss -2.842473 avg loss no lamb -2.842473 time 2020-06-26 15:55:59.790894
last batch sz 10
Pre: time 2020-06-26 15:56:13.860994: 
 	std: 0.0029382976
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9816, 0.976, 0.983, 0.9772]
	train_accs: [0.9816833, 0.9809, 0.97581667, 0.9816667, 0.9766167]
	best_train_sub_head: 0
	worst: 0.976
	avg: 0.98012
	best: 0.9828

Starting e_i: 810
Model ind 665 epoch 810 batch: 0 avg loss -2.951959 avg loss no lamb -2.951959 time 2020-06-26 15:56:14.877956
Model ind 665 epoch 810 batch: 100 avg loss -2.877970 avg loss no lamb -2.877970 time 2020-06-26 15:56:25.357089
Model ind 665 epoch 810 batch: 200 avg loss -2.842413 avg loss no lamb -2.842413 time 2020-06-26 15:56:35.982752
Model ind 665 epoch 810 batch: 300 avg loss -2.799958 avg loss no lamb -2.799958 time 2020-06-26 15:56:46.792513
Model ind 665 epoch 810 batch: 400 avg loss -2.810218 avg loss no lamb -2.810218 time 2020-06-26 15:56:57.549908
Model ind 665 epoch 810 batch: 500 avg loss -2.836481 avg loss no lamb -2.836481 time 2020-06-26 15:57:08.266655
Model ind 665 epoch 810 batch: 600 avg loss -2.847763 avg loss no lamb -2.847763 time 2020-06-26 15:57:18.860799
Model ind 665 epoch 810 batch: 700 avg loss -2.753150 avg loss no lamb -2.753150 time 2020-06-26 15:57:29.864731
Model ind 665 epoch 810 batch: 800 avg loss -2.844430 avg loss no lamb -2.844430 time 2020-06-26 15:57:40.721561
last batch sz 10
Pre: time 2020-06-26 15:57:54.633234: 
 	std: 0.0030650338
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9818, 0.9751, 0.9826, 0.9771]
	train_accs: [0.98148334, 0.9811, 0.97495, 0.98151666, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97976
	best: 0.9826

Starting e_i: 811
Model ind 665 epoch 811 batch: 0 avg loss -2.903443 avg loss no lamb -2.903443 time 2020-06-26 15:57:56.917260
Model ind 665 epoch 811 batch: 100 avg loss -2.890331 avg loss no lamb -2.890331 time 2020-06-26 15:58:07.652327
Model ind 665 epoch 811 batch: 200 avg loss -2.863055 avg loss no lamb -2.863055 time 2020-06-26 15:58:18.500767
Model ind 665 epoch 811 batch: 300 avg loss -2.826107 avg loss no lamb -2.826107 time 2020-06-26 15:58:29.242928
Model ind 665 epoch 811 batch: 400 avg loss -2.781936 avg loss no lamb -2.781936 time 2020-06-26 15:58:40.266271
Model ind 665 epoch 811 batch: 500 avg loss -2.801149 avg loss no lamb -2.801149 time 2020-06-26 15:58:50.956683
Model ind 665 epoch 811 batch: 600 avg loss -2.808321 avg loss no lamb -2.808321 time 2020-06-26 15:59:01.755367
Model ind 665 epoch 811 batch: 700 avg loss -2.726224 avg loss no lamb -2.726224 time 2020-06-26 15:59:12.669609
Model ind 665 epoch 811 batch: 800 avg loss -2.853292 avg loss no lamb -2.853292 time 2020-06-26 15:59:23.308341
last batch sz 10
Pre: time 2020-06-26 15:59:37.151060: 
 	std: 0.0028931699
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9823, 0.9763, 0.9832, 0.9776]
	train_accs: [0.9820333, 0.98165, 0.9763333, 0.98185, 0.97685]
	best_train_sub_head: 0
	worst: 0.9763
	avg: 0.98044
	best: 0.9828

Starting e_i: 812
Model ind 665 epoch 812 batch: 0 avg loss -2.976042 avg loss no lamb -2.976042 time 2020-06-26 15:59:38.135522
Model ind 665 epoch 812 batch: 100 avg loss -2.882709 avg loss no lamb -2.882709 time 2020-06-26 15:59:48.992647
Model ind 665 epoch 812 batch: 200 avg loss -2.878721 avg loss no lamb -2.878721 time 2020-06-26 15:59:59.570409
Model ind 665 epoch 812 batch: 300 avg loss -2.862466 avg loss no lamb -2.862466 time 2020-06-26 16:00:10.275675
Model ind 665 epoch 812 batch: 400 avg loss -2.758973 avg loss no lamb -2.758973 time 2020-06-26 16:00:20.982411
Model ind 665 epoch 812 batch: 500 avg loss -2.846893 avg loss no lamb -2.846893 time 2020-06-26 16:00:31.631012
Model ind 665 epoch 812 batch: 600 avg loss -2.908407 avg loss no lamb -2.908407 time 2020-06-26 16:00:42.447140
Model ind 665 epoch 812 batch: 700 avg loss -2.661162 avg loss no lamb -2.661162 time 2020-06-26 16:00:53.232842
Model ind 665 epoch 812 batch: 800 avg loss -2.800453 avg loss no lamb -2.800453 time 2020-06-26 16:01:04.051395
last batch sz 10
Pre: time 2020-06-26 16:01:17.954029: 
 	std: 0.0026993284
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9815, 0.9761, 0.9835, 0.9793]
	train_accs: [0.9820833, 0.98143333, 0.97573334, 0.98216665, 0.97748333]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.9806601
	best: 0.9835

Starting e_i: 813
Model ind 665 epoch 813 batch: 0 avg loss -2.940185 avg loss no lamb -2.940185 time 2020-06-26 16:01:19.092470
Model ind 665 epoch 813 batch: 100 avg loss -2.879201 avg loss no lamb -2.879201 time 2020-06-26 16:01:29.753439
Model ind 665 epoch 813 batch: 200 avg loss -2.841456 avg loss no lamb -2.841456 time 2020-06-26 16:01:40.440888
Model ind 665 epoch 813 batch: 300 avg loss -2.858381 avg loss no lamb -2.858381 time 2020-06-26 16:01:51.192822
Model ind 665 epoch 813 batch: 400 avg loss -2.726508 avg loss no lamb -2.726508 time 2020-06-26 16:02:01.813670
Model ind 665 epoch 813 batch: 500 avg loss -2.810669 avg loss no lamb -2.810669 time 2020-06-26 16:02:12.646735
Model ind 665 epoch 813 batch: 600 avg loss -2.862239 avg loss no lamb -2.862239 time 2020-06-26 16:02:23.493369
Model ind 665 epoch 813 batch: 700 avg loss -2.702777 avg loss no lamb -2.702777 time 2020-06-26 16:02:34.203301
Model ind 665 epoch 813 batch: 800 avg loss -2.808054 avg loss no lamb -2.808054 time 2020-06-26 16:02:44.891937
last batch sz 10
Pre: time 2020-06-26 16:02:58.505163: 
 	std: 0.0031044488
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9799, 0.975, 0.9824, 0.9759]
	train_accs: [0.9813667, 0.98081666, 0.97531664, 0.9815, 0.9762]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97908
	best: 0.9824

Starting e_i: 814
Model ind 665 epoch 814 batch: 0 avg loss -2.949651 avg loss no lamb -2.949651 time 2020-06-26 16:02:59.501106
Model ind 665 epoch 814 batch: 100 avg loss -2.927557 avg loss no lamb -2.927557 time 2020-06-26 16:03:10.461998
Model ind 665 epoch 814 batch: 200 avg loss -2.848408 avg loss no lamb -2.848408 time 2020-06-26 16:03:21.190561
Model ind 665 epoch 814 batch: 300 avg loss -2.814256 avg loss no lamb -2.814256 time 2020-06-26 16:03:31.871508
Model ind 665 epoch 814 batch: 400 avg loss -2.824968 avg loss no lamb -2.824968 time 2020-06-26 16:03:42.645508
Model ind 665 epoch 814 batch: 500 avg loss -2.865499 avg loss no lamb -2.865499 time 2020-06-26 16:03:53.757514
Model ind 665 epoch 814 batch: 600 avg loss -2.907707 avg loss no lamb -2.907707 time 2020-06-26 16:04:04.734693
Model ind 665 epoch 814 batch: 700 avg loss -2.710242 avg loss no lamb -2.710242 time 2020-06-26 16:04:15.446283
Model ind 665 epoch 814 batch: 800 avg loss -2.836578 avg loss no lamb -2.836578 time 2020-06-26 16:04:25.978033
last batch sz 10
Pre: time 2020-06-26 16:04:39.764319: 
 	std: 0.0030002613
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9811, 0.9753, 0.9816, 0.9761]
	train_accs: [0.98175, 0.98118335, 0.9759667, 0.9816667, 0.9766]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97932005
	best: 0.9825

Starting e_i: 815
Model ind 665 epoch 815 batch: 0 avg loss -2.896972 avg loss no lamb -2.896972 time 2020-06-26 16:04:40.898595
Model ind 665 epoch 815 batch: 100 avg loss -2.908511 avg loss no lamb -2.908511 time 2020-06-26 16:04:51.710528
Model ind 665 epoch 815 batch: 200 avg loss -2.817037 avg loss no lamb -2.817037 time 2020-06-26 16:05:02.558877
Model ind 665 epoch 815 batch: 300 avg loss -2.891474 avg loss no lamb -2.891474 time 2020-06-26 16:05:13.227104
Model ind 665 epoch 815 batch: 400 avg loss -2.764694 avg loss no lamb -2.764694 time 2020-06-26 16:05:23.822615
Model ind 665 epoch 815 batch: 500 avg loss -2.794450 avg loss no lamb -2.794450 time 2020-06-26 16:05:34.730768
Model ind 665 epoch 815 batch: 600 avg loss -2.867468 avg loss no lamb -2.867468 time 2020-06-26 16:05:45.752169
Model ind 665 epoch 815 batch: 700 avg loss -2.763156 avg loss no lamb -2.763156 time 2020-06-26 16:05:56.566198
Model ind 665 epoch 815 batch: 800 avg loss -2.891041 avg loss no lamb -2.891041 time 2020-06-26 16:06:07.272345
last batch sz 10
Pre: time 2020-06-26 16:06:21.207384: 
 	std: 0.0028082666
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9811, 0.9756, 0.9819, 0.9763]
	train_accs: [0.98158336, 0.98075, 0.97515, 0.9813167, 0.97585]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97936
	best: 0.9819

Starting e_i: 816
Model ind 665 epoch 816 batch: 0 avg loss -2.959597 avg loss no lamb -2.959597 time 2020-06-26 16:06:22.203077
Model ind 665 epoch 816 batch: 100 avg loss -2.895988 avg loss no lamb -2.895988 time 2020-06-26 16:06:32.994836
Model ind 665 epoch 816 batch: 200 avg loss -2.871805 avg loss no lamb -2.871805 time 2020-06-26 16:06:43.713228
Model ind 665 epoch 816 batch: 300 avg loss -2.910667 avg loss no lamb -2.910667 time 2020-06-26 16:06:54.348930
Model ind 665 epoch 816 batch: 400 avg loss -2.753780 avg loss no lamb -2.753780 time 2020-06-26 16:07:05.045314
Model ind 665 epoch 816 batch: 500 avg loss -2.747748 avg loss no lamb -2.747748 time 2020-06-26 16:07:15.823539
Model ind 665 epoch 816 batch: 600 avg loss -2.871614 avg loss no lamb -2.871614 time 2020-06-26 16:07:26.607432
Model ind 665 epoch 816 batch: 700 avg loss -2.792800 avg loss no lamb -2.792800 time 2020-06-26 16:07:37.343889
Model ind 665 epoch 816 batch: 800 avg loss -2.834424 avg loss no lamb -2.834424 time 2020-06-26 16:07:48.253990
last batch sz 10
Pre: time 2020-06-26 16:08:02.388934: 
 	std: 0.0031981233
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9794, 0.974, 0.9819, 0.9761]
	train_accs: [0.98165, 0.9806333, 0.97573334, 0.9817167, 0.9763167]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.9787
	best: 0.9819

Starting e_i: 817
Model ind 665 epoch 817 batch: 0 avg loss -2.909538 avg loss no lamb -2.909538 time 2020-06-26 16:08:03.536110
Model ind 665 epoch 817 batch: 100 avg loss -2.910473 avg loss no lamb -2.910473 time 2020-06-26 16:08:14.313706
Model ind 665 epoch 817 batch: 200 avg loss -2.866652 avg loss no lamb -2.866652 time 2020-06-26 16:08:24.978765
Model ind 665 epoch 817 batch: 300 avg loss -2.853737 avg loss no lamb -2.853737 time 2020-06-26 16:08:35.685392
Model ind 665 epoch 817 batch: 400 avg loss -2.742305 avg loss no lamb -2.742305 time 2020-06-26 16:08:46.305008
Model ind 665 epoch 817 batch: 500 avg loss -2.851629 avg loss no lamb -2.851629 time 2020-06-26 16:08:56.960215
Model ind 665 epoch 817 batch: 600 avg loss -2.862534 avg loss no lamb -2.862534 time 2020-06-26 16:09:07.882227
Model ind 665 epoch 817 batch: 700 avg loss -2.750998 avg loss no lamb -2.750998 time 2020-06-26 16:09:18.762578
Model ind 665 epoch 817 batch: 800 avg loss -2.896477 avg loss no lamb -2.896477 time 2020-06-26 16:09:29.537252
last batch sz 10
Pre: time 2020-06-26 16:09:43.362209: 
 	std: 0.002800426
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9808, 0.975, 0.9821, 0.9777]
	train_accs: [0.9816833, 0.98135, 0.97578335, 0.98175, 0.97695]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.9795601
	best: 0.9821

Starting e_i: 818
Model ind 665 epoch 818 batch: 0 avg loss -2.910806 avg loss no lamb -2.910806 time 2020-06-26 16:09:44.367906
Model ind 665 epoch 818 batch: 100 avg loss -2.899833 avg loss no lamb -2.899833 time 2020-06-26 16:09:55.042487
Model ind 665 epoch 818 batch: 200 avg loss -2.888970 avg loss no lamb -2.888970 time 2020-06-26 16:10:05.853142
Model ind 665 epoch 818 batch: 300 avg loss -2.821738 avg loss no lamb -2.821738 time 2020-06-26 16:10:16.852114
Model ind 665 epoch 818 batch: 400 avg loss -2.702310 avg loss no lamb -2.702310 time 2020-06-26 16:10:27.868473
Model ind 665 epoch 818 batch: 500 avg loss -2.770682 avg loss no lamb -2.770682 time 2020-06-26 16:10:38.746689
Model ind 665 epoch 818 batch: 600 avg loss -2.891062 avg loss no lamb -2.891062 time 2020-06-26 16:10:49.428481
Model ind 665 epoch 818 batch: 700 avg loss -2.751931 avg loss no lamb -2.751931 time 2020-06-26 16:11:00.359612
Model ind 665 epoch 818 batch: 800 avg loss -2.906432 avg loss no lamb -2.906432 time 2020-06-26 16:11:11.271108
last batch sz 10
Pre: time 2020-06-26 16:11:25.281995: 
 	std: 0.0033968154
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.981, 0.9739, 0.9818, 0.9756]
	train_accs: [0.9817, 0.98118335, 0.97538334, 0.98153335, 0.9761]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97884005
	best: 0.9819

Starting e_i: 819
Model ind 665 epoch 819 batch: 0 avg loss -2.926342 avg loss no lamb -2.926342 time 2020-06-26 16:11:26.428798
Model ind 665 epoch 819 batch: 100 avg loss -2.835804 avg loss no lamb -2.835804 time 2020-06-26 16:11:37.291431
Model ind 665 epoch 819 batch: 200 avg loss -2.814931 avg loss no lamb -2.814931 time 2020-06-26 16:11:47.975640
Model ind 665 epoch 819 batch: 300 avg loss -2.902197 avg loss no lamb -2.902197 time 2020-06-26 16:11:58.787520
Model ind 665 epoch 819 batch: 400 avg loss -2.806000 avg loss no lamb -2.806000 time 2020-06-26 16:12:09.777113
Model ind 665 epoch 819 batch: 500 avg loss -2.808861 avg loss no lamb -2.808861 time 2020-06-26 16:12:20.573189
Model ind 665 epoch 819 batch: 600 avg loss -2.837740 avg loss no lamb -2.837740 time 2020-06-26 16:12:31.319802
Model ind 665 epoch 819 batch: 700 avg loss -2.735579 avg loss no lamb -2.735579 time 2020-06-26 16:12:42.106446
Model ind 665 epoch 819 batch: 800 avg loss -2.874834 avg loss no lamb -2.874834 time 2020-06-26 16:12:52.994806
last batch sz 10
Pre: time 2020-06-26 16:13:07.116699: 
 	std: 0.0025570362
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9812, 0.9754, 0.9813, 0.9767]
	train_accs: [0.9815, 0.98111665, 0.97555, 0.98165, 0.97655]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97914
	best: 0.9813

Starting e_i: 820
Model ind 665 epoch 820 batch: 0 avg loss -2.907022 avg loss no lamb -2.907022 time 2020-06-26 16:13:08.094290
Model ind 665 epoch 820 batch: 100 avg loss -2.877121 avg loss no lamb -2.877121 time 2020-06-26 16:13:18.833712
Model ind 665 epoch 820 batch: 200 avg loss -2.835486 avg loss no lamb -2.835486 time 2020-06-26 16:13:29.258482
Model ind 665 epoch 820 batch: 300 avg loss -2.859935 avg loss no lamb -2.859935 time 2020-06-26 16:13:40.163026
Model ind 665 epoch 820 batch: 400 avg loss -2.792303 avg loss no lamb -2.792303 time 2020-06-26 16:13:51.229680
Model ind 665 epoch 820 batch: 500 avg loss -2.858261 avg loss no lamb -2.858261 time 2020-06-26 16:14:02.024847
Model ind 665 epoch 820 batch: 600 avg loss -2.808697 avg loss no lamb -2.808697 time 2020-06-26 16:14:12.759768
Model ind 665 epoch 820 batch: 700 avg loss -2.667704 avg loss no lamb -2.667704 time 2020-06-26 16:14:23.246188
Model ind 665 epoch 820 batch: 800 avg loss -2.793248 avg loss no lamb -2.793248 time 2020-06-26 16:14:33.993223
last batch sz 10
Pre: time 2020-06-26 16:14:47.799165: 
 	std: 0.0027652166
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9788, 0.9736, 0.9802, 0.9749]
	train_accs: [0.9810333, 0.9799833, 0.97508335, 0.98113334, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97754
	best: 0.9802

Starting e_i: 821
Model ind 665 epoch 821 batch: 0 avg loss -2.913233 avg loss no lamb -2.913233 time 2020-06-26 16:14:50.113842
Model ind 665 epoch 821 batch: 100 avg loss -2.890960 avg loss no lamb -2.890960 time 2020-06-26 16:15:00.787037
Model ind 665 epoch 821 batch: 200 avg loss -2.872143 avg loss no lamb -2.872143 time 2020-06-26 16:15:11.397136
Model ind 665 epoch 821 batch: 300 avg loss -2.849245 avg loss no lamb -2.849245 time 2020-06-26 16:15:22.030820
Model ind 665 epoch 821 batch: 400 avg loss -2.745926 avg loss no lamb -2.745926 time 2020-06-26 16:15:32.752951
Model ind 665 epoch 821 batch: 500 avg loss -2.837801 avg loss no lamb -2.837801 time 2020-06-26 16:15:43.742182
Model ind 665 epoch 821 batch: 600 avg loss -2.846494 avg loss no lamb -2.846494 time 2020-06-26 16:15:54.238638
Model ind 665 epoch 821 batch: 700 avg loss -2.715107 avg loss no lamb -2.715107 time 2020-06-26 16:16:05.177601
Model ind 665 epoch 821 batch: 800 avg loss -2.829963 avg loss no lamb -2.829963 time 2020-06-26 16:16:15.923019
last batch sz 10
Pre: time 2020-06-26 16:16:29.865788: 
 	std: 0.0023169029
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9796, 0.9752, 0.9806, 0.9758]
	train_accs: [0.9811, 0.9809167, 0.9762667, 0.98123336, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.9783
	best: 0.9806

Starting e_i: 822
Model ind 665 epoch 822 batch: 0 avg loss -2.922607 avg loss no lamb -2.922607 time 2020-06-26 16:16:30.860832
Model ind 665 epoch 822 batch: 100 avg loss -2.895782 avg loss no lamb -2.895782 time 2020-06-26 16:16:41.893475
Model ind 665 epoch 822 batch: 200 avg loss -2.894748 avg loss no lamb -2.894748 time 2020-06-26 16:16:52.578129
Model ind 665 epoch 822 batch: 300 avg loss -2.886826 avg loss no lamb -2.886826 time 2020-06-26 16:17:03.444094
Model ind 665 epoch 822 batch: 400 avg loss -2.778873 avg loss no lamb -2.778873 time 2020-06-26 16:17:14.452651
Model ind 665 epoch 822 batch: 500 avg loss -2.816775 avg loss no lamb -2.816775 time 2020-06-26 16:17:25.268569
Model ind 665 epoch 822 batch: 600 avg loss -2.820424 avg loss no lamb -2.820424 time 2020-06-26 16:17:36.011775
Model ind 665 epoch 822 batch: 700 avg loss -2.738996 avg loss no lamb -2.738996 time 2020-06-26 16:17:46.675492
Model ind 665 epoch 822 batch: 800 avg loss -2.824060 avg loss no lamb -2.824060 time 2020-06-26 16:17:57.483716
last batch sz 10
Pre: time 2020-06-26 16:18:11.600450: 
 	std: 0.0027111587
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.982, 0.9752, 0.9816, 0.9778]
	train_accs: [0.98175, 0.98146665, 0.9759833, 0.9818, 0.9772]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97966003
	best: 0.9816

Starting e_i: 823
Model ind 665 epoch 823 batch: 0 avg loss -2.887264 avg loss no lamb -2.887264 time 2020-06-26 16:18:12.671075
Model ind 665 epoch 823 batch: 100 avg loss -2.907918 avg loss no lamb -2.907918 time 2020-06-26 16:18:23.335807
Model ind 665 epoch 823 batch: 200 avg loss -2.870573 avg loss no lamb -2.870573 time 2020-06-26 16:18:34.042686
Model ind 665 epoch 823 batch: 300 avg loss -2.867595 avg loss no lamb -2.867595 time 2020-06-26 16:18:44.822820
Model ind 665 epoch 823 batch: 400 avg loss -2.762901 avg loss no lamb -2.762901 time 2020-06-26 16:18:55.530058
Model ind 665 epoch 823 batch: 500 avg loss -2.860434 avg loss no lamb -2.860434 time 2020-06-26 16:19:06.443951
Model ind 665 epoch 823 batch: 600 avg loss -2.897939 avg loss no lamb -2.897939 time 2020-06-26 16:19:17.150130
Model ind 665 epoch 823 batch: 700 avg loss -2.760876 avg loss no lamb -2.760876 time 2020-06-26 16:19:28.049878
Model ind 665 epoch 823 batch: 800 avg loss -2.879117 avg loss no lamb -2.879117 time 2020-06-26 16:19:39.007375
last batch sz 10
Pre: time 2020-06-26 16:19:53.216767: 
 	std: 0.002998256
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9809, 0.975, 0.9816, 0.9759]
	train_accs: [0.98145, 0.9806, 0.97531664, 0.98116666, 0.9759333]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97908
	best: 0.982

Starting e_i: 824
Model ind 665 epoch 824 batch: 0 avg loss -2.950470 avg loss no lamb -2.950470 time 2020-06-26 16:19:54.245695
Model ind 665 epoch 824 batch: 100 avg loss -2.867268 avg loss no lamb -2.867268 time 2020-06-26 16:20:04.860553
Model ind 665 epoch 824 batch: 200 avg loss -2.838369 avg loss no lamb -2.838369 time 2020-06-26 16:20:15.399602
Model ind 665 epoch 824 batch: 300 avg loss -2.849839 avg loss no lamb -2.849839 time 2020-06-26 16:20:26.486290
Model ind 665 epoch 824 batch: 400 avg loss -2.780481 avg loss no lamb -2.780481 time 2020-06-26 16:20:37.353697
Model ind 665 epoch 824 batch: 500 avg loss -2.836924 avg loss no lamb -2.836924 time 2020-06-26 16:20:48.088550
Model ind 665 epoch 824 batch: 600 avg loss -2.869672 avg loss no lamb -2.869672 time 2020-06-26 16:20:58.889635
Model ind 665 epoch 824 batch: 700 avg loss -2.783778 avg loss no lamb -2.783778 time 2020-06-26 16:21:09.681286
Model ind 665 epoch 824 batch: 800 avg loss -2.868962 avg loss no lamb -2.868962 time 2020-06-26 16:21:20.449447
last batch sz 10
Pre: time 2020-06-26 16:21:34.512170: 
 	std: 0.0032839044
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.983, 0.9816, 0.9754, 0.983, 0.9765]
	train_accs: [0.98188335, 0.98113334, 0.97595, 0.9820333, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.9799
	best: 0.983

Starting e_i: 825
Model ind 665 epoch 825 batch: 0 avg loss -2.870766 avg loss no lamb -2.870766 time 2020-06-26 16:21:35.609641
Model ind 665 epoch 825 batch: 100 avg loss -2.818949 avg loss no lamb -2.818949 time 2020-06-26 16:21:46.262583
Model ind 665 epoch 825 batch: 200 avg loss -2.848099 avg loss no lamb -2.848099 time 2020-06-26 16:21:57.108716
Model ind 665 epoch 825 batch: 300 avg loss -2.880522 avg loss no lamb -2.880522 time 2020-06-26 16:22:08.023575
Model ind 665 epoch 825 batch: 400 avg loss -2.784637 avg loss no lamb -2.784637 time 2020-06-26 16:22:18.940904
Model ind 665 epoch 825 batch: 500 avg loss -2.877326 avg loss no lamb -2.877326 time 2020-06-26 16:22:29.730966
Model ind 665 epoch 825 batch: 600 avg loss -2.855424 avg loss no lamb -2.855424 time 2020-06-26 16:22:40.675679
Model ind 665 epoch 825 batch: 700 avg loss -2.700222 avg loss no lamb -2.700222 time 2020-06-26 16:22:51.444154
Model ind 665 epoch 825 batch: 800 avg loss -2.888388 avg loss no lamb -2.888388 time 2020-06-26 16:23:02.245912
last batch sz 10
Pre: time 2020-06-26 16:23:16.312393: 
 	std: 0.0033677348
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9806, 0.9738, 0.9814, 0.9751]
	train_accs: [0.98148334, 0.98085, 0.9745167, 0.98155, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97852004
	best: 0.9814

Starting e_i: 826
Model ind 665 epoch 826 batch: 0 avg loss -2.942893 avg loss no lamb -2.942893 time 2020-06-26 16:23:17.299280
Model ind 665 epoch 826 batch: 100 avg loss -2.817594 avg loss no lamb -2.817594 time 2020-06-26 16:23:28.134572
Model ind 665 epoch 826 batch: 200 avg loss -2.868648 avg loss no lamb -2.868648 time 2020-06-26 16:23:38.994167
Model ind 665 epoch 826 batch: 300 avg loss -2.829781 avg loss no lamb -2.829781 time 2020-06-26 16:23:49.773272
Model ind 665 epoch 826 batch: 400 avg loss -2.788618 avg loss no lamb -2.788618 time 2020-06-26 16:24:00.578400
Model ind 665 epoch 826 batch: 500 avg loss -2.836693 avg loss no lamb -2.836693 time 2020-06-26 16:24:11.265204
Model ind 665 epoch 826 batch: 600 avg loss -2.847542 avg loss no lamb -2.847542 time 2020-06-26 16:24:22.138950
Model ind 665 epoch 826 batch: 700 avg loss -2.749885 avg loss no lamb -2.749885 time 2020-06-26 16:24:32.916805
Model ind 665 epoch 826 batch: 800 avg loss -2.874853 avg loss no lamb -2.874853 time 2020-06-26 16:24:43.775893
last batch sz 10
Pre: time 2020-06-26 16:24:58.313878: 
 	std: 0.0032178245
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9805, 0.9742, 0.9821, 0.9758]
	train_accs: [0.98148334, 0.98071665, 0.97506666, 0.9817167, 0.9763333]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97884
	best: 0.9821

Starting e_i: 827
Model ind 665 epoch 827 batch: 0 avg loss -2.980263 avg loss no lamb -2.980263 time 2020-06-26 16:24:59.439234
Model ind 665 epoch 827 batch: 100 avg loss -2.870830 avg loss no lamb -2.870830 time 2020-06-26 16:25:10.155589
Model ind 665 epoch 827 batch: 200 avg loss -2.831981 avg loss no lamb -2.831981 time 2020-06-26 16:25:20.944695
Model ind 665 epoch 827 batch: 300 avg loss -2.850692 avg loss no lamb -2.850692 time 2020-06-26 16:25:31.687723
Model ind 665 epoch 827 batch: 400 avg loss -2.750165 avg loss no lamb -2.750165 time 2020-06-26 16:25:42.109026
Model ind 665 epoch 827 batch: 500 avg loss -2.864495 avg loss no lamb -2.864495 time 2020-06-26 16:25:52.744254
Model ind 665 epoch 827 batch: 600 avg loss -2.894348 avg loss no lamb -2.894348 time 2020-06-26 16:26:03.623545
Model ind 665 epoch 827 batch: 700 avg loss -2.754363 avg loss no lamb -2.754363 time 2020-06-26 16:26:14.451260
Model ind 665 epoch 827 batch: 800 avg loss -2.859463 avg loss no lamb -2.859463 time 2020-06-26 16:26:25.247703
last batch sz 10
Pre: time 2020-06-26 16:26:39.167723: 
 	std: 0.0032879137
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9786, 0.9721, 0.9791, 0.9726]
	train_accs: [0.98156667, 0.9805667, 0.97503334, 0.98158336, 0.9755667]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97635996
	best: 0.9791

Starting e_i: 828
Model ind 665 epoch 828 batch: 0 avg loss -2.964920 avg loss no lamb -2.964920 time 2020-06-26 16:26:40.097892
Model ind 665 epoch 828 batch: 100 avg loss -2.787191 avg loss no lamb -2.787191 time 2020-06-26 16:26:51.058982
Model ind 665 epoch 828 batch: 200 avg loss -2.805222 avg loss no lamb -2.805222 time 2020-06-26 16:27:01.984858
Model ind 665 epoch 828 batch: 300 avg loss -2.825720 avg loss no lamb -2.825720 time 2020-06-26 16:27:12.862354
Model ind 665 epoch 828 batch: 400 avg loss -2.792768 avg loss no lamb -2.792768 time 2020-06-26 16:27:23.759876
Model ind 665 epoch 828 batch: 500 avg loss -2.783499 avg loss no lamb -2.783499 time 2020-06-26 16:27:34.371778
Model ind 665 epoch 828 batch: 600 avg loss -2.909243 avg loss no lamb -2.909243 time 2020-06-26 16:27:45.142899
Model ind 665 epoch 828 batch: 700 avg loss -2.724166 avg loss no lamb -2.724166 time 2020-06-26 16:27:56.117652
Model ind 665 epoch 828 batch: 800 avg loss -2.738886 avg loss no lamb -2.738886 time 2020-06-26 16:28:07.091207
last batch sz 10
Pre: time 2020-06-26 16:28:21.079089: 
 	std: 0.0040102806
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9797, 0.9718, 0.9809, 0.973]
	train_accs: [0.9807, 0.97995, 0.9736, 0.98081666, 0.9746]
	best_train_sub_head: 3
	worst: 0.9718
	avg: 0.97726
	best: 0.9809

Starting e_i: 829
Model ind 665 epoch 829 batch: 0 avg loss -2.917519 avg loss no lamb -2.917519 time 2020-06-26 16:28:22.213922
Model ind 665 epoch 829 batch: 100 avg loss -2.846819 avg loss no lamb -2.846819 time 2020-06-26 16:28:33.023420
Model ind 665 epoch 829 batch: 200 avg loss -2.922953 avg loss no lamb -2.922953 time 2020-06-26 16:28:43.822564
Model ind 665 epoch 829 batch: 300 avg loss -2.859403 avg loss no lamb -2.859403 time 2020-06-26 16:28:54.637772
Model ind 665 epoch 829 batch: 400 avg loss -2.778669 avg loss no lamb -2.778669 time 2020-06-26 16:29:05.560537
Model ind 665 epoch 829 batch: 500 avg loss -2.882685 avg loss no lamb -2.882685 time 2020-06-26 16:29:16.229543
Model ind 665 epoch 829 batch: 600 avg loss -2.838150 avg loss no lamb -2.838150 time 2020-06-26 16:29:27.143432
Model ind 665 epoch 829 batch: 700 avg loss -2.731645 avg loss no lamb -2.731645 time 2020-06-26 16:29:37.852331
Model ind 665 epoch 829 batch: 800 avg loss -2.836263 avg loss no lamb -2.836263 time 2020-06-26 16:29:48.762581
last batch sz 10
Pre: time 2020-06-26 16:30:02.811986: 
 	std: 0.0032506003
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9806, 0.9734, 0.9812, 0.9755]
	train_accs: [0.9816333, 0.9808667, 0.97495, 0.98178333, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97833997
	best: 0.9812

Starting e_i: 830
Model ind 665 epoch 830 batch: 0 avg loss -2.989484 avg loss no lamb -2.989484 time 2020-06-26 16:30:03.765301
Model ind 665 epoch 830 batch: 100 avg loss -2.899889 avg loss no lamb -2.899889 time 2020-06-26 16:30:14.359501
Model ind 665 epoch 830 batch: 200 avg loss -2.854160 avg loss no lamb -2.854160 time 2020-06-26 16:30:25.184471
Model ind 665 epoch 830 batch: 300 avg loss -2.880478 avg loss no lamb -2.880478 time 2020-06-26 16:30:36.246825
Model ind 665 epoch 830 batch: 400 avg loss -2.774602 avg loss no lamb -2.774602 time 2020-06-26 16:30:46.748087
Model ind 665 epoch 830 batch: 500 avg loss -2.918196 avg loss no lamb -2.918196 time 2020-06-26 16:30:57.559143
Model ind 665 epoch 830 batch: 600 avg loss -2.853863 avg loss no lamb -2.853863 time 2020-06-26 16:31:08.095215
Model ind 665 epoch 830 batch: 700 avg loss -2.783830 avg loss no lamb -2.783830 time 2020-06-26 16:31:18.600785
Model ind 665 epoch 830 batch: 800 avg loss -2.889555 avg loss no lamb -2.889555 time 2020-06-26 16:31:29.141123
last batch sz 10
Pre: time 2020-06-26 16:31:43.170332: 
 	std: 0.0036576493
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9809, 0.9738, 0.9817, 0.9746]
	train_accs: [0.9819667, 0.9807, 0.97463334, 0.98181665, 0.9752167]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97863996
	best: 0.9822

Starting e_i: 831
Model ind 665 epoch 831 batch: 0 avg loss -2.935998 avg loss no lamb -2.935998 time 2020-06-26 16:31:45.526530
Model ind 665 epoch 831 batch: 100 avg loss -2.923726 avg loss no lamb -2.923726 time 2020-06-26 16:31:56.152808
Model ind 665 epoch 831 batch: 200 avg loss -2.939320 avg loss no lamb -2.939320 time 2020-06-26 16:32:06.958610
Model ind 665 epoch 831 batch: 300 avg loss -2.841429 avg loss no lamb -2.841429 time 2020-06-26 16:32:17.721980
Model ind 665 epoch 831 batch: 400 avg loss -2.758100 avg loss no lamb -2.758100 time 2020-06-26 16:32:28.459686
Model ind 665 epoch 831 batch: 500 avg loss -2.858386 avg loss no lamb -2.858386 time 2020-06-26 16:32:39.185304
Model ind 665 epoch 831 batch: 600 avg loss -2.849778 avg loss no lamb -2.849778 time 2020-06-26 16:32:50.043225
Model ind 665 epoch 831 batch: 700 avg loss -2.769972 avg loss no lamb -2.769972 time 2020-06-26 16:33:00.720298
Model ind 665 epoch 831 batch: 800 avg loss -2.880510 avg loss no lamb -2.880510 time 2020-06-26 16:33:11.686331
last batch sz 10
Pre: time 2020-06-26 16:33:26.079496: 
 	std: 0.002940351
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9797, 0.9733, 0.9801, 0.9744]
	train_accs: [0.98088336, 0.98038334, 0.97415, 0.98075, 0.9755167]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97742
	best: 0.9796

Starting e_i: 832
Model ind 665 epoch 832 batch: 0 avg loss -2.996040 avg loss no lamb -2.996040 time 2020-06-26 16:33:27.091439
Model ind 665 epoch 832 batch: 100 avg loss -2.933267 avg loss no lamb -2.933267 time 2020-06-26 16:33:37.896862
Model ind 665 epoch 832 batch: 200 avg loss -2.904755 avg loss no lamb -2.904755 time 2020-06-26 16:33:48.737584
Model ind 665 epoch 832 batch: 300 avg loss -2.837706 avg loss no lamb -2.837706 time 2020-06-26 16:33:59.672151
Model ind 665 epoch 832 batch: 400 avg loss -2.822520 avg loss no lamb -2.822520 time 2020-06-26 16:34:10.528011
Model ind 665 epoch 832 batch: 500 avg loss -2.812719 avg loss no lamb -2.812719 time 2020-06-26 16:34:21.424224
Model ind 665 epoch 832 batch: 600 avg loss -2.788468 avg loss no lamb -2.788468 time 2020-06-26 16:34:32.253081
Model ind 665 epoch 832 batch: 700 avg loss -2.754831 avg loss no lamb -2.754831 time 2020-06-26 16:34:43.157218
Model ind 665 epoch 832 batch: 800 avg loss -2.837113 avg loss no lamb -2.837113 time 2020-06-26 16:34:54.030610
last batch sz 10
Pre: time 2020-06-26 16:35:08.461451: 
 	std: 0.002760149
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9803, 0.9741, 0.981, 0.9763]
	train_accs: [0.98146665, 0.98085, 0.9749333, 0.9817333, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97845995
	best: 0.981

Starting e_i: 833
Model ind 665 epoch 833 batch: 0 avg loss -2.989215 avg loss no lamb -2.989215 time 2020-06-26 16:35:09.581374
Model ind 665 epoch 833 batch: 100 avg loss -2.872814 avg loss no lamb -2.872814 time 2020-06-26 16:35:20.324732
Model ind 665 epoch 833 batch: 200 avg loss -2.929562 avg loss no lamb -2.929562 time 2020-06-26 16:35:31.351173
Model ind 665 epoch 833 batch: 300 avg loss -2.841632 avg loss no lamb -2.841632 time 2020-06-26 16:35:42.349327
Model ind 665 epoch 833 batch: 400 avg loss -2.768937 avg loss no lamb -2.768937 time 2020-06-26 16:35:53.176169
Model ind 665 epoch 833 batch: 500 avg loss -2.823403 avg loss no lamb -2.823403 time 2020-06-26 16:36:04.024294
Model ind 665 epoch 833 batch: 600 avg loss -2.815587 avg loss no lamb -2.815587 time 2020-06-26 16:36:14.846276
Model ind 665 epoch 833 batch: 700 avg loss -2.712086 avg loss no lamb -2.712086 time 2020-06-26 16:36:25.854278
Model ind 665 epoch 833 batch: 800 avg loss -2.910908 avg loss no lamb -2.910908 time 2020-06-26 16:36:36.609124
last batch sz 10
Pre: time 2020-06-26 16:36:50.643570: 
 	std: 0.0027760295
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9795, 0.9742, 0.9808, 0.9757]
	train_accs: [0.98175, 0.98035, 0.97533333, 0.9815, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97824
	best: 0.981

Starting e_i: 834
Model ind 665 epoch 834 batch: 0 avg loss -2.986042 avg loss no lamb -2.986042 time 2020-06-26 16:36:51.648739
Model ind 665 epoch 834 batch: 100 avg loss -2.817714 avg loss no lamb -2.817714 time 2020-06-26 16:37:02.362239
Model ind 665 epoch 834 batch: 200 avg loss -2.868824 avg loss no lamb -2.868824 time 2020-06-26 16:37:12.911835
Model ind 665 epoch 834 batch: 300 avg loss -2.860256 avg loss no lamb -2.860256 time 2020-06-26 16:37:23.634292
Model ind 665 epoch 834 batch: 400 avg loss -2.782723 avg loss no lamb -2.782723 time 2020-06-26 16:37:34.549526
Model ind 665 epoch 834 batch: 500 avg loss -2.828026 avg loss no lamb -2.828026 time 2020-06-26 16:37:45.400807
Model ind 665 epoch 834 batch: 600 avg loss -2.872168 avg loss no lamb -2.872168 time 2020-06-26 16:37:56.099701
Model ind 665 epoch 834 batch: 700 avg loss -2.641267 avg loss no lamb -2.641267 time 2020-06-26 16:38:06.774722
Model ind 665 epoch 834 batch: 800 avg loss -2.795618 avg loss no lamb -2.795618 time 2020-06-26 16:38:17.586866
last batch sz 10
Pre: time 2020-06-26 16:38:31.710555: 
 	std: 0.0026004708
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9792, 0.974, 0.98, 0.9755]
	train_accs: [0.98108333, 0.9803, 0.97555, 0.98108333, 0.9762]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97784007
	best: 0.9805

Starting e_i: 835
Model ind 665 epoch 835 batch: 0 avg loss -2.940604 avg loss no lamb -2.940604 time 2020-06-26 16:38:32.815899
Model ind 665 epoch 835 batch: 100 avg loss -2.858263 avg loss no lamb -2.858263 time 2020-06-26 16:38:43.685196
Model ind 665 epoch 835 batch: 200 avg loss -2.877275 avg loss no lamb -2.877275 time 2020-06-26 16:38:54.526104
Model ind 665 epoch 835 batch: 300 avg loss -2.835141 avg loss no lamb -2.835141 time 2020-06-26 16:39:05.458240
Model ind 665 epoch 835 batch: 400 avg loss -2.753714 avg loss no lamb -2.753714 time 2020-06-26 16:39:16.614124
Model ind 665 epoch 835 batch: 500 avg loss -2.817106 avg loss no lamb -2.817106 time 2020-06-26 16:39:27.611115
Model ind 665 epoch 835 batch: 600 avg loss -2.875041 avg loss no lamb -2.875041 time 2020-06-26 16:39:38.671494
Model ind 665 epoch 835 batch: 700 avg loss -2.702941 avg loss no lamb -2.702941 time 2020-06-26 16:39:49.578139
Model ind 665 epoch 835 batch: 800 avg loss -2.804542 avg loss no lamb -2.804542 time 2020-06-26 16:40:00.553656
last batch sz 10
Pre: time 2020-06-26 16:40:14.788249: 
 	std: 0.0037765685
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9813, 0.9733, 0.9821, 0.9758]
	train_accs: [0.9820167, 0.98115, 0.9748833, 0.98186666, 0.9759333]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97903997
	best: 0.9827

Starting e_i: 836
Model ind 665 epoch 836 batch: 0 avg loss -2.960151 avg loss no lamb -2.960151 time 2020-06-26 16:40:15.781882
Model ind 665 epoch 836 batch: 100 avg loss -2.923949 avg loss no lamb -2.923949 time 2020-06-26 16:40:26.584227
Model ind 665 epoch 836 batch: 200 avg loss -2.895969 avg loss no lamb -2.895969 time 2020-06-26 16:40:37.289354
Model ind 665 epoch 836 batch: 300 avg loss -2.859341 avg loss no lamb -2.859341 time 2020-06-26 16:40:48.092316
Model ind 665 epoch 836 batch: 400 avg loss -2.810246 avg loss no lamb -2.810246 time 2020-06-26 16:40:58.855477
Model ind 665 epoch 836 batch: 500 avg loss -2.864069 avg loss no lamb -2.864069 time 2020-06-26 16:41:10.049056
Model ind 665 epoch 836 batch: 600 avg loss -2.827419 avg loss no lamb -2.827419 time 2020-06-26 16:41:20.899977
Model ind 665 epoch 836 batch: 700 avg loss -2.846739 avg loss no lamb -2.846739 time 2020-06-26 16:41:31.827589
Model ind 665 epoch 836 batch: 800 avg loss -2.820886 avg loss no lamb -2.820886 time 2020-06-26 16:41:43.057366
last batch sz 10
Pre: time 2020-06-26 16:41:57.284943: 
 	std: 0.0029264395
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9833, 0.9814, 0.9758, 0.9832, 0.9783]
	train_accs: [0.98215, 0.9810333, 0.97605, 0.98226666, 0.9774167]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.98039997
	best: 0.9832

Starting e_i: 837
Model ind 665 epoch 837 batch: 0 avg loss -2.956022 avg loss no lamb -2.956022 time 2020-06-26 16:41:58.344827
Model ind 665 epoch 837 batch: 100 avg loss -2.856847 avg loss no lamb -2.856847 time 2020-06-26 16:42:09.374211
Model ind 665 epoch 837 batch: 200 avg loss -2.879754 avg loss no lamb -2.879754 time 2020-06-26 16:42:20.541731
Model ind 665 epoch 837 batch: 300 avg loss -2.858557 avg loss no lamb -2.858557 time 2020-06-26 16:42:31.603662
Model ind 665 epoch 837 batch: 400 avg loss -2.768432 avg loss no lamb -2.768432 time 2020-06-26 16:42:42.383526
Model ind 665 epoch 837 batch: 500 avg loss -2.801325 avg loss no lamb -2.801325 time 2020-06-26 16:42:53.429617
Model ind 665 epoch 837 batch: 600 avg loss -2.821843 avg loss no lamb -2.821843 time 2020-06-26 16:43:04.312256
Model ind 665 epoch 837 batch: 700 avg loss -2.701676 avg loss no lamb -2.701676 time 2020-06-26 16:43:15.090242
Model ind 665 epoch 837 batch: 800 avg loss -2.851216 avg loss no lamb -2.851216 time 2020-06-26 16:43:26.055771
last batch sz 10
Pre: time 2020-06-26 16:43:40.112494: 
 	std: 0.003084551
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9815, 0.9748, 0.9825, 0.977]
	train_accs: [0.98181665, 0.9809167, 0.97613335, 0.98193336, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.9795601
	best: 0.9825

Starting e_i: 838
Model ind 665 epoch 838 batch: 0 avg loss -2.976149 avg loss no lamb -2.976149 time 2020-06-26 16:43:41.113392
Model ind 665 epoch 838 batch: 100 avg loss -2.889117 avg loss no lamb -2.889117 time 2020-06-26 16:43:51.882410
Model ind 665 epoch 838 batch: 200 avg loss -2.863624 avg loss no lamb -2.863624 time 2020-06-26 16:44:02.713570
Model ind 665 epoch 838 batch: 300 avg loss -2.872637 avg loss no lamb -2.872637 time 2020-06-26 16:44:13.581152
Model ind 665 epoch 838 batch: 400 avg loss -2.758263 avg loss no lamb -2.758263 time 2020-06-26 16:44:24.691733
Model ind 665 epoch 838 batch: 500 avg loss -2.826468 avg loss no lamb -2.826468 time 2020-06-26 16:44:35.701507
Model ind 665 epoch 838 batch: 600 avg loss -2.893556 avg loss no lamb -2.893556 time 2020-06-26 16:44:46.468312
Model ind 665 epoch 838 batch: 700 avg loss -2.679962 avg loss no lamb -2.679962 time 2020-06-26 16:44:57.441654
Model ind 665 epoch 838 batch: 800 avg loss -2.870885 avg loss no lamb -2.870885 time 2020-06-26 16:45:08.413214
last batch sz 10
Pre: time 2020-06-26 16:45:22.382433: 
 	std: 0.0032360419
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9798, 0.9738, 0.9814, 0.9746]
	train_accs: [0.9813, 0.9806833, 0.9755833, 0.98135, 0.97578335]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.9781
	best: 0.9814

Starting e_i: 839
Model ind 665 epoch 839 batch: 0 avg loss -2.936053 avg loss no lamb -2.936053 time 2020-06-26 16:45:23.499169
Model ind 665 epoch 839 batch: 100 avg loss -2.883566 avg loss no lamb -2.883566 time 2020-06-26 16:45:34.572792
Model ind 665 epoch 839 batch: 200 avg loss -2.827802 avg loss no lamb -2.827802 time 2020-06-26 16:45:45.732672
Model ind 665 epoch 839 batch: 300 avg loss -2.860021 avg loss no lamb -2.860021 time 2020-06-26 16:45:56.630241
Model ind 665 epoch 839 batch: 400 avg loss -2.826154 avg loss no lamb -2.826154 time 2020-06-26 16:46:07.642150
Model ind 665 epoch 839 batch: 500 avg loss -2.859516 avg loss no lamb -2.859516 time 2020-06-26 16:46:18.460424
Model ind 665 epoch 839 batch: 600 avg loss -2.871300 avg loss no lamb -2.871300 time 2020-06-26 16:46:29.355391
Model ind 665 epoch 839 batch: 700 avg loss -2.796009 avg loss no lamb -2.796009 time 2020-06-26 16:46:40.426175
Model ind 665 epoch 839 batch: 800 avg loss -2.891572 avg loss no lamb -2.891572 time 2020-06-26 16:46:51.044119
last batch sz 10
Pre: time 2020-06-26 16:47:05.100082: 
 	std: 0.003592563
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9807, 0.9733, 0.982, 0.9754]
	train_accs: [0.9816833, 0.9809167, 0.9752167, 0.9817333, 0.9766]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97863996
	best: 0.982

Starting e_i: 840
Model ind 665 epoch 840 batch: 0 avg loss -2.940468 avg loss no lamb -2.940468 time 2020-06-26 16:47:06.101515
Model ind 665 epoch 840 batch: 100 avg loss -2.870992 avg loss no lamb -2.870992 time 2020-06-26 16:47:16.965712
Model ind 665 epoch 840 batch: 200 avg loss -2.761255 avg loss no lamb -2.761255 time 2020-06-26 16:47:27.748123
Model ind 665 epoch 840 batch: 300 avg loss -2.848310 avg loss no lamb -2.848310 time 2020-06-26 16:47:38.614194
Model ind 665 epoch 840 batch: 400 avg loss -2.795647 avg loss no lamb -2.795647 time 2020-06-26 16:47:49.448641
Model ind 665 epoch 840 batch: 500 avg loss -2.821614 avg loss no lamb -2.821614 time 2020-06-26 16:48:00.357885
Model ind 665 epoch 840 batch: 600 avg loss -2.836058 avg loss no lamb -2.836058 time 2020-06-26 16:48:11.277086
Model ind 665 epoch 840 batch: 700 avg loss -2.815618 avg loss no lamb -2.815618 time 2020-06-26 16:48:22.130728
Model ind 665 epoch 840 batch: 800 avg loss -2.843546 avg loss no lamb -2.843546 time 2020-06-26 16:48:33.045102
last batch sz 10
Pre: time 2020-06-26 16:48:47.200219: 
 	std: 0.0031550536
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.979, 0.9732, 0.9808, 0.9751]
	train_accs: [0.98215, 0.98115, 0.9754, 0.9821, 0.9763]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97783995
	best: 0.9811

Starting e_i: 841
Model ind 665 epoch 841 batch: 0 avg loss -2.949790 avg loss no lamb -2.949790 time 2020-06-26 16:48:49.529314
Model ind 665 epoch 841 batch: 100 avg loss -2.796086 avg loss no lamb -2.796086 time 2020-06-26 16:49:00.274088
Model ind 665 epoch 841 batch: 200 avg loss -2.857942 avg loss no lamb -2.857942 time 2020-06-26 16:49:11.409772
Model ind 665 epoch 841 batch: 300 avg loss -2.830145 avg loss no lamb -2.830145 time 2020-06-26 16:49:22.516934
Model ind 665 epoch 841 batch: 400 avg loss -2.781673 avg loss no lamb -2.781673 time 2020-06-26 16:49:33.466431
Model ind 665 epoch 841 batch: 500 avg loss -2.840289 avg loss no lamb -2.840289 time 2020-06-26 16:49:44.577283
Model ind 665 epoch 841 batch: 600 avg loss -2.874231 avg loss no lamb -2.874231 time 2020-06-26 16:49:55.573681
Model ind 665 epoch 841 batch: 700 avg loss -2.807846 avg loss no lamb -2.807846 time 2020-06-26 16:50:06.563501
Model ind 665 epoch 841 batch: 800 avg loss -2.865711 avg loss no lamb -2.865711 time 2020-06-26 16:50:17.652861
last batch sz 10
Pre: time 2020-06-26 16:50:31.990774: 
 	std: 0.0028118235
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9796, 0.9749, 0.9812, 0.9753]
	train_accs: [0.98158336, 0.98076665, 0.9756, 0.9815, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97846
	best: 0.9813

Starting e_i: 842
Model ind 665 epoch 842 batch: 0 avg loss -2.963663 avg loss no lamb -2.963663 time 2020-06-26 16:50:33.034361
Model ind 665 epoch 842 batch: 100 avg loss -2.847776 avg loss no lamb -2.847776 time 2020-06-26 16:50:44.139631
Model ind 665 epoch 842 batch: 200 avg loss -2.847459 avg loss no lamb -2.847459 time 2020-06-26 16:50:54.996179
Model ind 665 epoch 842 batch: 300 avg loss -2.826954 avg loss no lamb -2.826954 time 2020-06-26 16:51:05.953973
Model ind 665 epoch 842 batch: 400 avg loss -2.733366 avg loss no lamb -2.733366 time 2020-06-26 16:51:16.662912
Model ind 665 epoch 842 batch: 500 avg loss -2.843583 avg loss no lamb -2.843583 time 2020-06-26 16:51:27.230151
Model ind 665 epoch 842 batch: 600 avg loss -2.925045 avg loss no lamb -2.925045 time 2020-06-26 16:51:37.959414
Model ind 665 epoch 842 batch: 700 avg loss -2.679677 avg loss no lamb -2.679677 time 2020-06-26 16:51:49.058292
Model ind 665 epoch 842 batch: 800 avg loss -2.830553 avg loss no lamb -2.830553 time 2020-06-26 16:52:00.182298
last batch sz 10
Pre: time 2020-06-26 16:52:14.416266: 
 	std: 0.00264907
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9796, 0.9745, 0.9812, 0.976]
	train_accs: [0.9809333, 0.9806, 0.9757, 0.9812833, 0.97643334]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97838
	best: 0.9812

Starting e_i: 843
Model ind 665 epoch 843 batch: 0 avg loss -2.901758 avg loss no lamb -2.901758 time 2020-06-26 16:52:15.522118
Model ind 665 epoch 843 batch: 100 avg loss -2.929041 avg loss no lamb -2.929041 time 2020-06-26 16:52:26.358679
Model ind 665 epoch 843 batch: 200 avg loss -2.856029 avg loss no lamb -2.856029 time 2020-06-26 16:52:37.256295
Model ind 665 epoch 843 batch: 300 avg loss -2.907418 avg loss no lamb -2.907418 time 2020-06-26 16:52:48.202708
Model ind 665 epoch 843 batch: 400 avg loss -2.814796 avg loss no lamb -2.814796 time 2020-06-26 16:52:59.351868
Model ind 665 epoch 843 batch: 500 avg loss -2.821464 avg loss no lamb -2.821464 time 2020-06-26 16:53:09.993479
Model ind 665 epoch 843 batch: 600 avg loss -2.903339 avg loss no lamb -2.903339 time 2020-06-26 16:53:20.790075
Model ind 665 epoch 843 batch: 700 avg loss -2.709305 avg loss no lamb -2.709305 time 2020-06-26 16:53:31.566540
Model ind 665 epoch 843 batch: 800 avg loss -2.863295 avg loss no lamb -2.863295 time 2020-06-26 16:53:42.453605
last batch sz 10
Pre: time 2020-06-26 16:53:56.908944: 
 	std: 0.0025230225
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9799, 0.9755, 0.9816, 0.9766]
	train_accs: [0.98165, 0.98088336, 0.9759667, 0.9817333, 0.9766833]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97902
	best: 0.9816

Starting e_i: 844
Model ind 665 epoch 844 batch: 0 avg loss -2.951257 avg loss no lamb -2.951257 time 2020-06-26 16:53:57.853999
Model ind 665 epoch 844 batch: 100 avg loss -2.880826 avg loss no lamb -2.880826 time 2020-06-26 16:54:08.661908
Model ind 665 epoch 844 batch: 200 avg loss -2.868645 avg loss no lamb -2.868645 time 2020-06-26 16:54:19.520209
Model ind 665 epoch 844 batch: 300 avg loss -2.892237 avg loss no lamb -2.892237 time 2020-06-26 16:54:30.475123
Model ind 665 epoch 844 batch: 400 avg loss -2.832820 avg loss no lamb -2.832820 time 2020-06-26 16:54:41.385483
Model ind 665 epoch 844 batch: 500 avg loss -2.826982 avg loss no lamb -2.826982 time 2020-06-26 16:54:52.063141
Model ind 665 epoch 844 batch: 600 avg loss -2.858262 avg loss no lamb -2.858262 time 2020-06-26 16:55:02.883745
Model ind 665 epoch 844 batch: 700 avg loss -2.718481 avg loss no lamb -2.718481 time 2020-06-26 16:55:13.902305
Model ind 665 epoch 844 batch: 800 avg loss -2.861947 avg loss no lamb -2.861947 time 2020-06-26 16:55:24.814326
last batch sz 10
Pre: time 2020-06-26 16:55:39.128578: 
 	std: 0.0024245337
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9796, 0.9752, 0.981, 0.9761]
	train_accs: [0.9813333, 0.98081666, 0.97606665, 0.9813667, 0.97671664]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97854006
	best: 0.981

Starting e_i: 845
Model ind 665 epoch 845 batch: 0 avg loss -2.912043 avg loss no lamb -2.912043 time 2020-06-26 16:55:40.218013
Model ind 665 epoch 845 batch: 100 avg loss -2.863898 avg loss no lamb -2.863898 time 2020-06-26 16:55:51.168425
Model ind 665 epoch 845 batch: 200 avg loss -2.819887 avg loss no lamb -2.819887 time 2020-06-26 16:56:02.238431
Model ind 665 epoch 845 batch: 300 avg loss -2.832161 avg loss no lamb -2.832161 time 2020-06-26 16:56:13.382130
Model ind 665 epoch 845 batch: 400 avg loss -2.799041 avg loss no lamb -2.799041 time 2020-06-26 16:56:24.379653
Model ind 665 epoch 845 batch: 500 avg loss -2.850522 avg loss no lamb -2.850522 time 2020-06-26 16:56:35.344842
Model ind 665 epoch 845 batch: 600 avg loss -2.889938 avg loss no lamb -2.889938 time 2020-06-26 16:56:46.368247
Model ind 665 epoch 845 batch: 700 avg loss -2.713902 avg loss no lamb -2.713902 time 2020-06-26 16:56:57.500967
Model ind 665 epoch 845 batch: 800 avg loss -2.891868 avg loss no lamb -2.891868 time 2020-06-26 16:57:08.565609
last batch sz 10
Pre: time 2020-06-26 16:57:22.837969: 
 	std: 0.0029013138
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9809, 0.9751, 0.9821, 0.9766]
	train_accs: [0.9816667, 0.9812667, 0.97636664, 0.98195, 0.97686666]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97932005
	best: 0.9821

Starting e_i: 846
Model ind 665 epoch 846 batch: 0 avg loss -2.936392 avg loss no lamb -2.936392 time 2020-06-26 16:57:23.885073
Model ind 665 epoch 846 batch: 100 avg loss -2.813611 avg loss no lamb -2.813611 time 2020-06-26 16:57:34.713284
Model ind 665 epoch 846 batch: 200 avg loss -2.873207 avg loss no lamb -2.873207 time 2020-06-26 16:57:45.480372
Model ind 665 epoch 846 batch: 300 avg loss -2.896616 avg loss no lamb -2.896616 time 2020-06-26 16:57:56.452594
Model ind 665 epoch 846 batch: 400 avg loss -2.738728 avg loss no lamb -2.738728 time 2020-06-26 16:58:07.150025
Model ind 665 epoch 846 batch: 500 avg loss -2.837652 avg loss no lamb -2.837652 time 2020-06-26 16:58:17.657686
Model ind 665 epoch 846 batch: 600 avg loss -2.903726 avg loss no lamb -2.903726 time 2020-06-26 16:58:28.357103
Model ind 665 epoch 846 batch: 700 avg loss -2.775834 avg loss no lamb -2.775834 time 2020-06-26 16:58:39.122840
Model ind 665 epoch 846 batch: 800 avg loss -2.806782 avg loss no lamb -2.806782 time 2020-06-26 16:58:49.953293
last batch sz 10
Pre: time 2020-06-26 16:59:04.133593: 
 	std: 0.0026043165
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.981, 0.9754, 0.981, 0.9766]
	train_accs: [0.9813333, 0.981, 0.97565, 0.9813833, 0.9766833]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97914
	best: 0.981

Starting e_i: 847
Model ind 665 epoch 847 batch: 0 avg loss -2.934046 avg loss no lamb -2.934046 time 2020-06-26 16:59:05.293489
Model ind 665 epoch 847 batch: 100 avg loss -2.862510 avg loss no lamb -2.862510 time 2020-06-26 16:59:15.990521
Model ind 665 epoch 847 batch: 200 avg loss -2.840818 avg loss no lamb -2.840818 time 2020-06-26 16:59:26.717223
Model ind 665 epoch 847 batch: 300 avg loss -2.885914 avg loss no lamb -2.885914 time 2020-06-26 16:59:37.581061
Model ind 665 epoch 847 batch: 400 avg loss -2.766500 avg loss no lamb -2.766500 time 2020-06-26 16:59:48.477756
Model ind 665 epoch 847 batch: 500 avg loss -2.868526 avg loss no lamb -2.868526 time 2020-06-26 16:59:59.263319
Model ind 665 epoch 847 batch: 600 avg loss -2.894577 avg loss no lamb -2.894577 time 2020-06-26 17:00:10.109081
Model ind 665 epoch 847 batch: 700 avg loss -2.654392 avg loss no lamb -2.654392 time 2020-06-26 17:00:20.960132
Model ind 665 epoch 847 batch: 800 avg loss -2.875000 avg loss no lamb -2.875000 time 2020-06-26 17:00:31.626733
last batch sz 10
Pre: time 2020-06-26 17:00:45.673157: 
 	std: 0.0029136967
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9811, 0.9753, 0.9821, 0.9762]
	train_accs: [0.9817, 0.9812, 0.97611666, 0.98178333, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97928
	best: 0.9821

Starting e_i: 848
Model ind 665 epoch 848 batch: 0 avg loss -2.973504 avg loss no lamb -2.973504 time 2020-06-26 17:00:46.661946
Model ind 665 epoch 848 batch: 100 avg loss -2.888456 avg loss no lamb -2.888456 time 2020-06-26 17:00:57.563434
Model ind 665 epoch 848 batch: 200 avg loss -2.908389 avg loss no lamb -2.908389 time 2020-06-26 17:01:08.364780
Model ind 665 epoch 848 batch: 300 avg loss -2.822301 avg loss no lamb -2.822301 time 2020-06-26 17:01:19.049339
Model ind 665 epoch 848 batch: 400 avg loss -2.787728 avg loss no lamb -2.787728 time 2020-06-26 17:01:30.016290
Model ind 665 epoch 848 batch: 500 avg loss -2.827891 avg loss no lamb -2.827891 time 2020-06-26 17:01:40.740761
Model ind 665 epoch 848 batch: 600 avg loss -2.877818 avg loss no lamb -2.877818 time 2020-06-26 17:01:51.699221
Model ind 665 epoch 848 batch: 700 avg loss -2.773733 avg loss no lamb -2.773733 time 2020-06-26 17:02:02.509895
Model ind 665 epoch 848 batch: 800 avg loss -2.824868 avg loss no lamb -2.824868 time 2020-06-26 17:02:13.657238
last batch sz 10
Pre: time 2020-06-26 17:02:28.030868: 
 	std: 0.0026234363
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9817, 0.977, 0.9825, 0.9769]
	train_accs: [0.98186666, 0.98158336, 0.97681665, 0.9819, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9769
	avg: 0.98014003
	best: 0.9825

Starting e_i: 849
Model ind 665 epoch 849 batch: 0 avg loss -2.955499 avg loss no lamb -2.955499 time 2020-06-26 17:02:29.198644
Model ind 665 epoch 849 batch: 100 avg loss -2.895518 avg loss no lamb -2.895518 time 2020-06-26 17:02:39.912742
Model ind 665 epoch 849 batch: 200 avg loss -2.890374 avg loss no lamb -2.890374 time 2020-06-26 17:02:50.677225
Model ind 665 epoch 849 batch: 300 avg loss -2.834267 avg loss no lamb -2.834267 time 2020-06-26 17:03:01.548479
Model ind 665 epoch 849 batch: 400 avg loss -2.712715 avg loss no lamb -2.712715 time 2020-06-26 17:03:12.426148
Model ind 665 epoch 849 batch: 500 avg loss -2.810374 avg loss no lamb -2.810374 time 2020-06-26 17:03:23.185256
Model ind 665 epoch 849 batch: 600 avg loss -2.885927 avg loss no lamb -2.885927 time 2020-06-26 17:03:34.013333
Model ind 665 epoch 849 batch: 700 avg loss -2.731589 avg loss no lamb -2.731589 time 2020-06-26 17:03:45.102451
Model ind 665 epoch 849 batch: 800 avg loss -2.872257 avg loss no lamb -2.872257 time 2020-06-26 17:03:56.163566
last batch sz 10
Pre: time 2020-06-26 17:04:10.361809: 
 	std: 0.003128194
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9803, 0.9743, 0.9817, 0.9751]
	train_accs: [0.98143333, 0.98073334, 0.97543335, 0.98165, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97848
	best: 0.9817

Starting e_i: 850
Model ind 665 epoch 850 batch: 0 avg loss -2.960350 avg loss no lamb -2.960350 time 2020-06-26 17:04:11.359405
Model ind 665 epoch 850 batch: 100 avg loss -2.832173 avg loss no lamb -2.832173 time 2020-06-26 17:04:21.908854
Model ind 665 epoch 850 batch: 200 avg loss -2.877156 avg loss no lamb -2.877156 time 2020-06-26 17:04:32.912449
Model ind 665 epoch 850 batch: 300 avg loss -2.870945 avg loss no lamb -2.870945 time 2020-06-26 17:04:43.780428
Model ind 665 epoch 850 batch: 400 avg loss -2.761747 avg loss no lamb -2.761747 time 2020-06-26 17:04:54.599211
Model ind 665 epoch 850 batch: 500 avg loss -2.815197 avg loss no lamb -2.815197 time 2020-06-26 17:05:05.256354
Model ind 665 epoch 850 batch: 600 avg loss -2.894296 avg loss no lamb -2.894296 time 2020-06-26 17:05:15.858618
Model ind 665 epoch 850 batch: 700 avg loss -2.808450 avg loss no lamb -2.808450 time 2020-06-26 17:05:26.779308
Model ind 665 epoch 850 batch: 800 avg loss -2.890389 avg loss no lamb -2.890389 time 2020-06-26 17:05:37.875518
last batch sz 10
Pre: time 2020-06-26 17:05:51.844213: 
 	std: 0.0033755712
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9805, 0.974, 0.9815, 0.9752]
	train_accs: [0.98181665, 0.98106664, 0.97566664, 0.98155, 0.9763167]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97866
	best: 0.9821

Starting e_i: 851
Model ind 665 epoch 851 batch: 0 avg loss -2.974045 avg loss no lamb -2.974045 time 2020-06-26 17:05:54.174104
Model ind 665 epoch 851 batch: 100 avg loss -2.901721 avg loss no lamb -2.901721 time 2020-06-26 17:06:05.156157
Model ind 665 epoch 851 batch: 200 avg loss -2.850648 avg loss no lamb -2.850648 time 2020-06-26 17:06:15.815815
Model ind 665 epoch 851 batch: 300 avg loss -2.846245 avg loss no lamb -2.846245 time 2020-06-26 17:06:26.951063
Model ind 665 epoch 851 batch: 400 avg loss -2.743076 avg loss no lamb -2.743076 time 2020-06-26 17:06:37.945300
Model ind 665 epoch 851 batch: 500 avg loss -2.822231 avg loss no lamb -2.822231 time 2020-06-26 17:06:48.834413
Model ind 665 epoch 851 batch: 600 avg loss -2.863661 avg loss no lamb -2.863661 time 2020-06-26 17:06:59.573927
Model ind 665 epoch 851 batch: 700 avg loss -2.763677 avg loss no lamb -2.763677 time 2020-06-26 17:07:10.478134
Model ind 665 epoch 851 batch: 800 avg loss -2.835571 avg loss no lamb -2.835571 time 2020-06-26 17:07:21.555939
last batch sz 10
Pre: time 2020-06-26 17:07:35.550935: 
 	std: 0.0035756966
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9802, 0.973, 0.9812, 0.9744]
	train_accs: [0.9817, 0.98095, 0.9749, 0.9816, 0.97603333]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97802
	best: 0.9813

Starting e_i: 852
Model ind 665 epoch 852 batch: 0 avg loss -2.932678 avg loss no lamb -2.932678 time 2020-06-26 17:07:36.576589
Model ind 665 epoch 852 batch: 100 avg loss -2.911410 avg loss no lamb -2.911410 time 2020-06-26 17:07:47.443702
Model ind 665 epoch 852 batch: 200 avg loss -2.857455 avg loss no lamb -2.857455 time 2020-06-26 17:07:58.355464
Model ind 665 epoch 852 batch: 300 avg loss -2.856326 avg loss no lamb -2.856326 time 2020-06-26 17:08:08.859525
Model ind 665 epoch 852 batch: 400 avg loss -2.793073 avg loss no lamb -2.793073 time 2020-06-26 17:08:19.628812
Model ind 665 epoch 852 batch: 500 avg loss -2.807241 avg loss no lamb -2.807241 time 2020-06-26 17:08:30.660704
Model ind 665 epoch 852 batch: 600 avg loss -2.891094 avg loss no lamb -2.891094 time 2020-06-26 17:08:41.857709
Model ind 665 epoch 852 batch: 700 avg loss -2.747623 avg loss no lamb -2.747623 time 2020-06-26 17:08:52.532560
Model ind 665 epoch 852 batch: 800 avg loss -2.762366 avg loss no lamb -2.762366 time 2020-06-26 17:09:03.355676
last batch sz 10
Pre: time 2020-06-26 17:09:17.408038: 
 	std: 0.003575136
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.98, 0.9727, 0.9812, 0.9741]
	train_accs: [0.98183334, 0.98113334, 0.975, 0.98181665, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97771996
	best: 0.9806

Starting e_i: 853
Model ind 665 epoch 853 batch: 0 avg loss -2.943435 avg loss no lamb -2.943435 time 2020-06-26 17:09:18.497957
Model ind 665 epoch 853 batch: 100 avg loss -2.871577 avg loss no lamb -2.871577 time 2020-06-26 17:09:29.557557
Model ind 665 epoch 853 batch: 200 avg loss -2.849762 avg loss no lamb -2.849762 time 2020-06-26 17:09:40.554529
Model ind 665 epoch 853 batch: 300 avg loss -2.873803 avg loss no lamb -2.873803 time 2020-06-26 17:09:51.248670
Model ind 665 epoch 853 batch: 400 avg loss -2.825495 avg loss no lamb -2.825495 time 2020-06-26 17:10:01.985003
Model ind 665 epoch 853 batch: 500 avg loss -2.794215 avg loss no lamb -2.794215 time 2020-06-26 17:10:13.279123
Model ind 665 epoch 853 batch: 600 avg loss -2.850841 avg loss no lamb -2.850841 time 2020-06-26 17:10:24.381224
Model ind 665 epoch 853 batch: 700 avg loss -2.735227 avg loss no lamb -2.735227 time 2020-06-26 17:10:35.453099
Model ind 665 epoch 853 batch: 800 avg loss -2.914823 avg loss no lamb -2.914823 time 2020-06-26 17:10:46.157578
last batch sz 10
Pre: time 2020-06-26 17:11:00.090807: 
 	std: 0.003608543
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9802, 0.9728, 0.9813, 0.9741]
	train_accs: [0.9816, 0.9807, 0.97506666, 0.98165, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97782004
	best: 0.9813

Starting e_i: 854
Model ind 665 epoch 854 batch: 0 avg loss -2.947506 avg loss no lamb -2.947506 time 2020-06-26 17:11:01.068061
Model ind 665 epoch 854 batch: 100 avg loss -2.869673 avg loss no lamb -2.869673 time 2020-06-26 17:11:11.993627
Model ind 665 epoch 854 batch: 200 avg loss -2.878754 avg loss no lamb -2.878754 time 2020-06-26 17:11:22.996735
Model ind 665 epoch 854 batch: 300 avg loss -2.843379 avg loss no lamb -2.843379 time 2020-06-26 17:11:33.867702
Model ind 665 epoch 854 batch: 400 avg loss -2.744195 avg loss no lamb -2.744195 time 2020-06-26 17:11:44.714735
Model ind 665 epoch 854 batch: 500 avg loss -2.804531 avg loss no lamb -2.804531 time 2020-06-26 17:11:55.480981
Model ind 665 epoch 854 batch: 600 avg loss -2.845222 avg loss no lamb -2.845222 time 2020-06-26 17:12:06.388066
Model ind 665 epoch 854 batch: 700 avg loss -2.788560 avg loss no lamb -2.788560 time 2020-06-26 17:12:17.359560
Model ind 665 epoch 854 batch: 800 avg loss -2.889316 avg loss no lamb -2.889316 time 2020-06-26 17:12:27.871528
last batch sz 10
Pre: time 2020-06-26 17:12:41.924340: 
 	std: 0.0035768147
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9791, 0.9727, 0.9813, 0.9738]
	train_accs: [0.9819667, 0.9805667, 0.97555, 0.9821, 0.9762]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97752
	best: 0.9813

Starting e_i: 855
Model ind 665 epoch 855 batch: 0 avg loss -2.889745 avg loss no lamb -2.889745 time 2020-06-26 17:12:43.041757
Model ind 665 epoch 855 batch: 100 avg loss -2.851063 avg loss no lamb -2.851063 time 2020-06-26 17:12:53.837744
Model ind 665 epoch 855 batch: 200 avg loss -2.893060 avg loss no lamb -2.893060 time 2020-06-26 17:13:04.626933
Model ind 665 epoch 855 batch: 300 avg loss -2.881706 avg loss no lamb -2.881706 time 2020-06-26 17:13:15.651570
Model ind 665 epoch 855 batch: 400 avg loss -2.744394 avg loss no lamb -2.744394 time 2020-06-26 17:13:26.644467
Model ind 665 epoch 855 batch: 500 avg loss -2.845308 avg loss no lamb -2.845308 time 2020-06-26 17:13:37.502988
Model ind 665 epoch 855 batch: 600 avg loss -2.839258 avg loss no lamb -2.839258 time 2020-06-26 17:13:49.031319
Model ind 665 epoch 855 batch: 700 avg loss -2.699273 avg loss no lamb -2.699273 time 2020-06-26 17:13:59.891471
Model ind 665 epoch 855 batch: 800 avg loss -2.858797 avg loss no lamb -2.858797 time 2020-06-26 17:14:10.973625
last batch sz 10
Pre: time 2020-06-26 17:14:25.099828: 
 	std: 0.0031677121
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9799, 0.9731, 0.98, 0.9751]
	train_accs: [0.98176664, 0.98071665, 0.97541666, 0.98141664, 0.9767333]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97786
	best: 0.9812

Starting e_i: 856
Model ind 665 epoch 856 batch: 0 avg loss -2.917765 avg loss no lamb -2.917765 time 2020-06-26 17:14:26.068169
Model ind 665 epoch 856 batch: 100 avg loss -2.918211 avg loss no lamb -2.918211 time 2020-06-26 17:14:37.262220
Model ind 665 epoch 856 batch: 200 avg loss -2.819936 avg loss no lamb -2.819936 time 2020-06-26 17:14:48.254781
Model ind 665 epoch 856 batch: 300 avg loss -2.847328 avg loss no lamb -2.847328 time 2020-06-26 17:14:59.046987
Model ind 665 epoch 856 batch: 400 avg loss -2.755149 avg loss no lamb -2.755149 time 2020-06-26 17:15:09.987926
Model ind 665 epoch 856 batch: 500 avg loss -2.792874 avg loss no lamb -2.792874 time 2020-06-26 17:15:20.877356
Model ind 665 epoch 856 batch: 600 avg loss -2.875609 avg loss no lamb -2.875609 time 2020-06-26 17:15:31.856210
Model ind 665 epoch 856 batch: 700 avg loss -2.733080 avg loss no lamb -2.733080 time 2020-06-26 17:15:42.883439
Model ind 665 epoch 856 batch: 800 avg loss -2.853432 avg loss no lamb -2.853432 time 2020-06-26 17:15:53.806964
last batch sz 10
Pre: time 2020-06-26 17:16:08.020005: 
 	std: 0.002956608
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9792, 0.9735, 0.9802, 0.9743]
	train_accs: [0.9812, 0.98041666, 0.97506666, 0.98118335, 0.9763333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97748005
	best: 0.9802

Starting e_i: 857
Model ind 665 epoch 857 batch: 0 avg loss -2.971907 avg loss no lamb -2.971907 time 2020-06-26 17:16:09.097243
Model ind 665 epoch 857 batch: 100 avg loss -2.864002 avg loss no lamb -2.864002 time 2020-06-26 17:16:19.888084
Model ind 665 epoch 857 batch: 200 avg loss -2.865105 avg loss no lamb -2.865105 time 2020-06-26 17:16:30.652610
Model ind 665 epoch 857 batch: 300 avg loss -2.887943 avg loss no lamb -2.887943 time 2020-06-26 17:16:41.915420
Model ind 665 epoch 857 batch: 400 avg loss -2.788417 avg loss no lamb -2.788417 time 2020-06-26 17:16:52.759305
Model ind 665 epoch 857 batch: 500 avg loss -2.840319 avg loss no lamb -2.840319 time 2020-06-26 17:17:04.098936
Model ind 665 epoch 857 batch: 600 avg loss -2.834376 avg loss no lamb -2.834376 time 2020-06-26 17:17:14.927628
Model ind 665 epoch 857 batch: 700 avg loss -2.738434 avg loss no lamb -2.738434 time 2020-06-26 17:17:25.990388
Model ind 665 epoch 857 batch: 800 avg loss -2.893986 avg loss no lamb -2.893986 time 2020-06-26 17:17:36.893720
last batch sz 10
Pre: time 2020-06-26 17:17:50.879887: 
 	std: 0.0037594743
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9809, 0.9728, 0.9811, 0.974]
	train_accs: [0.9808667, 0.9802833, 0.9744667, 0.9812667, 0.9752333]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97798
	best: 0.9811

Starting e_i: 858
Model ind 665 epoch 858 batch: 0 avg loss -2.940479 avg loss no lamb -2.940479 time 2020-06-26 17:17:51.887172
Model ind 665 epoch 858 batch: 100 avg loss -2.842513 avg loss no lamb -2.842513 time 2020-06-26 17:18:02.689836
Model ind 665 epoch 858 batch: 200 avg loss -2.837526 avg loss no lamb -2.837526 time 2020-06-26 17:18:13.520397
Model ind 665 epoch 858 batch: 300 avg loss -2.903075 avg loss no lamb -2.903075 time 2020-06-26 17:18:24.488144
Model ind 665 epoch 858 batch: 400 avg loss -2.814819 avg loss no lamb -2.814819 time 2020-06-26 17:18:35.438814
Model ind 665 epoch 858 batch: 500 avg loss -2.774236 avg loss no lamb -2.774236 time 2020-06-26 17:18:46.049927
Model ind 665 epoch 858 batch: 600 avg loss -2.844685 avg loss no lamb -2.844685 time 2020-06-26 17:18:56.686595
Model ind 665 epoch 858 batch: 700 avg loss -2.729177 avg loss no lamb -2.729177 time 2020-06-26 17:19:07.667253
Model ind 665 epoch 858 batch: 800 avg loss -2.867008 avg loss no lamb -2.867008 time 2020-06-26 17:19:18.313250
last batch sz 10
Pre: time 2020-06-26 17:19:32.296685: 
 	std: 0.003349267
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9806, 0.9743, 0.9821, 0.9759]
	train_accs: [0.9822, 0.98116666, 0.9759, 0.982, 0.97685]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97908
	best: 0.9825

Starting e_i: 859
Model ind 665 epoch 859 batch: 0 avg loss -2.962520 avg loss no lamb -2.962520 time 2020-06-26 17:19:33.421745
Model ind 665 epoch 859 batch: 100 avg loss -2.847438 avg loss no lamb -2.847438 time 2020-06-26 17:19:44.160251
Model ind 665 epoch 859 batch: 200 avg loss -2.916976 avg loss no lamb -2.916976 time 2020-06-26 17:19:55.269910
Model ind 665 epoch 859 batch: 300 avg loss -2.852645 avg loss no lamb -2.852645 time 2020-06-26 17:20:06.109718
Model ind 665 epoch 859 batch: 400 avg loss -2.813891 avg loss no lamb -2.813891 time 2020-06-26 17:20:16.755652
Model ind 665 epoch 859 batch: 500 avg loss -2.814382 avg loss no lamb -2.814382 time 2020-06-26 17:20:27.404567
Model ind 665 epoch 859 batch: 600 avg loss -2.855137 avg loss no lamb -2.855137 time 2020-06-26 17:20:38.351009
Model ind 665 epoch 859 batch: 700 avg loss -2.794387 avg loss no lamb -2.794387 time 2020-06-26 17:20:49.121069
Model ind 665 epoch 859 batch: 800 avg loss -2.826870 avg loss no lamb -2.826870 time 2020-06-26 17:21:00.059323
last batch sz 10
Pre: time 2020-06-26 17:21:14.555217: 
 	std: 0.0031327906
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.983, 0.9814, 0.9756, 0.9827, 0.9766]
	train_accs: [0.98253334, 0.98156667, 0.97606665, 0.98221666, 0.9773]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97986
	best: 0.983

Starting e_i: 860
Model ind 665 epoch 860 batch: 0 avg loss -2.873073 avg loss no lamb -2.873073 time 2020-06-26 17:21:15.504427
Model ind 665 epoch 860 batch: 100 avg loss -2.899977 avg loss no lamb -2.899977 time 2020-06-26 17:21:26.386744
Model ind 665 epoch 860 batch: 200 avg loss -2.843642 avg loss no lamb -2.843642 time 2020-06-26 17:21:37.156356
Model ind 665 epoch 860 batch: 300 avg loss -2.859877 avg loss no lamb -2.859877 time 2020-06-26 17:21:47.949321
Model ind 665 epoch 860 batch: 400 avg loss -2.774773 avg loss no lamb -2.774773 time 2020-06-26 17:21:58.802068
Model ind 665 epoch 860 batch: 500 avg loss -2.815529 avg loss no lamb -2.815529 time 2020-06-26 17:22:09.990061
Model ind 665 epoch 860 batch: 600 avg loss -2.881641 avg loss no lamb -2.881641 time 2020-06-26 17:22:20.837543
Model ind 665 epoch 860 batch: 700 avg loss -2.681446 avg loss no lamb -2.681446 time 2020-06-26 17:22:31.566780
Model ind 665 epoch 860 batch: 800 avg loss -2.876079 avg loss no lamb -2.876079 time 2020-06-26 17:22:42.411168
last batch sz 10
Pre: time 2020-06-26 17:22:56.590744: 
 	std: 0.0030053384
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9804, 0.9745, 0.9817, 0.9759]
	train_accs: [0.98195, 0.98108333, 0.97605, 0.98223335, 0.97675]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.9788
	best: 0.9817

Starting e_i: 861
Model ind 665 epoch 861 batch: 0 avg loss -2.901129 avg loss no lamb -2.901129 time 2020-06-26 17:22:58.964148
Model ind 665 epoch 861 batch: 100 avg loss -2.871594 avg loss no lamb -2.871594 time 2020-06-26 17:23:09.841949
Model ind 665 epoch 861 batch: 200 avg loss -2.860271 avg loss no lamb -2.860271 time 2020-06-26 17:23:20.785968
Model ind 665 epoch 861 batch: 300 avg loss -2.793483 avg loss no lamb -2.793483 time 2020-06-26 17:23:31.488484
Model ind 665 epoch 861 batch: 400 avg loss -2.775317 avg loss no lamb -2.775317 time 2020-06-26 17:23:42.381634
Model ind 665 epoch 861 batch: 500 avg loss -2.867931 avg loss no lamb -2.867931 time 2020-06-26 17:23:53.526437
Model ind 665 epoch 861 batch: 600 avg loss -2.820368 avg loss no lamb -2.820368 time 2020-06-26 17:24:04.599448
Model ind 665 epoch 861 batch: 700 avg loss -2.792882 avg loss no lamb -2.792882 time 2020-06-26 17:24:15.292666
Model ind 665 epoch 861 batch: 800 avg loss -2.835302 avg loss no lamb -2.835302 time 2020-06-26 17:24:26.081583
last batch sz 10
Pre: time 2020-06-26 17:24:40.423132: 
 	std: 0.0033041881
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9795, 0.9723, 0.98, 0.9741]
	train_accs: [0.98111665, 0.9802, 0.9742333, 0.98123336, 0.97541666]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97718
	best: 0.98

Starting e_i: 862
Model ind 665 epoch 862 batch: 0 avg loss -2.893921 avg loss no lamb -2.893921 time 2020-06-26 17:24:41.434600
Model ind 665 epoch 862 batch: 100 avg loss -2.811048 avg loss no lamb -2.811048 time 2020-06-26 17:24:52.254165
Model ind 665 epoch 862 batch: 200 avg loss -2.902152 avg loss no lamb -2.902152 time 2020-06-26 17:25:03.551964
Model ind 665 epoch 862 batch: 300 avg loss -2.871967 avg loss no lamb -2.871967 time 2020-06-26 17:25:14.453825
Model ind 665 epoch 862 batch: 400 avg loss -2.805300 avg loss no lamb -2.805300 time 2020-06-26 17:25:25.459100
Model ind 665 epoch 862 batch: 500 avg loss -2.865755 avg loss no lamb -2.865755 time 2020-06-26 17:25:36.354496
Model ind 665 epoch 862 batch: 600 avg loss -2.848642 avg loss no lamb -2.848642 time 2020-06-26 17:25:47.276550
Model ind 665 epoch 862 batch: 700 avg loss -2.751415 avg loss no lamb -2.751415 time 2020-06-26 17:25:58.274603
Model ind 665 epoch 862 batch: 800 avg loss -2.820637 avg loss no lamb -2.820637 time 2020-06-26 17:26:09.175446
last batch sz 10
Pre: time 2020-06-26 17:26:23.488497: 
 	std: 0.0033037043
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9812, 0.974, 0.9821, 0.9762]
	train_accs: [0.98158336, 0.9808667, 0.97571665, 0.98185, 0.97675]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97903997
	best: 0.9821

Starting e_i: 863
Model ind 665 epoch 863 batch: 0 avg loss -2.951749 avg loss no lamb -2.951749 time 2020-06-26 17:26:24.600146
Model ind 665 epoch 863 batch: 100 avg loss -2.861395 avg loss no lamb -2.861395 time 2020-06-26 17:26:35.447431
Model ind 665 epoch 863 batch: 200 avg loss -2.832774 avg loss no lamb -2.832774 time 2020-06-26 17:26:46.394288
Model ind 665 epoch 863 batch: 300 avg loss -2.822966 avg loss no lamb -2.822966 time 2020-06-26 17:26:57.519190
Model ind 665 epoch 863 batch: 400 avg loss -2.781121 avg loss no lamb -2.781121 time 2020-06-26 17:27:08.493688
Model ind 665 epoch 863 batch: 500 avg loss -2.820119 avg loss no lamb -2.820119 time 2020-06-26 17:27:19.448030
Model ind 665 epoch 863 batch: 600 avg loss -2.871198 avg loss no lamb -2.871198 time 2020-06-26 17:27:30.512184
Model ind 665 epoch 863 batch: 700 avg loss -2.752496 avg loss no lamb -2.752496 time 2020-06-26 17:27:41.654408
Model ind 665 epoch 863 batch: 800 avg loss -2.903872 avg loss no lamb -2.903872 time 2020-06-26 17:27:52.757911
last batch sz 10
Pre: time 2020-06-26 17:28:06.763175: 
 	std: 0.0029387015
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9805, 0.9743, 0.9813, 0.9766]
	train_accs: [0.9818, 0.981, 0.9755667, 0.98193336, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.9789001
	best: 0.9813

Starting e_i: 864
Model ind 665 epoch 864 batch: 0 avg loss -2.944461 avg loss no lamb -2.944461 time 2020-06-26 17:28:07.773286
Model ind 665 epoch 864 batch: 100 avg loss -2.881665 avg loss no lamb -2.881665 time 2020-06-26 17:28:18.759679
Model ind 665 epoch 864 batch: 200 avg loss -2.883082 avg loss no lamb -2.883082 time 2020-06-26 17:28:29.965386
Model ind 665 epoch 864 batch: 300 avg loss -2.874657 avg loss no lamb -2.874657 time 2020-06-26 17:28:40.835999
Model ind 665 epoch 864 batch: 400 avg loss -2.748090 avg loss no lamb -2.748090 time 2020-06-26 17:28:51.360906
Model ind 665 epoch 864 batch: 500 avg loss -2.848811 avg loss no lamb -2.848811 time 2020-06-26 17:29:02.272332
Model ind 665 epoch 864 batch: 600 avg loss -2.879911 avg loss no lamb -2.879911 time 2020-06-26 17:29:13.264951
Model ind 665 epoch 864 batch: 700 avg loss -2.682421 avg loss no lamb -2.682421 time 2020-06-26 17:29:24.177582
Model ind 665 epoch 864 batch: 800 avg loss -2.890339 avg loss no lamb -2.890339 time 2020-06-26 17:29:34.983806
last batch sz 10
Pre: time 2020-06-26 17:29:49.475718: 
 	std: 0.0038765906
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.981, 0.9732, 0.9827, 0.9756]
	train_accs: [0.9821333, 0.98106664, 0.97531664, 0.98233336, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.9790001
	best: 0.9827

Starting e_i: 865
Model ind 665 epoch 865 batch: 0 avg loss -2.935776 avg loss no lamb -2.935776 time 2020-06-26 17:29:50.698297
Model ind 665 epoch 865 batch: 100 avg loss -2.945479 avg loss no lamb -2.945479 time 2020-06-26 17:30:01.873426
Model ind 665 epoch 865 batch: 200 avg loss -2.857793 avg loss no lamb -2.857793 time 2020-06-26 17:30:12.831125
Model ind 665 epoch 865 batch: 300 avg loss -2.893187 avg loss no lamb -2.893187 time 2020-06-26 17:30:23.673949
Model ind 665 epoch 865 batch: 400 avg loss -2.791073 avg loss no lamb -2.791073 time 2020-06-26 17:30:34.429092
Model ind 665 epoch 865 batch: 500 avg loss -2.843817 avg loss no lamb -2.843817 time 2020-06-26 17:30:45.394476
Model ind 665 epoch 865 batch: 600 avg loss -2.856609 avg loss no lamb -2.856609 time 2020-06-26 17:30:56.250272
Model ind 665 epoch 865 batch: 700 avg loss -2.766549 avg loss no lamb -2.766549 time 2020-06-26 17:31:06.896965
Model ind 665 epoch 865 batch: 800 avg loss -2.857390 avg loss no lamb -2.857390 time 2020-06-26 17:31:17.797166
last batch sz 10
Pre: time 2020-06-26 17:31:31.859961: 
 	std: 0.0026480204
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.981, 0.9752, 0.9815, 0.9764]
	train_accs: [0.98151666, 0.98116666, 0.97595, 0.98188335, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.979
	best: 0.9815

Starting e_i: 866
Model ind 665 epoch 866 batch: 0 avg loss -2.943032 avg loss no lamb -2.943032 time 2020-06-26 17:31:32.854271
Model ind 665 epoch 866 batch: 100 avg loss -2.943780 avg loss no lamb -2.943780 time 2020-06-26 17:31:43.941960
Model ind 665 epoch 866 batch: 200 avg loss -2.908441 avg loss no lamb -2.908441 time 2020-06-26 17:31:54.681667
Model ind 665 epoch 866 batch: 300 avg loss -2.859143 avg loss no lamb -2.859143 time 2020-06-26 17:32:05.506782
Model ind 665 epoch 866 batch: 400 avg loss -2.837217 avg loss no lamb -2.837217 time 2020-06-26 17:32:16.399264
Model ind 665 epoch 866 batch: 500 avg loss -2.842473 avg loss no lamb -2.842473 time 2020-06-26 17:32:27.277570
Model ind 665 epoch 866 batch: 600 avg loss -2.864810 avg loss no lamb -2.864810 time 2020-06-26 17:32:38.171676
Model ind 665 epoch 866 batch: 700 avg loss -2.743784 avg loss no lamb -2.743784 time 2020-06-26 17:32:49.162309
Model ind 665 epoch 866 batch: 800 avg loss -2.819225 avg loss no lamb -2.819225 time 2020-06-26 17:32:59.835995
last batch sz 10
Pre: time 2020-06-26 17:33:14.197850: 
 	std: 0.003044994
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9793, 0.9738, 0.981, 0.9752]
	train_accs: [0.9813333, 0.97995, 0.9756, 0.9813167, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.9781
	best: 0.9812

Starting e_i: 867
Model ind 665 epoch 867 batch: 0 avg loss -2.912061 avg loss no lamb -2.912061 time 2020-06-26 17:33:15.338298
Model ind 665 epoch 867 batch: 100 avg loss -2.884728 avg loss no lamb -2.884728 time 2020-06-26 17:33:26.379674
Model ind 665 epoch 867 batch: 200 avg loss -2.827958 avg loss no lamb -2.827958 time 2020-06-26 17:33:37.288401
Model ind 665 epoch 867 batch: 300 avg loss -2.845527 avg loss no lamb -2.845527 time 2020-06-26 17:33:47.882458
Model ind 665 epoch 867 batch: 400 avg loss -2.741206 avg loss no lamb -2.741206 time 2020-06-26 17:33:58.626815
Model ind 665 epoch 867 batch: 500 avg loss -2.862216 avg loss no lamb -2.862216 time 2020-06-26 17:34:09.650300
Model ind 665 epoch 867 batch: 600 avg loss -2.876508 avg loss no lamb -2.876508 time 2020-06-26 17:34:20.557344
Model ind 665 epoch 867 batch: 700 avg loss -2.724313 avg loss no lamb -2.724313 time 2020-06-26 17:34:31.362577
Model ind 665 epoch 867 batch: 800 avg loss -2.834113 avg loss no lamb -2.834113 time 2020-06-26 17:34:42.286370
last batch sz 10
Pre: time 2020-06-26 17:34:56.631387: 
 	std: 0.0036119875
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9804, 0.973, 0.9816, 0.9748]
	train_accs: [0.98141664, 0.98105, 0.9752667, 0.98176664, 0.9763]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97824
	best: 0.9816

Starting e_i: 868
Model ind 665 epoch 868 batch: 0 avg loss -2.962494 avg loss no lamb -2.962494 time 2020-06-26 17:34:57.617118
Model ind 665 epoch 868 batch: 100 avg loss -2.888689 avg loss no lamb -2.888689 time 2020-06-26 17:35:08.589116
Model ind 665 epoch 868 batch: 200 avg loss -2.922361 avg loss no lamb -2.922361 time 2020-06-26 17:35:19.585175
Model ind 665 epoch 868 batch: 300 avg loss -2.840374 avg loss no lamb -2.840374 time 2020-06-26 17:35:30.726053
Model ind 665 epoch 868 batch: 400 avg loss -2.675089 avg loss no lamb -2.675089 time 2020-06-26 17:35:41.948675
Model ind 665 epoch 868 batch: 500 avg loss -2.878735 avg loss no lamb -2.878735 time 2020-06-26 17:35:52.829374
Model ind 665 epoch 868 batch: 600 avg loss -2.876008 avg loss no lamb -2.876008 time 2020-06-26 17:36:03.722986
Model ind 665 epoch 868 batch: 700 avg loss -2.747148 avg loss no lamb -2.747148 time 2020-06-26 17:36:14.836310
Model ind 665 epoch 868 batch: 800 avg loss -2.854902 avg loss no lamb -2.854902 time 2020-06-26 17:36:25.813145
last batch sz 10
Pre: time 2020-06-26 17:36:40.160437: 
 	std: 0.003389739
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.981, 0.9753, 0.983, 0.9757]
	train_accs: [0.98193336, 0.98153335, 0.9759333, 0.9822, 0.9768]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97956
	best: 0.983

Starting e_i: 869
Model ind 665 epoch 869 batch: 0 avg loss -2.947289 avg loss no lamb -2.947289 time 2020-06-26 17:36:41.275321
Model ind 665 epoch 869 batch: 100 avg loss -2.907602 avg loss no lamb -2.907602 time 2020-06-26 17:36:52.052714
Model ind 665 epoch 869 batch: 200 avg loss -2.857809 avg loss no lamb -2.857809 time 2020-06-26 17:37:03.047207
Model ind 665 epoch 869 batch: 300 avg loss -2.812796 avg loss no lamb -2.812796 time 2020-06-26 17:37:13.815679
Model ind 665 epoch 869 batch: 400 avg loss -2.752782 avg loss no lamb -2.752782 time 2020-06-26 17:37:24.864233
Model ind 665 epoch 869 batch: 500 avg loss -2.886321 avg loss no lamb -2.886321 time 2020-06-26 17:37:35.871745
Model ind 665 epoch 869 batch: 600 avg loss -2.844846 avg loss no lamb -2.844846 time 2020-06-26 17:37:46.686134
Model ind 665 epoch 869 batch: 700 avg loss -2.767572 avg loss no lamb -2.767572 time 2020-06-26 17:37:57.450109
Model ind 665 epoch 869 batch: 800 avg loss -2.864555 avg loss no lamb -2.864555 time 2020-06-26 17:38:08.300338
last batch sz 10
Pre: time 2020-06-26 17:38:22.412775: 
 	std: 0.0035567384
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.981, 0.9744, 0.9819, 0.9742]
	train_accs: [0.98143333, 0.98105, 0.9751, 0.9813833, 0.9756167]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97863996
	best: 0.9817

Starting e_i: 870
Model ind 665 epoch 870 batch: 0 avg loss -2.968917 avg loss no lamb -2.968917 time 2020-06-26 17:38:23.434899
Model ind 665 epoch 870 batch: 100 avg loss -2.944713 avg loss no lamb -2.944713 time 2020-06-26 17:38:34.295329
Model ind 665 epoch 870 batch: 200 avg loss -2.851466 avg loss no lamb -2.851466 time 2020-06-26 17:38:44.987502
Model ind 665 epoch 870 batch: 300 avg loss -2.833571 avg loss no lamb -2.833571 time 2020-06-26 17:38:55.925559
Model ind 665 epoch 870 batch: 400 avg loss -2.778631 avg loss no lamb -2.778631 time 2020-06-26 17:39:07.159213
Model ind 665 epoch 870 batch: 500 avg loss -2.869290 avg loss no lamb -2.869290 time 2020-06-26 17:39:18.286824
Model ind 665 epoch 870 batch: 600 avg loss -2.860426 avg loss no lamb -2.860426 time 2020-06-26 17:39:29.164647
Model ind 665 epoch 870 batch: 700 avg loss -2.732033 avg loss no lamb -2.732033 time 2020-06-26 17:39:39.973324
Model ind 665 epoch 870 batch: 800 avg loss -2.882685 avg loss no lamb -2.882685 time 2020-06-26 17:39:50.865977
last batch sz 10
Pre: time 2020-06-26 17:40:05.316667: 
 	std: 0.0031480808
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9807, 0.9745, 0.9825, 0.9761]
	train_accs: [0.9815, 0.9809667, 0.97566664, 0.9817, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97904
	best: 0.9825

Starting e_i: 871
Model ind 665 epoch 871 batch: 0 avg loss -2.977657 avg loss no lamb -2.977657 time 2020-06-26 17:40:07.654760
Model ind 665 epoch 871 batch: 100 avg loss -2.881416 avg loss no lamb -2.881416 time 2020-06-26 17:40:18.380513
Model ind 665 epoch 871 batch: 200 avg loss -2.757360 avg loss no lamb -2.757360 time 2020-06-26 17:40:29.087430
Model ind 665 epoch 871 batch: 300 avg loss -2.852759 avg loss no lamb -2.852759 time 2020-06-26 17:40:40.032803
Model ind 665 epoch 871 batch: 400 avg loss -2.788141 avg loss no lamb -2.788141 time 2020-06-26 17:40:51.289867
Model ind 665 epoch 871 batch: 500 avg loss -2.834813 avg loss no lamb -2.834813 time 2020-06-26 17:41:02.126447
Model ind 665 epoch 871 batch: 600 avg loss -2.847157 avg loss no lamb -2.847157 time 2020-06-26 17:41:13.227100
Model ind 665 epoch 871 batch: 700 avg loss -2.788409 avg loss no lamb -2.788409 time 2020-06-26 17:41:24.170148
Model ind 665 epoch 871 batch: 800 avg loss -2.897314 avg loss no lamb -2.897314 time 2020-06-26 17:41:35.143419
last batch sz 10
Pre: time 2020-06-26 17:41:49.252016: 
 	std: 0.0026279953
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9812, 0.9756, 0.9808, 0.9759]
	train_accs: [0.98153335, 0.98085, 0.9755333, 0.9813333, 0.976]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.9789599
	best: 0.9813

Starting e_i: 872
Model ind 665 epoch 872 batch: 0 avg loss -2.955246 avg loss no lamb -2.955246 time 2020-06-26 17:41:50.258244
Model ind 665 epoch 872 batch: 100 avg loss -2.817489 avg loss no lamb -2.817489 time 2020-06-26 17:42:01.072782
Model ind 665 epoch 872 batch: 200 avg loss -2.847889 avg loss no lamb -2.847889 time 2020-06-26 17:42:11.855386
Model ind 665 epoch 872 batch: 300 avg loss -2.851405 avg loss no lamb -2.851405 time 2020-06-26 17:42:22.533204
Model ind 665 epoch 872 batch: 400 avg loss -2.793538 avg loss no lamb -2.793538 time 2020-06-26 17:42:33.370004
Model ind 665 epoch 872 batch: 500 avg loss -2.823520 avg loss no lamb -2.823520 time 2020-06-26 17:42:44.142393
Model ind 665 epoch 872 batch: 600 avg loss -2.858707 avg loss no lamb -2.858707 time 2020-06-26 17:42:55.015159
Model ind 665 epoch 872 batch: 700 avg loss -2.727282 avg loss no lamb -2.727282 time 2020-06-26 17:43:05.677998
Model ind 665 epoch 872 batch: 800 avg loss -2.839331 avg loss no lamb -2.839331 time 2020-06-26 17:43:16.478234
last batch sz 10
Pre: time 2020-06-26 17:43:30.338308: 
 	std: 0.002558443
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.979, 0.974, 0.9792, 0.9742]
	train_accs: [0.98081666, 0.98001665, 0.97456664, 0.9806167, 0.9761]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97721994
	best: 0.9797

Starting e_i: 873
Model ind 665 epoch 873 batch: 0 avg loss -2.941750 avg loss no lamb -2.941750 time 2020-06-26 17:43:31.475835
Model ind 665 epoch 873 batch: 100 avg loss -2.834509 avg loss no lamb -2.834509 time 2020-06-26 17:43:42.338253
Model ind 665 epoch 873 batch: 200 avg loss -2.853065 avg loss no lamb -2.853065 time 2020-06-26 17:43:53.846480
Model ind 665 epoch 873 batch: 300 avg loss -2.888362 avg loss no lamb -2.888362 time 2020-06-26 17:44:04.840853
Model ind 665 epoch 873 batch: 400 avg loss -2.798218 avg loss no lamb -2.798218 time 2020-06-26 17:44:15.744622
Model ind 665 epoch 873 batch: 500 avg loss -2.852395 avg loss no lamb -2.852395 time 2020-06-26 17:44:26.541678
Model ind 665 epoch 873 batch: 600 avg loss -2.848742 avg loss no lamb -2.848742 time 2020-06-26 17:44:37.586866
Model ind 665 epoch 873 batch: 700 avg loss -2.731653 avg loss no lamb -2.731653 time 2020-06-26 17:44:48.538739
Model ind 665 epoch 873 batch: 800 avg loss -2.872804 avg loss no lamb -2.872804 time 2020-06-26 17:44:59.460816
last batch sz 10
Pre: time 2020-06-26 17:45:13.539987: 
 	std: 0.0026560135
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9805, 0.9754, 0.981, 0.9756]
	train_accs: [0.9813, 0.9806167, 0.976, 0.98181665, 0.9767]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.9787399
	best: 0.981

Starting e_i: 874
Model ind 665 epoch 874 batch: 0 avg loss -2.932099 avg loss no lamb -2.932099 time 2020-06-26 17:45:14.512041
Model ind 665 epoch 874 batch: 100 avg loss -2.880209 avg loss no lamb -2.880209 time 2020-06-26 17:45:25.433078
Model ind 665 epoch 874 batch: 200 avg loss -2.810804 avg loss no lamb -2.810804 time 2020-06-26 17:45:36.426882
Model ind 665 epoch 874 batch: 300 avg loss -2.863599 avg loss no lamb -2.863599 time 2020-06-26 17:45:47.251713
Model ind 665 epoch 874 batch: 400 avg loss -2.773591 avg loss no lamb -2.773591 time 2020-06-26 17:45:58.123949
Model ind 665 epoch 874 batch: 500 avg loss -2.808432 avg loss no lamb -2.808432 time 2020-06-26 17:46:09.258570
Model ind 665 epoch 874 batch: 600 avg loss -2.879274 avg loss no lamb -2.879274 time 2020-06-26 17:46:20.197230
Model ind 665 epoch 874 batch: 700 avg loss -2.739742 avg loss no lamb -2.739742 time 2020-06-26 17:46:30.789564
Model ind 665 epoch 874 batch: 800 avg loss -2.902349 avg loss no lamb -2.902349 time 2020-06-26 17:46:41.873069
last batch sz 10
Pre: time 2020-06-26 17:46:56.429624: 
 	std: 0.0030737608
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9793, 0.9732, 0.9799, 0.9737]
	train_accs: [0.9810333, 0.98095, 0.9756, 0.9813833, 0.97555]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97720003
	best: 0.9799

Starting e_i: 875
Model ind 665 epoch 875 batch: 0 avg loss -2.943538 avg loss no lamb -2.943538 time 2020-06-26 17:46:57.662309
Model ind 665 epoch 875 batch: 100 avg loss -2.866649 avg loss no lamb -2.866649 time 2020-06-26 17:47:08.679040
Model ind 665 epoch 875 batch: 200 avg loss -2.875637 avg loss no lamb -2.875637 time 2020-06-26 17:47:19.670293
Model ind 665 epoch 875 batch: 300 avg loss -2.873564 avg loss no lamb -2.873564 time 2020-06-26 17:47:30.749598
Model ind 665 epoch 875 batch: 400 avg loss -2.792841 avg loss no lamb -2.792841 time 2020-06-26 17:47:41.844058
Model ind 665 epoch 875 batch: 500 avg loss -2.830419 avg loss no lamb -2.830419 time 2020-06-26 17:47:52.778073
Model ind 665 epoch 875 batch: 600 avg loss -2.854679 avg loss no lamb -2.854679 time 2020-06-26 17:48:03.511290
Model ind 665 epoch 875 batch: 700 avg loss -2.764104 avg loss no lamb -2.764104 time 2020-06-26 17:48:14.366154
Model ind 665 epoch 875 batch: 800 avg loss -2.837130 avg loss no lamb -2.837130 time 2020-06-26 17:48:25.374945
last batch sz 10
Pre: time 2020-06-26 17:48:39.436134: 
 	std: 0.002934893
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9806, 0.9752, 0.9823, 0.976]
	train_accs: [0.98115, 0.98065, 0.97543335, 0.98135, 0.9762333]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97911996
	best: 0.9823

Starting e_i: 876
Model ind 665 epoch 876 batch: 0 avg loss -2.987139 avg loss no lamb -2.987139 time 2020-06-26 17:48:40.438464
Model ind 665 epoch 876 batch: 100 avg loss -2.915430 avg loss no lamb -2.915430 time 2020-06-26 17:48:51.269002
Model ind 665 epoch 876 batch: 200 avg loss -2.852571 avg loss no lamb -2.852571 time 2020-06-26 17:49:02.151864
Model ind 665 epoch 876 batch: 300 avg loss -2.885836 avg loss no lamb -2.885836 time 2020-06-26 17:49:12.887690
Model ind 665 epoch 876 batch: 400 avg loss -2.671526 avg loss no lamb -2.671526 time 2020-06-26 17:49:23.652386
Model ind 665 epoch 876 batch: 500 avg loss -2.840926 avg loss no lamb -2.840926 time 2020-06-26 17:49:34.577396
Model ind 665 epoch 876 batch: 600 avg loss -2.913402 avg loss no lamb -2.913402 time 2020-06-26 17:49:45.682397
Model ind 665 epoch 876 batch: 700 avg loss -2.767460 avg loss no lamb -2.767460 time 2020-06-26 17:49:56.555507
Model ind 665 epoch 876 batch: 800 avg loss -2.854519 avg loss no lamb -2.854519 time 2020-06-26 17:50:07.507773
last batch sz 10
Pre: time 2020-06-26 17:50:21.303970: 
 	std: 0.0029196003
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9803, 0.9748, 0.9822, 0.9763]
	train_accs: [0.98111665, 0.98045, 0.9759, 0.9816667, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.9790001
	best: 0.9822

Starting e_i: 877
Model ind 665 epoch 877 batch: 0 avg loss -2.958538 avg loss no lamb -2.958538 time 2020-06-26 17:50:22.408723
Model ind 665 epoch 877 batch: 100 avg loss -2.869290 avg loss no lamb -2.869290 time 2020-06-26 17:50:33.572449
Model ind 665 epoch 877 batch: 200 avg loss -2.865618 avg loss no lamb -2.865618 time 2020-06-26 17:50:44.369185
Model ind 665 epoch 877 batch: 300 avg loss -2.905109 avg loss no lamb -2.905109 time 2020-06-26 17:50:55.339216
Model ind 665 epoch 877 batch: 400 avg loss -2.753447 avg loss no lamb -2.753447 time 2020-06-26 17:51:06.395282
Model ind 665 epoch 877 batch: 500 avg loss -2.797948 avg loss no lamb -2.797948 time 2020-06-26 17:51:17.185069
Model ind 665 epoch 877 batch: 600 avg loss -2.906612 avg loss no lamb -2.906612 time 2020-06-26 17:51:28.182787
Model ind 665 epoch 877 batch: 700 avg loss -2.790374 avg loss no lamb -2.790374 time 2020-06-26 17:51:39.064692
Model ind 665 epoch 877 batch: 800 avg loss -2.856462 avg loss no lamb -2.856462 time 2020-06-26 17:51:49.870915
last batch sz 10
Pre: time 2020-06-26 17:52:04.155172: 
 	std: 0.0035682044
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9811, 0.9744, 0.9828, 0.9754]
	train_accs: [0.9816, 0.98095, 0.97546667, 0.9818, 0.976]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.9792
	best: 0.9828

Starting e_i: 878
Model ind 665 epoch 878 batch: 0 avg loss -2.934065 avg loss no lamb -2.934065 time 2020-06-26 17:52:05.130570
Model ind 665 epoch 878 batch: 100 avg loss -2.863242 avg loss no lamb -2.863242 time 2020-06-26 17:52:16.107897
Model ind 665 epoch 878 batch: 200 avg loss -2.840868 avg loss no lamb -2.840868 time 2020-06-26 17:52:27.057212
Model ind 665 epoch 878 batch: 300 avg loss -2.843223 avg loss no lamb -2.843223 time 2020-06-26 17:52:37.970492
Model ind 665 epoch 878 batch: 400 avg loss -2.722492 avg loss no lamb -2.722492 time 2020-06-26 17:52:48.752632
Model ind 665 epoch 878 batch: 500 avg loss -2.849968 avg loss no lamb -2.849968 time 2020-06-26 17:52:59.780165
Model ind 665 epoch 878 batch: 600 avg loss -2.867090 avg loss no lamb -2.867090 time 2020-06-26 17:53:10.669018
Model ind 665 epoch 878 batch: 700 avg loss -2.762521 avg loss no lamb -2.762521 time 2020-06-26 17:53:21.355953
Model ind 665 epoch 878 batch: 800 avg loss -2.850543 avg loss no lamb -2.850543 time 2020-06-26 17:53:31.992434
last batch sz 10
Pre: time 2020-06-26 17:53:45.964220: 
 	std: 0.002746206
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9798, 0.9752, 0.9811, 0.9748]
	train_accs: [0.98141664, 0.9811, 0.976, 0.98143333, 0.97643334]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97832
	best: 0.9811

Starting e_i: 879
Model ind 665 epoch 879 batch: 0 avg loss -2.916462 avg loss no lamb -2.916462 time 2020-06-26 17:53:47.075208
Model ind 665 epoch 879 batch: 100 avg loss -2.876113 avg loss no lamb -2.876113 time 2020-06-26 17:53:58.312783
Model ind 665 epoch 879 batch: 200 avg loss -2.859765 avg loss no lamb -2.859765 time 2020-06-26 17:54:09.158339
Model ind 665 epoch 879 batch: 300 avg loss -2.830693 avg loss no lamb -2.830693 time 2020-06-26 17:54:20.153377
Model ind 665 epoch 879 batch: 400 avg loss -2.752730 avg loss no lamb -2.752730 time 2020-06-26 17:54:31.311898
Model ind 665 epoch 879 batch: 500 avg loss -2.819614 avg loss no lamb -2.819614 time 2020-06-26 17:54:42.147477
Model ind 665 epoch 879 batch: 600 avg loss -2.891954 avg loss no lamb -2.891954 time 2020-06-26 17:54:53.120051
Model ind 665 epoch 879 batch: 700 avg loss -2.790671 avg loss no lamb -2.790671 time 2020-06-26 17:55:04.088597
Model ind 665 epoch 879 batch: 800 avg loss -2.885031 avg loss no lamb -2.885031 time 2020-06-26 17:55:15.058906
last batch sz 10
Pre: time 2020-06-26 17:55:29.148038: 
 	std: 0.0030506488
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9812, 0.9751, 0.9821, 0.9762]
	train_accs: [0.98181665, 0.9813167, 0.9759167, 0.98141664, 0.97716665]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97933996
	best: 0.9821

Starting e_i: 880
Model ind 665 epoch 880 batch: 0 avg loss -2.947738 avg loss no lamb -2.947738 time 2020-06-26 17:55:30.234522
Model ind 665 epoch 880 batch: 100 avg loss -2.815014 avg loss no lamb -2.815014 time 2020-06-26 17:55:41.254209
Model ind 665 epoch 880 batch: 200 avg loss -2.826591 avg loss no lamb -2.826591 time 2020-06-26 17:55:51.930858
Model ind 665 epoch 880 batch: 300 avg loss -2.840141 avg loss no lamb -2.840141 time 2020-06-26 17:56:02.898203
Model ind 665 epoch 880 batch: 400 avg loss -2.712870 avg loss no lamb -2.712870 time 2020-06-26 17:56:13.973946
Model ind 665 epoch 880 batch: 500 avg loss -2.857977 avg loss no lamb -2.857977 time 2020-06-26 17:56:24.669912
Model ind 665 epoch 880 batch: 600 avg loss -2.839920 avg loss no lamb -2.839920 time 2020-06-26 17:56:35.545824
Model ind 665 epoch 880 batch: 700 avg loss -2.753733 avg loss no lamb -2.753733 time 2020-06-26 17:56:46.468521
Model ind 665 epoch 880 batch: 800 avg loss -2.884618 avg loss no lamb -2.884618 time 2020-06-26 17:56:57.490816
last batch sz 10
Pre: time 2020-06-26 17:57:11.710449: 
 	std: 0.0029977292
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9789, 0.9731, 0.98, 0.9742]
	train_accs: [0.98108333, 0.9805167, 0.9755667, 0.98106664, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97726
	best: 0.9801

Starting e_i: 881
Model ind 665 epoch 881 batch: 0 avg loss -2.972447 avg loss no lamb -2.972447 time 2020-06-26 17:57:14.029960
Model ind 665 epoch 881 batch: 100 avg loss -2.901181 avg loss no lamb -2.901181 time 2020-06-26 17:57:24.781266
Model ind 665 epoch 881 batch: 200 avg loss -2.842235 avg loss no lamb -2.842235 time 2020-06-26 17:57:35.921857
Model ind 665 epoch 881 batch: 300 avg loss -2.804296 avg loss no lamb -2.804296 time 2020-06-26 17:57:46.627275
Model ind 665 epoch 881 batch: 400 avg loss -2.771671 avg loss no lamb -2.771671 time 2020-06-26 17:57:57.298948
Model ind 665 epoch 881 batch: 500 avg loss -2.806036 avg loss no lamb -2.806036 time 2020-06-26 17:58:08.270976
Model ind 665 epoch 881 batch: 600 avg loss -2.853549 avg loss no lamb -2.853549 time 2020-06-26 17:58:19.191792
Model ind 665 epoch 881 batch: 700 avg loss -2.738859 avg loss no lamb -2.738859 time 2020-06-26 17:58:29.995209
Model ind 665 epoch 881 batch: 800 avg loss -2.870168 avg loss no lamb -2.870168 time 2020-06-26 17:58:40.672610
last batch sz 10
Pre: time 2020-06-26 17:58:54.531524: 
 	std: 0.0037456057
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9799, 0.9727, 0.9814, 0.9739]
	train_accs: [0.9816333, 0.9806, 0.9745333, 0.98191667, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97782004
	best: 0.9814

Starting e_i: 882
Model ind 665 epoch 882 batch: 0 avg loss -2.952547 avg loss no lamb -2.952547 time 2020-06-26 17:58:55.523518
Model ind 665 epoch 882 batch: 100 avg loss -2.890903 avg loss no lamb -2.890903 time 2020-06-26 17:59:06.757732
Model ind 665 epoch 882 batch: 200 avg loss -2.836080 avg loss no lamb -2.836080 time 2020-06-26 17:59:17.693060
Model ind 665 epoch 882 batch: 300 avg loss -2.854581 avg loss no lamb -2.854581 time 2020-06-26 17:59:28.483539
Model ind 665 epoch 882 batch: 400 avg loss -2.718908 avg loss no lamb -2.718908 time 2020-06-26 17:59:39.297000
Model ind 665 epoch 882 batch: 500 avg loss -2.836385 avg loss no lamb -2.836385 time 2020-06-26 17:59:50.156373
Model ind 665 epoch 882 batch: 600 avg loss -2.843721 avg loss no lamb -2.843721 time 2020-06-26 18:00:01.061784
Model ind 665 epoch 882 batch: 700 avg loss -2.732810 avg loss no lamb -2.732810 time 2020-06-26 18:00:11.964530
Model ind 665 epoch 882 batch: 800 avg loss -2.885585 avg loss no lamb -2.885585 time 2020-06-26 18:00:22.576576
last batch sz 10
Pre: time 2020-06-26 18:00:36.614446: 
 	std: 0.0036913848
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9818, 0.9742, 0.9826, 0.9753]
	train_accs: [0.98246664, 0.9816833, 0.97531664, 0.98246664, 0.97645]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97924006
	best: 0.9823

Starting e_i: 883
Model ind 665 epoch 883 batch: 0 avg loss -2.966139 avg loss no lamb -2.966139 time 2020-06-26 18:00:37.768686
Model ind 665 epoch 883 batch: 100 avg loss -2.869210 avg loss no lamb -2.869210 time 2020-06-26 18:00:48.796946
Model ind 665 epoch 883 batch: 200 avg loss -2.836456 avg loss no lamb -2.836456 time 2020-06-26 18:00:59.754917
Model ind 665 epoch 883 batch: 300 avg loss -2.868430 avg loss no lamb -2.868430 time 2020-06-26 18:01:10.660198
Model ind 665 epoch 883 batch: 400 avg loss -2.765781 avg loss no lamb -2.765781 time 2020-06-26 18:01:21.652467
Model ind 665 epoch 883 batch: 500 avg loss -2.802269 avg loss no lamb -2.802269 time 2020-06-26 18:01:32.636334
Model ind 665 epoch 883 batch: 600 avg loss -2.876286 avg loss no lamb -2.876286 time 2020-06-26 18:01:43.603409
Model ind 665 epoch 883 batch: 700 avg loss -2.739537 avg loss no lamb -2.739537 time 2020-06-26 18:01:54.551343
Model ind 665 epoch 883 batch: 800 avg loss -2.898023 avg loss no lamb -2.898023 time 2020-06-26 18:02:05.285460
last batch sz 10
Pre: time 2020-06-26 18:02:19.203682: 
 	std: 0.0032890162
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9798, 0.9736, 0.9819, 0.9754]
	train_accs: [0.98178333, 0.9805167, 0.97503334, 0.9821, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97838
	best: 0.9819

Starting e_i: 884
Model ind 665 epoch 884 batch: 0 avg loss -2.941190 avg loss no lamb -2.941190 time 2020-06-26 18:02:20.197078
Model ind 665 epoch 884 batch: 100 avg loss -2.927665 avg loss no lamb -2.927665 time 2020-06-26 18:02:31.158935
Model ind 665 epoch 884 batch: 200 avg loss -2.865526 avg loss no lamb -2.865526 time 2020-06-26 18:02:42.052226
Model ind 665 epoch 884 batch: 300 avg loss -2.848889 avg loss no lamb -2.848889 time 2020-06-26 18:02:52.844978
Model ind 665 epoch 884 batch: 400 avg loss -2.741065 avg loss no lamb -2.741065 time 2020-06-26 18:03:04.063542
Model ind 665 epoch 884 batch: 500 avg loss -2.848171 avg loss no lamb -2.848171 time 2020-06-26 18:03:15.028910
Model ind 665 epoch 884 batch: 600 avg loss -2.906645 avg loss no lamb -2.906645 time 2020-06-26 18:03:25.792051
Model ind 665 epoch 884 batch: 700 avg loss -2.775351 avg loss no lamb -2.775351 time 2020-06-26 18:03:36.491674
Model ind 665 epoch 884 batch: 800 avg loss -2.863191 avg loss no lamb -2.863191 time 2020-06-26 18:03:47.071838
last batch sz 10
Pre: time 2020-06-26 18:04:01.131956: 
 	std: 0.0036103155
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9808, 0.9741, 0.982, 0.9744]
	train_accs: [0.9819667, 0.98106664, 0.9762667, 0.98211664, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97863996
	best: 0.982

Starting e_i: 885
Model ind 665 epoch 885 batch: 0 avg loss -2.908991 avg loss no lamb -2.908991 time 2020-06-26 18:04:02.275889
Model ind 665 epoch 885 batch: 100 avg loss -2.841448 avg loss no lamb -2.841448 time 2020-06-26 18:04:13.072227
Model ind 665 epoch 885 batch: 200 avg loss -2.845404 avg loss no lamb -2.845404 time 2020-06-26 18:04:24.069323
Model ind 665 epoch 885 batch: 300 avg loss -2.853536 avg loss no lamb -2.853536 time 2020-06-26 18:04:34.837035
Model ind 665 epoch 885 batch: 400 avg loss -2.788520 avg loss no lamb -2.788520 time 2020-06-26 18:04:45.748217
Model ind 665 epoch 885 batch: 500 avg loss -2.815998 avg loss no lamb -2.815998 time 2020-06-26 18:04:56.673896
Model ind 665 epoch 885 batch: 600 avg loss -2.866078 avg loss no lamb -2.866078 time 2020-06-26 18:05:07.667241
Model ind 665 epoch 885 batch: 700 avg loss -2.707480 avg loss no lamb -2.707480 time 2020-06-26 18:05:18.551864
Model ind 665 epoch 885 batch: 800 avg loss -2.874928 avg loss no lamb -2.874928 time 2020-06-26 18:05:29.164752
last batch sz 10
Pre: time 2020-06-26 18:05:43.354869: 
 	std: 0.0032224106
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9812, 0.9746, 0.9821, 0.975]
	train_accs: [0.9816333, 0.98145, 0.9763333, 0.98221666, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.9787
	best: 0.9821

Starting e_i: 886
Model ind 665 epoch 886 batch: 0 avg loss -2.937750 avg loss no lamb -2.937750 time 2020-06-26 18:05:44.391334
Model ind 665 epoch 886 batch: 100 avg loss -2.862768 avg loss no lamb -2.862768 time 2020-06-26 18:05:55.381944
Model ind 665 epoch 886 batch: 200 avg loss -2.868980 avg loss no lamb -2.868980 time 2020-06-26 18:06:06.442914
Model ind 665 epoch 886 batch: 300 avg loss -2.877041 avg loss no lamb -2.877041 time 2020-06-26 18:06:17.186671
Model ind 665 epoch 886 batch: 400 avg loss -2.740145 avg loss no lamb -2.740145 time 2020-06-26 18:06:28.187763
Model ind 665 epoch 886 batch: 500 avg loss -2.802530 avg loss no lamb -2.802530 time 2020-06-26 18:06:39.048324
Model ind 665 epoch 886 batch: 600 avg loss -2.872294 avg loss no lamb -2.872294 time 2020-06-26 18:06:50.048323
Model ind 665 epoch 886 batch: 700 avg loss -2.790312 avg loss no lamb -2.790312 time 2020-06-26 18:07:01.064286
Model ind 665 epoch 886 batch: 800 avg loss -2.810186 avg loss no lamb -2.810186 time 2020-06-26 18:07:11.880128
last batch sz 10
Pre: time 2020-06-26 18:07:25.798124: 
 	std: 0.0033938815
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.98, 0.9741, 0.9821, 0.9751]
	train_accs: [0.98228335, 0.98118335, 0.97573334, 0.9822, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97863996
	best: 0.9819

Starting e_i: 887
Model ind 665 epoch 887 batch: 0 avg loss -2.939394 avg loss no lamb -2.939394 time 2020-06-26 18:07:26.877436
Model ind 665 epoch 887 batch: 100 avg loss -2.875725 avg loss no lamb -2.875725 time 2020-06-26 18:07:37.632615
Model ind 665 epoch 887 batch: 200 avg loss -2.874078 avg loss no lamb -2.874078 time 2020-06-26 18:07:48.504267
Model ind 665 epoch 887 batch: 300 avg loss -2.877187 avg loss no lamb -2.877187 time 2020-06-26 18:07:59.369472
Model ind 665 epoch 887 batch: 400 avg loss -2.774277 avg loss no lamb -2.774277 time 2020-06-26 18:08:10.263798
Model ind 665 epoch 887 batch: 500 avg loss -2.880934 avg loss no lamb -2.880934 time 2020-06-26 18:08:21.105486
Model ind 665 epoch 887 batch: 600 avg loss -2.833117 avg loss no lamb -2.833117 time 2020-06-26 18:08:32.080609
Model ind 665 epoch 887 batch: 700 avg loss -2.717968 avg loss no lamb -2.717968 time 2020-06-26 18:08:43.165276
Model ind 665 epoch 887 batch: 800 avg loss -2.842406 avg loss no lamb -2.842406 time 2020-06-26 18:08:54.257929
last batch sz 10
Pre: time 2020-06-26 18:09:08.251116: 
 	std: 0.0031972497
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.981, 0.9748, 0.982, 0.9749]
	train_accs: [0.9812667, 0.9808, 0.97578335, 0.98155, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.9787399
	best: 0.982

Starting e_i: 888
Model ind 665 epoch 888 batch: 0 avg loss -2.951221 avg loss no lamb -2.951221 time 2020-06-26 18:09:09.244283
Model ind 665 epoch 888 batch: 100 avg loss -2.868460 avg loss no lamb -2.868460 time 2020-06-26 18:09:20.149837
Model ind 665 epoch 888 batch: 200 avg loss -2.925770 avg loss no lamb -2.925770 time 2020-06-26 18:09:30.973623
Model ind 665 epoch 888 batch: 300 avg loss -2.855747 avg loss no lamb -2.855747 time 2020-06-26 18:09:42.005982
Model ind 665 epoch 888 batch: 400 avg loss -2.799080 avg loss no lamb -2.799080 time 2020-06-26 18:09:52.832686
Model ind 665 epoch 888 batch: 500 avg loss -2.885806 avg loss no lamb -2.885806 time 2020-06-26 18:10:03.898175
Model ind 665 epoch 888 batch: 600 avg loss -2.882266 avg loss no lamb -2.882266 time 2020-06-26 18:10:14.688160
Model ind 665 epoch 888 batch: 700 avg loss -2.718299 avg loss no lamb -2.718299 time 2020-06-26 18:10:25.481184
Model ind 665 epoch 888 batch: 800 avg loss -2.811822 avg loss no lamb -2.811822 time 2020-06-26 18:10:36.490071
last batch sz 10
Pre: time 2020-06-26 18:10:50.520183: 
 	std: 0.0031502247
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9798, 0.9743, 0.9819, 0.9752]
	train_accs: [0.98155, 0.9809167, 0.97571665, 0.98156667, 0.97603333]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.9785
	best: 0.9819

Starting e_i: 889
Model ind 665 epoch 889 batch: 0 avg loss -2.900945 avg loss no lamb -2.900945 time 2020-06-26 18:10:51.682787
Model ind 665 epoch 889 batch: 100 avg loss -2.890272 avg loss no lamb -2.890272 time 2020-06-26 18:11:03.195200
Model ind 665 epoch 889 batch: 200 avg loss -2.854626 avg loss no lamb -2.854626 time 2020-06-26 18:11:14.165464
Model ind 665 epoch 889 batch: 300 avg loss -2.873413 avg loss no lamb -2.873413 time 2020-06-26 18:11:25.147075
Model ind 665 epoch 889 batch: 400 avg loss -2.785617 avg loss no lamb -2.785617 time 2020-06-26 18:11:35.895199
Model ind 665 epoch 889 batch: 500 avg loss -2.828394 avg loss no lamb -2.828394 time 2020-06-26 18:11:46.814251
Model ind 665 epoch 889 batch: 600 avg loss -2.886185 avg loss no lamb -2.886185 time 2020-06-26 18:11:57.904211
Model ind 665 epoch 889 batch: 700 avg loss -2.697828 avg loss no lamb -2.697828 time 2020-06-26 18:12:08.906658
Model ind 665 epoch 889 batch: 800 avg loss -2.850481 avg loss no lamb -2.850481 time 2020-06-26 18:12:19.773277
last batch sz 10
Pre: time 2020-06-26 18:12:33.910597: 
 	std: 0.0031673482
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.981, 0.9754, 0.982, 0.9747]
	train_accs: [0.98145, 0.98066664, 0.9756333, 0.9815, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97889996
	best: 0.982

Starting e_i: 890
Model ind 665 epoch 890 batch: 0 avg loss -2.930998 avg loss no lamb -2.930998 time 2020-06-26 18:12:34.917025
Model ind 665 epoch 890 batch: 100 avg loss -2.883200 avg loss no lamb -2.883200 time 2020-06-26 18:12:45.698181
Model ind 665 epoch 890 batch: 200 avg loss -2.899164 avg loss no lamb -2.899164 time 2020-06-26 18:12:56.459275
Model ind 665 epoch 890 batch: 300 avg loss -2.887827 avg loss no lamb -2.887827 time 2020-06-26 18:13:07.293912
Model ind 665 epoch 890 batch: 400 avg loss -2.743513 avg loss no lamb -2.743513 time 2020-06-26 18:13:18.154084
Model ind 665 epoch 890 batch: 500 avg loss -2.840118 avg loss no lamb -2.840118 time 2020-06-26 18:13:29.091769
Model ind 665 epoch 890 batch: 600 avg loss -2.895623 avg loss no lamb -2.895623 time 2020-06-26 18:13:40.036486
Model ind 665 epoch 890 batch: 700 avg loss -2.740876 avg loss no lamb -2.740876 time 2020-06-26 18:13:50.859916
Model ind 665 epoch 890 batch: 800 avg loss -2.797322 avg loss no lamb -2.797322 time 2020-06-26 18:14:01.570676
last batch sz 10
Pre: time 2020-06-26 18:14:15.687057: 
 	std: 0.0029065525
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9815, 0.9755, 0.9813, 0.9758]
	train_accs: [0.9820833, 0.9816167, 0.97641665, 0.98223335, 0.97711664]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.9792
	best: 0.9813

Starting e_i: 891
Model ind 665 epoch 891 batch: 0 avg loss -2.960241 avg loss no lamb -2.960241 time 2020-06-26 18:14:17.987207
Model ind 665 epoch 891 batch: 100 avg loss -2.855602 avg loss no lamb -2.855602 time 2020-06-26 18:14:29.069875
Model ind 665 epoch 891 batch: 200 avg loss -2.854547 avg loss no lamb -2.854547 time 2020-06-26 18:14:40.048473
Model ind 665 epoch 891 batch: 300 avg loss -2.896168 avg loss no lamb -2.896168 time 2020-06-26 18:14:50.959247
Model ind 665 epoch 891 batch: 400 avg loss -2.743556 avg loss no lamb -2.743556 time 2020-06-26 18:15:01.951834
Model ind 665 epoch 891 batch: 500 avg loss -2.783368 avg loss no lamb -2.783368 time 2020-06-26 18:15:12.855809
Model ind 665 epoch 891 batch: 600 avg loss -2.869908 avg loss no lamb -2.869908 time 2020-06-26 18:15:23.919766
Model ind 665 epoch 891 batch: 700 avg loss -2.710874 avg loss no lamb -2.710874 time 2020-06-26 18:15:34.779457
Model ind 665 epoch 891 batch: 800 avg loss -2.869049 avg loss no lamb -2.869049 time 2020-06-26 18:15:45.744065
last batch sz 10
Pre: time 2020-06-26 18:15:59.579479: 
 	std: 0.0022622119
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9785, 0.9775, 0.9732, 0.9784, 0.974]
	train_accs: [0.9809, 0.9791667, 0.9747, 0.98073334, 0.97575]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.9763201
	best: 0.9785

Starting e_i: 892
Model ind 665 epoch 892 batch: 0 avg loss -2.919845 avg loss no lamb -2.919845 time 2020-06-26 18:16:00.598477
Model ind 665 epoch 892 batch: 100 avg loss -2.867436 avg loss no lamb -2.867436 time 2020-06-26 18:16:11.363606
Model ind 665 epoch 892 batch: 200 avg loss -2.877165 avg loss no lamb -2.877165 time 2020-06-26 18:16:22.246745
Model ind 665 epoch 892 batch: 300 avg loss -2.863159 avg loss no lamb -2.863159 time 2020-06-26 18:16:33.141170
Model ind 665 epoch 892 batch: 400 avg loss -2.825822 avg loss no lamb -2.825822 time 2020-06-26 18:16:43.946491
Model ind 665 epoch 892 batch: 500 avg loss -2.831676 avg loss no lamb -2.831676 time 2020-06-26 18:16:54.680891
Model ind 665 epoch 892 batch: 600 avg loss -2.858773 avg loss no lamb -2.858773 time 2020-06-26 18:17:05.627458
Model ind 665 epoch 892 batch: 700 avg loss -2.740004 avg loss no lamb -2.740004 time 2020-06-26 18:17:16.571684
Model ind 665 epoch 892 batch: 800 avg loss -2.862779 avg loss no lamb -2.862779 time 2020-06-26 18:17:27.477151
last batch sz 10
Pre: time 2020-06-26 18:17:41.550603: 
 	std: 0.0028366218
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9805, 0.9744, 0.9818, 0.9764]
	train_accs: [0.9816667, 0.9813167, 0.97625, 0.9817167, 0.97725]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97874004
	best: 0.9818

Starting e_i: 893
Model ind 665 epoch 893 batch: 0 avg loss -2.917686 avg loss no lamb -2.917686 time 2020-06-26 18:17:42.671390
Model ind 665 epoch 893 batch: 100 avg loss -2.877975 avg loss no lamb -2.877975 time 2020-06-26 18:17:53.541097
Model ind 665 epoch 893 batch: 200 avg loss -2.842635 avg loss no lamb -2.842635 time 2020-06-26 18:18:04.541467
Model ind 665 epoch 893 batch: 300 avg loss -2.873419 avg loss no lamb -2.873419 time 2020-06-26 18:18:15.581323
Model ind 665 epoch 893 batch: 400 avg loss -2.779806 avg loss no lamb -2.779806 time 2020-06-26 18:18:26.541212
Model ind 665 epoch 893 batch: 500 avg loss -2.874198 avg loss no lamb -2.874198 time 2020-06-26 18:18:37.353848
Model ind 665 epoch 893 batch: 600 avg loss -2.836028 avg loss no lamb -2.836028 time 2020-06-26 18:18:48.559917
Model ind 665 epoch 893 batch: 700 avg loss -2.758425 avg loss no lamb -2.758425 time 2020-06-26 18:18:59.340140
Model ind 665 epoch 893 batch: 800 avg loss -2.796229 avg loss no lamb -2.796229 time 2020-06-26 18:19:10.228503
last batch sz 10
Pre: time 2020-06-26 18:19:24.440139: 
 	std: 0.0027213234
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.98, 0.9752, 0.9821, 0.9767]
	train_accs: [0.9819667, 0.9809333, 0.9766, 0.98211664, 0.97715]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97911996
	best: 0.9821

Starting e_i: 894
Model ind 665 epoch 894 batch: 0 avg loss -2.983349 avg loss no lamb -2.983349 time 2020-06-26 18:19:25.429799
Model ind 665 epoch 894 batch: 100 avg loss -2.865352 avg loss no lamb -2.865352 time 2020-06-26 18:19:36.177095
Model ind 665 epoch 894 batch: 200 avg loss -2.793306 avg loss no lamb -2.793306 time 2020-06-26 18:19:46.992345
Model ind 665 epoch 894 batch: 300 avg loss -2.782145 avg loss no lamb -2.782145 time 2020-06-26 18:19:57.781134
Model ind 665 epoch 894 batch: 400 avg loss -2.834316 avg loss no lamb -2.834316 time 2020-06-26 18:20:08.608056
Model ind 665 epoch 894 batch: 500 avg loss -2.817799 avg loss no lamb -2.817799 time 2020-06-26 18:20:19.451259
Model ind 665 epoch 894 batch: 600 avg loss -2.900973 avg loss no lamb -2.900973 time 2020-06-26 18:20:30.550247
Model ind 665 epoch 894 batch: 700 avg loss -2.709501 avg loss no lamb -2.709501 time 2020-06-26 18:20:41.430984
Model ind 665 epoch 894 batch: 800 avg loss -2.866659 avg loss no lamb -2.866659 time 2020-06-26 18:20:52.307351
last batch sz 10
Pre: time 2020-06-26 18:21:06.509774: 
 	std: 0.0032633771
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9802, 0.9741, 0.9809, 0.9741]
	train_accs: [0.9813167, 0.98025, 0.97548336, 0.9813, 0.97613335]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97808
	best: 0.9811

Starting e_i: 895
Model ind 665 epoch 895 batch: 0 avg loss -2.937173 avg loss no lamb -2.937173 time 2020-06-26 18:21:07.710932
Model ind 665 epoch 895 batch: 100 avg loss -2.944079 avg loss no lamb -2.944079 time 2020-06-26 18:21:18.593981
Model ind 665 epoch 895 batch: 200 avg loss -2.832228 avg loss no lamb -2.832228 time 2020-06-26 18:21:29.625799
Model ind 665 epoch 895 batch: 300 avg loss -2.846204 avg loss no lamb -2.846204 time 2020-06-26 18:21:40.472028
Model ind 665 epoch 895 batch: 400 avg loss -2.720860 avg loss no lamb -2.720860 time 2020-06-26 18:21:51.248965
Model ind 665 epoch 895 batch: 500 avg loss -2.800678 avg loss no lamb -2.800678 time 2020-06-26 18:22:02.079169
Model ind 665 epoch 895 batch: 600 avg loss -2.927503 avg loss no lamb -2.927503 time 2020-06-26 18:22:12.687080
Model ind 665 epoch 895 batch: 700 avg loss -2.717296 avg loss no lamb -2.717296 time 2020-06-26 18:22:23.731081
Model ind 665 epoch 895 batch: 800 avg loss -2.825934 avg loss no lamb -2.825934 time 2020-06-26 18:22:34.847497
last batch sz 10
Pre: time 2020-06-26 18:22:48.952410: 
 	std: 0.0027737303
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9785, 0.9738, 0.9799, 0.9746]
	train_accs: [0.98106664, 0.98025, 0.9753, 0.9812667, 0.97595]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97747993
	best: 0.9799

Starting e_i: 896
Model ind 665 epoch 896 batch: 0 avg loss -2.944636 avg loss no lamb -2.944636 time 2020-06-26 18:22:49.963386
Model ind 665 epoch 896 batch: 100 avg loss -2.846504 avg loss no lamb -2.846504 time 2020-06-26 18:23:00.695631
Model ind 665 epoch 896 batch: 200 avg loss -2.853046 avg loss no lamb -2.853046 time 2020-06-26 18:23:11.401599
Model ind 665 epoch 896 batch: 300 avg loss -2.878432 avg loss no lamb -2.878432 time 2020-06-26 18:23:22.350402
Model ind 665 epoch 896 batch: 400 avg loss -2.748152 avg loss no lamb -2.748152 time 2020-06-26 18:23:32.975075
Model ind 665 epoch 896 batch: 500 avg loss -2.843498 avg loss no lamb -2.843498 time 2020-06-26 18:23:43.781184
Model ind 665 epoch 896 batch: 600 avg loss -2.787269 avg loss no lamb -2.787269 time 2020-06-26 18:23:54.711754
Model ind 665 epoch 896 batch: 700 avg loss -2.836195 avg loss no lamb -2.836195 time 2020-06-26 18:24:05.855962
Model ind 665 epoch 896 batch: 800 avg loss -2.893565 avg loss no lamb -2.893565 time 2020-06-26 18:24:16.873263
last batch sz 10
Pre: time 2020-06-26 18:24:30.985473: 
 	std: 0.0031776642
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9806, 0.9743, 0.982, 0.9762]
	train_accs: [0.98188335, 0.9809833, 0.9755, 0.98191667, 0.977]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97902
	best: 0.982

Starting e_i: 897
Model ind 665 epoch 897 batch: 0 avg loss -2.916445 avg loss no lamb -2.916445 time 2020-06-26 18:24:32.142642
Model ind 665 epoch 897 batch: 100 avg loss -2.834982 avg loss no lamb -2.834982 time 2020-06-26 18:24:43.059512
Model ind 665 epoch 897 batch: 200 avg loss -2.845463 avg loss no lamb -2.845463 time 2020-06-26 18:24:53.927464
Model ind 665 epoch 897 batch: 300 avg loss -2.893333 avg loss no lamb -2.893333 time 2020-06-26 18:25:05.028494
Model ind 665 epoch 897 batch: 400 avg loss -2.808264 avg loss no lamb -2.808264 time 2020-06-26 18:25:15.874458
Model ind 665 epoch 897 batch: 500 avg loss -2.857980 avg loss no lamb -2.857980 time 2020-06-26 18:25:26.572237
Model ind 665 epoch 897 batch: 600 avg loss -2.832915 avg loss no lamb -2.832915 time 2020-06-26 18:25:37.227005
Model ind 665 epoch 897 batch: 700 avg loss -2.795850 avg loss no lamb -2.795850 time 2020-06-26 18:25:48.030203
Model ind 665 epoch 897 batch: 800 avg loss -2.829760 avg loss no lamb -2.829760 time 2020-06-26 18:25:58.944715
last batch sz 10
Pre: time 2020-06-26 18:26:13.230012: 
 	std: 0.0027080574
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9795, 0.9747, 0.9808, 0.975]
	train_accs: [0.9813167, 0.98076665, 0.9754, 0.9812833, 0.976]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97811997
	best: 0.9806

Starting e_i: 898
Model ind 665 epoch 898 batch: 0 avg loss -2.959119 avg loss no lamb -2.959119 time 2020-06-26 18:26:14.245506
Model ind 665 epoch 898 batch: 100 avg loss -2.846985 avg loss no lamb -2.846985 time 2020-06-26 18:26:24.905500
Model ind 665 epoch 898 batch: 200 avg loss -2.876092 avg loss no lamb -2.876092 time 2020-06-26 18:26:35.776469
Model ind 665 epoch 898 batch: 300 avg loss -2.884209 avg loss no lamb -2.884209 time 2020-06-26 18:26:46.742302
Model ind 665 epoch 898 batch: 400 avg loss -2.743195 avg loss no lamb -2.743195 time 2020-06-26 18:26:57.486438
Model ind 665 epoch 898 batch: 500 avg loss -2.823490 avg loss no lamb -2.823490 time 2020-06-26 18:27:08.280125
Model ind 665 epoch 898 batch: 600 avg loss -2.841811 avg loss no lamb -2.841811 time 2020-06-26 18:27:19.012171
Model ind 665 epoch 898 batch: 700 avg loss -2.673756 avg loss no lamb -2.673756 time 2020-06-26 18:27:29.582281
Model ind 665 epoch 898 batch: 800 avg loss -2.884268 avg loss no lamb -2.884268 time 2020-06-26 18:27:40.689130
last batch sz 10
Pre: time 2020-06-26 18:27:54.976910: 
 	std: 0.0027327586
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9788, 0.9743, 0.9801, 0.9737]
	train_accs: [0.9805667, 0.9799, 0.9746, 0.9805667, 0.97548336]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.9773
	best: 0.9796

Starting e_i: 899
Model ind 665 epoch 899 batch: 0 avg loss -2.936289 avg loss no lamb -2.936289 time 2020-06-26 18:27:56.096398
Model ind 665 epoch 899 batch: 100 avg loss -2.884005 avg loss no lamb -2.884005 time 2020-06-26 18:28:07.778162
Model ind 665 epoch 899 batch: 200 avg loss -2.764451 avg loss no lamb -2.764451 time 2020-06-26 18:28:18.595327
Model ind 665 epoch 899 batch: 300 avg loss -2.848661 avg loss no lamb -2.848661 time 2020-06-26 18:28:29.198785
Model ind 665 epoch 899 batch: 400 avg loss -2.777895 avg loss no lamb -2.777895 time 2020-06-26 18:28:39.913405
Model ind 665 epoch 899 batch: 500 avg loss -2.853579 avg loss no lamb -2.853579 time 2020-06-26 18:28:50.791580
Model ind 665 epoch 899 batch: 600 avg loss -2.876026 avg loss no lamb -2.876026 time 2020-06-26 18:29:01.484958
Model ind 665 epoch 899 batch: 700 avg loss -2.792389 avg loss no lamb -2.792389 time 2020-06-26 18:29:12.207405
Model ind 665 epoch 899 batch: 800 avg loss -2.861639 avg loss no lamb -2.861639 time 2020-06-26 18:29:23.088259
last batch sz 10
Pre: time 2020-06-26 18:29:37.314929: 
 	std: 0.0025428997
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9827, 0.9809, 0.9768, 0.9828, 0.9775]
	train_accs: [0.9820333, 0.98106664, 0.97693336, 0.98186666, 0.9774]
	best_train_sub_head: 0
	worst: 0.9768
	avg: 0.98014003
	best: 0.9827

Starting e_i: 900
Model ind 665 epoch 900 batch: 0 avg loss -2.916055 avg loss no lamb -2.916055 time 2020-06-26 18:29:38.289594
Model ind 665 epoch 900 batch: 100 avg loss -2.923669 avg loss no lamb -2.923669 time 2020-06-26 18:29:49.224424
Model ind 665 epoch 900 batch: 200 avg loss -2.867809 avg loss no lamb -2.867809 time 2020-06-26 18:30:00.246356
Model ind 665 epoch 900 batch: 300 avg loss -2.870842 avg loss no lamb -2.870842 time 2020-06-26 18:30:10.888199
Model ind 665 epoch 900 batch: 400 avg loss -2.794123 avg loss no lamb -2.794123 time 2020-06-26 18:30:21.560566
Model ind 665 epoch 900 batch: 500 avg loss -2.868808 avg loss no lamb -2.868808 time 2020-06-26 18:30:32.341304
Model ind 665 epoch 900 batch: 600 avg loss -2.862053 avg loss no lamb -2.862053 time 2020-06-26 18:30:43.246408
Model ind 665 epoch 900 batch: 700 avg loss -2.780362 avg loss no lamb -2.780362 time 2020-06-26 18:30:54.086361
Model ind 665 epoch 900 batch: 800 avg loss -2.870526 avg loss no lamb -2.870526 time 2020-06-26 18:31:04.760888
last batch sz 10
Pre: time 2020-06-26 18:31:18.850396: 
 	std: 0.0035171553
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9815, 0.9743, 0.9822, 0.975]
	train_accs: [0.9816667, 0.9809833, 0.9754, 0.98148334, 0.9763]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97894
	best: 0.9817

Starting e_i: 901
Model ind 665 epoch 901 batch: 0 avg loss -2.922325 avg loss no lamb -2.922325 time 2020-06-26 18:31:21.194726
Model ind 665 epoch 901 batch: 100 avg loss -2.868776 avg loss no lamb -2.868776 time 2020-06-26 18:31:32.026035
Model ind 665 epoch 901 batch: 200 avg loss -2.837333 avg loss no lamb -2.837333 time 2020-06-26 18:31:42.843385
Model ind 665 epoch 901 batch: 300 avg loss -2.914422 avg loss no lamb -2.914422 time 2020-06-26 18:31:53.665527
Model ind 665 epoch 901 batch: 400 avg loss -2.814437 avg loss no lamb -2.814437 time 2020-06-26 18:32:04.668315
Model ind 665 epoch 901 batch: 500 avg loss -2.819453 avg loss no lamb -2.819453 time 2020-06-26 18:32:15.244706
Model ind 665 epoch 901 batch: 600 avg loss -2.841953 avg loss no lamb -2.841953 time 2020-06-26 18:32:26.263464
Model ind 665 epoch 901 batch: 700 avg loss -2.769040 avg loss no lamb -2.769040 time 2020-06-26 18:32:37.385993
Model ind 665 epoch 901 batch: 800 avg loss -2.919484 avg loss no lamb -2.919484 time 2020-06-26 18:32:48.743248
last batch sz 10
Pre: time 2020-06-26 18:33:02.966121: 
 	std: 0.002626494
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9796, 0.9752, 0.9815, 0.9759]
	train_accs: [0.98146665, 0.9804, 0.97648335, 0.9816167, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97866
	best: 0.9815

Starting e_i: 902
Model ind 665 epoch 902 batch: 0 avg loss -2.972668 avg loss no lamb -2.972668 time 2020-06-26 18:33:03.999294
Model ind 665 epoch 902 batch: 100 avg loss -2.790857 avg loss no lamb -2.790857 time 2020-06-26 18:33:14.746806
Model ind 665 epoch 902 batch: 200 avg loss -2.864880 avg loss no lamb -2.864880 time 2020-06-26 18:33:25.659036
Model ind 665 epoch 902 batch: 300 avg loss -2.845759 avg loss no lamb -2.845759 time 2020-06-26 18:33:36.514368
Model ind 665 epoch 902 batch: 400 avg loss -2.809409 avg loss no lamb -2.809409 time 2020-06-26 18:33:47.528047
Model ind 665 epoch 902 batch: 500 avg loss -2.871189 avg loss no lamb -2.871189 time 2020-06-26 18:33:58.386023
Model ind 665 epoch 902 batch: 600 avg loss -2.833544 avg loss no lamb -2.833544 time 2020-06-26 18:34:09.470359
Model ind 665 epoch 902 batch: 700 avg loss -2.753971 avg loss no lamb -2.753971 time 2020-06-26 18:34:20.553369
Model ind 665 epoch 902 batch: 800 avg loss -2.810745 avg loss no lamb -2.810745 time 2020-06-26 18:34:31.470020
last batch sz 10
Pre: time 2020-06-26 18:34:45.478912: 
 	std: 0.0030740032
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9805, 0.9743, 0.9816, 0.9753]
	train_accs: [0.98121667, 0.98106664, 0.97585, 0.9817167, 0.9767]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97852004
	best: 0.9816

Starting e_i: 903
Model ind 665 epoch 903 batch: 0 avg loss -2.957591 avg loss no lamb -2.957591 time 2020-06-26 18:34:46.603212
Model ind 665 epoch 903 batch: 100 avg loss -2.841986 avg loss no lamb -2.841986 time 2020-06-26 18:34:57.546013
Model ind 665 epoch 903 batch: 200 avg loss -2.838161 avg loss no lamb -2.838161 time 2020-06-26 18:35:08.550540
Model ind 665 epoch 903 batch: 300 avg loss -2.848661 avg loss no lamb -2.848661 time 2020-06-26 18:35:19.358314
Model ind 665 epoch 903 batch: 400 avg loss -2.766296 avg loss no lamb -2.766296 time 2020-06-26 18:35:30.080225
Model ind 665 epoch 903 batch: 500 avg loss -2.870796 avg loss no lamb -2.870796 time 2020-06-26 18:35:40.917492
Model ind 665 epoch 903 batch: 600 avg loss -2.918052 avg loss no lamb -2.918052 time 2020-06-26 18:35:52.047137
Model ind 665 epoch 903 batch: 700 avg loss -2.716725 avg loss no lamb -2.716725 time 2020-06-26 18:36:03.001369
Model ind 665 epoch 903 batch: 800 avg loss -2.797177 avg loss no lamb -2.797177 time 2020-06-26 18:36:13.844979
last batch sz 10
Pre: time 2020-06-26 18:36:27.990651: 
 	std: 0.0033722548
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9813, 0.9744, 0.9818, 0.9754]
	train_accs: [0.98223335, 0.98158336, 0.9758833, 0.9820667, 0.97685]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.979
	best: 0.9821

Starting e_i: 904
Model ind 665 epoch 904 batch: 0 avg loss -2.971580 avg loss no lamb -2.971580 time 2020-06-26 18:36:29.019364
Model ind 665 epoch 904 batch: 100 avg loss -2.901191 avg loss no lamb -2.901191 time 2020-06-26 18:36:39.920519
Model ind 665 epoch 904 batch: 200 avg loss -2.807461 avg loss no lamb -2.807461 time 2020-06-26 18:36:50.638926
Model ind 665 epoch 904 batch: 300 avg loss -2.864621 avg loss no lamb -2.864621 time 2020-06-26 18:37:01.574443
Model ind 665 epoch 904 batch: 400 avg loss -2.752575 avg loss no lamb -2.752575 time 2020-06-26 18:37:12.355105
Model ind 665 epoch 904 batch: 500 avg loss -2.827299 avg loss no lamb -2.827299 time 2020-06-26 18:37:23.246772
Model ind 665 epoch 904 batch: 600 avg loss -2.880193 avg loss no lamb -2.880193 time 2020-06-26 18:37:33.876971
Model ind 665 epoch 904 batch: 700 avg loss -2.754861 avg loss no lamb -2.754861 time 2020-06-26 18:37:44.941586
Model ind 665 epoch 904 batch: 800 avg loss -2.903039 avg loss no lamb -2.903039 time 2020-06-26 18:37:55.894834
last batch sz 10
Pre: time 2020-06-26 18:38:10.146111: 
 	std: 0.0030179464
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9809, 0.9751, 0.9826, 0.977]
	train_accs: [0.98178333, 0.9813167, 0.97651666, 0.98183334, 0.9774333]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97959995
	best: 0.9826

Starting e_i: 905
Model ind 665 epoch 905 batch: 0 avg loss -2.895595 avg loss no lamb -2.895595 time 2020-06-26 18:38:11.320572
Model ind 665 epoch 905 batch: 100 avg loss -2.872659 avg loss no lamb -2.872659 time 2020-06-26 18:38:22.039585
Model ind 665 epoch 905 batch: 200 avg loss -2.888705 avg loss no lamb -2.888705 time 2020-06-26 18:38:32.921398
Model ind 665 epoch 905 batch: 300 avg loss -2.848653 avg loss no lamb -2.848653 time 2020-06-26 18:38:43.836256
Model ind 665 epoch 905 batch: 400 avg loss -2.786118 avg loss no lamb -2.786118 time 2020-06-26 18:38:54.677224
Model ind 665 epoch 905 batch: 500 avg loss -2.838732 avg loss no lamb -2.838732 time 2020-06-26 18:39:05.185859
Model ind 665 epoch 905 batch: 600 avg loss -2.882332 avg loss no lamb -2.882332 time 2020-06-26 18:39:16.102083
Model ind 665 epoch 905 batch: 700 avg loss -2.764678 avg loss no lamb -2.764678 time 2020-06-26 18:39:27.031996
Model ind 665 epoch 905 batch: 800 avg loss -2.877862 avg loss no lamb -2.877862 time 2020-06-26 18:39:37.901276
last batch sz 10
Pre: time 2020-06-26 18:39:51.891530: 
 	std: 0.0033789964
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.98, 0.9738, 0.9808, 0.9736]
	train_accs: [0.9813167, 0.9808667, 0.9748667, 0.98121667, 0.9755667]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97782004
	best: 0.9809

Starting e_i: 906
Model ind 665 epoch 906 batch: 0 avg loss -2.969699 avg loss no lamb -2.969699 time 2020-06-26 18:39:52.868755
Model ind 665 epoch 906 batch: 100 avg loss -2.865215 avg loss no lamb -2.865215 time 2020-06-26 18:40:03.593337
Model ind 665 epoch 906 batch: 200 avg loss -2.846873 avg loss no lamb -2.846873 time 2020-06-26 18:40:14.279301
Model ind 665 epoch 906 batch: 300 avg loss -2.858314 avg loss no lamb -2.858314 time 2020-06-26 18:40:25.046037
Model ind 665 epoch 906 batch: 400 avg loss -2.805546 avg loss no lamb -2.805546 time 2020-06-26 18:40:35.577543
Model ind 665 epoch 906 batch: 500 avg loss -2.784184 avg loss no lamb -2.784184 time 2020-06-26 18:40:46.243592
Model ind 665 epoch 906 batch: 600 avg loss -2.884062 avg loss no lamb -2.884062 time 2020-06-26 18:40:57.011093
Model ind 665 epoch 906 batch: 700 avg loss -2.757960 avg loss no lamb -2.757960 time 2020-06-26 18:41:08.148236
Model ind 665 epoch 906 batch: 800 avg loss -2.927688 avg loss no lamb -2.927688 time 2020-06-26 18:41:18.906479
last batch sz 10
Pre: time 2020-06-26 18:41:32.808973: 
 	std: 0.0025126769
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9801, 0.9757, 0.9819, 0.9765]
	train_accs: [0.9816, 0.98105, 0.9764, 0.98185, 0.97685]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.9790799
	best: 0.9819

Starting e_i: 907
Model ind 665 epoch 907 batch: 0 avg loss -2.970417 avg loss no lamb -2.970417 time 2020-06-26 18:41:33.989673
Model ind 665 epoch 907 batch: 100 avg loss -2.887569 avg loss no lamb -2.887569 time 2020-06-26 18:41:44.885098
Model ind 665 epoch 907 batch: 200 avg loss -2.822732 avg loss no lamb -2.822732 time 2020-06-26 18:41:55.907981
Model ind 665 epoch 907 batch: 300 avg loss -2.862159 avg loss no lamb -2.862159 time 2020-06-26 18:42:06.983507
Model ind 665 epoch 907 batch: 400 avg loss -2.724650 avg loss no lamb -2.724650 time 2020-06-26 18:42:17.613900
Model ind 665 epoch 907 batch: 500 avg loss -2.860514 avg loss no lamb -2.860514 time 2020-06-26 18:42:28.313706
Model ind 665 epoch 907 batch: 600 avg loss -2.867188 avg loss no lamb -2.867188 time 2020-06-26 18:42:39.211845
Model ind 665 epoch 907 batch: 700 avg loss -2.767461 avg loss no lamb -2.767461 time 2020-06-26 18:42:50.442184
Model ind 665 epoch 907 batch: 800 avg loss -2.827251 avg loss no lamb -2.827251 time 2020-06-26 18:43:01.276071
last batch sz 10
Pre: time 2020-06-26 18:43:15.297784: 
 	std: 0.003515908
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9819, 0.9744, 0.9821, 0.9757]
	train_accs: [0.9819833, 0.9812833, 0.97616667, 0.9821, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97932005
	best: 0.9821

Starting e_i: 908
Model ind 665 epoch 908 batch: 0 avg loss -2.981498 avg loss no lamb -2.981498 time 2020-06-26 18:43:16.307732
Model ind 665 epoch 908 batch: 100 avg loss -2.889941 avg loss no lamb -2.889941 time 2020-06-26 18:43:27.147658
Model ind 665 epoch 908 batch: 200 avg loss -2.761774 avg loss no lamb -2.761774 time 2020-06-26 18:43:38.076171
Model ind 665 epoch 908 batch: 300 avg loss -2.856579 avg loss no lamb -2.856579 time 2020-06-26 18:43:49.092428
Model ind 665 epoch 908 batch: 400 avg loss -2.687198 avg loss no lamb -2.687198 time 2020-06-26 18:43:59.790472
Model ind 665 epoch 908 batch: 500 avg loss -2.795656 avg loss no lamb -2.795656 time 2020-06-26 18:44:10.639617
Model ind 665 epoch 908 batch: 600 avg loss -2.848968 avg loss no lamb -2.848968 time 2020-06-26 18:44:21.640341
Model ind 665 epoch 908 batch: 700 avg loss -2.811326 avg loss no lamb -2.811326 time 2020-06-26 18:44:32.360147
Model ind 665 epoch 908 batch: 800 avg loss -2.885188 avg loss no lamb -2.885188 time 2020-06-26 18:44:43.027343
last batch sz 10
Pre: time 2020-06-26 18:44:57.197239: 
 	std: 0.0035801665
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9808, 0.9732, 0.9818, 0.9755]
	train_accs: [0.98216665, 0.98143333, 0.97501665, 0.98226666, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97862005
	best: 0.9818

Starting e_i: 909
Model ind 665 epoch 909 batch: 0 avg loss -2.915426 avg loss no lamb -2.915426 time 2020-06-26 18:44:58.304780
Model ind 665 epoch 909 batch: 100 avg loss -2.897777 avg loss no lamb -2.897777 time 2020-06-26 18:45:09.068688
Model ind 665 epoch 909 batch: 200 avg loss -2.869546 avg loss no lamb -2.869546 time 2020-06-26 18:45:19.947492
Model ind 665 epoch 909 batch: 300 avg loss -2.853593 avg loss no lamb -2.853593 time 2020-06-26 18:45:30.953578
Model ind 665 epoch 909 batch: 400 avg loss -2.727022 avg loss no lamb -2.727022 time 2020-06-26 18:45:41.815462
Model ind 665 epoch 909 batch: 500 avg loss -2.779785 avg loss no lamb -2.779785 time 2020-06-26 18:45:52.444842
Model ind 665 epoch 909 batch: 600 avg loss -2.851203 avg loss no lamb -2.851203 time 2020-06-26 18:46:03.542306
Model ind 665 epoch 909 batch: 700 avg loss -2.756711 avg loss no lamb -2.756711 time 2020-06-26 18:46:14.396246
Model ind 665 epoch 909 batch: 800 avg loss -2.836313 avg loss no lamb -2.836313 time 2020-06-26 18:46:25.196786
last batch sz 10
Pre: time 2020-06-26 18:46:39.331459: 
 	std: 0.002552642
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9809, 0.9764, 0.9827, 0.9776]
	train_accs: [0.9822, 0.98113334, 0.97645, 0.9824833, 0.9776667]
	best_train_sub_head: 3
	worst: 0.9764
	avg: 0.98
	best: 0.9827

Starting e_i: 910
Model ind 665 epoch 910 batch: 0 avg loss -2.939116 avg loss no lamb -2.939116 time 2020-06-26 18:46:40.354519
Model ind 665 epoch 910 batch: 100 avg loss -2.893761 avg loss no lamb -2.893761 time 2020-06-26 18:46:51.451131
Model ind 665 epoch 910 batch: 200 avg loss -2.871694 avg loss no lamb -2.871694 time 2020-06-26 18:47:02.540168
Model ind 665 epoch 910 batch: 300 avg loss -2.887377 avg loss no lamb -2.887377 time 2020-06-26 18:47:13.484534
Model ind 665 epoch 910 batch: 400 avg loss -2.804131 avg loss no lamb -2.804131 time 2020-06-26 18:47:24.236217
Model ind 665 epoch 910 batch: 500 avg loss -2.811559 avg loss no lamb -2.811559 time 2020-06-26 18:47:35.093660
Model ind 665 epoch 910 batch: 600 avg loss -2.820388 avg loss no lamb -2.820388 time 2020-06-26 18:47:45.937106
Model ind 665 epoch 910 batch: 700 avg loss -2.740861 avg loss no lamb -2.740861 time 2020-06-26 18:47:56.791865
Model ind 665 epoch 910 batch: 800 avg loss -2.811362 avg loss no lamb -2.811362 time 2020-06-26 18:48:07.602675
last batch sz 10
Pre: time 2020-06-26 18:48:21.485553: 
 	std: 0.0028756913
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9793, 0.9734, 0.9791, 0.9734]
	train_accs: [0.9814, 0.9805167, 0.9755333, 0.9813333, 0.97655]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97691995
	best: 0.9794

Starting e_i: 911
Model ind 665 epoch 911 batch: 0 avg loss -2.936404 avg loss no lamb -2.936404 time 2020-06-26 18:48:23.851333
Model ind 665 epoch 911 batch: 100 avg loss -2.838097 avg loss no lamb -2.838097 time 2020-06-26 18:48:34.787592
Model ind 665 epoch 911 batch: 200 avg loss -2.844689 avg loss no lamb -2.844689 time 2020-06-26 18:48:45.659923
Model ind 665 epoch 911 batch: 300 avg loss -2.876256 avg loss no lamb -2.876256 time 2020-06-26 18:48:56.664122
Model ind 665 epoch 911 batch: 400 avg loss -2.765316 avg loss no lamb -2.765316 time 2020-06-26 18:49:07.397711
Model ind 665 epoch 911 batch: 500 avg loss -2.839455 avg loss no lamb -2.839455 time 2020-06-26 18:49:17.910391
Model ind 665 epoch 911 batch: 600 avg loss -2.822612 avg loss no lamb -2.822612 time 2020-06-26 18:49:28.741131
Model ind 665 epoch 911 batch: 700 avg loss -2.743926 avg loss no lamb -2.743926 time 2020-06-26 18:49:39.772539
Model ind 665 epoch 911 batch: 800 avg loss -2.838083 avg loss no lamb -2.838083 time 2020-06-26 18:49:50.750701
last batch sz 10
Pre: time 2020-06-26 18:50:04.723668: 
 	std: 0.0024832254
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9817, 0.9767, 0.9825, 0.9778]
	train_accs: [0.9823833, 0.98135, 0.9772, 0.98228335, 0.97785]
	best_train_sub_head: 0
	worst: 0.9767
	avg: 0.98024005
	best: 0.9825

Starting e_i: 912
Model ind 665 epoch 912 batch: 0 avg loss -2.946299 avg loss no lamb -2.946299 time 2020-06-26 18:50:05.778607
Model ind 665 epoch 912 batch: 100 avg loss -2.865314 avg loss no lamb -2.865314 time 2020-06-26 18:50:16.640656
Model ind 665 epoch 912 batch: 200 avg loss -2.828925 avg loss no lamb -2.828925 time 2020-06-26 18:50:27.392374
Model ind 665 epoch 912 batch: 300 avg loss -2.871055 avg loss no lamb -2.871055 time 2020-06-26 18:50:38.090030
Model ind 665 epoch 912 batch: 400 avg loss -2.708532 avg loss no lamb -2.708532 time 2020-06-26 18:50:48.831419
Model ind 665 epoch 912 batch: 500 avg loss -2.847448 avg loss no lamb -2.847448 time 2020-06-26 18:50:59.900012
Model ind 665 epoch 912 batch: 600 avg loss -2.870687 avg loss no lamb -2.870687 time 2020-06-26 18:51:10.949677
Model ind 665 epoch 912 batch: 700 avg loss -2.758020 avg loss no lamb -2.758020 time 2020-06-26 18:51:21.697332
Model ind 665 epoch 912 batch: 800 avg loss -2.855925 avg loss no lamb -2.855925 time 2020-06-26 18:51:32.501914
last batch sz 10
Pre: time 2020-06-26 18:51:46.518726: 
 	std: 0.0028309769
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9813, 0.9758, 0.9822, 0.9764]
	train_accs: [0.98223335, 0.9812667, 0.97636664, 0.98211664, 0.97713333]
	best_train_sub_head: 0
	worst: 0.9758
	avg: 0.97954
	best: 0.982

Starting e_i: 913
Model ind 665 epoch 913 batch: 0 avg loss -2.981602 avg loss no lamb -2.981602 time 2020-06-26 18:51:47.676215
Model ind 665 epoch 913 batch: 100 avg loss -2.864656 avg loss no lamb -2.864656 time 2020-06-26 18:51:58.387871
Model ind 665 epoch 913 batch: 200 avg loss -2.883065 avg loss no lamb -2.883065 time 2020-06-26 18:52:09.092538
Model ind 665 epoch 913 batch: 300 avg loss -2.871158 avg loss no lamb -2.871158 time 2020-06-26 18:52:19.925012
Model ind 665 epoch 913 batch: 400 avg loss -2.767320 avg loss no lamb -2.767320 time 2020-06-26 18:52:30.777600
Model ind 665 epoch 913 batch: 500 avg loss -2.766540 avg loss no lamb -2.766540 time 2020-06-26 18:52:41.781845
Model ind 665 epoch 913 batch: 600 avg loss -2.878852 avg loss no lamb -2.878852 time 2020-06-26 18:52:52.720374
Model ind 665 epoch 913 batch: 700 avg loss -2.726457 avg loss no lamb -2.726457 time 2020-06-26 18:53:03.638355
Model ind 665 epoch 913 batch: 800 avg loss -2.857293 avg loss no lamb -2.857293 time 2020-06-26 18:53:14.532079
last batch sz 10
Pre: time 2020-06-26 18:53:28.552496: 
 	std: 0.002525533
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9801, 0.9746, 0.9808, 0.9763]
	train_accs: [0.98148334, 0.9809, 0.97625, 0.98135, 0.97748333]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97845995
	best: 0.9805

Starting e_i: 914
Model ind 665 epoch 914 batch: 0 avg loss -2.906092 avg loss no lamb -2.906092 time 2020-06-26 18:53:29.584212
Model ind 665 epoch 914 batch: 100 avg loss -2.869431 avg loss no lamb -2.869431 time 2020-06-26 18:53:40.572965
Model ind 665 epoch 914 batch: 200 avg loss -2.858997 avg loss no lamb -2.858997 time 2020-06-26 18:53:51.370575
Model ind 665 epoch 914 batch: 300 avg loss -2.838066 avg loss no lamb -2.838066 time 2020-06-26 18:54:02.113133
Model ind 665 epoch 914 batch: 400 avg loss -2.829692 avg loss no lamb -2.829692 time 2020-06-26 18:54:13.087984
Model ind 665 epoch 914 batch: 500 avg loss -2.813828 avg loss no lamb -2.813828 time 2020-06-26 18:54:24.158296
Model ind 665 epoch 914 batch: 600 avg loss -2.914105 avg loss no lamb -2.914105 time 2020-06-26 18:54:35.054276
Model ind 665 epoch 914 batch: 700 avg loss -2.760439 avg loss no lamb -2.760439 time 2020-06-26 18:54:45.878673
Model ind 665 epoch 914 batch: 800 avg loss -2.840857 avg loss no lamb -2.840857 time 2020-06-26 18:54:56.785560
last batch sz 10
Pre: time 2020-06-26 18:55:11.004479: 
 	std: 0.0028394428
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9789, 0.9734, 0.9796, 0.9738]
	train_accs: [0.9809, 0.97985, 0.9745333, 0.98076665, 0.97578335]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97706
	best: 0.9796

Starting e_i: 915
Model ind 665 epoch 915 batch: 0 avg loss -2.974534 avg loss no lamb -2.974534 time 2020-06-26 18:55:12.154853
Model ind 665 epoch 915 batch: 100 avg loss -2.886499 avg loss no lamb -2.886499 time 2020-06-26 18:55:23.159669
Model ind 665 epoch 915 batch: 200 avg loss -2.852080 avg loss no lamb -2.852080 time 2020-06-26 18:55:33.764491
Model ind 665 epoch 915 batch: 300 avg loss -2.880005 avg loss no lamb -2.880005 time 2020-06-26 18:55:44.596685
Model ind 665 epoch 915 batch: 400 avg loss -2.789756 avg loss no lamb -2.789756 time 2020-06-26 18:55:55.509724
Model ind 665 epoch 915 batch: 500 avg loss -2.823769 avg loss no lamb -2.823769 time 2020-06-26 18:56:06.602891
Model ind 665 epoch 915 batch: 600 avg loss -2.846074 avg loss no lamb -2.846074 time 2020-06-26 18:56:17.724286
Model ind 665 epoch 915 batch: 700 avg loss -2.706601 avg loss no lamb -2.706601 time 2020-06-26 18:56:28.813656
Model ind 665 epoch 915 batch: 800 avg loss -2.795555 avg loss no lamb -2.795555 time 2020-06-26 18:56:39.673180
last batch sz 10
Pre: time 2020-06-26 18:56:53.675007: 
 	std: 0.0030668548
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9803, 0.9745, 0.9814, 0.9746]
	train_accs: [0.9813, 0.98036665, 0.97533333, 0.98123336, 0.976]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97827995
	best: 0.9806

Starting e_i: 916
Model ind 665 epoch 916 batch: 0 avg loss -2.941821 avg loss no lamb -2.941821 time 2020-06-26 18:56:54.755589
Model ind 665 epoch 916 batch: 100 avg loss -2.883462 avg loss no lamb -2.883462 time 2020-06-26 18:57:05.661313
Model ind 665 epoch 916 batch: 200 avg loss -2.884404 avg loss no lamb -2.884404 time 2020-06-26 18:57:16.641207
Model ind 665 epoch 916 batch: 300 avg loss -2.852314 avg loss no lamb -2.852314 time 2020-06-26 18:57:27.707348
Model ind 665 epoch 916 batch: 400 avg loss -2.784503 avg loss no lamb -2.784503 time 2020-06-26 18:57:38.519057
Model ind 665 epoch 916 batch: 500 avg loss -2.827298 avg loss no lamb -2.827298 time 2020-06-26 18:57:49.427032
Model ind 665 epoch 916 batch: 600 avg loss -2.820976 avg loss no lamb -2.820976 time 2020-06-26 18:58:00.548272
Model ind 665 epoch 916 batch: 700 avg loss -2.685350 avg loss no lamb -2.685350 time 2020-06-26 18:58:11.662955
Model ind 665 epoch 916 batch: 800 avg loss -2.893795 avg loss no lamb -2.893795 time 2020-06-26 18:58:22.425880
last batch sz 10
Pre: time 2020-06-26 18:58:36.299189: 
 	std: 0.0027089573
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9804, 0.9755, 0.9818, 0.9761]
	train_accs: [0.9813667, 0.9805167, 0.9763, 0.9813167, 0.9769]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97906
	best: 0.9815

Starting e_i: 917
Model ind 665 epoch 917 batch: 0 avg loss -2.930870 avg loss no lamb -2.930870 time 2020-06-26 18:58:37.448830
Model ind 665 epoch 917 batch: 100 avg loss -2.885396 avg loss no lamb -2.885396 time 2020-06-26 18:58:48.627124
Model ind 665 epoch 917 batch: 200 avg loss -2.842361 avg loss no lamb -2.842361 time 2020-06-26 18:58:59.478790
Model ind 665 epoch 917 batch: 300 avg loss -2.814499 avg loss no lamb -2.814499 time 2020-06-26 18:59:10.379601
Model ind 665 epoch 917 batch: 400 avg loss -2.790900 avg loss no lamb -2.790900 time 2020-06-26 18:59:21.060060
Model ind 665 epoch 917 batch: 500 avg loss -2.853907 avg loss no lamb -2.853907 time 2020-06-26 18:59:32.076172
Model ind 665 epoch 917 batch: 600 avg loss -2.881974 avg loss no lamb -2.881974 time 2020-06-26 18:59:43.056293
Model ind 665 epoch 917 batch: 700 avg loss -2.712330 avg loss no lamb -2.712330 time 2020-06-26 18:59:54.060892
Model ind 665 epoch 917 batch: 800 avg loss -2.829001 avg loss no lamb -2.829001 time 2020-06-26 19:00:05.000153
last batch sz 10
Pre: time 2020-06-26 19:00:18.849012: 
 	std: 0.0025255494
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9806, 0.9755, 0.9806, 0.9756]
	train_accs: [0.98175, 0.98125, 0.97651666, 0.9815, 0.97721666]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97863996
	best: 0.9809

Starting e_i: 918
Model ind 665 epoch 918 batch: 0 avg loss -2.919491 avg loss no lamb -2.919491 time 2020-06-26 19:00:19.868974
Model ind 665 epoch 918 batch: 100 avg loss -2.882301 avg loss no lamb -2.882301 time 2020-06-26 19:00:30.662291
Model ind 665 epoch 918 batch: 200 avg loss -2.835622 avg loss no lamb -2.835622 time 2020-06-26 19:00:41.459993
Model ind 665 epoch 918 batch: 300 avg loss -2.858909 avg loss no lamb -2.858909 time 2020-06-26 19:00:52.172061
Model ind 665 epoch 918 batch: 400 avg loss -2.735199 avg loss no lamb -2.735199 time 2020-06-26 19:01:02.874945
Model ind 665 epoch 918 batch: 500 avg loss -2.827266 avg loss no lamb -2.827266 time 2020-06-26 19:01:13.555654
Model ind 665 epoch 918 batch: 600 avg loss -2.890821 avg loss no lamb -2.890821 time 2020-06-26 19:01:24.505277
Model ind 665 epoch 918 batch: 700 avg loss -2.765079 avg loss no lamb -2.765079 time 2020-06-26 19:01:35.297863
Model ind 665 epoch 918 batch: 800 avg loss -2.894652 avg loss no lamb -2.894652 time 2020-06-26 19:01:46.288769
last batch sz 10
Pre: time 2020-06-26 19:02:00.312354: 
 	std: 0.0021657345
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9804, 0.9759, 0.9808, 0.9763]
	train_accs: [0.98185, 0.9813667, 0.9767333, 0.98186666, 0.9777667]
	best_train_sub_head: 3
	worst: 0.9759
	avg: 0.9787399
	best: 0.9808

Starting e_i: 919
Model ind 665 epoch 919 batch: 0 avg loss -2.954576 avg loss no lamb -2.954576 time 2020-06-26 19:02:01.455932
Model ind 665 epoch 919 batch: 100 avg loss -2.907177 avg loss no lamb -2.907177 time 2020-06-26 19:02:12.249629
Model ind 665 epoch 919 batch: 200 avg loss -2.891874 avg loss no lamb -2.891874 time 2020-06-26 19:02:23.660722
Model ind 665 epoch 919 batch: 300 avg loss -2.865762 avg loss no lamb -2.865762 time 2020-06-26 19:02:34.587521
Model ind 665 epoch 919 batch: 400 avg loss -2.812135 avg loss no lamb -2.812135 time 2020-06-26 19:02:45.300259
Model ind 665 epoch 919 batch: 500 avg loss -2.794291 avg loss no lamb -2.794291 time 2020-06-26 19:02:56.214125
Model ind 665 epoch 919 batch: 600 avg loss -2.888221 avg loss no lamb -2.888221 time 2020-06-26 19:03:07.243362
Model ind 665 epoch 919 batch: 700 avg loss -2.707708 avg loss no lamb -2.707708 time 2020-06-26 19:03:18.148246
Model ind 665 epoch 919 batch: 800 avg loss -2.849145 avg loss no lamb -2.849145 time 2020-06-26 19:03:29.164176
last batch sz 10
Pre: time 2020-06-26 19:03:43.444233: 
 	std: 0.0023489657
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9805, 0.9755, 0.9802, 0.9754]
	train_accs: [0.9813167, 0.98118335, 0.9762667, 0.98146665, 0.97685]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97831994
	best: 0.9802

Starting e_i: 920
Model ind 665 epoch 920 batch: 0 avg loss -2.926116 avg loss no lamb -2.926116 time 2020-06-26 19:03:44.480992
Model ind 665 epoch 920 batch: 100 avg loss -2.830042 avg loss no lamb -2.830042 time 2020-06-26 19:03:55.437250
Model ind 665 epoch 920 batch: 200 avg loss -2.795199 avg loss no lamb -2.795199 time 2020-06-26 19:04:06.358603
Model ind 665 epoch 920 batch: 300 avg loss -2.803028 avg loss no lamb -2.803028 time 2020-06-26 19:04:17.149679
Model ind 665 epoch 920 batch: 400 avg loss -2.769711 avg loss no lamb -2.769711 time 2020-06-26 19:04:28.122161
Model ind 665 epoch 920 batch: 500 avg loss -2.838156 avg loss no lamb -2.838156 time 2020-06-26 19:04:39.082874
Model ind 665 epoch 920 batch: 600 avg loss -2.872473 avg loss no lamb -2.872473 time 2020-06-26 19:04:50.179380
Model ind 665 epoch 920 batch: 700 avg loss -2.787116 avg loss no lamb -2.787116 time 2020-06-26 19:05:01.150118
Model ind 665 epoch 920 batch: 800 avg loss -2.860805 avg loss no lamb -2.860805 time 2020-06-26 19:05:11.829554
last batch sz 10
Pre: time 2020-06-26 19:05:25.866052: 
 	std: 0.002678354
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9815, 0.9755, 0.9812, 0.9768]
	train_accs: [0.98188335, 0.98143333, 0.9762833, 0.9819833, 0.9770333]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97938
	best: 0.9812

Starting e_i: 921
Model ind 665 epoch 921 batch: 0 avg loss -2.986393 avg loss no lamb -2.986393 time 2020-06-26 19:05:28.269115
Model ind 665 epoch 921 batch: 100 avg loss -2.886710 avg loss no lamb -2.886710 time 2020-06-26 19:05:38.919291
Model ind 665 epoch 921 batch: 200 avg loss -2.803728 avg loss no lamb -2.803728 time 2020-06-26 19:05:49.746668
Model ind 665 epoch 921 batch: 300 avg loss -2.870080 avg loss no lamb -2.870080 time 2020-06-26 19:06:00.590221
Model ind 665 epoch 921 batch: 400 avg loss -2.848889 avg loss no lamb -2.848889 time 2020-06-26 19:06:11.509793
Model ind 665 epoch 921 batch: 500 avg loss -2.791176 avg loss no lamb -2.791176 time 2020-06-26 19:06:22.229397
Model ind 665 epoch 921 batch: 600 avg loss -2.856688 avg loss no lamb -2.856688 time 2020-06-26 19:06:33.053500
Model ind 665 epoch 921 batch: 700 avg loss -2.728157 avg loss no lamb -2.728157 time 2020-06-26 19:06:43.956943
Model ind 665 epoch 921 batch: 800 avg loss -2.802156 avg loss no lamb -2.802156 time 2020-06-26 19:06:54.750083
last batch sz 10
Pre: time 2020-06-26 19:07:08.929712: 
 	std: 0.0021207666
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9804, 0.9755, 0.9804, 0.977]
	train_accs: [0.98175, 0.9813167, 0.9758667, 0.9816333, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97878
	best: 0.9806

Starting e_i: 922
Model ind 665 epoch 922 batch: 0 avg loss -2.896088 avg loss no lamb -2.896088 time 2020-06-26 19:07:09.956301
Model ind 665 epoch 922 batch: 100 avg loss -2.909981 avg loss no lamb -2.909981 time 2020-06-26 19:07:20.630468
Model ind 665 epoch 922 batch: 200 avg loss -2.849543 avg loss no lamb -2.849543 time 2020-06-26 19:07:31.285440
Model ind 665 epoch 922 batch: 300 avg loss -2.877469 avg loss no lamb -2.877469 time 2020-06-26 19:07:42.169386
Model ind 665 epoch 922 batch: 400 avg loss -2.777283 avg loss no lamb -2.777283 time 2020-06-26 19:07:53.153575
Model ind 665 epoch 922 batch: 500 avg loss -2.828602 avg loss no lamb -2.828602 time 2020-06-26 19:08:04.126662
Model ind 665 epoch 922 batch: 600 avg loss -2.868028 avg loss no lamb -2.868028 time 2020-06-26 19:08:15.124416
Model ind 665 epoch 922 batch: 700 avg loss -2.728557 avg loss no lamb -2.728557 time 2020-06-26 19:08:25.950715
Model ind 665 epoch 922 batch: 800 avg loss -2.911256 avg loss no lamb -2.911256 time 2020-06-26 19:08:36.628785
last batch sz 10
Pre: time 2020-06-26 19:08:50.554249: 
 	std: 0.0027975559
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9799, 0.9743, 0.9808, 0.9752]
	train_accs: [0.9816333, 0.98113334, 0.97615, 0.9816833, 0.9766667]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97814
	best: 0.9808

Starting e_i: 923
Model ind 665 epoch 923 batch: 0 avg loss -2.939564 avg loss no lamb -2.939564 time 2020-06-26 19:08:51.715946
Model ind 665 epoch 923 batch: 100 avg loss -2.850832 avg loss no lamb -2.850832 time 2020-06-26 19:09:02.682287
Model ind 665 epoch 923 batch: 200 avg loss -2.836727 avg loss no lamb -2.836727 time 2020-06-26 19:09:13.373858
Model ind 665 epoch 923 batch: 300 avg loss -2.852462 avg loss no lamb -2.852462 time 2020-06-26 19:09:24.392836
Model ind 665 epoch 923 batch: 400 avg loss -2.789989 avg loss no lamb -2.789989 time 2020-06-26 19:09:35.337766
Model ind 665 epoch 923 batch: 500 avg loss -2.855844 avg loss no lamb -2.855844 time 2020-06-26 19:09:46.359959
Model ind 665 epoch 923 batch: 600 avg loss -2.868697 avg loss no lamb -2.868697 time 2020-06-26 19:09:57.302826
Model ind 665 epoch 923 batch: 700 avg loss -2.743818 avg loss no lamb -2.743818 time 2020-06-26 19:10:08.121233
Model ind 665 epoch 923 batch: 800 avg loss -2.908468 avg loss no lamb -2.908468 time 2020-06-26 19:10:18.933509
last batch sz 10
Pre: time 2020-06-26 19:10:33.146204: 
 	std: 0.003331433
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9803, 0.9736, 0.9816, 0.9753]
	train_accs: [0.98188335, 0.9812, 0.97625, 0.9819667, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97844
	best: 0.9816

Starting e_i: 924
Model ind 665 epoch 924 batch: 0 avg loss -2.921161 avg loss no lamb -2.921161 time 2020-06-26 19:10:34.192090
Model ind 665 epoch 924 batch: 100 avg loss -2.880455 avg loss no lamb -2.880455 time 2020-06-26 19:10:45.199960
Model ind 665 epoch 924 batch: 200 avg loss -2.931859 avg loss no lamb -2.931859 time 2020-06-26 19:10:56.087104
Model ind 665 epoch 924 batch: 300 avg loss -2.844055 avg loss no lamb -2.844055 time 2020-06-26 19:11:06.695152
Model ind 665 epoch 924 batch: 400 avg loss -2.811008 avg loss no lamb -2.811008 time 2020-06-26 19:11:17.798457
Model ind 665 epoch 924 batch: 500 avg loss -2.849508 avg loss no lamb -2.849508 time 2020-06-26 19:11:28.662488
Model ind 665 epoch 924 batch: 600 avg loss -2.916065 avg loss no lamb -2.916065 time 2020-06-26 19:11:39.855151
Model ind 665 epoch 924 batch: 700 avg loss -2.742042 avg loss no lamb -2.742042 time 2020-06-26 19:11:50.945521
Model ind 665 epoch 924 batch: 800 avg loss -2.880599 avg loss no lamb -2.880599 time 2020-06-26 19:12:01.654022
last batch sz 10
Pre: time 2020-06-26 19:12:15.816492: 
 	std: 0.0022645125
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9809, 0.9763, 0.9813, 0.977]
	train_accs: [0.98176664, 0.98135, 0.97688335, 0.98195, 0.9776667]
	best_train_sub_head: 3
	worst: 0.9763
	avg: 0.97940004
	best: 0.9813

Starting e_i: 925
Model ind 665 epoch 925 batch: 0 avg loss -2.952754 avg loss no lamb -2.952754 time 2020-06-26 19:12:16.950759
Model ind 665 epoch 925 batch: 100 avg loss -2.853243 avg loss no lamb -2.853243 time 2020-06-26 19:12:27.732246
Model ind 665 epoch 925 batch: 200 avg loss -2.837236 avg loss no lamb -2.837236 time 2020-06-26 19:12:38.394663
Model ind 665 epoch 925 batch: 300 avg loss -2.846001 avg loss no lamb -2.846001 time 2020-06-26 19:12:49.555734
Model ind 665 epoch 925 batch: 400 avg loss -2.834081 avg loss no lamb -2.834081 time 2020-06-26 19:13:00.651802
Model ind 665 epoch 925 batch: 500 avg loss -2.862321 avg loss no lamb -2.862321 time 2020-06-26 19:13:11.481203
Model ind 665 epoch 925 batch: 600 avg loss -2.903930 avg loss no lamb -2.903930 time 2020-06-26 19:13:22.264902
Model ind 665 epoch 925 batch: 700 avg loss -2.710298 avg loss no lamb -2.710298 time 2020-06-26 19:13:33.089340
Model ind 665 epoch 925 batch: 800 avg loss -2.857512 avg loss no lamb -2.857512 time 2020-06-26 19:13:44.329348
last batch sz 10
Pre: time 2020-06-26 19:13:58.497855: 
 	std: 0.002471752
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9808, 0.9759, 0.9813, 0.9761]
	train_accs: [0.98146665, 0.98115, 0.9763167, 0.98135, 0.9770167]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.97902
	best: 0.981

Starting e_i: 926
Model ind 665 epoch 926 batch: 0 avg loss -2.940926 avg loss no lamb -2.940926 time 2020-06-26 19:13:59.529989
Model ind 665 epoch 926 batch: 100 avg loss -2.884120 avg loss no lamb -2.884120 time 2020-06-26 19:14:10.363006
Model ind 665 epoch 926 batch: 200 avg loss -2.868178 avg loss no lamb -2.868178 time 2020-06-26 19:14:20.952113
Model ind 665 epoch 926 batch: 300 avg loss -2.853302 avg loss no lamb -2.853302 time 2020-06-26 19:14:31.854503
Model ind 665 epoch 926 batch: 400 avg loss -2.785706 avg loss no lamb -2.785706 time 2020-06-26 19:14:42.859821
Model ind 665 epoch 926 batch: 500 avg loss -2.803857 avg loss no lamb -2.803857 time 2020-06-26 19:14:53.870202
Model ind 665 epoch 926 batch: 600 avg loss -2.871424 avg loss no lamb -2.871424 time 2020-06-26 19:15:04.530578
Model ind 665 epoch 926 batch: 700 avg loss -2.753381 avg loss no lamb -2.753381 time 2020-06-26 19:15:15.320507
Model ind 665 epoch 926 batch: 800 avg loss -2.913788 avg loss no lamb -2.913788 time 2020-06-26 19:15:26.392231
last batch sz 10
Pre: time 2020-06-26 19:15:40.470316: 
 	std: 0.003081559
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9816, 0.9749, 0.9824, 0.9772]
	train_accs: [0.9821333, 0.98123336, 0.9757, 0.9820333, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97969997
	best: 0.9824

Starting e_i: 927
Model ind 665 epoch 927 batch: 0 avg loss -2.938554 avg loss no lamb -2.938554 time 2020-06-26 19:15:41.626799
Model ind 665 epoch 927 batch: 100 avg loss -2.868831 avg loss no lamb -2.868831 time 2020-06-26 19:15:52.593850
Model ind 665 epoch 927 batch: 200 avg loss -2.859442 avg loss no lamb -2.859442 time 2020-06-26 19:16:03.416190
Model ind 665 epoch 927 batch: 300 avg loss -2.792806 avg loss no lamb -2.792806 time 2020-06-26 19:16:14.294474
Model ind 665 epoch 927 batch: 400 avg loss -2.781067 avg loss no lamb -2.781067 time 2020-06-26 19:16:25.304165
Model ind 665 epoch 927 batch: 500 avg loss -2.850966 avg loss no lamb -2.850966 time 2020-06-26 19:16:36.264153
Model ind 665 epoch 927 batch: 600 avg loss -2.863432 avg loss no lamb -2.863432 time 2020-06-26 19:16:46.937500
Model ind 665 epoch 927 batch: 700 avg loss -2.772057 avg loss no lamb -2.772057 time 2020-06-26 19:16:57.812380
Model ind 665 epoch 927 batch: 800 avg loss -2.846755 avg loss no lamb -2.846755 time 2020-06-26 19:17:08.584235
last batch sz 10
Pre: time 2020-06-26 19:17:22.590535: 
 	std: 0.002552948
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9809, 0.9746, 0.9809, 0.9771]
	train_accs: [0.9815, 0.9813333, 0.97608334, 0.98191667, 0.97728336]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97882
	best: 0.9809

Starting e_i: 928
Model ind 665 epoch 928 batch: 0 avg loss -2.973845 avg loss no lamb -2.973845 time 2020-06-26 19:17:23.611144
Model ind 665 epoch 928 batch: 100 avg loss -2.842979 avg loss no lamb -2.842979 time 2020-06-26 19:17:34.288230
Model ind 665 epoch 928 batch: 200 avg loss -2.888531 avg loss no lamb -2.888531 time 2020-06-26 19:17:45.437603
Model ind 665 epoch 928 batch: 300 avg loss -2.871198 avg loss no lamb -2.871198 time 2020-06-26 19:17:56.159684
Model ind 665 epoch 928 batch: 400 avg loss -2.742430 avg loss no lamb -2.742430 time 2020-06-26 19:18:07.077824
Model ind 665 epoch 928 batch: 500 avg loss -2.878750 avg loss no lamb -2.878750 time 2020-06-26 19:18:17.769831
Model ind 665 epoch 928 batch: 600 avg loss -2.860610 avg loss no lamb -2.860610 time 2020-06-26 19:18:28.642545
Model ind 665 epoch 928 batch: 700 avg loss -2.703781 avg loss no lamb -2.703781 time 2020-06-26 19:18:39.457024
Model ind 665 epoch 928 batch: 800 avg loss -2.858883 avg loss no lamb -2.858883 time 2020-06-26 19:18:50.481734
last batch sz 10
Pre: time 2020-06-26 19:19:04.610859: 
 	std: 0.002628305
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9807, 0.976, 0.982, 0.9767]
	train_accs: [0.9819833, 0.9815, 0.97648335, 0.98218334, 0.9774167]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.9795
	best: 0.982

Starting e_i: 929
Model ind 665 epoch 929 batch: 0 avg loss -2.964921 avg loss no lamb -2.964921 time 2020-06-26 19:19:05.747381
Model ind 665 epoch 929 batch: 100 avg loss -2.872982 avg loss no lamb -2.872982 time 2020-06-26 19:19:16.304865
Model ind 665 epoch 929 batch: 200 avg loss -2.877369 avg loss no lamb -2.877369 time 2020-06-26 19:19:27.120881
Model ind 665 epoch 929 batch: 300 avg loss -2.845026 avg loss no lamb -2.845026 time 2020-06-26 19:19:37.985080
Model ind 665 epoch 929 batch: 400 avg loss -2.766598 avg loss no lamb -2.766598 time 2020-06-26 19:19:48.865409
Model ind 665 epoch 929 batch: 500 avg loss -2.857885 avg loss no lamb -2.857885 time 2020-06-26 19:19:59.729814
Model ind 665 epoch 929 batch: 600 avg loss -2.847868 avg loss no lamb -2.847868 time 2020-06-26 19:20:10.440100
Model ind 665 epoch 929 batch: 700 avg loss -2.765763 avg loss no lamb -2.765763 time 2020-06-26 19:20:21.148748
Model ind 665 epoch 929 batch: 800 avg loss -2.804600 avg loss no lamb -2.804600 time 2020-06-26 19:20:31.979290
last batch sz 10
Pre: time 2020-06-26 19:20:45.991004: 
 	std: 0.0029863697
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.981, 0.9753, 0.9819, 0.9755]
	train_accs: [0.9817333, 0.98118335, 0.9759, 0.98176664, 0.97676665]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97904
	best: 0.9819

Starting e_i: 930
Model ind 665 epoch 930 batch: 0 avg loss -2.958375 avg loss no lamb -2.958375 time 2020-06-26 19:20:46.954142
Model ind 665 epoch 930 batch: 100 avg loss -2.888641 avg loss no lamb -2.888641 time 2020-06-26 19:20:57.554908
Model ind 665 epoch 930 batch: 200 avg loss -2.845371 avg loss no lamb -2.845371 time 2020-06-26 19:21:08.199755
Model ind 665 epoch 930 batch: 300 avg loss -2.914092 avg loss no lamb -2.914092 time 2020-06-26 19:21:18.958933
Model ind 665 epoch 930 batch: 400 avg loss -2.746883 avg loss no lamb -2.746883 time 2020-06-26 19:21:29.738947
Model ind 665 epoch 930 batch: 500 avg loss -2.817210 avg loss no lamb -2.817210 time 2020-06-26 19:21:40.519004
Model ind 665 epoch 930 batch: 600 avg loss -2.898989 avg loss no lamb -2.898989 time 2020-06-26 19:21:51.326622
Model ind 665 epoch 930 batch: 700 avg loss -2.729607 avg loss no lamb -2.729607 time 2020-06-26 19:22:01.920391
Model ind 665 epoch 930 batch: 800 avg loss -2.870511 avg loss no lamb -2.870511 time 2020-06-26 19:22:12.804331
last batch sz 10
Pre: time 2020-06-26 19:22:27.177235: 
 	std: 0.0029917166
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9808, 0.9746, 0.9806, 0.9748]
	train_accs: [0.9812833, 0.9810333, 0.9755833, 0.9812, 0.97625]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97836
	best: 0.981

Starting e_i: 931
Model ind 665 epoch 931 batch: 0 avg loss -2.920747 avg loss no lamb -2.920747 time 2020-06-26 19:22:29.560054
Model ind 665 epoch 931 batch: 100 avg loss -2.904182 avg loss no lamb -2.904182 time 2020-06-26 19:22:40.189408
Model ind 665 epoch 931 batch: 200 avg loss -2.850182 avg loss no lamb -2.850182 time 2020-06-26 19:22:50.676445
Model ind 665 epoch 931 batch: 300 avg loss -2.860853 avg loss no lamb -2.860853 time 2020-06-26 19:23:01.690945
Model ind 665 epoch 931 batch: 400 avg loss -2.810856 avg loss no lamb -2.810856 time 2020-06-26 19:23:12.665878
Model ind 665 epoch 931 batch: 500 avg loss -2.827648 avg loss no lamb -2.827648 time 2020-06-26 19:23:23.483438
Model ind 665 epoch 931 batch: 600 avg loss -2.885079 avg loss no lamb -2.885079 time 2020-06-26 19:23:34.256056
Model ind 665 epoch 931 batch: 700 avg loss -2.737792 avg loss no lamb -2.737792 time 2020-06-26 19:23:45.085250
Model ind 665 epoch 931 batch: 800 avg loss -2.905514 avg loss no lamb -2.905514 time 2020-06-26 19:23:56.060843
last batch sz 10
Pre: time 2020-06-26 19:24:10.104822: 
 	std: 0.0028251752
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.98, 0.9738, 0.9799, 0.9749]
	train_accs: [0.9813833, 0.9805833, 0.97576666, 0.98108333, 0.97613335]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97778
	best: 0.9803

Starting e_i: 932
Model ind 665 epoch 932 batch: 0 avg loss -2.939649 avg loss no lamb -2.939649 time 2020-06-26 19:24:11.117241
Model ind 665 epoch 932 batch: 100 avg loss -2.929027 avg loss no lamb -2.929027 time 2020-06-26 19:24:21.852950
Model ind 665 epoch 932 batch: 200 avg loss -2.871620 avg loss no lamb -2.871620 time 2020-06-26 19:24:32.564954
Model ind 665 epoch 932 batch: 300 avg loss -2.807025 avg loss no lamb -2.807025 time 2020-06-26 19:24:43.607150
Model ind 665 epoch 932 batch: 400 avg loss -2.781197 avg loss no lamb -2.781197 time 2020-06-26 19:24:54.442628
Model ind 665 epoch 932 batch: 500 avg loss -2.837476 avg loss no lamb -2.837476 time 2020-06-26 19:25:05.303649
Model ind 665 epoch 932 batch: 600 avg loss -2.852482 avg loss no lamb -2.852482 time 2020-06-26 19:25:16.215498
Model ind 665 epoch 932 batch: 700 avg loss -2.824706 avg loss no lamb -2.824706 time 2020-06-26 19:25:26.906924
Model ind 665 epoch 932 batch: 800 avg loss -2.816048 avg loss no lamb -2.816048 time 2020-06-26 19:25:37.913857
last batch sz 10
Pre: time 2020-06-26 19:25:51.857901: 
 	std: 0.0028674754
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9805, 0.9751, 0.9817, 0.9759]
	train_accs: [0.9817333, 0.98073334, 0.97575, 0.9819667, 0.97655]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97896004
	best: 0.9817

Starting e_i: 933
Model ind 665 epoch 933 batch: 0 avg loss -2.963992 avg loss no lamb -2.963992 time 2020-06-26 19:25:53.039080
Model ind 665 epoch 933 batch: 100 avg loss -2.919411 avg loss no lamb -2.919411 time 2020-06-26 19:26:04.145296
Model ind 665 epoch 933 batch: 200 avg loss -2.794928 avg loss no lamb -2.794928 time 2020-06-26 19:26:14.993822
Model ind 665 epoch 933 batch: 300 avg loss -2.852969 avg loss no lamb -2.852969 time 2020-06-26 19:26:25.815207
Model ind 665 epoch 933 batch: 400 avg loss -2.786380 avg loss no lamb -2.786380 time 2020-06-26 19:26:36.587482
Model ind 665 epoch 933 batch: 500 avg loss -2.748089 avg loss no lamb -2.748089 time 2020-06-26 19:26:47.603670
Model ind 665 epoch 933 batch: 600 avg loss -2.861999 avg loss no lamb -2.861999 time 2020-06-26 19:26:58.655588
Model ind 665 epoch 933 batch: 700 avg loss -2.803416 avg loss no lamb -2.803416 time 2020-06-26 19:27:09.578701
Model ind 665 epoch 933 batch: 800 avg loss -2.907871 avg loss no lamb -2.907871 time 2020-06-26 19:27:20.470585
last batch sz 10
Pre: time 2020-06-26 19:27:34.514163: 
 	std: 0.0033364645
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9817, 0.9749, 0.9825, 0.976]
	train_accs: [0.98233336, 0.98145, 0.9763333, 0.98218334, 0.9774167]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.9795
	best: 0.9824

Starting e_i: 934
Model ind 665 epoch 934 batch: 0 avg loss -2.953282 avg loss no lamb -2.953282 time 2020-06-26 19:27:35.554826
Model ind 665 epoch 934 batch: 100 avg loss -2.897446 avg loss no lamb -2.897446 time 2020-06-26 19:27:46.361117
Model ind 665 epoch 934 batch: 200 avg loss -2.888571 avg loss no lamb -2.888571 time 2020-06-26 19:27:57.132830
Model ind 665 epoch 934 batch: 300 avg loss -2.872216 avg loss no lamb -2.872216 time 2020-06-26 19:28:08.139469
Model ind 665 epoch 934 batch: 400 avg loss -2.782803 avg loss no lamb -2.782803 time 2020-06-26 19:28:18.901376
Model ind 665 epoch 934 batch: 500 avg loss -2.787419 avg loss no lamb -2.787419 time 2020-06-26 19:28:29.279368
Model ind 665 epoch 934 batch: 600 avg loss -2.850201 avg loss no lamb -2.850201 time 2020-06-26 19:28:40.231473
Model ind 665 epoch 934 batch: 700 avg loss -2.727139 avg loss no lamb -2.727139 time 2020-06-26 19:28:51.104549
Model ind 665 epoch 934 batch: 800 avg loss -2.850833 avg loss no lamb -2.850833 time 2020-06-26 19:29:01.942758
last batch sz 10
Pre: time 2020-06-26 19:29:16.158171: 
 	std: 0.003512494
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9813, 0.9744, 0.982, 0.975]
	train_accs: [0.98185, 0.9808667, 0.97555, 0.9817167, 0.97646666]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97897995
	best: 0.9822

Starting e_i: 935
Model ind 665 epoch 935 batch: 0 avg loss -2.933525 avg loss no lamb -2.933525 time 2020-06-26 19:29:17.356455
Model ind 665 epoch 935 batch: 100 avg loss -2.902081 avg loss no lamb -2.902081 time 2020-06-26 19:29:28.348026
Model ind 665 epoch 935 batch: 200 avg loss -2.869323 avg loss no lamb -2.869323 time 2020-06-26 19:29:39.002151
Model ind 665 epoch 935 batch: 300 avg loss -2.859219 avg loss no lamb -2.859219 time 2020-06-26 19:29:50.064814
Model ind 665 epoch 935 batch: 400 avg loss -2.811738 avg loss no lamb -2.811738 time 2020-06-26 19:30:00.878498
Model ind 665 epoch 935 batch: 500 avg loss -2.824831 avg loss no lamb -2.824831 time 2020-06-26 19:30:11.759594
Model ind 665 epoch 935 batch: 600 avg loss -2.852212 avg loss no lamb -2.852212 time 2020-06-26 19:30:22.279663
Model ind 665 epoch 935 batch: 700 avg loss -2.737213 avg loss no lamb -2.737213 time 2020-06-26 19:30:33.121338
Model ind 665 epoch 935 batch: 800 avg loss -2.881132 avg loss no lamb -2.881132 time 2020-06-26 19:30:43.857575
last batch sz 10
Pre: time 2020-06-26 19:30:57.990406: 
 	std: 0.0030925672
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9795, 0.9735, 0.981, 0.9749]
	train_accs: [0.98145, 0.98078334, 0.97608334, 0.9813167, 0.97713333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.9778999
	best: 0.9806

Starting e_i: 936
Model ind 665 epoch 936 batch: 0 avg loss -2.879205 avg loss no lamb -2.879205 time 2020-06-26 19:30:58.957735
Model ind 665 epoch 936 batch: 100 avg loss -2.913220 avg loss no lamb -2.913220 time 2020-06-26 19:31:09.762437
Model ind 665 epoch 936 batch: 200 avg loss -2.854215 avg loss no lamb -2.854215 time 2020-06-26 19:31:20.624004
Model ind 665 epoch 936 batch: 300 avg loss -2.838018 avg loss no lamb -2.838018 time 2020-06-26 19:31:31.440434
Model ind 665 epoch 936 batch: 400 avg loss -2.815213 avg loss no lamb -2.815213 time 2020-06-26 19:31:42.437450
Model ind 665 epoch 936 batch: 500 avg loss -2.868502 avg loss no lamb -2.868502 time 2020-06-26 19:31:52.965707
Model ind 665 epoch 936 batch: 600 avg loss -2.846823 avg loss no lamb -2.846823 time 2020-06-26 19:32:03.774411
Model ind 665 epoch 936 batch: 700 avg loss -2.765906 avg loss no lamb -2.765906 time 2020-06-26 19:32:14.561494
Model ind 665 epoch 936 batch: 800 avg loss -2.871256 avg loss no lamb -2.871256 time 2020-06-26 19:32:25.547037
last batch sz 10
Pre: time 2020-06-26 19:32:39.565092: 
 	std: 0.0030596731
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9812, 0.9749, 0.9814, 0.9756]
	train_accs: [0.9818, 0.98141664, 0.97635, 0.9816667, 0.97705]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97898006
	best: 0.9818

Starting e_i: 937
Model ind 665 epoch 937 batch: 0 avg loss -2.948927 avg loss no lamb -2.948927 time 2020-06-26 19:32:40.746588
Model ind 665 epoch 937 batch: 100 avg loss -2.865861 avg loss no lamb -2.865861 time 2020-06-26 19:32:51.698321
Model ind 665 epoch 937 batch: 200 avg loss -2.867217 avg loss no lamb -2.867217 time 2020-06-26 19:33:02.345307
Model ind 665 epoch 937 batch: 300 avg loss -2.895206 avg loss no lamb -2.895206 time 2020-06-26 19:33:13.246220
Model ind 665 epoch 937 batch: 400 avg loss -2.798488 avg loss no lamb -2.798488 time 2020-06-26 19:33:24.026469
Model ind 665 epoch 937 batch: 500 avg loss -2.872434 avg loss no lamb -2.872434 time 2020-06-26 19:33:34.921934
Model ind 665 epoch 937 batch: 600 avg loss -2.901492 avg loss no lamb -2.901492 time 2020-06-26 19:33:45.728614
Model ind 665 epoch 937 batch: 700 avg loss -2.721741 avg loss no lamb -2.721741 time 2020-06-26 19:33:56.571567
Model ind 665 epoch 937 batch: 800 avg loss -2.843146 avg loss no lamb -2.843146 time 2020-06-26 19:34:07.591893
last batch sz 10
Pre: time 2020-06-26 19:34:21.920072: 
 	std: 0.0028470429
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9796, 0.974, 0.9814, 0.9764]
	train_accs: [0.98176664, 0.9806167, 0.9758833, 0.98181665, 0.9774333]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97848
	best: 0.9814

Starting e_i: 938
Model ind 665 epoch 938 batch: 0 avg loss -2.939962 avg loss no lamb -2.939962 time 2020-06-26 19:34:22.979411
Model ind 665 epoch 938 batch: 100 avg loss -2.896730 avg loss no lamb -2.896730 time 2020-06-26 19:34:33.590318
Model ind 665 epoch 938 batch: 200 avg loss -2.834719 avg loss no lamb -2.834719 time 2020-06-26 19:34:44.455658
Model ind 665 epoch 938 batch: 300 avg loss -2.807600 avg loss no lamb -2.807600 time 2020-06-26 19:34:55.085732
Model ind 665 epoch 938 batch: 400 avg loss -2.791303 avg loss no lamb -2.791303 time 2020-06-26 19:35:05.977218
Model ind 665 epoch 938 batch: 500 avg loss -2.840669 avg loss no lamb -2.840669 time 2020-06-26 19:35:16.873552
Model ind 665 epoch 938 batch: 600 avg loss -2.949121 avg loss no lamb -2.949121 time 2020-06-26 19:35:27.384748
Model ind 665 epoch 938 batch: 700 avg loss -2.810049 avg loss no lamb -2.810049 time 2020-06-26 19:35:37.974775
Model ind 665 epoch 938 batch: 800 avg loss -2.793881 avg loss no lamb -2.793881 time 2020-06-26 19:35:49.051303
last batch sz 10
Pre: time 2020-06-26 19:36:03.359197: 
 	std: 0.0029417106
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9796, 0.9736, 0.9805, 0.9751]
	train_accs: [0.9812833, 0.9806, 0.9759667, 0.9814, 0.97688335]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97788
	best: 0.9805

Starting e_i: 939
Model ind 665 epoch 939 batch: 0 avg loss -3.002595 avg loss no lamb -3.002595 time 2020-06-26 19:36:04.545530
Model ind 665 epoch 939 batch: 100 avg loss -2.913574 avg loss no lamb -2.913574 time 2020-06-26 19:36:15.390067
Model ind 665 epoch 939 batch: 200 avg loss -2.840065 avg loss no lamb -2.840065 time 2020-06-26 19:36:26.295678
Model ind 665 epoch 939 batch: 300 avg loss -2.839119 avg loss no lamb -2.839119 time 2020-06-26 19:36:37.187530
Model ind 665 epoch 939 batch: 400 avg loss -2.778340 avg loss no lamb -2.778340 time 2020-06-26 19:36:47.865984
Model ind 665 epoch 939 batch: 500 avg loss -2.800282 avg loss no lamb -2.800282 time 2020-06-26 19:36:58.793576
Model ind 665 epoch 939 batch: 600 avg loss -2.890974 avg loss no lamb -2.890974 time 2020-06-26 19:37:09.481655
Model ind 665 epoch 939 batch: 700 avg loss -2.687338 avg loss no lamb -2.687338 time 2020-06-26 19:37:20.526301
Model ind 665 epoch 939 batch: 800 avg loss -2.854456 avg loss no lamb -2.854456 time 2020-06-26 19:37:31.276035
last batch sz 10
Pre: time 2020-06-26 19:37:45.582443: 
 	std: 0.002987028
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9801, 0.9742, 0.9812, 0.9755]
	train_accs: [0.98143333, 0.98106664, 0.976, 0.9816333, 0.97671664]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97844
	best: 0.9812

Starting e_i: 940
Model ind 665 epoch 940 batch: 0 avg loss -2.918897 avg loss no lamb -2.918897 time 2020-06-26 19:37:46.579634
Model ind 665 epoch 940 batch: 100 avg loss -2.876475 avg loss no lamb -2.876475 time 2020-06-26 19:37:57.258345
Model ind 665 epoch 940 batch: 200 avg loss -2.866309 avg loss no lamb -2.866309 time 2020-06-26 19:38:08.340894
Model ind 665 epoch 940 batch: 300 avg loss -2.812270 avg loss no lamb -2.812270 time 2020-06-26 19:38:19.369586
Model ind 665 epoch 940 batch: 400 avg loss -2.794187 avg loss no lamb -2.794187 time 2020-06-26 19:38:30.164488
Model ind 665 epoch 940 batch: 500 avg loss -2.816362 avg loss no lamb -2.816362 time 2020-06-26 19:38:41.079464
Model ind 665 epoch 940 batch: 600 avg loss -2.938328 avg loss no lamb -2.938328 time 2020-06-26 19:38:51.887313
Model ind 665 epoch 940 batch: 700 avg loss -2.797359 avg loss no lamb -2.797359 time 2020-06-26 19:39:02.841152
Model ind 665 epoch 940 batch: 800 avg loss -2.879317 avg loss no lamb -2.879317 time 2020-06-26 19:39:13.664648
last batch sz 10
Pre: time 2020-06-26 19:39:27.619607: 
 	std: 0.0026732727
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9799, 0.9749, 0.9814, 0.9756]
	train_accs: [0.9812, 0.98116666, 0.97583336, 0.9817333, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97845995
	best: 0.9814

Starting e_i: 941
Model ind 665 epoch 941 batch: 0 avg loss -2.975834 avg loss no lamb -2.975834 time 2020-06-26 19:39:29.996446
Model ind 665 epoch 941 batch: 100 avg loss -2.865523 avg loss no lamb -2.865523 time 2020-06-26 19:39:40.961739
Model ind 665 epoch 941 batch: 200 avg loss -2.879789 avg loss no lamb -2.879789 time 2020-06-26 19:39:51.783877
Model ind 665 epoch 941 batch: 300 avg loss -2.897080 avg loss no lamb -2.897080 time 2020-06-26 19:40:02.804897
Model ind 665 epoch 941 batch: 400 avg loss -2.767975 avg loss no lamb -2.767975 time 2020-06-26 19:40:13.595413
Model ind 665 epoch 941 batch: 500 avg loss -2.824569 avg loss no lamb -2.824569 time 2020-06-26 19:40:24.159184
Model ind 665 epoch 941 batch: 600 avg loss -2.868251 avg loss no lamb -2.868251 time 2020-06-26 19:40:35.239421
Model ind 665 epoch 941 batch: 700 avg loss -2.623874 avg loss no lamb -2.623874 time 2020-06-26 19:40:46.100403
Model ind 665 epoch 941 batch: 800 avg loss -2.853245 avg loss no lamb -2.853245 time 2020-06-26 19:40:57.014943
last batch sz 10
Pre: time 2020-06-26 19:41:11.213566: 
 	std: 0.0033837233
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9811, 0.9739, 0.9816, 0.9749]
	train_accs: [0.98193336, 0.98143333, 0.97531664, 0.98211664, 0.9766833]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97852004
	best: 0.9816

Starting e_i: 942
Model ind 665 epoch 942 batch: 0 avg loss -2.981538 avg loss no lamb -2.981538 time 2020-06-26 19:41:12.236725
Model ind 665 epoch 942 batch: 100 avg loss -2.931606 avg loss no lamb -2.931606 time 2020-06-26 19:41:23.300726
Model ind 665 epoch 942 batch: 200 avg loss -2.931750 avg loss no lamb -2.931750 time 2020-06-26 19:41:34.394697
Model ind 665 epoch 942 batch: 300 avg loss -2.836062 avg loss no lamb -2.836062 time 2020-06-26 19:41:45.435084
Model ind 665 epoch 942 batch: 400 avg loss -2.759363 avg loss no lamb -2.759363 time 2020-06-26 19:41:56.335342
Model ind 665 epoch 942 batch: 500 avg loss -2.858017 avg loss no lamb -2.858017 time 2020-06-26 19:42:07.116555
Model ind 665 epoch 942 batch: 600 avg loss -2.823223 avg loss no lamb -2.823223 time 2020-06-26 19:42:17.677362
Model ind 665 epoch 942 batch: 700 avg loss -2.732100 avg loss no lamb -2.732100 time 2020-06-26 19:42:28.490243
Model ind 665 epoch 942 batch: 800 avg loss -2.840534 avg loss no lamb -2.840534 time 2020-06-26 19:42:39.596568
last batch sz 10
Pre: time 2020-06-26 19:42:53.563982: 
 	std: 0.002968757
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9794, 0.9735, 0.9808, 0.9752]
	train_accs: [0.98148334, 0.9804, 0.9752, 0.98143333, 0.97641665]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97788
	best: 0.9805

Starting e_i: 943
Model ind 665 epoch 943 batch: 0 avg loss -2.890898 avg loss no lamb -2.890898 time 2020-06-26 19:42:54.755104
Model ind 665 epoch 943 batch: 100 avg loss -2.909383 avg loss no lamb -2.909383 time 2020-06-26 19:43:05.590588
Model ind 665 epoch 943 batch: 200 avg loss -2.891570 avg loss no lamb -2.891570 time 2020-06-26 19:43:16.278053
Model ind 665 epoch 943 batch: 300 avg loss -2.852985 avg loss no lamb -2.852985 time 2020-06-26 19:43:27.264614
Model ind 665 epoch 943 batch: 400 avg loss -2.833116 avg loss no lamb -2.833116 time 2020-06-26 19:43:37.994071
Model ind 665 epoch 943 batch: 500 avg loss -2.846327 avg loss no lamb -2.846327 time 2020-06-26 19:43:48.635134
Model ind 665 epoch 943 batch: 600 avg loss -2.912093 avg loss no lamb -2.912093 time 2020-06-26 19:43:59.247221
Model ind 665 epoch 943 batch: 700 avg loss -2.801865 avg loss no lamb -2.801865 time 2020-06-26 19:44:10.000100
Model ind 665 epoch 943 batch: 800 avg loss -2.845139 avg loss no lamb -2.845139 time 2020-06-26 19:44:20.846907
last batch sz 10
Pre: time 2020-06-26 19:44:34.934502: 
 	std: 0.0031984942
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9811, 0.9745, 0.9817, 0.9761]
	train_accs: [0.98186666, 0.98121667, 0.97576666, 0.9817333, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97914
	best: 0.9823

Starting e_i: 944
Model ind 665 epoch 944 batch: 0 avg loss -2.945258 avg loss no lamb -2.945258 time 2020-06-26 19:44:35.948938
Model ind 665 epoch 944 batch: 100 avg loss -2.856309 avg loss no lamb -2.856309 time 2020-06-26 19:44:46.642185
Model ind 665 epoch 944 batch: 200 avg loss -2.856520 avg loss no lamb -2.856520 time 2020-06-26 19:44:57.324491
Model ind 665 epoch 944 batch: 300 avg loss -2.848296 avg loss no lamb -2.848296 time 2020-06-26 19:45:08.203776
Model ind 665 epoch 944 batch: 400 avg loss -2.812455 avg loss no lamb -2.812455 time 2020-06-26 19:45:19.149633
Model ind 665 epoch 944 batch: 500 avg loss -2.840554 avg loss no lamb -2.840554 time 2020-06-26 19:45:29.976225
Model ind 665 epoch 944 batch: 600 avg loss -2.850784 avg loss no lamb -2.850784 time 2020-06-26 19:45:40.772467
Model ind 665 epoch 944 batch: 700 avg loss -2.768165 avg loss no lamb -2.768165 time 2020-06-26 19:45:51.148974
Model ind 665 epoch 944 batch: 800 avg loss -2.825765 avg loss no lamb -2.825765 time 2020-06-26 19:46:01.872116
last batch sz 10
Pre: time 2020-06-26 19:46:16.176821: 
 	std: 0.0029687725
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9807, 0.9749, 0.982, 0.9759]
	train_accs: [0.98121667, 0.98085, 0.97603333, 0.9816333, 0.97676665]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97898006
	best: 0.982

Starting e_i: 945
Model ind 665 epoch 945 batch: 0 avg loss -2.953331 avg loss no lamb -2.953331 time 2020-06-26 19:46:17.336839
Model ind 665 epoch 945 batch: 100 avg loss -2.954049 avg loss no lamb -2.954049 time 2020-06-26 19:46:28.010199
Model ind 665 epoch 945 batch: 200 avg loss -2.873087 avg loss no lamb -2.873087 time 2020-06-26 19:46:38.813574
Model ind 665 epoch 945 batch: 300 avg loss -2.867773 avg loss no lamb -2.867773 time 2020-06-26 19:46:49.408465
Model ind 665 epoch 945 batch: 400 avg loss -2.791633 avg loss no lamb -2.791633 time 2020-06-26 19:47:00.329608
Model ind 665 epoch 945 batch: 500 avg loss -2.810749 avg loss no lamb -2.810749 time 2020-06-26 19:47:11.017956
Model ind 665 epoch 945 batch: 600 avg loss -2.849793 avg loss no lamb -2.849793 time 2020-06-26 19:47:21.799815
Model ind 665 epoch 945 batch: 700 avg loss -2.797598 avg loss no lamb -2.797598 time 2020-06-26 19:47:32.401493
Model ind 665 epoch 945 batch: 800 avg loss -2.899136 avg loss no lamb -2.899136 time 2020-06-26 19:47:43.322172
last batch sz 10
Pre: time 2020-06-26 19:47:57.267885: 
 	std: 0.0027846845
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9804, 0.9747, 0.981, 0.9761]
	train_accs: [0.98186666, 0.98118335, 0.9759, 0.98186666, 0.977]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97874004
	best: 0.9815

Starting e_i: 946
Model ind 665 epoch 946 batch: 0 avg loss -2.994630 avg loss no lamb -2.994630 time 2020-06-26 19:47:58.259117
Model ind 665 epoch 946 batch: 100 avg loss -2.900413 avg loss no lamb -2.900413 time 2020-06-26 19:48:09.071588
Model ind 665 epoch 946 batch: 200 avg loss -2.918693 avg loss no lamb -2.918693 time 2020-06-26 19:48:19.944341
Model ind 665 epoch 946 batch: 300 avg loss -2.832600 avg loss no lamb -2.832600 time 2020-06-26 19:48:30.951165
Model ind 665 epoch 946 batch: 400 avg loss -2.730235 avg loss no lamb -2.730235 time 2020-06-26 19:48:41.734227
Model ind 665 epoch 946 batch: 500 avg loss -2.866987 avg loss no lamb -2.866987 time 2020-06-26 19:48:52.390789
Model ind 665 epoch 946 batch: 600 avg loss -2.905191 avg loss no lamb -2.905191 time 2020-06-26 19:49:03.243431
Model ind 665 epoch 946 batch: 700 avg loss -2.787010 avg loss no lamb -2.787010 time 2020-06-26 19:49:13.793172
Model ind 665 epoch 946 batch: 800 avg loss -2.900973 avg loss no lamb -2.900973 time 2020-06-26 19:49:24.700468
last batch sz 10
Pre: time 2020-06-26 19:49:38.950400: 
 	std: 0.0026963705
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9812, 0.9756, 0.9822, 0.9773]
	train_accs: [0.9817, 0.98156667, 0.9766167, 0.98215, 0.9772]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97966003
	best: 0.9822

Starting e_i: 947
Model ind 665 epoch 947 batch: 0 avg loss -2.951589 avg loss no lamb -2.951589 time 2020-06-26 19:49:40.109516
Model ind 665 epoch 947 batch: 100 avg loss -2.888595 avg loss no lamb -2.888595 time 2020-06-26 19:49:50.980505
Model ind 665 epoch 947 batch: 200 avg loss -2.891390 avg loss no lamb -2.891390 time 2020-06-26 19:50:02.140289
Model ind 665 epoch 947 batch: 300 avg loss -2.873972 avg loss no lamb -2.873972 time 2020-06-26 19:50:12.795490
Model ind 665 epoch 947 batch: 400 avg loss -2.800445 avg loss no lamb -2.800445 time 2020-06-26 19:50:23.681720
Model ind 665 epoch 947 batch: 500 avg loss -2.801459 avg loss no lamb -2.801459 time 2020-06-26 19:50:34.671005
Model ind 665 epoch 947 batch: 600 avg loss -2.850867 avg loss no lamb -2.850867 time 2020-06-26 19:50:45.679345
Model ind 665 epoch 947 batch: 700 avg loss -2.704851 avg loss no lamb -2.704851 time 2020-06-26 19:50:56.562814
Model ind 665 epoch 947 batch: 800 avg loss -2.812852 avg loss no lamb -2.812852 time 2020-06-26 19:51:07.588933
last batch sz 10
Pre: time 2020-06-26 19:51:21.669626: 
 	std: 0.002316897
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9804, 0.9756, 0.9814, 0.9771]
	train_accs: [0.9816333, 0.98146665, 0.9765, 0.98191667, 0.9773833]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97910005
	best: 0.9814

Starting e_i: 948
Model ind 665 epoch 948 batch: 0 avg loss -2.979589 avg loss no lamb -2.979589 time 2020-06-26 19:51:22.728817
Model ind 665 epoch 948 batch: 100 avg loss -2.875945 avg loss no lamb -2.875945 time 2020-06-26 19:51:33.636078
Model ind 665 epoch 948 batch: 200 avg loss -2.920824 avg loss no lamb -2.920824 time 2020-06-26 19:51:44.427665
Model ind 665 epoch 948 batch: 300 avg loss -2.858450 avg loss no lamb -2.858450 time 2020-06-26 19:51:55.041185
Model ind 665 epoch 948 batch: 400 avg loss -2.777949 avg loss no lamb -2.777949 time 2020-06-26 19:52:05.572649
Model ind 665 epoch 948 batch: 500 avg loss -2.842058 avg loss no lamb -2.842058 time 2020-06-26 19:52:16.496367
Model ind 665 epoch 948 batch: 600 avg loss -2.883833 avg loss no lamb -2.883833 time 2020-06-26 19:52:27.298382
Model ind 665 epoch 948 batch: 700 avg loss -2.792848 avg loss no lamb -2.792848 time 2020-06-26 19:52:38.049900
Model ind 665 epoch 948 batch: 800 avg loss -2.858395 avg loss no lamb -2.858395 time 2020-06-26 19:52:48.773618
last batch sz 10
Pre: time 2020-06-26 19:53:02.701012: 
 	std: 0.0024408218
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9802, 0.9753, 0.9809, 0.9758]
	train_accs: [0.981, 0.98095, 0.97658336, 0.98141664, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97852004
	best: 0.9809

Starting e_i: 949
Model ind 665 epoch 949 batch: 0 avg loss -2.929776 avg loss no lamb -2.929776 time 2020-06-26 19:53:03.900467
Model ind 665 epoch 949 batch: 100 avg loss -2.846532 avg loss no lamb -2.846532 time 2020-06-26 19:53:14.793740
Model ind 665 epoch 949 batch: 200 avg loss -2.888333 avg loss no lamb -2.888333 time 2020-06-26 19:53:25.856952
Model ind 665 epoch 949 batch: 300 avg loss -2.909283 avg loss no lamb -2.909283 time 2020-06-26 19:53:36.502020
Model ind 665 epoch 949 batch: 400 avg loss -2.824126 avg loss no lamb -2.824126 time 2020-06-26 19:53:47.469456
Model ind 665 epoch 949 batch: 500 avg loss -2.857947 avg loss no lamb -2.857947 time 2020-06-26 19:53:58.258839
Model ind 665 epoch 949 batch: 600 avg loss -2.846455 avg loss no lamb -2.846455 time 2020-06-26 19:54:09.300952
Model ind 665 epoch 949 batch: 700 avg loss -2.738919 avg loss no lamb -2.738919 time 2020-06-26 19:54:20.026411
Model ind 665 epoch 949 batch: 800 avg loss -2.838611 avg loss no lamb -2.838611 time 2020-06-26 19:54:30.761297
last batch sz 10
Pre: time 2020-06-26 19:54:44.736294: 
 	std: 0.002725151
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9804, 0.9737, 0.9799, 0.9757]
	train_accs: [0.9815, 0.98118335, 0.97625, 0.98145, 0.97695]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97793996
	best: 0.98

Starting e_i: 950
Model ind 665 epoch 950 batch: 0 avg loss -2.944284 avg loss no lamb -2.944284 time 2020-06-26 19:54:45.737946
Model ind 665 epoch 950 batch: 100 avg loss -2.859965 avg loss no lamb -2.859965 time 2020-06-26 19:54:56.665122
Model ind 665 epoch 950 batch: 200 avg loss -2.858206 avg loss no lamb -2.858206 time 2020-06-26 19:55:07.473710
Model ind 665 epoch 950 batch: 300 avg loss -2.844239 avg loss no lamb -2.844239 time 2020-06-26 19:55:18.430182
Model ind 665 epoch 950 batch: 400 avg loss -2.832695 avg loss no lamb -2.832695 time 2020-06-26 19:55:29.245495
Model ind 665 epoch 950 batch: 500 avg loss -2.863886 avg loss no lamb -2.863886 time 2020-06-26 19:55:40.249711
Model ind 665 epoch 950 batch: 600 avg loss -2.902609 avg loss no lamb -2.902609 time 2020-06-26 19:55:51.156946
Model ind 665 epoch 950 batch: 700 avg loss -2.788170 avg loss no lamb -2.788170 time 2020-06-26 19:56:02.060176
Model ind 665 epoch 950 batch: 800 avg loss -2.819711 avg loss no lamb -2.819711 time 2020-06-26 19:56:13.269449
last batch sz 10
Pre: time 2020-06-26 19:56:27.150035: 
 	std: 0.002478237
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9797, 0.975, 0.98, 0.9747]
	train_accs: [0.98111665, 0.9806833, 0.9758833, 0.98125, 0.97625]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97788
	best: 0.98

Starting e_i: 951
Model ind 665 epoch 951 batch: 0 avg loss -2.902138 avg loss no lamb -2.902138 time 2020-06-26 19:56:29.551764
Model ind 665 epoch 951 batch: 100 avg loss -2.899565 avg loss no lamb -2.899565 time 2020-06-26 19:56:40.749862
Model ind 665 epoch 951 batch: 200 avg loss -2.847153 avg loss no lamb -2.847153 time 2020-06-26 19:56:51.837435
Model ind 665 epoch 951 batch: 300 avg loss -2.808300 avg loss no lamb -2.808300 time 2020-06-26 19:57:02.815796
Model ind 665 epoch 951 batch: 400 avg loss -2.784782 avg loss no lamb -2.784782 time 2020-06-26 19:57:13.563851
Model ind 665 epoch 951 batch: 500 avg loss -2.834967 avg loss no lamb -2.834967 time 2020-06-26 19:57:24.549934
Model ind 665 epoch 951 batch: 600 avg loss -2.876444 avg loss no lamb -2.876444 time 2020-06-26 19:57:35.543223
Model ind 665 epoch 951 batch: 700 avg loss -2.739650 avg loss no lamb -2.739650 time 2020-06-26 19:57:46.433795
Model ind 665 epoch 951 batch: 800 avg loss -2.833112 avg loss no lamb -2.833112 time 2020-06-26 19:57:57.321417
last batch sz 10
Pre: time 2020-06-26 19:58:11.338983: 
 	std: 0.0030235103
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9801, 0.9736, 0.9798, 0.9744]
	train_accs: [0.9812667, 0.98076665, 0.97583336, 0.9811, 0.9759]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.9776799
	best: 0.9805

Starting e_i: 952
Model ind 665 epoch 952 batch: 0 avg loss -2.919236 avg loss no lamb -2.919236 time 2020-06-26 19:58:12.369581
Model ind 665 epoch 952 batch: 100 avg loss -2.910575 avg loss no lamb -2.910575 time 2020-06-26 19:58:23.407963
Model ind 665 epoch 952 batch: 200 avg loss -2.864411 avg loss no lamb -2.864411 time 2020-06-26 19:58:34.407774
Model ind 665 epoch 952 batch: 300 avg loss -2.803236 avg loss no lamb -2.803236 time 2020-06-26 19:58:45.364101
Model ind 665 epoch 952 batch: 400 avg loss -2.729804 avg loss no lamb -2.729804 time 2020-06-26 19:58:56.144993
Model ind 665 epoch 952 batch: 500 avg loss -2.869536 avg loss no lamb -2.869536 time 2020-06-26 19:59:06.950762
Model ind 665 epoch 952 batch: 600 avg loss -2.875818 avg loss no lamb -2.875818 time 2020-06-26 19:59:17.782407
Model ind 665 epoch 952 batch: 700 avg loss -2.774498 avg loss no lamb -2.774498 time 2020-06-26 19:59:28.844805
Model ind 665 epoch 952 batch: 800 avg loss -2.840261 avg loss no lamb -2.840261 time 2020-06-26 19:59:39.832778
last batch sz 10
Pre: time 2020-06-26 19:59:54.206085: 
 	std: 0.0029123263
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9787, 0.9733, 0.9797, 0.9739]
	train_accs: [0.98148334, 0.98075, 0.97575, 0.9815, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97712004
	best: 0.9797

Starting e_i: 953
Model ind 665 epoch 953 batch: 0 avg loss -2.930461 avg loss no lamb -2.930461 time 2020-06-26 19:59:55.368851
Model ind 665 epoch 953 batch: 100 avg loss -2.902275 avg loss no lamb -2.902275 time 2020-06-26 20:00:06.617029
Model ind 665 epoch 953 batch: 200 avg loss -2.839220 avg loss no lamb -2.839220 time 2020-06-26 20:00:17.437364
Model ind 665 epoch 953 batch: 300 avg loss -2.879105 avg loss no lamb -2.879105 time 2020-06-26 20:00:28.192221
Model ind 665 epoch 953 batch: 400 avg loss -2.779362 avg loss no lamb -2.779362 time 2020-06-26 20:00:38.816247
Model ind 665 epoch 953 batch: 500 avg loss -2.823366 avg loss no lamb -2.823366 time 2020-06-26 20:00:49.952071
Model ind 665 epoch 953 batch: 600 avg loss -2.911886 avg loss no lamb -2.911886 time 2020-06-26 20:01:01.049195
Model ind 665 epoch 953 batch: 700 avg loss -2.729504 avg loss no lamb -2.729504 time 2020-06-26 20:01:12.013700
Model ind 665 epoch 953 batch: 800 avg loss -2.816631 avg loss no lamb -2.816631 time 2020-06-26 20:01:22.963317
last batch sz 10
Pre: time 2020-06-26 20:01:37.089259: 
 	std: 0.002616565
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9796, 0.974, 0.9803, 0.975]
	train_accs: [0.9816333, 0.98106664, 0.97625, 0.98158336, 0.9769833]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97766
	best: 0.9794

Starting e_i: 954
Model ind 665 epoch 954 batch: 0 avg loss -2.950587 avg loss no lamb -2.950587 time 2020-06-26 20:01:38.101381
Model ind 665 epoch 954 batch: 100 avg loss -2.870949 avg loss no lamb -2.870949 time 2020-06-26 20:01:48.866875
Model ind 665 epoch 954 batch: 200 avg loss -2.835750 avg loss no lamb -2.835750 time 2020-06-26 20:01:59.747858
Model ind 665 epoch 954 batch: 300 avg loss -2.756253 avg loss no lamb -2.756253 time 2020-06-26 20:02:10.850493
Model ind 665 epoch 954 batch: 400 avg loss -2.745248 avg loss no lamb -2.745248 time 2020-06-26 20:02:21.849893
Model ind 665 epoch 954 batch: 500 avg loss -2.838425 avg loss no lamb -2.838425 time 2020-06-26 20:02:32.750834
Model ind 665 epoch 954 batch: 600 avg loss -2.857526 avg loss no lamb -2.857526 time 2020-06-26 20:02:43.658411
Model ind 665 epoch 954 batch: 700 avg loss -2.768454 avg loss no lamb -2.768454 time 2020-06-26 20:02:54.684745
Model ind 665 epoch 954 batch: 800 avg loss -2.874249 avg loss no lamb -2.874249 time 2020-06-26 20:03:05.629050
last batch sz 10
Pre: time 2020-06-26 20:03:19.825310: 
 	std: 0.0030571853
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9803, 0.973, 0.9801, 0.975]
	train_accs: [0.9817167, 0.98135, 0.97618335, 0.9814, 0.97695]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97766
	best: 0.9799

Starting e_i: 955
Model ind 665 epoch 955 batch: 0 avg loss -2.942249 avg loss no lamb -2.942249 time 2020-06-26 20:03:20.976173
Model ind 665 epoch 955 batch: 100 avg loss -2.851315 avg loss no lamb -2.851315 time 2020-06-26 20:03:32.087866
Model ind 665 epoch 955 batch: 200 avg loss -2.860613 avg loss no lamb -2.860613 time 2020-06-26 20:03:42.956429
Model ind 665 epoch 955 batch: 300 avg loss -2.907037 avg loss no lamb -2.907037 time 2020-06-26 20:03:53.779471
Model ind 665 epoch 955 batch: 400 avg loss -2.791617 avg loss no lamb -2.791617 time 2020-06-26 20:04:04.483376
Model ind 665 epoch 955 batch: 500 avg loss -2.855369 avg loss no lamb -2.855369 time 2020-06-26 20:04:15.269162
Model ind 665 epoch 955 batch: 600 avg loss -2.889308 avg loss no lamb -2.889308 time 2020-06-26 20:04:26.359848
Model ind 665 epoch 955 batch: 700 avg loss -2.720339 avg loss no lamb -2.720339 time 2020-06-26 20:04:37.371494
Model ind 665 epoch 955 batch: 800 avg loss -2.833512 avg loss no lamb -2.833512 time 2020-06-26 20:04:48.167970
last batch sz 10
Pre: time 2020-06-26 20:05:02.054970: 
 	std: 0.0031993764
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9782, 0.9719, 0.9792, 0.9732]
	train_accs: [0.981, 0.9802167, 0.97511667, 0.9809167, 0.97548336]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.9764
	best: 0.9795

Starting e_i: 956
Model ind 665 epoch 956 batch: 0 avg loss -2.926092 avg loss no lamb -2.926092 time 2020-06-26 20:05:03.155501
Model ind 665 epoch 956 batch: 100 avg loss -2.849329 avg loss no lamb -2.849329 time 2020-06-26 20:05:14.160193
Model ind 665 epoch 956 batch: 200 avg loss -2.874606 avg loss no lamb -2.874606 time 2020-06-26 20:05:25.162107
Model ind 665 epoch 956 batch: 300 avg loss -2.818810 avg loss no lamb -2.818810 time 2020-06-26 20:05:36.064190
Model ind 665 epoch 956 batch: 400 avg loss -2.732473 avg loss no lamb -2.732473 time 2020-06-26 20:05:46.707714
Model ind 665 epoch 956 batch: 500 avg loss -2.839277 avg loss no lamb -2.839277 time 2020-06-26 20:05:57.378631
Model ind 665 epoch 956 batch: 600 avg loss -2.904910 avg loss no lamb -2.904910 time 2020-06-26 20:06:08.134914
Model ind 665 epoch 956 batch: 700 avg loss -2.737979 avg loss no lamb -2.737979 time 2020-06-26 20:06:19.201676
Model ind 665 epoch 956 batch: 800 avg loss -2.873522 avg loss no lamb -2.873522 time 2020-06-26 20:06:29.900367
last batch sz 10
Pre: time 2020-06-26 20:06:44.088889: 
 	std: 0.0035762482
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9802, 0.9727, 0.9809, 0.9742]
	train_accs: [0.9817, 0.98085, 0.97525, 0.9817333, 0.9763333]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97778
	best: 0.9809

Starting e_i: 957
Model ind 665 epoch 957 batch: 0 avg loss -2.970495 avg loss no lamb -2.970495 time 2020-06-26 20:06:45.278206
Model ind 665 epoch 957 batch: 100 avg loss -2.871029 avg loss no lamb -2.871029 time 2020-06-26 20:06:56.192819
Model ind 665 epoch 957 batch: 200 avg loss -2.844904 avg loss no lamb -2.844904 time 2020-06-26 20:07:07.314885
Model ind 665 epoch 957 batch: 300 avg loss -2.810897 avg loss no lamb -2.810897 time 2020-06-26 20:07:18.182499
Model ind 665 epoch 957 batch: 400 avg loss -2.800003 avg loss no lamb -2.800003 time 2020-06-26 20:07:29.266045
Model ind 665 epoch 957 batch: 500 avg loss -2.829813 avg loss no lamb -2.829813 time 2020-06-26 20:07:40.363383
Model ind 665 epoch 957 batch: 600 avg loss -2.853643 avg loss no lamb -2.853643 time 2020-06-26 20:07:51.478349
Model ind 665 epoch 957 batch: 700 avg loss -2.735851 avg loss no lamb -2.735851 time 2020-06-26 20:08:02.580155
Model ind 665 epoch 957 batch: 800 avg loss -2.813262 avg loss no lamb -2.813262 time 2020-06-26 20:08:13.448755
last batch sz 10
Pre: time 2020-06-26 20:08:27.676877: 
 	std: 0.0029815491
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9793, 0.9733, 0.9801, 0.9747]
	train_accs: [0.98153335, 0.98078334, 0.975, 0.98143333, 0.9765667]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97757995
	best: 0.9805

Starting e_i: 958
Model ind 665 epoch 958 batch: 0 avg loss -2.922290 avg loss no lamb -2.922290 time 2020-06-26 20:08:28.686165
Model ind 665 epoch 958 batch: 100 avg loss -2.937058 avg loss no lamb -2.937058 time 2020-06-26 20:08:39.425443
Model ind 665 epoch 958 batch: 200 avg loss -2.905549 avg loss no lamb -2.905549 time 2020-06-26 20:08:50.599259
Model ind 665 epoch 958 batch: 300 avg loss -2.791104 avg loss no lamb -2.791104 time 2020-06-26 20:09:01.751199
Model ind 665 epoch 958 batch: 400 avg loss -2.811179 avg loss no lamb -2.811179 time 2020-06-26 20:09:12.759093
Model ind 665 epoch 958 batch: 500 avg loss -2.844590 avg loss no lamb -2.844590 time 2020-06-26 20:09:23.667267
Model ind 665 epoch 958 batch: 600 avg loss -2.862838 avg loss no lamb -2.862838 time 2020-06-26 20:09:34.488805
Model ind 665 epoch 958 batch: 700 avg loss -2.723675 avg loss no lamb -2.723675 time 2020-06-26 20:09:45.523120
Model ind 665 epoch 958 batch: 800 avg loss -2.839915 avg loss no lamb -2.839915 time 2020-06-26 20:09:56.686976
last batch sz 10
Pre: time 2020-06-26 20:10:10.875083: 
 	std: 0.003372005
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9798, 0.9731, 0.9807, 0.9736]
	train_accs: [0.98121667, 0.98018336, 0.9748833, 0.98153335, 0.97571665]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97746
	best: 0.9807

Starting e_i: 959
Model ind 665 epoch 959 batch: 0 avg loss -2.975897 avg loss no lamb -2.975897 time 2020-06-26 20:10:12.056584
Model ind 665 epoch 959 batch: 100 avg loss -2.937815 avg loss no lamb -2.937815 time 2020-06-26 20:10:22.895356
Model ind 665 epoch 959 batch: 200 avg loss -2.861362 avg loss no lamb -2.861362 time 2020-06-26 20:10:33.687761
Model ind 665 epoch 959 batch: 300 avg loss -2.856748 avg loss no lamb -2.856748 time 2020-06-26 20:10:44.617194
Model ind 665 epoch 959 batch: 400 avg loss -2.823957 avg loss no lamb -2.823957 time 2020-06-26 20:10:55.846984
Model ind 665 epoch 959 batch: 500 avg loss -2.821350 avg loss no lamb -2.821350 time 2020-06-26 20:11:06.942338
Model ind 665 epoch 959 batch: 600 avg loss -2.851464 avg loss no lamb -2.851464 time 2020-06-26 20:11:17.988970
Model ind 665 epoch 959 batch: 700 avg loss -2.762305 avg loss no lamb -2.762305 time 2020-06-26 20:11:29.155513
Model ind 665 epoch 959 batch: 800 avg loss -2.882904 avg loss no lamb -2.882904 time 2020-06-26 20:11:40.113457
last batch sz 10
Pre: time 2020-06-26 20:11:54.285249: 
 	std: 0.0033511673
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9811, 0.9732, 0.9808, 0.975]
	train_accs: [0.98141664, 0.9813, 0.9756, 0.98146665, 0.9765]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97814
	best: 0.9808

Starting e_i: 960
Model ind 665 epoch 960 batch: 0 avg loss -2.928428 avg loss no lamb -2.928428 time 2020-06-26 20:11:55.336788
Model ind 665 epoch 960 batch: 100 avg loss -2.901207 avg loss no lamb -2.901207 time 2020-06-26 20:12:06.324165
Model ind 665 epoch 960 batch: 200 avg loss -2.889294 avg loss no lamb -2.889294 time 2020-06-26 20:12:16.889009
Model ind 665 epoch 960 batch: 300 avg loss -2.845155 avg loss no lamb -2.845155 time 2020-06-26 20:12:27.534638
Model ind 665 epoch 960 batch: 400 avg loss -2.743265 avg loss no lamb -2.743265 time 2020-06-26 20:12:38.191451
Model ind 665 epoch 960 batch: 500 avg loss -2.792984 avg loss no lamb -2.792984 time 2020-06-26 20:12:49.355681
Model ind 665 epoch 960 batch: 600 avg loss -2.861579 avg loss no lamb -2.861579 time 2020-06-26 20:13:00.149385
Model ind 665 epoch 960 batch: 700 avg loss -2.753704 avg loss no lamb -2.753704 time 2020-06-26 20:13:11.079301
Model ind 665 epoch 960 batch: 800 avg loss -2.826025 avg loss no lamb -2.826025 time 2020-06-26 20:13:21.861262
last batch sz 10
Pre: time 2020-06-26 20:13:35.913406: 
 	std: 0.003101348
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9802, 0.9735, 0.981, 0.9759]
	train_accs: [0.9812, 0.98075, 0.97513336, 0.98135, 0.97625]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97836
	best: 0.981

Starting e_i: 961
Model ind 665 epoch 961 batch: 0 avg loss -2.942438 avg loss no lamb -2.942438 time 2020-06-26 20:13:38.236084
Model ind 665 epoch 961 batch: 100 avg loss -2.919857 avg loss no lamb -2.919857 time 2020-06-26 20:13:49.169468
Model ind 665 epoch 961 batch: 200 avg loss -2.903239 avg loss no lamb -2.903239 time 2020-06-26 20:14:00.219836
Model ind 665 epoch 961 batch: 300 avg loss -2.835662 avg loss no lamb -2.835662 time 2020-06-26 20:14:11.190764
Model ind 665 epoch 961 batch: 400 avg loss -2.721967 avg loss no lamb -2.721967 time 2020-06-26 20:14:22.159036
Model ind 665 epoch 961 batch: 500 avg loss -2.861327 avg loss no lamb -2.861327 time 2020-06-26 20:14:33.387600
Model ind 665 epoch 961 batch: 600 avg loss -2.867344 avg loss no lamb -2.867344 time 2020-06-26 20:14:44.373763
Model ind 665 epoch 961 batch: 700 avg loss -2.743340 avg loss no lamb -2.743340 time 2020-06-26 20:14:55.278657
Model ind 665 epoch 961 batch: 800 avg loss -2.826767 avg loss no lamb -2.826767 time 2020-06-26 20:15:06.355065
last batch sz 10
Pre: time 2020-06-26 20:15:20.657503: 
 	std: 0.0031296066
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9803, 0.9738, 0.981, 0.9756]
	train_accs: [0.98195, 0.98085, 0.9752, 0.98191667, 0.97645]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97844
	best: 0.9815

Starting e_i: 962
Model ind 665 epoch 962 batch: 0 avg loss -2.884751 avg loss no lamb -2.884751 time 2020-06-26 20:15:21.698938
Model ind 665 epoch 962 batch: 100 avg loss -2.918906 avg loss no lamb -2.918906 time 2020-06-26 20:15:32.443722
Model ind 665 epoch 962 batch: 200 avg loss -2.857983 avg loss no lamb -2.857983 time 2020-06-26 20:15:43.208351
Model ind 665 epoch 962 batch: 300 avg loss -2.788777 avg loss no lamb -2.788777 time 2020-06-26 20:15:53.975091
Model ind 665 epoch 962 batch: 400 avg loss -2.812038 avg loss no lamb -2.812038 time 2020-06-26 20:16:05.149162
Model ind 665 epoch 962 batch: 500 avg loss -2.789384 avg loss no lamb -2.789384 time 2020-06-26 20:16:16.359943
Model ind 665 epoch 962 batch: 600 avg loss -2.854412 avg loss no lamb -2.854412 time 2020-06-26 20:16:27.124282
Model ind 665 epoch 962 batch: 700 avg loss -2.782878 avg loss no lamb -2.782878 time 2020-06-26 20:16:38.167325
Model ind 665 epoch 962 batch: 800 avg loss -2.879857 avg loss no lamb -2.879857 time 2020-06-26 20:16:48.931357
last batch sz 10
Pre: time 2020-06-26 20:17:03.579757: 
 	std: 0.0024808003
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.979, 0.9783, 0.9736, 0.9795, 0.9743]
	train_accs: [0.9810167, 0.9805, 0.97496665, 0.9809, 0.97581667]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97694
	best: 0.979

Starting e_i: 963
Model ind 665 epoch 963 batch: 0 avg loss -2.957588 avg loss no lamb -2.957588 time 2020-06-26 20:17:04.833430
Model ind 665 epoch 963 batch: 100 avg loss -2.981649 avg loss no lamb -2.981649 time 2020-06-26 20:17:15.786915
Model ind 665 epoch 963 batch: 200 avg loss -2.875774 avg loss no lamb -2.875774 time 2020-06-26 20:17:26.663469
Model ind 665 epoch 963 batch: 300 avg loss -2.869526 avg loss no lamb -2.869526 time 2020-06-26 20:17:37.409569
Model ind 665 epoch 963 batch: 400 avg loss -2.809410 avg loss no lamb -2.809410 time 2020-06-26 20:17:48.478852
Model ind 665 epoch 963 batch: 500 avg loss -2.851407 avg loss no lamb -2.851407 time 2020-06-26 20:17:59.251145
Model ind 665 epoch 963 batch: 600 avg loss -2.903688 avg loss no lamb -2.903688 time 2020-06-26 20:18:10.287231
Model ind 665 epoch 963 batch: 700 avg loss -2.790205 avg loss no lamb -2.790205 time 2020-06-26 20:18:20.991323
Model ind 665 epoch 963 batch: 800 avg loss -2.860687 avg loss no lamb -2.860687 time 2020-06-26 20:18:32.032849
last batch sz 10
Pre: time 2020-06-26 20:18:46.037527: 
 	std: 0.0026157983
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9808, 0.9754, 0.9808, 0.9759]
	train_accs: [0.9815, 0.98088336, 0.9755167, 0.9813833, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97884
	best: 0.9813

Starting e_i: 964
Model ind 665 epoch 964 batch: 0 avg loss -2.909563 avg loss no lamb -2.909563 time 2020-06-26 20:18:47.081854
Model ind 665 epoch 964 batch: 100 avg loss -2.874383 avg loss no lamb -2.874383 time 2020-06-26 20:18:57.939986
Model ind 665 epoch 964 batch: 200 avg loss -2.842619 avg loss no lamb -2.842619 time 2020-06-26 20:19:08.666374
Model ind 665 epoch 964 batch: 300 avg loss -2.886156 avg loss no lamb -2.886156 time 2020-06-26 20:19:19.361186
Model ind 665 epoch 964 batch: 400 avg loss -2.805620 avg loss no lamb -2.805620 time 2020-06-26 20:19:30.185577
Model ind 665 epoch 964 batch: 500 avg loss -2.791019 avg loss no lamb -2.791019 time 2020-06-26 20:19:40.750527
Model ind 665 epoch 964 batch: 600 avg loss -2.922223 avg loss no lamb -2.922223 time 2020-06-26 20:19:51.654423
Model ind 665 epoch 964 batch: 700 avg loss -2.725864 avg loss no lamb -2.725864 time 2020-06-26 20:20:02.562608
Model ind 665 epoch 964 batch: 800 avg loss -2.826105 avg loss no lamb -2.826105 time 2020-06-26 20:20:13.073684
last batch sz 10
Pre: time 2020-06-26 20:20:27.088072: 
 	std: 0.0029586481
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9808, 0.9753, 0.9822, 0.976]
	train_accs: [0.98158336, 0.98065, 0.97576666, 0.98175, 0.9763333]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97922003
	best: 0.9822

Starting e_i: 965
Model ind 665 epoch 965 batch: 0 avg loss -2.919245 avg loss no lamb -2.919245 time 2020-06-26 20:20:28.266816
Model ind 665 epoch 965 batch: 100 avg loss -2.884990 avg loss no lamb -2.884990 time 2020-06-26 20:20:39.199700
Model ind 665 epoch 965 batch: 200 avg loss -2.837857 avg loss no lamb -2.837857 time 2020-06-26 20:20:49.791886
Model ind 665 epoch 965 batch: 300 avg loss -2.852189 avg loss no lamb -2.852189 time 2020-06-26 20:21:00.552986
Model ind 665 epoch 965 batch: 400 avg loss -2.718259 avg loss no lamb -2.718259 time 2020-06-26 20:21:11.439132
Model ind 665 epoch 965 batch: 500 avg loss -2.833095 avg loss no lamb -2.833095 time 2020-06-26 20:21:22.493689
Model ind 665 epoch 965 batch: 600 avg loss -2.857480 avg loss no lamb -2.857480 time 2020-06-26 20:21:33.274163
Model ind 665 epoch 965 batch: 700 avg loss -2.778222 avg loss no lamb -2.778222 time 2020-06-26 20:21:44.256252
Model ind 665 epoch 965 batch: 800 avg loss -2.844353 avg loss no lamb -2.844353 time 2020-06-26 20:21:55.129068
last batch sz 10
Pre: time 2020-06-26 20:22:09.315733: 
 	std: 0.0030637337
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9811, 0.9748, 0.9825, 0.9774]
	train_accs: [0.98178333, 0.98105, 0.97585, 0.98188335, 0.97713333]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97966003
	best: 0.9825

Starting e_i: 966
Model ind 665 epoch 966 batch: 0 avg loss -2.932495 avg loss no lamb -2.932495 time 2020-06-26 20:22:10.370310
Model ind 665 epoch 966 batch: 100 avg loss -2.946557 avg loss no lamb -2.946557 time 2020-06-26 20:22:21.085299
Model ind 665 epoch 966 batch: 200 avg loss -2.897743 avg loss no lamb -2.897743 time 2020-06-26 20:22:31.814055
Model ind 665 epoch 966 batch: 300 avg loss -2.867212 avg loss no lamb -2.867212 time 2020-06-26 20:22:42.867045
Model ind 665 epoch 966 batch: 400 avg loss -2.697065 avg loss no lamb -2.697065 time 2020-06-26 20:22:53.770672
Model ind 665 epoch 966 batch: 500 avg loss -2.821573 avg loss no lamb -2.821573 time 2020-06-26 20:23:04.672435
Model ind 665 epoch 966 batch: 600 avg loss -2.810577 avg loss no lamb -2.810577 time 2020-06-26 20:23:15.478443
Model ind 665 epoch 966 batch: 700 avg loss -2.836574 avg loss no lamb -2.836574 time 2020-06-26 20:23:26.171851
Model ind 665 epoch 966 batch: 800 avg loss -2.871066 avg loss no lamb -2.871066 time 2020-06-26 20:23:36.886670
last batch sz 10
Pre: time 2020-06-26 20:23:50.980978: 
 	std: 0.002695476
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.98, 0.9744, 0.9808, 0.9759]
	train_accs: [0.98121667, 0.98066664, 0.9751667, 0.9814, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97838
	best: 0.9808

Starting e_i: 967
Model ind 665 epoch 967 batch: 0 avg loss -2.900711 avg loss no lamb -2.900711 time 2020-06-26 20:23:52.155470
Model ind 665 epoch 967 batch: 100 avg loss -2.932965 avg loss no lamb -2.932965 time 2020-06-26 20:24:02.954074
Model ind 665 epoch 967 batch: 200 avg loss -2.905198 avg loss no lamb -2.905198 time 2020-06-26 20:24:13.770490
Model ind 665 epoch 967 batch: 300 avg loss -2.887475 avg loss no lamb -2.887475 time 2020-06-26 20:24:24.645508
Model ind 665 epoch 967 batch: 400 avg loss -2.782800 avg loss no lamb -2.782800 time 2020-06-26 20:24:35.678459
Model ind 665 epoch 967 batch: 500 avg loss -2.825839 avg loss no lamb -2.825839 time 2020-06-26 20:24:46.569029
Model ind 665 epoch 967 batch: 600 avg loss -2.905760 avg loss no lamb -2.905760 time 2020-06-26 20:24:57.499751
Model ind 665 epoch 967 batch: 700 avg loss -2.749058 avg loss no lamb -2.749058 time 2020-06-26 20:25:08.518270
Model ind 665 epoch 967 batch: 800 avg loss -2.897690 avg loss no lamb -2.897690 time 2020-06-26 20:25:19.385507
last batch sz 10
Pre: time 2020-06-26 20:25:33.489749: 
 	std: 0.0025150578
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9799, 0.9746, 0.9794, 0.9743]
	train_accs: [0.9810167, 0.98071665, 0.97573334, 0.9809, 0.97665]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97752
	best: 0.9794

Starting e_i: 968
Model ind 665 epoch 968 batch: 0 avg loss -2.917961 avg loss no lamb -2.917961 time 2020-06-26 20:25:34.581681
Model ind 665 epoch 968 batch: 100 avg loss -2.820012 avg loss no lamb -2.820012 time 2020-06-26 20:25:45.386251
Model ind 665 epoch 968 batch: 200 avg loss -2.861276 avg loss no lamb -2.861276 time 2020-06-26 20:25:56.248228
Model ind 665 epoch 968 batch: 300 avg loss -2.868017 avg loss no lamb -2.868017 time 2020-06-26 20:26:07.098298
Model ind 665 epoch 968 batch: 400 avg loss -2.772257 avg loss no lamb -2.772257 time 2020-06-26 20:26:18.043125
Model ind 665 epoch 968 batch: 500 avg loss -2.886910 avg loss no lamb -2.886910 time 2020-06-26 20:26:28.855538
Model ind 665 epoch 968 batch: 600 avg loss -2.839518 avg loss no lamb -2.839518 time 2020-06-26 20:26:39.753542
Model ind 665 epoch 968 batch: 700 avg loss -2.804356 avg loss no lamb -2.804356 time 2020-06-26 20:26:50.458431
Model ind 665 epoch 968 batch: 800 avg loss -2.872811 avg loss no lamb -2.872811 time 2020-06-26 20:27:01.457993
last batch sz 10
Pre: time 2020-06-26 20:27:15.664009: 
 	std: 0.003361314
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9812, 0.9747, 0.9818, 0.9752]
	train_accs: [0.9817167, 0.98075, 0.97575, 0.9816167, 0.9765]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97904
	best: 0.9823

Starting e_i: 969
Model ind 665 epoch 969 batch: 0 avg loss -2.880354 avg loss no lamb -2.880354 time 2020-06-26 20:27:16.820044
Model ind 665 epoch 969 batch: 100 avg loss -2.927851 avg loss no lamb -2.927851 time 2020-06-26 20:27:27.464452
Model ind 665 epoch 969 batch: 200 avg loss -2.903894 avg loss no lamb -2.903894 time 2020-06-26 20:27:38.095081
Model ind 665 epoch 969 batch: 300 avg loss -2.803746 avg loss no lamb -2.803746 time 2020-06-26 20:27:48.864175
Model ind 665 epoch 969 batch: 400 avg loss -2.755844 avg loss no lamb -2.755844 time 2020-06-26 20:27:59.681071
Model ind 665 epoch 969 batch: 500 avg loss -2.835396 avg loss no lamb -2.835396 time 2020-06-26 20:28:10.649721
Model ind 665 epoch 969 batch: 600 avg loss -2.883283 avg loss no lamb -2.883283 time 2020-06-26 20:28:21.358184
Model ind 665 epoch 969 batch: 700 avg loss -2.780659 avg loss no lamb -2.780659 time 2020-06-26 20:28:32.132982
Model ind 665 epoch 969 batch: 800 avg loss -2.891302 avg loss no lamb -2.891302 time 2020-06-26 20:28:42.965875
last batch sz 10
Pre: time 2020-06-26 20:28:57.047606: 
 	std: 0.0028277056
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9812, 0.9757, 0.9819, 0.976]
	train_accs: [0.98178333, 0.9812667, 0.97578335, 0.9816333, 0.9763833]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.9793
	best: 0.9817

Starting e_i: 970
Model ind 665 epoch 970 batch: 0 avg loss -2.953514 avg loss no lamb -2.953514 time 2020-06-26 20:28:58.134011
Model ind 665 epoch 970 batch: 100 avg loss -2.904037 avg loss no lamb -2.904037 time 2020-06-26 20:29:08.959401
Model ind 665 epoch 970 batch: 200 avg loss -2.892292 avg loss no lamb -2.892292 time 2020-06-26 20:29:19.745198
Model ind 665 epoch 970 batch: 300 avg loss -2.870326 avg loss no lamb -2.870326 time 2020-06-26 20:29:30.523808
Model ind 665 epoch 970 batch: 400 avg loss -2.770784 avg loss no lamb -2.770784 time 2020-06-26 20:29:41.626010
Model ind 665 epoch 970 batch: 500 avg loss -2.836527 avg loss no lamb -2.836527 time 2020-06-26 20:29:52.391179
Model ind 665 epoch 970 batch: 600 avg loss -2.840948 avg loss no lamb -2.840948 time 2020-06-26 20:30:03.178882
Model ind 665 epoch 970 batch: 700 avg loss -2.804901 avg loss no lamb -2.804901 time 2020-06-26 20:30:13.802859
Model ind 665 epoch 970 batch: 800 avg loss -2.891855 avg loss no lamb -2.891855 time 2020-06-26 20:30:24.636744
last batch sz 10
Pre: time 2020-06-26 20:30:38.832731: 
 	std: 0.002846468
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9824, 0.976, 0.9824, 0.977]
	train_accs: [0.98205, 0.9816, 0.97653335, 0.98226666, 0.9773167]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.9799601
	best: 0.9824

Starting e_i: 971
Model ind 665 epoch 971 batch: 0 avg loss -2.954466 avg loss no lamb -2.954466 time 2020-06-26 20:30:41.136880
Model ind 665 epoch 971 batch: 100 avg loss -2.919419 avg loss no lamb -2.919419 time 2020-06-26 20:30:51.994629
Model ind 665 epoch 971 batch: 200 avg loss -2.820419 avg loss no lamb -2.820419 time 2020-06-26 20:31:02.491181
Model ind 665 epoch 971 batch: 300 avg loss -2.867793 avg loss no lamb -2.867793 time 2020-06-26 20:31:13.585118
Model ind 665 epoch 971 batch: 400 avg loss -2.760615 avg loss no lamb -2.760615 time 2020-06-26 20:31:24.554287
Model ind 665 epoch 971 batch: 500 avg loss -2.873828 avg loss no lamb -2.873828 time 2020-06-26 20:31:35.426802
Model ind 665 epoch 971 batch: 600 avg loss -2.784846 avg loss no lamb -2.784846 time 2020-06-26 20:31:46.244015
Model ind 665 epoch 971 batch: 700 avg loss -2.745404 avg loss no lamb -2.745404 time 2020-06-26 20:31:57.283106
Model ind 665 epoch 971 batch: 800 avg loss -2.860034 avg loss no lamb -2.860034 time 2020-06-26 20:32:07.994298
last batch sz 10
Pre: time 2020-06-26 20:32:22.129772: 
 	std: 0.0029924111
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9814, 0.9747, 0.9804, 0.9751]
	train_accs: [0.9821, 0.98158336, 0.97595, 0.9820833, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97854006
	best: 0.9811

Starting e_i: 972
Model ind 665 epoch 972 batch: 0 avg loss -2.953122 avg loss no lamb -2.953122 time 2020-06-26 20:32:23.135888
Model ind 665 epoch 972 batch: 100 avg loss -2.923670 avg loss no lamb -2.923670 time 2020-06-26 20:32:33.987694
Model ind 665 epoch 972 batch: 200 avg loss -2.820818 avg loss no lamb -2.820818 time 2020-06-26 20:32:44.751329
Model ind 665 epoch 972 batch: 300 avg loss -2.867027 avg loss no lamb -2.867027 time 2020-06-26 20:32:55.549789
Model ind 665 epoch 972 batch: 400 avg loss -2.805338 avg loss no lamb -2.805338 time 2020-06-26 20:33:06.662785
Model ind 665 epoch 972 batch: 500 avg loss -2.837899 avg loss no lamb -2.837899 time 2020-06-26 20:33:17.706705
Model ind 665 epoch 972 batch: 600 avg loss -2.922520 avg loss no lamb -2.922520 time 2020-06-26 20:33:28.547455
Model ind 665 epoch 972 batch: 700 avg loss -2.742728 avg loss no lamb -2.742728 time 2020-06-26 20:33:39.440605
Model ind 665 epoch 972 batch: 800 avg loss -2.891952 avg loss no lamb -2.891952 time 2020-06-26 20:33:50.557841
last batch sz 10
Pre: time 2020-06-26 20:34:04.737228: 
 	std: 0.003219329
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9821, 0.9748, 0.9822, 0.9762]
	train_accs: [0.98176664, 0.9815, 0.9756167, 0.98205, 0.97665]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97940004
	best: 0.9822

Starting e_i: 973
Model ind 665 epoch 973 batch: 0 avg loss -2.908634 avg loss no lamb -2.908634 time 2020-06-26 20:34:05.937958
Model ind 665 epoch 973 batch: 100 avg loss -2.851187 avg loss no lamb -2.851187 time 2020-06-26 20:34:17.000643
Model ind 665 epoch 973 batch: 200 avg loss -2.858850 avg loss no lamb -2.858850 time 2020-06-26 20:34:27.955867
Model ind 665 epoch 973 batch: 300 avg loss -2.850133 avg loss no lamb -2.850133 time 2020-06-26 20:34:39.008807
Model ind 665 epoch 973 batch: 400 avg loss -2.839179 avg loss no lamb -2.839179 time 2020-06-26 20:34:49.950380
Model ind 665 epoch 973 batch: 500 avg loss -2.835894 avg loss no lamb -2.835894 time 2020-06-26 20:35:01.195436
Model ind 665 epoch 973 batch: 600 avg loss -2.885264 avg loss no lamb -2.885264 time 2020-06-26 20:35:11.899546
Model ind 665 epoch 973 batch: 700 avg loss -2.828600 avg loss no lamb -2.828600 time 2020-06-26 20:35:22.978871
Model ind 665 epoch 973 batch: 800 avg loss -2.838464 avg loss no lamb -2.838464 time 2020-06-26 20:35:33.743777
last batch sz 10
Pre: time 2020-06-26 20:35:48.023957: 
 	std: 0.00309774
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9801, 0.9733, 0.9806, 0.9746]
	train_accs: [0.98113334, 0.98035, 0.9749333, 0.98155, 0.9762167]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97769994
	best: 0.9806

Starting e_i: 974
Model ind 665 epoch 974 batch: 0 avg loss -2.946228 avg loss no lamb -2.946228 time 2020-06-26 20:35:49.095845
Model ind 665 epoch 974 batch: 100 avg loss -2.871189 avg loss no lamb -2.871189 time 2020-06-26 20:35:59.770809
Model ind 665 epoch 974 batch: 200 avg loss -2.895298 avg loss no lamb -2.895298 time 2020-06-26 20:36:10.510048
Model ind 665 epoch 974 batch: 300 avg loss -2.846240 avg loss no lamb -2.846240 time 2020-06-26 20:36:21.227905
Model ind 665 epoch 974 batch: 400 avg loss -2.745492 avg loss no lamb -2.745492 time 2020-06-26 20:36:32.331168
Model ind 665 epoch 974 batch: 500 avg loss -2.809265 avg loss no lamb -2.809265 time 2020-06-26 20:36:43.273660
Model ind 665 epoch 974 batch: 600 avg loss -2.884202 avg loss no lamb -2.884202 time 2020-06-26 20:36:54.071845
Model ind 665 epoch 974 batch: 700 avg loss -2.744204 avg loss no lamb -2.744204 time 2020-06-26 20:37:04.974691
Model ind 665 epoch 974 batch: 800 avg loss -2.812341 avg loss no lamb -2.812341 time 2020-06-26 20:37:15.735231
last batch sz 10
Pre: time 2020-06-26 20:37:30.006135: 
 	std: 0.002495274
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9804, 0.9757, 0.9815, 0.9764]
	train_accs: [0.98146665, 0.98116666, 0.97613335, 0.9816667, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97906
	best: 0.9815

Starting e_i: 975
Model ind 665 epoch 975 batch: 0 avg loss -2.936856 avg loss no lamb -2.936856 time 2020-06-26 20:37:31.180553
Model ind 665 epoch 975 batch: 100 avg loss -2.835128 avg loss no lamb -2.835128 time 2020-06-26 20:37:42.395688
Model ind 665 epoch 975 batch: 200 avg loss -2.855021 avg loss no lamb -2.855021 time 2020-06-26 20:37:53.561293
Model ind 665 epoch 975 batch: 300 avg loss -2.858441 avg loss no lamb -2.858441 time 2020-06-26 20:38:04.616280
Model ind 665 epoch 975 batch: 400 avg loss -2.820491 avg loss no lamb -2.820491 time 2020-06-26 20:38:15.830509
Model ind 665 epoch 975 batch: 500 avg loss -2.793714 avg loss no lamb -2.793714 time 2020-06-26 20:38:26.830642
Model ind 665 epoch 975 batch: 600 avg loss -2.875544 avg loss no lamb -2.875544 time 2020-06-26 20:38:37.850956
Model ind 665 epoch 975 batch: 700 avg loss -2.829607 avg loss no lamb -2.829607 time 2020-06-26 20:38:49.049727
Model ind 665 epoch 975 batch: 800 avg loss -2.838481 avg loss no lamb -2.838481 time 2020-06-26 20:38:59.834171
last batch sz 10
Pre: time 2020-06-26 20:39:14.039753: 
 	std: 0.003910813
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9806, 0.9727, 0.9815, 0.9737]
	train_accs: [0.98165, 0.9809333, 0.9747, 0.98158336, 0.97565]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97796
	best: 0.9813

Starting e_i: 976
Model ind 665 epoch 976 batch: 0 avg loss -2.969279 avg loss no lamb -2.969279 time 2020-06-26 20:39:15.066605
Model ind 665 epoch 976 batch: 100 avg loss -2.861259 avg loss no lamb -2.861259 time 2020-06-26 20:39:25.766321
Model ind 665 epoch 976 batch: 200 avg loss -2.869373 avg loss no lamb -2.869373 time 2020-06-26 20:39:36.247960
Model ind 665 epoch 976 batch: 300 avg loss -2.867763 avg loss no lamb -2.867763 time 2020-06-26 20:39:46.991875
Model ind 665 epoch 976 batch: 400 avg loss -2.773149 avg loss no lamb -2.773149 time 2020-06-26 20:39:57.597492
Model ind 665 epoch 976 batch: 500 avg loss -2.845400 avg loss no lamb -2.845400 time 2020-06-26 20:40:08.639292
Model ind 665 epoch 976 batch: 600 avg loss -2.893453 avg loss no lamb -2.893453 time 2020-06-26 20:40:19.843437
Model ind 665 epoch 976 batch: 700 avg loss -2.804995 avg loss no lamb -2.804995 time 2020-06-26 20:40:30.541117
Model ind 665 epoch 976 batch: 800 avg loss -2.878565 avg loss no lamb -2.878565 time 2020-06-26 20:40:41.189826
last batch sz 10
Pre: time 2020-06-26 20:40:55.216688: 
 	std: 0.0032165917
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9817, 0.9748, 0.9822, 0.9759]
	train_accs: [0.98215, 0.9816, 0.97641665, 0.98226666, 0.97735]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.9792601
	best: 0.9822

Starting e_i: 977
Model ind 665 epoch 977 batch: 0 avg loss -2.918155 avg loss no lamb -2.918155 time 2020-06-26 20:40:56.365704
Model ind 665 epoch 977 batch: 100 avg loss -2.840419 avg loss no lamb -2.840419 time 2020-06-26 20:41:07.449047
Model ind 665 epoch 977 batch: 200 avg loss -2.885141 avg loss no lamb -2.885141 time 2020-06-26 20:41:18.603012
Model ind 665 epoch 977 batch: 300 avg loss -2.844008 avg loss no lamb -2.844008 time 2020-06-26 20:41:29.475217
Model ind 665 epoch 977 batch: 400 avg loss -2.791095 avg loss no lamb -2.791095 time 2020-06-26 20:41:40.412151
Model ind 665 epoch 977 batch: 500 avg loss -2.864264 avg loss no lamb -2.864264 time 2020-06-26 20:41:51.184315
Model ind 665 epoch 977 batch: 600 avg loss -2.882239 avg loss no lamb -2.882239 time 2020-06-26 20:42:02.247642
Model ind 665 epoch 977 batch: 700 avg loss -2.765038 avg loss no lamb -2.765038 time 2020-06-26 20:42:12.688001
Model ind 665 epoch 977 batch: 800 avg loss -2.833309 avg loss no lamb -2.833309 time 2020-06-26 20:42:23.654848
last batch sz 10
Pre: time 2020-06-26 20:42:37.766458: 
 	std: 0.0032189467
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9812, 0.9744, 0.9824, 0.9766]
	train_accs: [0.98193336, 0.98118335, 0.9759833, 0.98181665, 0.9772]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97931993
	best: 0.982

Starting e_i: 978
Model ind 665 epoch 978 batch: 0 avg loss -2.944995 avg loss no lamb -2.944995 time 2020-06-26 20:42:38.763878
Model ind 665 epoch 978 batch: 100 avg loss -2.855071 avg loss no lamb -2.855071 time 2020-06-26 20:42:49.562943
Model ind 665 epoch 978 batch: 200 avg loss -2.875562 avg loss no lamb -2.875562 time 2020-06-26 20:43:00.200879
Model ind 665 epoch 978 batch: 300 avg loss -2.874155 avg loss no lamb -2.874155 time 2020-06-26 20:43:11.207658
Model ind 665 epoch 978 batch: 400 avg loss -2.717664 avg loss no lamb -2.717664 time 2020-06-26 20:43:22.325428
Model ind 665 epoch 978 batch: 500 avg loss -2.857271 avg loss no lamb -2.857271 time 2020-06-26 20:43:33.286002
Model ind 665 epoch 978 batch: 600 avg loss -2.827418 avg loss no lamb -2.827418 time 2020-06-26 20:43:44.279398
Model ind 665 epoch 978 batch: 700 avg loss -2.713900 avg loss no lamb -2.713900 time 2020-06-26 20:43:55.232489
Model ind 665 epoch 978 batch: 800 avg loss -2.852721 avg loss no lamb -2.852721 time 2020-06-26 20:44:06.441862
last batch sz 10
Pre: time 2020-06-26 20:44:20.641778: 
 	std: 0.0030770174
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9803, 0.9735, 0.9803, 0.9744]
	train_accs: [0.98125, 0.98075, 0.9754, 0.9812667, 0.9766667]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97769994
	best: 0.9803

Starting e_i: 979
Model ind 665 epoch 979 batch: 0 avg loss -2.956090 avg loss no lamb -2.956090 time 2020-06-26 20:44:21.842628
Model ind 665 epoch 979 batch: 100 avg loss -2.884647 avg loss no lamb -2.884647 time 2020-06-26 20:44:32.555679
Model ind 665 epoch 979 batch: 200 avg loss -2.771459 avg loss no lamb -2.771459 time 2020-06-26 20:44:43.185942
Model ind 665 epoch 979 batch: 300 avg loss -2.853008 avg loss no lamb -2.853008 time 2020-06-26 20:44:54.049539
Model ind 665 epoch 979 batch: 400 avg loss -2.843180 avg loss no lamb -2.843180 time 2020-06-26 20:45:04.976791
Model ind 665 epoch 979 batch: 500 avg loss -2.836215 avg loss no lamb -2.836215 time 2020-06-26 20:45:15.878400
Model ind 665 epoch 979 batch: 600 avg loss -2.894898 avg loss no lamb -2.894898 time 2020-06-26 20:45:26.704290
Model ind 665 epoch 979 batch: 700 avg loss -2.671535 avg loss no lamb -2.671535 time 2020-06-26 20:45:37.781053
Model ind 665 epoch 979 batch: 800 avg loss -2.846134 avg loss no lamb -2.846134 time 2020-06-26 20:45:48.872955
last batch sz 10
Pre: time 2020-06-26 20:46:03.316539: 
 	std: 0.0029226078
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9802, 0.9744, 0.9813, 0.9756]
	train_accs: [0.98193336, 0.98116666, 0.97595, 0.982, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97852004
	best: 0.9813

Starting e_i: 980
Model ind 665 epoch 980 batch: 0 avg loss -2.937032 avg loss no lamb -2.937032 time 2020-06-26 20:46:04.360367
Model ind 665 epoch 980 batch: 100 avg loss -2.901018 avg loss no lamb -2.901018 time 2020-06-26 20:46:15.084323
Model ind 665 epoch 980 batch: 200 avg loss -2.917417 avg loss no lamb -2.917417 time 2020-06-26 20:46:26.073330
Model ind 665 epoch 980 batch: 300 avg loss -2.863157 avg loss no lamb -2.863157 time 2020-06-26 20:46:36.902017
Model ind 665 epoch 980 batch: 400 avg loss -2.797764 avg loss no lamb -2.797764 time 2020-06-26 20:46:48.027857
Model ind 665 epoch 980 batch: 500 avg loss -2.883866 avg loss no lamb -2.883866 time 2020-06-26 20:46:58.783523
Model ind 665 epoch 980 batch: 600 avg loss -2.856321 avg loss no lamb -2.856321 time 2020-06-26 20:47:09.804910
Model ind 665 epoch 980 batch: 700 avg loss -2.756644 avg loss no lamb -2.756644 time 2020-06-26 20:47:20.609607
Model ind 665 epoch 980 batch: 800 avg loss -2.817295 avg loss no lamb -2.817295 time 2020-06-26 20:47:31.520727
last batch sz 10
Pre: time 2020-06-26 20:47:45.695181: 
 	std: 0.0028378936
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.981, 0.9745, 0.9805, 0.9758]
	train_accs: [0.98153335, 0.9809833, 0.97603333, 0.98156667, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97858
	best: 0.9805

Starting e_i: 981
Model ind 665 epoch 981 batch: 0 avg loss -2.986264 avg loss no lamb -2.986264 time 2020-06-26 20:47:48.019289
Model ind 665 epoch 981 batch: 100 avg loss -2.916023 avg loss no lamb -2.916023 time 2020-06-26 20:47:58.793683
Model ind 665 epoch 981 batch: 200 avg loss -2.827487 avg loss no lamb -2.827487 time 2020-06-26 20:48:09.687702
Model ind 665 epoch 981 batch: 300 avg loss -2.818428 avg loss no lamb -2.818428 time 2020-06-26 20:48:20.483360
Model ind 665 epoch 981 batch: 400 avg loss -2.738543 avg loss no lamb -2.738543 time 2020-06-26 20:48:31.694864
Model ind 665 epoch 981 batch: 500 avg loss -2.827307 avg loss no lamb -2.827307 time 2020-06-26 20:48:42.592785
Model ind 665 epoch 981 batch: 600 avg loss -2.911297 avg loss no lamb -2.911297 time 2020-06-26 20:48:53.631965
Model ind 665 epoch 981 batch: 700 avg loss -2.755176 avg loss no lamb -2.755176 time 2020-06-26 20:49:04.475237
Model ind 665 epoch 981 batch: 800 avg loss -2.799590 avg loss no lamb -2.799590 time 2020-06-26 20:49:15.246011
last batch sz 10
Pre: time 2020-06-26 20:49:29.518026: 
 	std: 0.003346402
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9803, 0.9734, 0.981, 0.9746]
	train_accs: [0.9816, 0.98075, 0.97546667, 0.9817167, 0.976]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97805995
	best: 0.981

Starting e_i: 982
Model ind 665 epoch 982 batch: 0 avg loss -2.954432 avg loss no lamb -2.954432 time 2020-06-26 20:49:30.536706
Model ind 665 epoch 982 batch: 100 avg loss -2.929067 avg loss no lamb -2.929067 time 2020-06-26 20:49:41.275267
Model ind 665 epoch 982 batch: 200 avg loss -2.863005 avg loss no lamb -2.863005 time 2020-06-26 20:49:52.183392
Model ind 665 epoch 982 batch: 300 avg loss -2.887719 avg loss no lamb -2.887719 time 2020-06-26 20:50:03.048977
Model ind 665 epoch 982 batch: 400 avg loss -2.763571 avg loss no lamb -2.763571 time 2020-06-26 20:50:14.069318
Model ind 665 epoch 982 batch: 500 avg loss -2.909052 avg loss no lamb -2.909052 time 2020-06-26 20:50:24.969971
Model ind 665 epoch 982 batch: 600 avg loss -2.842325 avg loss no lamb -2.842325 time 2020-06-26 20:50:35.956383
Model ind 665 epoch 982 batch: 700 avg loss -2.820403 avg loss no lamb -2.820403 time 2020-06-26 20:50:46.846414
Model ind 665 epoch 982 batch: 800 avg loss -2.870629 avg loss no lamb -2.870629 time 2020-06-26 20:50:57.597900
last batch sz 10
Pre: time 2020-06-26 20:51:11.531405: 
 	std: 0.002962848
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9804, 0.9747, 0.9813, 0.9748]
	train_accs: [0.98158336, 0.9806833, 0.97566664, 0.98186666, 0.97655]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97836
	best: 0.9813

Starting e_i: 983
Model ind 665 epoch 983 batch: 0 avg loss -2.924291 avg loss no lamb -2.924291 time 2020-06-26 20:51:12.754577
Model ind 665 epoch 983 batch: 100 avg loss -2.845351 avg loss no lamb -2.845351 time 2020-06-26 20:51:23.725854
Model ind 665 epoch 983 batch: 200 avg loss -2.901325 avg loss no lamb -2.901325 time 2020-06-26 20:51:34.406190
Model ind 665 epoch 983 batch: 300 avg loss -2.868750 avg loss no lamb -2.868750 time 2020-06-26 20:51:45.228172
Model ind 665 epoch 983 batch: 400 avg loss -2.756589 avg loss no lamb -2.756589 time 2020-06-26 20:51:56.433026
Model ind 665 epoch 983 batch: 500 avg loss -2.799134 avg loss no lamb -2.799134 time 2020-06-26 20:52:07.428528
Model ind 665 epoch 983 batch: 600 avg loss -2.869121 avg loss no lamb -2.869121 time 2020-06-26 20:52:17.997349
Model ind 665 epoch 983 batch: 700 avg loss -2.783318 avg loss no lamb -2.783318 time 2020-06-26 20:52:28.765714
Model ind 665 epoch 983 batch: 800 avg loss -2.906155 avg loss no lamb -2.906155 time 2020-06-26 20:52:39.647988
last batch sz 10
Pre: time 2020-06-26 20:52:54.001560: 
 	std: 0.003182081
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9802, 0.9733, 0.9813, 0.9756]
	train_accs: [0.9814, 0.98078334, 0.97535, 0.98158336, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97822
	best: 0.9813

Starting e_i: 984
Model ind 665 epoch 984 batch: 0 avg loss -2.944765 avg loss no lamb -2.944765 time 2020-06-26 20:52:55.066476
Model ind 665 epoch 984 batch: 100 avg loss -2.896523 avg loss no lamb -2.896523 time 2020-06-26 20:53:05.871613
Model ind 665 epoch 984 batch: 200 avg loss -2.888771 avg loss no lamb -2.888771 time 2020-06-26 20:53:16.524528
Model ind 665 epoch 984 batch: 300 avg loss -2.881521 avg loss no lamb -2.881521 time 2020-06-26 20:53:27.345216
Model ind 665 epoch 984 batch: 400 avg loss -2.727934 avg loss no lamb -2.727934 time 2020-06-26 20:53:38.571979
Model ind 665 epoch 984 batch: 500 avg loss -2.854499 avg loss no lamb -2.854499 time 2020-06-26 20:53:49.749097
Model ind 665 epoch 984 batch: 600 avg loss -2.853801 avg loss no lamb -2.853801 time 2020-06-26 20:54:00.550984
Model ind 665 epoch 984 batch: 700 avg loss -2.740685 avg loss no lamb -2.740685 time 2020-06-26 20:54:11.345785
Model ind 665 epoch 984 batch: 800 avg loss -2.841648 avg loss no lamb -2.841648 time 2020-06-26 20:54:22.152822
last batch sz 10
Pre: time 2020-06-26 20:54:36.358379: 
 	std: 0.002723662
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9805, 0.9757, 0.981, 0.9756]
	train_accs: [0.98183334, 0.98111665, 0.97616667, 0.98193336, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97894
	best: 0.981

Starting e_i: 985
Model ind 665 epoch 985 batch: 0 avg loss -2.927608 avg loss no lamb -2.927608 time 2020-06-26 20:54:37.549576
Model ind 665 epoch 985 batch: 100 avg loss -2.850037 avg loss no lamb -2.850037 time 2020-06-26 20:54:48.536876
Model ind 665 epoch 985 batch: 200 avg loss -2.864826 avg loss no lamb -2.864826 time 2020-06-26 20:54:59.594872
Model ind 665 epoch 985 batch: 300 avg loss -2.879692 avg loss no lamb -2.879692 time 2020-06-26 20:55:10.809232
Model ind 665 epoch 985 batch: 400 avg loss -2.739483 avg loss no lamb -2.739483 time 2020-06-26 20:55:21.606490
Model ind 665 epoch 985 batch: 500 avg loss -2.846217 avg loss no lamb -2.846217 time 2020-06-26 20:55:32.400261
Model ind 665 epoch 985 batch: 600 avg loss -2.862763 avg loss no lamb -2.862763 time 2020-06-26 20:55:43.432628
Model ind 665 epoch 985 batch: 700 avg loss -2.741830 avg loss no lamb -2.741830 time 2020-06-26 20:55:54.348180
Model ind 665 epoch 985 batch: 800 avg loss -2.819813 avg loss no lamb -2.819813 time 2020-06-26 20:56:05.325343
last batch sz 10
Pre: time 2020-06-26 20:56:19.519691: 
 	std: 0.0032812343
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9814, 0.9736, 0.9811, 0.9756]
	train_accs: [0.9816833, 0.9809833, 0.9751833, 0.9817, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97854006
	best: 0.9811

Starting e_i: 986
Model ind 665 epoch 986 batch: 0 avg loss -2.929974 avg loss no lamb -2.929974 time 2020-06-26 20:56:20.606663
Model ind 665 epoch 986 batch: 100 avg loss -2.910651 avg loss no lamb -2.910651 time 2020-06-26 20:56:31.594187
Model ind 665 epoch 986 batch: 200 avg loss -2.792100 avg loss no lamb -2.792100 time 2020-06-26 20:56:42.581915
Model ind 665 epoch 986 batch: 300 avg loss -2.877081 avg loss no lamb -2.877081 time 2020-06-26 20:56:53.551092
Model ind 665 epoch 986 batch: 400 avg loss -2.765329 avg loss no lamb -2.765329 time 2020-06-26 20:57:04.494346
Model ind 665 epoch 986 batch: 500 avg loss -2.853691 avg loss no lamb -2.853691 time 2020-06-26 20:57:15.508677
Model ind 665 epoch 986 batch: 600 avg loss -2.886726 avg loss no lamb -2.886726 time 2020-06-26 20:57:26.318861
Model ind 665 epoch 986 batch: 700 avg loss -2.785896 avg loss no lamb -2.785896 time 2020-06-26 20:57:37.331572
Model ind 665 epoch 986 batch: 800 avg loss -2.839779 avg loss no lamb -2.839779 time 2020-06-26 20:57:48.331622
last batch sz 10
Pre: time 2020-06-26 20:58:02.435204: 
 	std: 0.0035130624
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9799, 0.9726, 0.981, 0.9743]
	train_accs: [0.98158336, 0.98065, 0.97508335, 0.98158336, 0.9759167]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97768
	best: 0.9806

Starting e_i: 987
Model ind 665 epoch 987 batch: 0 avg loss -2.987719 avg loss no lamb -2.987719 time 2020-06-26 20:58:03.643712
Model ind 665 epoch 987 batch: 100 avg loss -2.917864 avg loss no lamb -2.917864 time 2020-06-26 20:58:14.429986
Model ind 665 epoch 987 batch: 200 avg loss -2.862383 avg loss no lamb -2.862383 time 2020-06-26 20:58:25.499302
Model ind 665 epoch 987 batch: 300 avg loss -2.882020 avg loss no lamb -2.882020 time 2020-06-26 20:58:36.387673
Model ind 665 epoch 987 batch: 400 avg loss -2.760207 avg loss no lamb -2.760207 time 2020-06-26 20:58:47.363401
Model ind 665 epoch 987 batch: 500 avg loss -2.871922 avg loss no lamb -2.871922 time 2020-06-26 20:58:58.056310
Model ind 665 epoch 987 batch: 600 avg loss -2.845749 avg loss no lamb -2.845749 time 2020-06-26 20:59:09.149507
Model ind 665 epoch 987 batch: 700 avg loss -2.819230 avg loss no lamb -2.819230 time 2020-06-26 20:59:20.160714
Model ind 665 epoch 987 batch: 800 avg loss -2.817622 avg loss no lamb -2.817622 time 2020-06-26 20:59:31.009885
last batch sz 10
Pre: time 2020-06-26 20:59:45.130362: 
 	std: 0.0031390523
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9804, 0.9738, 0.9813, 0.9759]
	train_accs: [0.98135, 0.98071665, 0.97508335, 0.9813833, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97858
	best: 0.9813

Starting e_i: 988
Model ind 665 epoch 988 batch: 0 avg loss -2.972578 avg loss no lamb -2.972578 time 2020-06-26 20:59:46.191188
Model ind 665 epoch 988 batch: 100 avg loss -2.884155 avg loss no lamb -2.884155 time 2020-06-26 20:59:56.885193
Model ind 665 epoch 988 batch: 200 avg loss -2.869640 avg loss no lamb -2.869640 time 2020-06-26 21:00:07.816104
Model ind 665 epoch 988 batch: 300 avg loss -2.917863 avg loss no lamb -2.917863 time 2020-06-26 21:00:18.943912
Model ind 665 epoch 988 batch: 400 avg loss -2.784875 avg loss no lamb -2.784875 time 2020-06-26 21:00:29.965874
Model ind 665 epoch 988 batch: 500 avg loss -2.817653 avg loss no lamb -2.817653 time 2020-06-26 21:00:40.844849
Model ind 665 epoch 988 batch: 600 avg loss -2.906775 avg loss no lamb -2.906775 time 2020-06-26 21:00:51.680264
Model ind 665 epoch 988 batch: 700 avg loss -2.803810 avg loss no lamb -2.803810 time 2020-06-26 21:01:02.591394
Model ind 665 epoch 988 batch: 800 avg loss -2.842015 avg loss no lamb -2.842015 time 2020-06-26 21:01:13.470834
last batch sz 10
Pre: time 2020-06-26 21:01:27.594106: 
 	std: 0.002770123
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9809, 0.9751, 0.9809, 0.9757]
	train_accs: [0.98183334, 0.98135, 0.9766833, 0.98153335, 0.9767333]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97878
	best: 0.9813

Starting e_i: 989
Model ind 665 epoch 989 batch: 0 avg loss -2.935301 avg loss no lamb -2.935301 time 2020-06-26 21:01:28.760112
Model ind 665 epoch 989 batch: 100 avg loss -2.950376 avg loss no lamb -2.950376 time 2020-06-26 21:01:39.460296
Model ind 665 epoch 989 batch: 200 avg loss -2.862004 avg loss no lamb -2.862004 time 2020-06-26 21:01:50.135676
Model ind 665 epoch 989 batch: 300 avg loss -2.864689 avg loss no lamb -2.864689 time 2020-06-26 21:02:01.468837
Model ind 665 epoch 989 batch: 400 avg loss -2.778128 avg loss no lamb -2.778128 time 2020-06-26 21:02:12.488770
Model ind 665 epoch 989 batch: 500 avg loss -2.814354 avg loss no lamb -2.814354 time 2020-06-26 21:02:23.407141
Model ind 665 epoch 989 batch: 600 avg loss -2.891002 avg loss no lamb -2.891002 time 2020-06-26 21:02:34.281225
Model ind 665 epoch 989 batch: 700 avg loss -2.694860 avg loss no lamb -2.694860 time 2020-06-26 21:02:45.183061
Model ind 665 epoch 989 batch: 800 avg loss -2.885330 avg loss no lamb -2.885330 time 2020-06-26 21:02:56.281365
last batch sz 10
Pre: time 2020-06-26 21:03:10.650253: 
 	std: 0.0026852272
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9814, 0.9752, 0.9809, 0.9762]
	train_accs: [0.9820667, 0.98095, 0.9764, 0.98175, 0.97718334]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97896004
	best: 0.9811

Starting e_i: 990
Model ind 665 epoch 990 batch: 0 avg loss -2.950251 avg loss no lamb -2.950251 time 2020-06-26 21:03:11.732891
Model ind 665 epoch 990 batch: 100 avg loss -2.921202 avg loss no lamb -2.921202 time 2020-06-26 21:03:22.357868
Model ind 665 epoch 990 batch: 200 avg loss -2.858408 avg loss no lamb -2.858408 time 2020-06-26 21:03:32.732261
Model ind 665 epoch 990 batch: 300 avg loss -2.875264 avg loss no lamb -2.875264 time 2020-06-26 21:03:43.800144
Model ind 665 epoch 990 batch: 400 avg loss -2.808236 avg loss no lamb -2.808236 time 2020-06-26 21:03:54.742176
Model ind 665 epoch 990 batch: 500 avg loss -2.816713 avg loss no lamb -2.816713 time 2020-06-26 21:04:05.683184
Model ind 665 epoch 990 batch: 600 avg loss -2.863503 avg loss no lamb -2.863503 time 2020-06-26 21:04:16.552311
Model ind 665 epoch 990 batch: 700 avg loss -2.755894 avg loss no lamb -2.755894 time 2020-06-26 21:04:27.178604
Model ind 665 epoch 990 batch: 800 avg loss -2.833167 avg loss no lamb -2.833167 time 2020-06-26 21:04:38.066080
last batch sz 10
Pre: time 2020-06-26 21:04:52.294772: 
 	std: 0.002881383
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9817, 0.9752, 0.9813, 0.9757]
	train_accs: [0.9813833, 0.9805667, 0.9755833, 0.9817333, 0.9766]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.9789599
	best: 0.9813

Starting e_i: 991
Model ind 665 epoch 991 batch: 0 avg loss -2.912591 avg loss no lamb -2.912591 time 2020-06-26 21:04:54.667527
Model ind 665 epoch 991 batch: 100 avg loss -2.892914 avg loss no lamb -2.892914 time 2020-06-26 21:05:05.272114
Model ind 665 epoch 991 batch: 200 avg loss -2.865752 avg loss no lamb -2.865752 time 2020-06-26 21:05:16.122840
Model ind 665 epoch 991 batch: 300 avg loss -2.848389 avg loss no lamb -2.848389 time 2020-06-26 21:05:27.051502
Model ind 665 epoch 991 batch: 400 avg loss -2.768378 avg loss no lamb -2.768378 time 2020-06-26 21:05:38.171824
Model ind 665 epoch 991 batch: 500 avg loss -2.902864 avg loss no lamb -2.902864 time 2020-06-26 21:05:49.064912
Model ind 665 epoch 991 batch: 600 avg loss -2.876718 avg loss no lamb -2.876718 time 2020-06-26 21:05:59.984328
Model ind 665 epoch 991 batch: 700 avg loss -2.774676 avg loss no lamb -2.774676 time 2020-06-26 21:06:10.811485
Model ind 665 epoch 991 batch: 800 avg loss -2.824889 avg loss no lamb -2.824889 time 2020-06-26 21:06:21.894489
last batch sz 10
Pre: time 2020-06-26 21:06:36.080562: 
 	std: 0.0033873902
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9796, 0.9726, 0.9799, 0.9732]
	train_accs: [0.9809667, 0.98025, 0.9748, 0.98123336, 0.9755833]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97704
	best: 0.9799

Starting e_i: 992
Model ind 665 epoch 992 batch: 0 avg loss -2.977773 avg loss no lamb -2.977773 time 2020-06-26 21:06:37.156640
Model ind 665 epoch 992 batch: 100 avg loss -2.929225 avg loss no lamb -2.929225 time 2020-06-26 21:06:47.827550
Model ind 665 epoch 992 batch: 200 avg loss -2.833265 avg loss no lamb -2.833265 time 2020-06-26 21:06:58.581279
Model ind 665 epoch 992 batch: 300 avg loss -2.846793 avg loss no lamb -2.846793 time 2020-06-26 21:07:09.706584
Model ind 665 epoch 992 batch: 400 avg loss -2.775060 avg loss no lamb -2.775060 time 2020-06-26 21:07:20.559099
Model ind 665 epoch 992 batch: 500 avg loss -2.827890 avg loss no lamb -2.827890 time 2020-06-26 21:07:31.524831
Model ind 665 epoch 992 batch: 600 avg loss -2.873162 avg loss no lamb -2.873162 time 2020-06-26 21:07:42.493930
Model ind 665 epoch 992 batch: 700 avg loss -2.756295 avg loss no lamb -2.756295 time 2020-06-26 21:07:53.390962
Model ind 665 epoch 992 batch: 800 avg loss -2.818652 avg loss no lamb -2.818652 time 2020-06-26 21:08:04.192563
last batch sz 10
Pre: time 2020-06-26 21:08:18.270056: 
 	std: 0.0029803405
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.979, 0.9739, 0.9803, 0.9736]
	train_accs: [0.9813833, 0.98036665, 0.97538334, 0.9816833, 0.97616667]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97735995
	best: 0.9803

Starting e_i: 993
Model ind 665 epoch 993 batch: 0 avg loss -2.930825 avg loss no lamb -2.930825 time 2020-06-26 21:08:19.502350
Model ind 665 epoch 993 batch: 100 avg loss -2.831429 avg loss no lamb -2.831429 time 2020-06-26 21:08:30.412766
Model ind 665 epoch 993 batch: 200 avg loss -2.835329 avg loss no lamb -2.835329 time 2020-06-26 21:08:41.307079
Model ind 665 epoch 993 batch: 300 avg loss -2.872004 avg loss no lamb -2.872004 time 2020-06-26 21:08:52.191328
Model ind 665 epoch 993 batch: 400 avg loss -2.739860 avg loss no lamb -2.739860 time 2020-06-26 21:09:02.975545
Model ind 665 epoch 993 batch: 500 avg loss -2.864912 avg loss no lamb -2.864912 time 2020-06-26 21:09:13.828105
Model ind 665 epoch 993 batch: 600 avg loss -2.888389 avg loss no lamb -2.888389 time 2020-06-26 21:09:24.706717
Model ind 665 epoch 993 batch: 700 avg loss -2.811271 avg loss no lamb -2.811271 time 2020-06-26 21:09:35.649162
Model ind 665 epoch 993 batch: 800 avg loss -2.877500 avg loss no lamb -2.877500 time 2020-06-26 21:09:46.546561
last batch sz 10
Pre: time 2020-06-26 21:10:00.606620: 
 	std: 0.0032927839
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9789, 0.9733, 0.9808, 0.9742]
	train_accs: [0.98165, 0.98036665, 0.9755167, 0.98123336, 0.9764]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97766
	best: 0.9811

Starting e_i: 994
Model ind 665 epoch 994 batch: 0 avg loss -2.964409 avg loss no lamb -2.964409 time 2020-06-26 21:10:01.664778
Model ind 665 epoch 994 batch: 100 avg loss -2.876088 avg loss no lamb -2.876088 time 2020-06-26 21:10:12.380366
Model ind 665 epoch 994 batch: 200 avg loss -2.866316 avg loss no lamb -2.866316 time 2020-06-26 21:10:23.128454
Model ind 665 epoch 994 batch: 300 avg loss -2.841829 avg loss no lamb -2.841829 time 2020-06-26 21:10:33.997763
Model ind 665 epoch 994 batch: 400 avg loss -2.757696 avg loss no lamb -2.757696 time 2020-06-26 21:10:44.992497
Model ind 665 epoch 994 batch: 500 avg loss -2.835162 avg loss no lamb -2.835162 time 2020-06-26 21:10:56.259140
Model ind 665 epoch 994 batch: 600 avg loss -2.861776 avg loss no lamb -2.861776 time 2020-06-26 21:11:06.895527
Model ind 665 epoch 994 batch: 700 avg loss -2.793949 avg loss no lamb -2.793949 time 2020-06-26 21:11:17.757918
Model ind 665 epoch 994 batch: 800 avg loss -2.853724 avg loss no lamb -2.853724 time 2020-06-26 21:11:28.269532
last batch sz 10
Pre: time 2020-06-26 21:11:42.299326: 
 	std: 0.0031815688
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9805, 0.9738, 0.9807, 0.9743]
	train_accs: [0.9814, 0.98071665, 0.97538334, 0.98156667, 0.97648335]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97793996
	best: 0.9807

Starting e_i: 995
Model ind 665 epoch 995 batch: 0 avg loss -2.947589 avg loss no lamb -2.947589 time 2020-06-26 21:11:43.474998
Model ind 665 epoch 995 batch: 100 avg loss -2.890905 avg loss no lamb -2.890905 time 2020-06-26 21:11:54.364285
Model ind 665 epoch 995 batch: 200 avg loss -2.906527 avg loss no lamb -2.906527 time 2020-06-26 21:12:05.390440
Model ind 665 epoch 995 batch: 300 avg loss -2.850683 avg loss no lamb -2.850683 time 2020-06-26 21:12:16.271734
Model ind 665 epoch 995 batch: 400 avg loss -2.775457 avg loss no lamb -2.775457 time 2020-06-26 21:12:27.039186
Model ind 665 epoch 995 batch: 500 avg loss -2.831672 avg loss no lamb -2.831672 time 2020-06-26 21:12:37.796211
Model ind 665 epoch 995 batch: 600 avg loss -2.895688 avg loss no lamb -2.895688 time 2020-06-26 21:12:48.842119
Model ind 665 epoch 995 batch: 700 avg loss -2.765141 avg loss no lamb -2.765141 time 2020-06-26 21:12:59.925645
Model ind 665 epoch 995 batch: 800 avg loss -2.840029 avg loss no lamb -2.840029 time 2020-06-26 21:13:10.776185
last batch sz 10
Pre: time 2020-06-26 21:13:24.763645: 
 	std: 0.0031575812
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9786, 0.9728, 0.9808, 0.9745]
	train_accs: [0.98158336, 0.98048335, 0.9754, 0.98165, 0.97693336]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97735995
	best: 0.9808

Starting e_i: 996
Model ind 665 epoch 996 batch: 0 avg loss -2.916336 avg loss no lamb -2.916336 time 2020-06-26 21:13:25.877318
Model ind 665 epoch 996 batch: 100 avg loss -2.839153 avg loss no lamb -2.839153 time 2020-06-26 21:13:36.576825
Model ind 665 epoch 996 batch: 200 avg loss -2.873949 avg loss no lamb -2.873949 time 2020-06-26 21:13:47.559798
Model ind 665 epoch 996 batch: 300 avg loss -2.816191 avg loss no lamb -2.816191 time 2020-06-26 21:13:58.594953
Model ind 665 epoch 996 batch: 400 avg loss -2.805769 avg loss no lamb -2.805769 time 2020-06-26 21:14:09.695805
Model ind 665 epoch 996 batch: 500 avg loss -2.813432 avg loss no lamb -2.813432 time 2020-06-26 21:14:20.851441
Model ind 665 epoch 996 batch: 600 avg loss -2.899617 avg loss no lamb -2.899617 time 2020-06-26 21:14:31.729353
Model ind 665 epoch 996 batch: 700 avg loss -2.794994 avg loss no lamb -2.794994 time 2020-06-26 21:14:42.612200
Model ind 665 epoch 996 batch: 800 avg loss -2.875397 avg loss no lamb -2.875397 time 2020-06-26 21:14:53.518084
last batch sz 10
Pre: time 2020-06-26 21:15:07.484944: 
 	std: 0.0029178124
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9784, 0.9726, 0.9799, 0.975]
	train_accs: [0.9810333, 0.9799167, 0.97533333, 0.98083335, 0.97643334]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97718
	best: 0.98

Starting e_i: 997
Model ind 665 epoch 997 batch: 0 avg loss -2.958181 avg loss no lamb -2.958181 time 2020-06-26 21:15:08.694219
Model ind 665 epoch 997 batch: 100 avg loss -2.841855 avg loss no lamb -2.841855 time 2020-06-26 21:15:19.545246
Model ind 665 epoch 997 batch: 200 avg loss -2.858198 avg loss no lamb -2.858198 time 2020-06-26 21:15:30.605205
Model ind 665 epoch 997 batch: 300 avg loss -2.887779 avg loss no lamb -2.887779 time 2020-06-26 21:15:41.357090
Model ind 665 epoch 997 batch: 400 avg loss -2.757430 avg loss no lamb -2.757430 time 2020-06-26 21:15:52.198534
Model ind 665 epoch 997 batch: 500 avg loss -2.848062 avg loss no lamb -2.848062 time 2020-06-26 21:16:02.806061
Model ind 665 epoch 997 batch: 600 avg loss -2.901958 avg loss no lamb -2.901958 time 2020-06-26 21:16:13.633846
Model ind 665 epoch 997 batch: 700 avg loss -2.768072 avg loss no lamb -2.768072 time 2020-06-26 21:16:24.548502
Model ind 665 epoch 997 batch: 800 avg loss -2.834908 avg loss no lamb -2.834908 time 2020-06-26 21:16:35.264641
last batch sz 10
Pre: time 2020-06-26 21:16:49.111119: 
 	std: 0.0036472473
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9797, 0.9725, 0.9815, 0.9748]
	train_accs: [0.9819667, 0.9809333, 0.97485, 0.98188335, 0.97653335]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97796
	best: 0.9813

Starting e_i: 998
Model ind 665 epoch 998 batch: 0 avg loss -2.946509 avg loss no lamb -2.946509 time 2020-06-26 21:16:50.136790
Model ind 665 epoch 998 batch: 100 avg loss -2.891109 avg loss no lamb -2.891109 time 2020-06-26 21:17:00.974767
Model ind 665 epoch 998 batch: 200 avg loss -2.888411 avg loss no lamb -2.888411 time 2020-06-26 21:17:12.065232
Model ind 665 epoch 998 batch: 300 avg loss -2.884059 avg loss no lamb -2.884059 time 2020-06-26 21:17:22.868962
Model ind 665 epoch 998 batch: 400 avg loss -2.786640 avg loss no lamb -2.786640 time 2020-06-26 21:17:33.634091
Model ind 665 epoch 998 batch: 500 avg loss -2.851275 avg loss no lamb -2.851275 time 2020-06-26 21:17:44.438999
Model ind 665 epoch 998 batch: 600 avg loss -2.856509 avg loss no lamb -2.856509 time 2020-06-26 21:17:55.512091
Model ind 665 epoch 998 batch: 700 avg loss -2.733316 avg loss no lamb -2.733316 time 2020-06-26 21:18:06.339151
Model ind 665 epoch 998 batch: 800 avg loss -2.862412 avg loss no lamb -2.862412 time 2020-06-26 21:18:17.281502
last batch sz 10
Pre: time 2020-06-26 21:18:31.229408: 
 	std: 0.0035130607
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9809, 0.9737, 0.9816, 0.9748]
	train_accs: [0.98188335, 0.9812833, 0.9758667, 0.9817, 0.9766]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97852004
	best: 0.9816

Starting e_i: 999
Model ind 665 epoch 999 batch: 0 avg loss -2.909559 avg loss no lamb -2.909559 time 2020-06-26 21:18:32.416462
Model ind 665 epoch 999 batch: 100 avg loss -2.919354 avg loss no lamb -2.919354 time 2020-06-26 21:18:43.252522
Model ind 665 epoch 999 batch: 200 avg loss -2.844462 avg loss no lamb -2.844462 time 2020-06-26 21:18:54.600333
Model ind 665 epoch 999 batch: 300 avg loss -2.831369 avg loss no lamb -2.831369 time 2020-06-26 21:19:05.571449
Model ind 665 epoch 999 batch: 400 avg loss -2.785933 avg loss no lamb -2.785933 time 2020-06-26 21:19:16.543879
Model ind 665 epoch 999 batch: 500 avg loss -2.815496 avg loss no lamb -2.815496 time 2020-06-26 21:19:27.228853
Model ind 665 epoch 999 batch: 600 avg loss -2.843846 avg loss no lamb -2.843846 time 2020-06-26 21:19:38.090384
Model ind 665 epoch 999 batch: 700 avg loss -2.768710 avg loss no lamb -2.768710 time 2020-06-26 21:19:49.064413
Model ind 665 epoch 999 batch: 800 avg loss -2.805428 avg loss no lamb -2.805428 time 2020-06-26 21:19:59.685234
last batch sz 10
Pre: time 2020-06-26 21:20:13.679681: 
 	std: 0.0035419774
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.98, 0.972, 0.9802, 0.9738]
	train_accs: [0.98151666, 0.9813167, 0.97531664, 0.98135, 0.9762333]
	best_train_sub_head: 0
	worst: 0.972
	avg: 0.97718
	best: 0.9799

Starting e_i: 1000
Model ind 665 epoch 1000 batch: 0 avg loss -2.921194 avg loss no lamb -2.921194 time 2020-06-26 21:20:14.737053
Model ind 665 epoch 1000 batch: 100 avg loss -2.932635 avg loss no lamb -2.932635 time 2020-06-26 21:20:25.717630
Model ind 665 epoch 1000 batch: 200 avg loss -2.853575 avg loss no lamb -2.853575 time 2020-06-26 21:20:36.681188
Model ind 665 epoch 1000 batch: 300 avg loss -2.878956 avg loss no lamb -2.878956 time 2020-06-26 21:20:47.361723
Model ind 665 epoch 1000 batch: 400 avg loss -2.783550 avg loss no lamb -2.783550 time 2020-06-26 21:20:58.116794
Model ind 665 epoch 1000 batch: 500 avg loss -2.863177 avg loss no lamb -2.863177 time 2020-06-26 21:21:08.978710
Model ind 665 epoch 1000 batch: 600 avg loss -2.874029 avg loss no lamb -2.874029 time 2020-06-26 21:21:19.752500
Model ind 665 epoch 1000 batch: 700 avg loss -2.726551 avg loss no lamb -2.726551 time 2020-06-26 21:21:30.295639
Model ind 665 epoch 1000 batch: 800 avg loss -2.832514 avg loss no lamb -2.832514 time 2020-06-26 21:21:41.098424
last batch sz 10
Pre: time 2020-06-26 21:21:55.244995: 
 	std: 0.0028187947
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9806, 0.974, 0.9812, 0.9774]
	train_accs: [0.9818, 0.98088336, 0.97578335, 0.98156667, 0.97735]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97888005
	best: 0.9812

Starting e_i: 1001
Model ind 665 epoch 1001 batch: 0 avg loss -2.945951 avg loss no lamb -2.945951 time 2020-06-26 21:21:57.648405
Model ind 665 epoch 1001 batch: 100 avg loss -2.876705 avg loss no lamb -2.876705 time 2020-06-26 21:22:08.655942
Model ind 665 epoch 1001 batch: 200 avg loss -2.871079 avg loss no lamb -2.871079 time 2020-06-26 21:22:19.444023
Model ind 665 epoch 1001 batch: 300 avg loss -2.818370 avg loss no lamb -2.818370 time 2020-06-26 21:22:30.294555
Model ind 665 epoch 1001 batch: 400 avg loss -2.796935 avg loss no lamb -2.796935 time 2020-06-26 21:22:41.208275
Model ind 665 epoch 1001 batch: 500 avg loss -2.828052 avg loss no lamb -2.828052 time 2020-06-26 21:22:52.153464
Model ind 665 epoch 1001 batch: 600 avg loss -2.894332 avg loss no lamb -2.894332 time 2020-06-26 21:23:03.091743
Model ind 665 epoch 1001 batch: 700 avg loss -2.708201 avg loss no lamb -2.708201 time 2020-06-26 21:23:13.981485
Model ind 665 epoch 1001 batch: 800 avg loss -2.888340 avg loss no lamb -2.888340 time 2020-06-26 21:23:24.840762
last batch sz 10
Pre: time 2020-06-26 21:23:39.148167: 
 	std: 0.0037248502
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9805, 0.9736, 0.9814, 0.9741]
	train_accs: [0.98221666, 0.9811, 0.9752833, 0.98185, 0.97605]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97836
	best: 0.9822

Starting e_i: 1002
Model ind 665 epoch 1002 batch: 0 avg loss -2.960046 avg loss no lamb -2.960046 time 2020-06-26 21:23:40.207295
Model ind 665 epoch 1002 batch: 100 avg loss -2.922200 avg loss no lamb -2.922200 time 2020-06-26 21:23:51.199310
Model ind 665 epoch 1002 batch: 200 avg loss -2.919802 avg loss no lamb -2.919802 time 2020-06-26 21:24:02.057496
Model ind 665 epoch 1002 batch: 300 avg loss -2.889560 avg loss no lamb -2.889560 time 2020-06-26 21:24:12.781066
Model ind 665 epoch 1002 batch: 400 avg loss -2.846020 avg loss no lamb -2.846020 time 2020-06-26 21:24:23.679908
Model ind 665 epoch 1002 batch: 500 avg loss -2.858346 avg loss no lamb -2.858346 time 2020-06-26 21:24:34.642918
Model ind 665 epoch 1002 batch: 600 avg loss -2.873731 avg loss no lamb -2.873731 time 2020-06-26 21:24:45.183894
Model ind 665 epoch 1002 batch: 700 avg loss -2.776743 avg loss no lamb -2.776743 time 2020-06-26 21:24:55.874710
Model ind 665 epoch 1002 batch: 800 avg loss -2.802958 avg loss no lamb -2.802958 time 2020-06-26 21:25:06.789517
last batch sz 10
Pre: time 2020-06-26 21:25:20.631409: 
 	std: 0.0032178399
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.98, 0.9736, 0.9807, 0.9747]
	train_accs: [0.98156667, 0.9806333, 0.97511667, 0.9812833, 0.9763]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.9780399
	best: 0.9812

Starting e_i: 1003
Model ind 665 epoch 1003 batch: 0 avg loss -2.952754 avg loss no lamb -2.952754 time 2020-06-26 21:25:21.855628
Model ind 665 epoch 1003 batch: 100 avg loss -2.926614 avg loss no lamb -2.926614 time 2020-06-26 21:25:32.806868
Model ind 665 epoch 1003 batch: 200 avg loss -2.818415 avg loss no lamb -2.818415 time 2020-06-26 21:25:43.873097
Model ind 665 epoch 1003 batch: 300 avg loss -2.827888 avg loss no lamb -2.827888 time 2020-06-26 21:25:54.764141
Model ind 665 epoch 1003 batch: 400 avg loss -2.741416 avg loss no lamb -2.741416 time 2020-06-26 21:26:05.694584
Model ind 665 epoch 1003 batch: 500 avg loss -2.838451 avg loss no lamb -2.838451 time 2020-06-26 21:26:16.691547
Model ind 665 epoch 1003 batch: 600 avg loss -2.894013 avg loss no lamb -2.894013 time 2020-06-26 21:26:27.586474
Model ind 665 epoch 1003 batch: 700 avg loss -2.794155 avg loss no lamb -2.794155 time 2020-06-26 21:26:38.680819
Model ind 665 epoch 1003 batch: 800 avg loss -2.885148 avg loss no lamb -2.885148 time 2020-06-26 21:26:49.602559
last batch sz 10
Pre: time 2020-06-26 21:27:03.863597: 
 	std: 0.0037507557
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9811, 0.974, 0.9825, 0.9747]
	train_accs: [0.9821333, 0.9815, 0.97548336, 0.98256665, 0.97641665]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97889996
	best: 0.9825

Starting e_i: 1004
Model ind 665 epoch 1004 batch: 0 avg loss -2.899014 avg loss no lamb -2.899014 time 2020-06-26 21:27:04.897403
Model ind 665 epoch 1004 batch: 100 avg loss -2.849696 avg loss no lamb -2.849696 time 2020-06-26 21:27:15.868958
Model ind 665 epoch 1004 batch: 200 avg loss -2.891295 avg loss no lamb -2.891295 time 2020-06-26 21:27:26.728211
Model ind 665 epoch 1004 batch: 300 avg loss -2.811157 avg loss no lamb -2.811157 time 2020-06-26 21:27:37.301769
Model ind 665 epoch 1004 batch: 400 avg loss -2.752861 avg loss no lamb -2.752861 time 2020-06-26 21:27:48.446403
Model ind 665 epoch 1004 batch: 500 avg loss -2.830055 avg loss no lamb -2.830055 time 2020-06-26 21:27:59.400286
Model ind 665 epoch 1004 batch: 600 avg loss -2.876145 avg loss no lamb -2.876145 time 2020-06-26 21:28:10.153985
Model ind 665 epoch 1004 batch: 700 avg loss -2.809121 avg loss no lamb -2.809121 time 2020-06-26 21:28:20.976846
Model ind 665 epoch 1004 batch: 800 avg loss -2.832261 avg loss no lamb -2.832261 time 2020-06-26 21:28:31.847372
last batch sz 10
Pre: time 2020-06-26 21:28:45.851845: 
 	std: 0.003382555
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9803, 0.9729, 0.9807, 0.9748]
	train_accs: [0.98188335, 0.98088336, 0.97485, 0.9819, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97791994
	best: 0.9807

Starting e_i: 1005
Model ind 665 epoch 1005 batch: 0 avg loss -2.943823 avg loss no lamb -2.943823 time 2020-06-26 21:28:47.130951
Model ind 665 epoch 1005 batch: 100 avg loss -2.864258 avg loss no lamb -2.864258 time 2020-06-26 21:28:58.151267
Model ind 665 epoch 1005 batch: 200 avg loss -2.873207 avg loss no lamb -2.873207 time 2020-06-26 21:29:08.911056
Model ind 665 epoch 1005 batch: 300 avg loss -2.838478 avg loss no lamb -2.838478 time 2020-06-26 21:29:19.578103
Model ind 665 epoch 1005 batch: 400 avg loss -2.793779 avg loss no lamb -2.793779 time 2020-06-26 21:29:30.306870
Model ind 665 epoch 1005 batch: 500 avg loss -2.834122 avg loss no lamb -2.834122 time 2020-06-26 21:29:41.333190
Model ind 665 epoch 1005 batch: 600 avg loss -2.902122 avg loss no lamb -2.902122 time 2020-06-26 21:29:52.272785
Model ind 665 epoch 1005 batch: 700 avg loss -2.785931 avg loss no lamb -2.785931 time 2020-06-26 21:30:03.115946
Model ind 665 epoch 1005 batch: 800 avg loss -2.846864 avg loss no lamb -2.846864 time 2020-06-26 21:30:13.878750
last batch sz 10
Pre: time 2020-06-26 21:30:27.929458: 
 	std: 0.003455196
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9806, 0.9727, 0.981, 0.9749]
	train_accs: [0.98083335, 0.9805, 0.9741833, 0.98115, 0.9759667]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97793996
	best: 0.981

Starting e_i: 1006
Model ind 665 epoch 1006 batch: 0 avg loss -2.947796 avg loss no lamb -2.947796 time 2020-06-26 21:30:28.963053
Model ind 665 epoch 1006 batch: 100 avg loss -2.911032 avg loss no lamb -2.911032 time 2020-06-26 21:30:39.967800
Model ind 665 epoch 1006 batch: 200 avg loss -2.830252 avg loss no lamb -2.830252 time 2020-06-26 21:30:50.729047
Model ind 665 epoch 1006 batch: 300 avg loss -2.904441 avg loss no lamb -2.904441 time 2020-06-26 21:31:01.510494
Model ind 665 epoch 1006 batch: 400 avg loss -2.764340 avg loss no lamb -2.764340 time 2020-06-26 21:31:12.501571
Model ind 665 epoch 1006 batch: 500 avg loss -2.817322 avg loss no lamb -2.817322 time 2020-06-26 21:31:23.599396
Model ind 665 epoch 1006 batch: 600 avg loss -2.858074 avg loss no lamb -2.858074 time 2020-06-26 21:31:34.532567
Model ind 665 epoch 1006 batch: 700 avg loss -2.710434 avg loss no lamb -2.710434 time 2020-06-26 21:31:45.631386
Model ind 665 epoch 1006 batch: 800 avg loss -2.840541 avg loss no lamb -2.840541 time 2020-06-26 21:31:56.187818
last batch sz 10
Pre: time 2020-06-26 21:32:10.522530: 
 	std: 0.002507114
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9801, 0.9751, 0.981, 0.976]
	train_accs: [0.9818, 0.9809167, 0.9759667, 0.98176664, 0.97685]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97858
	best: 0.9807

Starting e_i: 1007
Model ind 665 epoch 1007 batch: 0 avg loss -2.915677 avg loss no lamb -2.915677 time 2020-06-26 21:32:11.722425
Model ind 665 epoch 1007 batch: 100 avg loss -2.864965 avg loss no lamb -2.864965 time 2020-06-26 21:32:22.686335
Model ind 665 epoch 1007 batch: 200 avg loss -2.827511 avg loss no lamb -2.827511 time 2020-06-26 21:32:33.627058
Model ind 665 epoch 1007 batch: 300 avg loss -2.873936 avg loss no lamb -2.873936 time 2020-06-26 21:32:44.561053
Model ind 665 epoch 1007 batch: 400 avg loss -2.759447 avg loss no lamb -2.759447 time 2020-06-26 21:32:55.460462
Model ind 665 epoch 1007 batch: 500 avg loss -2.815772 avg loss no lamb -2.815772 time 2020-06-26 21:33:06.491175
Model ind 665 epoch 1007 batch: 600 avg loss -2.891597 avg loss no lamb -2.891597 time 2020-06-26 21:33:17.157772
Model ind 665 epoch 1007 batch: 700 avg loss -2.670539 avg loss no lamb -2.670539 time 2020-06-26 21:33:28.068589
Model ind 665 epoch 1007 batch: 800 avg loss -2.832643 avg loss no lamb -2.832643 time 2020-06-26 21:33:38.847092
last batch sz 10
Pre: time 2020-06-26 21:33:52.659874: 
 	std: 0.0036657231
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9811, 0.9732, 0.9813, 0.975]
	train_accs: [0.9821333, 0.9812, 0.97495, 0.9816, 0.9764]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97852004
	best: 0.982

Starting e_i: 1008
Model ind 665 epoch 1008 batch: 0 avg loss -2.962504 avg loss no lamb -2.962504 time 2020-06-26 21:33:53.693383
Model ind 665 epoch 1008 batch: 100 avg loss -2.874643 avg loss no lamb -2.874643 time 2020-06-26 21:34:04.560385
Model ind 665 epoch 1008 batch: 200 avg loss -2.888036 avg loss no lamb -2.888036 time 2020-06-26 21:34:15.540595
Model ind 665 epoch 1008 batch: 300 avg loss -2.883385 avg loss no lamb -2.883385 time 2020-06-26 21:34:26.557856
Model ind 665 epoch 1008 batch: 400 avg loss -2.709136 avg loss no lamb -2.709136 time 2020-06-26 21:34:37.398261
Model ind 665 epoch 1008 batch: 500 avg loss -2.805252 avg loss no lamb -2.805252 time 2020-06-26 21:34:48.375218
Model ind 665 epoch 1008 batch: 600 avg loss -2.883945 avg loss no lamb -2.883945 time 2020-06-26 21:34:59.248398
Model ind 665 epoch 1008 batch: 700 avg loss -2.771543 avg loss no lamb -2.771543 time 2020-06-26 21:35:10.137005
Model ind 665 epoch 1008 batch: 800 avg loss -2.900613 avg loss no lamb -2.900613 time 2020-06-26 21:35:20.956005
last batch sz 10
Pre: time 2020-06-26 21:35:35.004565: 
 	std: 0.0028806948
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9798, 0.9737, 0.9799, 0.9751]
	train_accs: [0.9809833, 0.9802833, 0.9751, 0.98108333, 0.97625]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97786
	best: 0.9799

Starting e_i: 1009
Model ind 665 epoch 1009 batch: 0 avg loss -2.958817 avg loss no lamb -2.958817 time 2020-06-26 21:35:36.269702
Model ind 665 epoch 1009 batch: 100 avg loss -2.930344 avg loss no lamb -2.930344 time 2020-06-26 21:35:47.265818
Model ind 665 epoch 1009 batch: 200 avg loss -2.881706 avg loss no lamb -2.881706 time 2020-06-26 21:35:58.298393
Model ind 665 epoch 1009 batch: 300 avg loss -2.875721 avg loss no lamb -2.875721 time 2020-06-26 21:36:09.256565
Model ind 665 epoch 1009 batch: 400 avg loss -2.788450 avg loss no lamb -2.788450 time 2020-06-26 21:36:20.270302
Model ind 665 epoch 1009 batch: 500 avg loss -2.835479 avg loss no lamb -2.835479 time 2020-06-26 21:36:31.241368
Model ind 665 epoch 1009 batch: 600 avg loss -2.907776 avg loss no lamb -2.907776 time 2020-06-26 21:36:42.239101
Model ind 665 epoch 1009 batch: 700 avg loss -2.728163 avg loss no lamb -2.728163 time 2020-06-26 21:36:53.186827
Model ind 665 epoch 1009 batch: 800 avg loss -2.856574 avg loss no lamb -2.856574 time 2020-06-26 21:37:04.265497
last batch sz 10
Pre: time 2020-06-26 21:37:18.607656: 
 	std: 0.0027235183
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9793, 0.9741, 0.9801, 0.9743]
	train_accs: [0.98116666, 0.9802333, 0.9756167, 0.9811, 0.976]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97752
	best: 0.9798

Starting e_i: 1010
Model ind 665 epoch 1010 batch: 0 avg loss -2.955051 avg loss no lamb -2.955051 time 2020-06-26 21:37:19.682346
Model ind 665 epoch 1010 batch: 100 avg loss -2.841340 avg loss no lamb -2.841340 time 2020-06-26 21:37:30.559059
Model ind 665 epoch 1010 batch: 200 avg loss -2.859810 avg loss no lamb -2.859810 time 2020-06-26 21:37:41.576487
Model ind 665 epoch 1010 batch: 300 avg loss -2.801814 avg loss no lamb -2.801814 time 2020-06-26 21:37:52.542302
Model ind 665 epoch 1010 batch: 400 avg loss -2.802632 avg loss no lamb -2.802632 time 2020-06-26 21:38:03.542667
Model ind 665 epoch 1010 batch: 500 avg loss -2.784602 avg loss no lamb -2.784602 time 2020-06-26 21:38:14.436632
Model ind 665 epoch 1010 batch: 600 avg loss -2.883583 avg loss no lamb -2.883583 time 2020-06-26 21:38:25.094463
Model ind 665 epoch 1010 batch: 700 avg loss -2.815922 avg loss no lamb -2.815922 time 2020-06-26 21:38:35.853848
Model ind 665 epoch 1010 batch: 800 avg loss -2.865222 avg loss no lamb -2.865222 time 2020-06-26 21:38:46.856425
last batch sz 10
Pre: time 2020-06-26 21:39:01.009017: 
 	std: 0.002492705
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9803, 0.9756, 0.9812, 0.9759]
	train_accs: [0.98135, 0.98036665, 0.97611666, 0.9816667, 0.9765]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97878
	best: 0.9812

Starting e_i: 1011
Model ind 665 epoch 1011 batch: 0 avg loss -2.890903 avg loss no lamb -2.890903 time 2020-06-26 21:39:03.447251
Model ind 665 epoch 1011 batch: 100 avg loss -2.835664 avg loss no lamb -2.835664 time 2020-06-26 21:39:14.391570
Model ind 665 epoch 1011 batch: 200 avg loss -2.904916 avg loss no lamb -2.904916 time 2020-06-26 21:39:25.280674
Model ind 665 epoch 1011 batch: 300 avg loss -2.890308 avg loss no lamb -2.890308 time 2020-06-26 21:39:36.460432
Model ind 665 epoch 1011 batch: 400 avg loss -2.799186 avg loss no lamb -2.799186 time 2020-06-26 21:39:47.473312
Model ind 665 epoch 1011 batch: 500 avg loss -2.817145 avg loss no lamb -2.817145 time 2020-06-26 21:39:58.175424
Model ind 665 epoch 1011 batch: 600 avg loss -2.838807 avg loss no lamb -2.838807 time 2020-06-26 21:40:09.096709
Model ind 665 epoch 1011 batch: 700 avg loss -2.785860 avg loss no lamb -2.785860 time 2020-06-26 21:40:19.768074
Model ind 665 epoch 1011 batch: 800 avg loss -2.945334 avg loss no lamb -2.945334 time 2020-06-26 21:40:30.730086
last batch sz 10
Pre: time 2020-06-26 21:40:44.969219: 
 	std: 0.0037110744
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9809, 0.9723, 0.9804, 0.9733]
	train_accs: [0.98108333, 0.9812, 0.97443336, 0.98145, 0.97573334]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97730005
	best: 0.9804

Starting e_i: 1012
Model ind 665 epoch 1012 batch: 0 avg loss -2.919582 avg loss no lamb -2.919582 time 2020-06-26 21:40:46.056019
Model ind 665 epoch 1012 batch: 100 avg loss -2.919737 avg loss no lamb -2.919737 time 2020-06-26 21:40:57.029548
Model ind 665 epoch 1012 batch: 200 avg loss -2.801419 avg loss no lamb -2.801419 time 2020-06-26 21:41:07.557818
Model ind 665 epoch 1012 batch: 300 avg loss -2.823617 avg loss no lamb -2.823617 time 2020-06-26 21:41:18.656485
Model ind 665 epoch 1012 batch: 400 avg loss -2.795105 avg loss no lamb -2.795105 time 2020-06-26 21:41:29.368656
Model ind 665 epoch 1012 batch: 500 avg loss -2.816713 avg loss no lamb -2.816713 time 2020-06-26 21:41:40.265910
Model ind 665 epoch 1012 batch: 600 avg loss -2.821713 avg loss no lamb -2.821713 time 2020-06-26 21:41:50.997894
Model ind 665 epoch 1012 batch: 700 avg loss -2.779963 avg loss no lamb -2.779963 time 2020-06-26 21:42:01.872174
Model ind 665 epoch 1012 batch: 800 avg loss -2.849581 avg loss no lamb -2.849581 time 2020-06-26 21:42:12.700813
last batch sz 10
Pre: time 2020-06-26 21:42:26.717921: 
 	std: 0.0026618834
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9809, 0.9748, 0.9806, 0.9759]
	train_accs: [0.98121667, 0.9809833, 0.9756, 0.98106664, 0.9761]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97858
	best: 0.9807

Starting e_i: 1013
Model ind 665 epoch 1013 batch: 0 avg loss -2.918679 avg loss no lamb -2.918679 time 2020-06-26 21:42:27.927688
Model ind 665 epoch 1013 batch: 100 avg loss -2.933852 avg loss no lamb -2.933852 time 2020-06-26 21:42:38.884214
Model ind 665 epoch 1013 batch: 200 avg loss -2.829626 avg loss no lamb -2.829626 time 2020-06-26 21:42:49.841002
Model ind 665 epoch 1013 batch: 300 avg loss -2.902429 avg loss no lamb -2.902429 time 2020-06-26 21:43:00.757944
Model ind 665 epoch 1013 batch: 400 avg loss -2.828145 avg loss no lamb -2.828145 time 2020-06-26 21:43:11.724034
Model ind 665 epoch 1013 batch: 500 avg loss -2.840680 avg loss no lamb -2.840680 time 2020-06-26 21:43:22.609942
Model ind 665 epoch 1013 batch: 600 avg loss -2.833276 avg loss no lamb -2.833276 time 2020-06-26 21:43:33.409483
Model ind 665 epoch 1013 batch: 700 avg loss -2.736350 avg loss no lamb -2.736350 time 2020-06-26 21:43:44.662682
Model ind 665 epoch 1013 batch: 800 avg loss -2.834174 avg loss no lamb -2.834174 time 2020-06-26 21:43:55.561196
last batch sz 10
Pre: time 2020-06-26 21:44:09.937730: 
 	std: 0.0028554413
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9801, 0.9745, 0.9805, 0.9749]
	train_accs: [0.9813833, 0.9811, 0.9756333, 0.9812833, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97817993
	best: 0.9809

Starting e_i: 1014
Model ind 665 epoch 1014 batch: 0 avg loss -2.971503 avg loss no lamb -2.971503 time 2020-06-26 21:44:10.989844
Model ind 665 epoch 1014 batch: 100 avg loss -2.866311 avg loss no lamb -2.866311 time 2020-06-26 21:44:21.963002
Model ind 665 epoch 1014 batch: 200 avg loss -2.891389 avg loss no lamb -2.891389 time 2020-06-26 21:44:32.956167
Model ind 665 epoch 1014 batch: 300 avg loss -2.851851 avg loss no lamb -2.851851 time 2020-06-26 21:44:43.854768
Model ind 665 epoch 1014 batch: 400 avg loss -2.753886 avg loss no lamb -2.753886 time 2020-06-26 21:44:54.718528
Model ind 665 epoch 1014 batch: 500 avg loss -2.861248 avg loss no lamb -2.861248 time 2020-06-26 21:45:05.761631
Model ind 665 epoch 1014 batch: 600 avg loss -2.880373 avg loss no lamb -2.880373 time 2020-06-26 21:45:16.584432
Model ind 665 epoch 1014 batch: 700 avg loss -2.827900 avg loss no lamb -2.827900 time 2020-06-26 21:45:27.675194
Model ind 665 epoch 1014 batch: 800 avg loss -2.872260 avg loss no lamb -2.872260 time 2020-06-26 21:45:38.750786
last batch sz 10
Pre: time 2020-06-26 21:45:53.214654: 
 	std: 0.0035704458
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9798, 0.9726, 0.9811, 0.9747]
	train_accs: [0.9814, 0.9806333, 0.9748667, 0.98155, 0.9759333]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.9779
	best: 0.9811

Starting e_i: 1015
Model ind 665 epoch 1015 batch: 0 avg loss -2.964578 avg loss no lamb -2.964578 time 2020-06-26 21:45:54.410387
Model ind 665 epoch 1015 batch: 100 avg loss -2.881894 avg loss no lamb -2.881894 time 2020-06-26 21:46:05.284954
Model ind 665 epoch 1015 batch: 200 avg loss -2.887536 avg loss no lamb -2.887536 time 2020-06-26 21:46:16.299173
Model ind 665 epoch 1015 batch: 300 avg loss -2.880043 avg loss no lamb -2.880043 time 2020-06-26 21:46:27.402288
Model ind 665 epoch 1015 batch: 400 avg loss -2.738292 avg loss no lamb -2.738292 time 2020-06-26 21:46:38.195291
Model ind 665 epoch 1015 batch: 500 avg loss -2.803159 avg loss no lamb -2.803159 time 2020-06-26 21:46:49.084458
Model ind 665 epoch 1015 batch: 600 avg loss -2.873187 avg loss no lamb -2.873187 time 2020-06-26 21:46:59.873387
Model ind 665 epoch 1015 batch: 700 avg loss -2.755433 avg loss no lamb -2.755433 time 2020-06-26 21:47:10.851897
Model ind 665 epoch 1015 batch: 800 avg loss -2.912266 avg loss no lamb -2.912266 time 2020-06-26 21:47:21.792728
last batch sz 10
Pre: time 2020-06-26 21:47:36.336662: 
 	std: 0.003496063
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9804, 0.973, 0.9816, 0.9759]
	train_accs: [0.9816167, 0.98073334, 0.97491664, 0.9817333, 0.97665]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97854006
	best: 0.9816

Starting e_i: 1016
Model ind 665 epoch 1016 batch: 0 avg loss -2.956842 avg loss no lamb -2.956842 time 2020-06-26 21:47:37.467790
Model ind 665 epoch 1016 batch: 100 avg loss -2.896224 avg loss no lamb -2.896224 time 2020-06-26 21:47:48.107671
Model ind 665 epoch 1016 batch: 200 avg loss -2.837147 avg loss no lamb -2.837147 time 2020-06-26 21:47:58.989693
Model ind 665 epoch 1016 batch: 300 avg loss -2.797166 avg loss no lamb -2.797166 time 2020-06-26 21:48:10.054915
Model ind 665 epoch 1016 batch: 400 avg loss -2.785269 avg loss no lamb -2.785269 time 2020-06-26 21:48:21.035310
Model ind 665 epoch 1016 batch: 500 avg loss -2.837021 avg loss no lamb -2.837021 time 2020-06-26 21:48:31.853562
Model ind 665 epoch 1016 batch: 600 avg loss -2.895691 avg loss no lamb -2.895691 time 2020-06-26 21:48:42.852280
Model ind 665 epoch 1016 batch: 700 avg loss -2.757493 avg loss no lamb -2.757493 time 2020-06-26 21:48:53.572505
Model ind 665 epoch 1016 batch: 800 avg loss -2.848614 avg loss no lamb -2.848614 time 2020-06-26 21:49:04.360023
last batch sz 10
Pre: time 2020-06-26 21:49:18.699795: 
 	std: 0.002925344
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9813, 0.9755, 0.9821, 0.9766]
	train_accs: [0.98141664, 0.9806333, 0.97606665, 0.9813, 0.9766833]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97958004
	best: 0.9824

Starting e_i: 1017
Model ind 665 epoch 1017 batch: 0 avg loss -2.962994 avg loss no lamb -2.962994 time 2020-06-26 21:49:19.896043
Model ind 665 epoch 1017 batch: 100 avg loss -2.888790 avg loss no lamb -2.888790 time 2020-06-26 21:49:30.595059
Model ind 665 epoch 1017 batch: 200 avg loss -2.859619 avg loss no lamb -2.859619 time 2020-06-26 21:49:41.447255
Model ind 665 epoch 1017 batch: 300 avg loss -2.877577 avg loss no lamb -2.877577 time 2020-06-26 21:49:52.299687
Model ind 665 epoch 1017 batch: 400 avg loss -2.777229 avg loss no lamb -2.777229 time 2020-06-26 21:50:03.298011
Model ind 665 epoch 1017 batch: 500 avg loss -2.839057 avg loss no lamb -2.839057 time 2020-06-26 21:50:14.155645
Model ind 665 epoch 1017 batch: 600 avg loss -2.886541 avg loss no lamb -2.886541 time 2020-06-26 21:50:24.896515
Model ind 665 epoch 1017 batch: 700 avg loss -2.742939 avg loss no lamb -2.742939 time 2020-06-26 21:50:35.893712
Model ind 665 epoch 1017 batch: 800 avg loss -2.852367 avg loss no lamb -2.852367 time 2020-06-26 21:50:47.050677
last batch sz 10
Pre: time 2020-06-26 21:51:01.129817: 
 	std: 0.0033915155
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9809, 0.9737, 0.9812, 0.9745]
	train_accs: [0.98106664, 0.9808, 0.97466666, 0.98113334, 0.9755167]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97824
	best: 0.9812

Starting e_i: 1018
Model ind 665 epoch 1018 batch: 0 avg loss -2.912955 avg loss no lamb -2.912955 time 2020-06-26 21:51:02.169844
Model ind 665 epoch 1018 batch: 100 avg loss -2.885708 avg loss no lamb -2.885708 time 2020-06-26 21:51:12.925257
Model ind 665 epoch 1018 batch: 200 avg loss -2.914984 avg loss no lamb -2.914984 time 2020-06-26 21:51:23.586497
Model ind 665 epoch 1018 batch: 300 avg loss -2.853690 avg loss no lamb -2.853690 time 2020-06-26 21:51:34.556299
Model ind 665 epoch 1018 batch: 400 avg loss -2.799382 avg loss no lamb -2.799382 time 2020-06-26 21:51:45.374982
Model ind 665 epoch 1018 batch: 500 avg loss -2.871122 avg loss no lamb -2.871122 time 2020-06-26 21:51:56.362252
Model ind 665 epoch 1018 batch: 600 avg loss -2.881934 avg loss no lamb -2.881934 time 2020-06-26 21:52:07.243555
Model ind 665 epoch 1018 batch: 700 avg loss -2.778879 avg loss no lamb -2.778879 time 2020-06-26 21:52:18.290122
Model ind 665 epoch 1018 batch: 800 avg loss -2.884313 avg loss no lamb -2.884313 time 2020-06-26 21:52:29.074336
last batch sz 10
Pre: time 2020-06-26 21:52:43.333232: 
 	std: 0.0031814473
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9805, 0.9746, 0.9814, 0.9744]
	train_accs: [0.981, 0.9806, 0.97531664, 0.98141664, 0.9758]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97838
	best: 0.9814

Starting e_i: 1019
Model ind 665 epoch 1019 batch: 0 avg loss -2.999146 avg loss no lamb -2.999146 time 2020-06-26 21:52:44.532346
Model ind 665 epoch 1019 batch: 100 avg loss -2.896479 avg loss no lamb -2.896479 time 2020-06-26 21:52:55.583008
Model ind 665 epoch 1019 batch: 200 avg loss -2.814766 avg loss no lamb -2.814766 time 2020-06-26 21:53:06.514855
Model ind 665 epoch 1019 batch: 300 avg loss -2.879135 avg loss no lamb -2.879135 time 2020-06-26 21:53:17.553430
Model ind 665 epoch 1019 batch: 400 avg loss -2.764379 avg loss no lamb -2.764379 time 2020-06-26 21:53:28.557370
Model ind 665 epoch 1019 batch: 500 avg loss -2.819362 avg loss no lamb -2.819362 time 2020-06-26 21:53:39.652004
Model ind 665 epoch 1019 batch: 600 avg loss -2.878298 avg loss no lamb -2.878298 time 2020-06-26 21:53:50.563279
Model ind 665 epoch 1019 batch: 700 avg loss -2.803983 avg loss no lamb -2.803983 time 2020-06-26 21:54:01.488373
Model ind 665 epoch 1019 batch: 800 avg loss -2.853667 avg loss no lamb -2.853667 time 2020-06-26 21:54:12.485207
last batch sz 10
Pre: time 2020-06-26 21:54:26.577287: 
 	std: 0.0030450113
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9808, 0.9747, 0.9803, 0.9747]
	train_accs: [0.9817167, 0.98125, 0.97536665, 0.9817, 0.97603333]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97840005
	best: 0.9815

Starting e_i: 1020
Model ind 665 epoch 1020 batch: 0 avg loss -2.923597 avg loss no lamb -2.923597 time 2020-06-26 21:54:27.634437
Model ind 665 epoch 1020 batch: 100 avg loss -2.924851 avg loss no lamb -2.924851 time 2020-06-26 21:54:38.282716
Model ind 665 epoch 1020 batch: 200 avg loss -2.853166 avg loss no lamb -2.853166 time 2020-06-26 21:54:49.070920
Model ind 665 epoch 1020 batch: 300 avg loss -2.859155 avg loss no lamb -2.859155 time 2020-06-26 21:54:59.740100
Model ind 665 epoch 1020 batch: 400 avg loss -2.752710 avg loss no lamb -2.752710 time 2020-06-26 21:55:10.849287
Model ind 665 epoch 1020 batch: 500 avg loss -2.868580 avg loss no lamb -2.868580 time 2020-06-26 21:55:21.547800
Model ind 665 epoch 1020 batch: 600 avg loss -2.871315 avg loss no lamb -2.871315 time 2020-06-26 21:55:32.296087
Model ind 665 epoch 1020 batch: 700 avg loss -2.697978 avg loss no lamb -2.697978 time 2020-06-26 21:55:43.110226
Model ind 665 epoch 1020 batch: 800 avg loss -2.850593 avg loss no lamb -2.850593 time 2020-06-26 21:55:54.353446
last batch sz 10
Pre: time 2020-06-26 21:56:08.644042: 
 	std: 0.0027437122
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9795, 0.9738, 0.9795, 0.9743]
	train_accs: [0.9809667, 0.98013335, 0.97535, 0.98076665, 0.9762]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.9774
	best: 0.9799

Starting e_i: 1021
Model ind 665 epoch 1021 batch: 0 avg loss -2.961553 avg loss no lamb -2.961553 time 2020-06-26 21:56:11.064944
Model ind 665 epoch 1021 batch: 100 avg loss -2.836998 avg loss no lamb -2.836998 time 2020-06-26 21:56:22.058028
Model ind 665 epoch 1021 batch: 200 avg loss -2.851058 avg loss no lamb -2.851058 time 2020-06-26 21:56:33.291502
Model ind 665 epoch 1021 batch: 300 avg loss -2.852953 avg loss no lamb -2.852953 time 2020-06-26 21:56:44.305306
Model ind 665 epoch 1021 batch: 400 avg loss -2.824183 avg loss no lamb -2.824183 time 2020-06-26 21:56:55.314785
Model ind 665 epoch 1021 batch: 500 avg loss -2.909393 avg loss no lamb -2.909393 time 2020-06-26 21:57:06.475694
Model ind 665 epoch 1021 batch: 600 avg loss -2.871713 avg loss no lamb -2.871713 time 2020-06-26 21:57:17.671553
Model ind 665 epoch 1021 batch: 700 avg loss -2.763273 avg loss no lamb -2.763273 time 2020-06-26 21:57:28.584821
Model ind 665 epoch 1021 batch: 800 avg loss -2.928548 avg loss no lamb -2.928548 time 2020-06-26 21:57:39.414456
last batch sz 10
Pre: time 2020-06-26 21:57:53.347152: 
 	std: 0.0025120615
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.979, 0.9744, 0.98, 0.9748]
	train_accs: [0.9812, 0.98038334, 0.9751833, 0.98115, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97764
	best: 0.98

Starting e_i: 1022
Model ind 665 epoch 1022 batch: 0 avg loss -2.949692 avg loss no lamb -2.949692 time 2020-06-26 21:57:54.384547
Model ind 665 epoch 1022 batch: 100 avg loss -2.899120 avg loss no lamb -2.899120 time 2020-06-26 21:58:05.354378
Model ind 665 epoch 1022 batch: 200 avg loss -2.864856 avg loss no lamb -2.864856 time 2020-06-26 21:58:16.475072
Model ind 665 epoch 1022 batch: 300 avg loss -2.814467 avg loss no lamb -2.814467 time 2020-06-26 21:58:27.692563
Model ind 665 epoch 1022 batch: 400 avg loss -2.721714 avg loss no lamb -2.721714 time 2020-06-26 21:58:38.680290
Model ind 665 epoch 1022 batch: 500 avg loss -2.890714 avg loss no lamb -2.890714 time 2020-06-26 21:58:49.752547
Model ind 665 epoch 1022 batch: 600 avg loss -2.831669 avg loss no lamb -2.831669 time 2020-06-26 21:59:00.872297
Model ind 665 epoch 1022 batch: 700 avg loss -2.773968 avg loss no lamb -2.773968 time 2020-06-26 21:59:11.873804
Model ind 665 epoch 1022 batch: 800 avg loss -2.853919 avg loss no lamb -2.853919 time 2020-06-26 21:59:23.053744
last batch sz 10
Pre: time 2020-06-26 21:59:37.377316: 
 	std: 0.0028421206
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9802, 0.9744, 0.9804, 0.9749]
	train_accs: [0.98155, 0.98066664, 0.97575, 0.98108333, 0.97601664]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97812
	best: 0.9807

Starting e_i: 1023
Model ind 665 epoch 1023 batch: 0 avg loss -2.984610 avg loss no lamb -2.984610 time 2020-06-26 21:59:38.589785
Model ind 665 epoch 1023 batch: 100 avg loss -2.843025 avg loss no lamb -2.843025 time 2020-06-26 21:59:49.735907
Model ind 665 epoch 1023 batch: 200 avg loss -2.854699 avg loss no lamb -2.854699 time 2020-06-26 22:00:00.801015
Model ind 665 epoch 1023 batch: 300 avg loss -2.916251 avg loss no lamb -2.916251 time 2020-06-26 22:00:12.004590
Model ind 665 epoch 1023 batch: 400 avg loss -2.778184 avg loss no lamb -2.778184 time 2020-06-26 22:00:22.924947
Model ind 665 epoch 1023 batch: 500 avg loss -2.834881 avg loss no lamb -2.834881 time 2020-06-26 22:00:34.109449
Model ind 665 epoch 1023 batch: 600 avg loss -2.890425 avg loss no lamb -2.890425 time 2020-06-26 22:00:44.996432
Model ind 665 epoch 1023 batch: 700 avg loss -2.756384 avg loss no lamb -2.756384 time 2020-06-26 22:00:55.674178
Model ind 665 epoch 1023 batch: 800 avg loss -2.866522 avg loss no lamb -2.866522 time 2020-06-26 22:01:06.616162
last batch sz 10
Pre: time 2020-06-26 22:01:20.948948: 
 	std: 0.0031922392
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9816, 0.9744, 0.9816, 0.976]
	train_accs: [0.9820833, 0.98141664, 0.9758667, 0.98205, 0.9770333]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97906
	best: 0.9817

Starting e_i: 1024
Model ind 665 epoch 1024 batch: 0 avg loss -2.935422 avg loss no lamb -2.935422 time 2020-06-26 22:01:22.061582
Model ind 665 epoch 1024 batch: 100 avg loss -2.857524 avg loss no lamb -2.857524 time 2020-06-26 22:01:32.999402
Model ind 665 epoch 1024 batch: 200 avg loss -2.842890 avg loss no lamb -2.842890 time 2020-06-26 22:01:43.867744
Model ind 665 epoch 1024 batch: 300 avg loss -2.857773 avg loss no lamb -2.857773 time 2020-06-26 22:01:54.955486
Model ind 665 epoch 1024 batch: 400 avg loss -2.747101 avg loss no lamb -2.747101 time 2020-06-26 22:02:05.954994
Model ind 665 epoch 1024 batch: 500 avg loss -2.819854 avg loss no lamb -2.819854 time 2020-06-26 22:02:16.945456
Model ind 665 epoch 1024 batch: 600 avg loss -2.865845 avg loss no lamb -2.865845 time 2020-06-26 22:02:27.713287
Model ind 665 epoch 1024 batch: 700 avg loss -2.809120 avg loss no lamb -2.809120 time 2020-06-26 22:02:38.463512
Model ind 665 epoch 1024 batch: 800 avg loss -2.862823 avg loss no lamb -2.862823 time 2020-06-26 22:02:49.471577
last batch sz 10
Pre: time 2020-06-26 22:03:03.728131: 
 	std: 0.003556759
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9807, 0.974, 0.9821, 0.9747]
	train_accs: [0.98185, 0.9809667, 0.9755833, 0.9819, 0.9765667]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97866
	best: 0.9821

Starting e_i: 1025
Model ind 665 epoch 1025 batch: 0 avg loss -2.903764 avg loss no lamb -2.903764 time 2020-06-26 22:03:04.922201
Model ind 665 epoch 1025 batch: 100 avg loss -2.900564 avg loss no lamb -2.900564 time 2020-06-26 22:03:15.663069
Model ind 665 epoch 1025 batch: 200 avg loss -2.865626 avg loss no lamb -2.865626 time 2020-06-26 22:03:26.436124
Model ind 665 epoch 1025 batch: 300 avg loss -2.899723 avg loss no lamb -2.899723 time 2020-06-26 22:03:37.262576
Model ind 665 epoch 1025 batch: 400 avg loss -2.741138 avg loss no lamb -2.741138 time 2020-06-26 22:03:48.261699
Model ind 665 epoch 1025 batch: 500 avg loss -2.857100 avg loss no lamb -2.857100 time 2020-06-26 22:03:59.478829
Model ind 665 epoch 1025 batch: 600 avg loss -2.895593 avg loss no lamb -2.895593 time 2020-06-26 22:04:10.568256
Model ind 665 epoch 1025 batch: 700 avg loss -2.732809 avg loss no lamb -2.732809 time 2020-06-26 22:04:21.510927
Model ind 665 epoch 1025 batch: 800 avg loss -2.812043 avg loss no lamb -2.812043 time 2020-06-26 22:04:32.446769
last batch sz 10
Pre: time 2020-06-26 22:04:46.773095: 
 	std: 0.0028345187
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.98, 0.9733, 0.98, 0.9752]
	train_accs: [0.9805167, 0.9799, 0.9745167, 0.98046666, 0.97595]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97764003
	best: 0.9797

Starting e_i: 1026
Model ind 665 epoch 1026 batch: 0 avg loss -2.910482 avg loss no lamb -2.910482 time 2020-06-26 22:04:47.852668
Model ind 665 epoch 1026 batch: 100 avg loss -2.866956 avg loss no lamb -2.866956 time 2020-06-26 22:04:58.695037
Model ind 665 epoch 1026 batch: 200 avg loss -2.904368 avg loss no lamb -2.904368 time 2020-06-26 22:05:09.688939
Model ind 665 epoch 1026 batch: 300 avg loss -2.838382 avg loss no lamb -2.838382 time 2020-06-26 22:05:20.693643
Model ind 665 epoch 1026 batch: 400 avg loss -2.761840 avg loss no lamb -2.761840 time 2020-06-26 22:05:31.997652
Model ind 665 epoch 1026 batch: 500 avg loss -2.853925 avg loss no lamb -2.853925 time 2020-06-26 22:05:42.785243
Model ind 665 epoch 1026 batch: 600 avg loss -2.872078 avg loss no lamb -2.872078 time 2020-06-26 22:05:53.530356
Model ind 665 epoch 1026 batch: 700 avg loss -2.804392 avg loss no lamb -2.804392 time 2020-06-26 22:06:04.790838
Model ind 665 epoch 1026 batch: 800 avg loss -2.902379 avg loss no lamb -2.902379 time 2020-06-26 22:06:16.059278
last batch sz 10
Pre: time 2020-06-26 22:06:30.380208: 
 	std: 0.0031025275
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9795, 0.9729, 0.9802, 0.9747]
	train_accs: [0.98115, 0.9801667, 0.9748667, 0.9810167, 0.976]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97752
	best: 0.9803

Starting e_i: 1027
Model ind 665 epoch 1027 batch: 0 avg loss -2.942229 avg loss no lamb -2.942229 time 2020-06-26 22:06:31.603410
Model ind 665 epoch 1027 batch: 100 avg loss -2.911829 avg loss no lamb -2.911829 time 2020-06-26 22:06:42.394103
Model ind 665 epoch 1027 batch: 200 avg loss -2.874038 avg loss no lamb -2.874038 time 2020-06-26 22:06:53.244163
Model ind 665 epoch 1027 batch: 300 avg loss -2.826702 avg loss no lamb -2.826702 time 2020-06-26 22:07:04.250913
Model ind 665 epoch 1027 batch: 400 avg loss -2.789553 avg loss no lamb -2.789553 time 2020-06-26 22:07:15.241492
Model ind 665 epoch 1027 batch: 500 avg loss -2.856011 avg loss no lamb -2.856011 time 2020-06-26 22:07:26.009458
Model ind 665 epoch 1027 batch: 600 avg loss -2.879323 avg loss no lamb -2.879323 time 2020-06-26 22:07:36.974711
Model ind 665 epoch 1027 batch: 700 avg loss -2.759617 avg loss no lamb -2.759617 time 2020-06-26 22:07:48.180054
Model ind 665 epoch 1027 batch: 800 avg loss -2.816824 avg loss no lamb -2.816824 time 2020-06-26 22:07:59.079012
last batch sz 10
Pre: time 2020-06-26 22:08:13.321864: 
 	std: 0.002481609
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9805, 0.9759, 0.9819, 0.9769]
	train_accs: [0.98178333, 0.98083335, 0.97616667, 0.98176664, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9759
	avg: 0.97936
	best: 0.9816

Starting e_i: 1028
Model ind 665 epoch 1028 batch: 0 avg loss -2.976243 avg loss no lamb -2.976243 time 2020-06-26 22:08:14.393511
Model ind 665 epoch 1028 batch: 100 avg loss -2.927760 avg loss no lamb -2.927760 time 2020-06-26 22:08:25.183765
Model ind 665 epoch 1028 batch: 200 avg loss -2.888952 avg loss no lamb -2.888952 time 2020-06-26 22:08:36.231460
Model ind 665 epoch 1028 batch: 300 avg loss -2.863535 avg loss no lamb -2.863535 time 2020-06-26 22:08:47.099781
Model ind 665 epoch 1028 batch: 400 avg loss -2.764408 avg loss no lamb -2.764408 time 2020-06-26 22:08:58.215783
Model ind 665 epoch 1028 batch: 500 avg loss -2.763826 avg loss no lamb -2.763826 time 2020-06-26 22:09:09.178927
Model ind 665 epoch 1028 batch: 600 avg loss -2.903466 avg loss no lamb -2.903466 time 2020-06-26 22:09:20.157405
Model ind 665 epoch 1028 batch: 700 avg loss -2.803736 avg loss no lamb -2.803736 time 2020-06-26 22:09:30.864883
Model ind 665 epoch 1028 batch: 800 avg loss -2.846858 avg loss no lamb -2.846858 time 2020-06-26 22:09:41.872539
last batch sz 10
Pre: time 2020-06-26 22:09:56.118895: 
 	std: 0.0031689762
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.982, 0.9751, 0.981, 0.9749]
	train_accs: [0.9817, 0.9813, 0.97606665, 0.9813667, 0.97615]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97886
	best: 0.9813

Starting e_i: 1029
Model ind 665 epoch 1029 batch: 0 avg loss -2.888625 avg loss no lamb -2.888625 time 2020-06-26 22:09:57.346409
Model ind 665 epoch 1029 batch: 100 avg loss -2.901211 avg loss no lamb -2.901211 time 2020-06-26 22:10:08.327456
Model ind 665 epoch 1029 batch: 200 avg loss -2.837843 avg loss no lamb -2.837843 time 2020-06-26 22:10:19.273710
Model ind 665 epoch 1029 batch: 300 avg loss -2.905065 avg loss no lamb -2.905065 time 2020-06-26 22:10:30.060966
Model ind 665 epoch 1029 batch: 400 avg loss -2.729245 avg loss no lamb -2.729245 time 2020-06-26 22:10:41.068260
Model ind 665 epoch 1029 batch: 500 avg loss -2.849872 avg loss no lamb -2.849872 time 2020-06-26 22:10:52.173510
Model ind 665 epoch 1029 batch: 600 avg loss -2.880089 avg loss no lamb -2.880089 time 2020-06-26 22:11:03.176916
Model ind 665 epoch 1029 batch: 700 avg loss -2.842683 avg loss no lamb -2.842683 time 2020-06-26 22:11:14.247172
Model ind 665 epoch 1029 batch: 800 avg loss -2.890130 avg loss no lamb -2.890130 time 2020-06-26 22:11:25.204845
last batch sz 10
Pre: time 2020-06-26 22:11:39.921683: 
 	std: 0.0029566318
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9796, 0.9737, 0.9804, 0.9747]
	train_accs: [0.9813333, 0.98045, 0.97548336, 0.98143333, 0.97606665]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97778
	best: 0.9804

Starting e_i: 1030
Model ind 665 epoch 1030 batch: 0 avg loss -2.891290 avg loss no lamb -2.891290 time 2020-06-26 22:11:41.052592
Model ind 665 epoch 1030 batch: 100 avg loss -2.823354 avg loss no lamb -2.823354 time 2020-06-26 22:11:52.070887
Model ind 665 epoch 1030 batch: 200 avg loss -2.871164 avg loss no lamb -2.871164 time 2020-06-26 22:12:03.081483
Model ind 665 epoch 1030 batch: 300 avg loss -2.821098 avg loss no lamb -2.821098 time 2020-06-26 22:12:13.778868
Model ind 665 epoch 1030 batch: 400 avg loss -2.751275 avg loss no lamb -2.751275 time 2020-06-26 22:12:24.471263
Model ind 665 epoch 1030 batch: 500 avg loss -2.861407 avg loss no lamb -2.861407 time 2020-06-26 22:12:35.161369
Model ind 665 epoch 1030 batch: 600 avg loss -2.827653 avg loss no lamb -2.827653 time 2020-06-26 22:12:46.044926
Model ind 665 epoch 1030 batch: 700 avg loss -2.741277 avg loss no lamb -2.741277 time 2020-06-26 22:12:56.962789
Model ind 665 epoch 1030 batch: 800 avg loss -2.852430 avg loss no lamb -2.852430 time 2020-06-26 22:13:07.989925
last batch sz 10
Pre: time 2020-06-26 22:13:21.899063: 
 	std: 0.0027080681
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9797, 0.9745, 0.981, 0.9758]
	train_accs: [0.98178333, 0.98045, 0.9751833, 0.9816833, 0.9766167]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97838
	best: 0.9809

Starting e_i: 1031
Model ind 665 epoch 1031 batch: 0 avg loss -2.947545 avg loss no lamb -2.947545 time 2020-06-26 22:13:24.273289
Model ind 665 epoch 1031 batch: 100 avg loss -2.843170 avg loss no lamb -2.843170 time 2020-06-26 22:13:35.360877
Model ind 665 epoch 1031 batch: 200 avg loss -2.858361 avg loss no lamb -2.858361 time 2020-06-26 22:13:46.224005
Model ind 665 epoch 1031 batch: 300 avg loss -2.837115 avg loss no lamb -2.837115 time 2020-06-26 22:13:56.857248
Model ind 665 epoch 1031 batch: 400 avg loss -2.803018 avg loss no lamb -2.803018 time 2020-06-26 22:14:07.992380
Model ind 665 epoch 1031 batch: 500 avg loss -2.808123 avg loss no lamb -2.808123 time 2020-06-26 22:14:18.995704
Model ind 665 epoch 1031 batch: 600 avg loss -2.878537 avg loss no lamb -2.878537 time 2020-06-26 22:14:29.877602
Model ind 665 epoch 1031 batch: 700 avg loss -2.721874 avg loss no lamb -2.721874 time 2020-06-26 22:14:40.933349
Model ind 665 epoch 1031 batch: 800 avg loss -2.898627 avg loss no lamb -2.898627 time 2020-06-26 22:14:51.722506
last batch sz 10
Pre: time 2020-06-26 22:15:05.792071: 
 	std: 0.0030590235
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.98, 0.9734, 0.9811, 0.976]
	train_accs: [0.9816167, 0.98065, 0.9751, 0.9816833, 0.9766667]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97827995
	best: 0.9811

Starting e_i: 1032
Model ind 665 epoch 1032 batch: 0 avg loss -2.916175 avg loss no lamb -2.916175 time 2020-06-26 22:15:06.853912
Model ind 665 epoch 1032 batch: 100 avg loss -2.935284 avg loss no lamb -2.935284 time 2020-06-26 22:15:17.769254
Model ind 665 epoch 1032 batch: 200 avg loss -2.907003 avg loss no lamb -2.907003 time 2020-06-26 22:15:28.458983
Model ind 665 epoch 1032 batch: 300 avg loss -2.806499 avg loss no lamb -2.806499 time 2020-06-26 22:15:39.155836
Model ind 665 epoch 1032 batch: 400 avg loss -2.781110 avg loss no lamb -2.781110 time 2020-06-26 22:15:49.907349
Model ind 665 epoch 1032 batch: 500 avg loss -2.813785 avg loss no lamb -2.813785 time 2020-06-26 22:16:00.910668
Model ind 665 epoch 1032 batch: 600 avg loss -2.851068 avg loss no lamb -2.851068 time 2020-06-26 22:16:11.894755
Model ind 665 epoch 1032 batch: 700 avg loss -2.788738 avg loss no lamb -2.788738 time 2020-06-26 22:16:22.456780
Model ind 665 epoch 1032 batch: 800 avg loss -2.886649 avg loss no lamb -2.886649 time 2020-06-26 22:16:33.195619
last batch sz 10
Pre: time 2020-06-26 22:16:47.291281: 
 	std: 0.0033196404
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.979, 0.9728, 0.9814, 0.9751]
	train_accs: [0.98141664, 0.9799833, 0.97525, 0.98123336, 0.97613335]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.9778
	best: 0.9807

Starting e_i: 1033
Model ind 665 epoch 1033 batch: 0 avg loss -2.966617 avg loss no lamb -2.966617 time 2020-06-26 22:16:48.560382
Model ind 665 epoch 1033 batch: 100 avg loss -2.923304 avg loss no lamb -2.923304 time 2020-06-26 22:16:59.358109
Model ind 665 epoch 1033 batch: 200 avg loss -2.830698 avg loss no lamb -2.830698 time 2020-06-26 22:17:10.242197
Model ind 665 epoch 1033 batch: 300 avg loss -2.850970 avg loss no lamb -2.850970 time 2020-06-26 22:17:21.187529
Model ind 665 epoch 1033 batch: 400 avg loss -2.762534 avg loss no lamb -2.762534 time 2020-06-26 22:17:32.153570
Model ind 665 epoch 1033 batch: 500 avg loss -2.840464 avg loss no lamb -2.840464 time 2020-06-26 22:17:43.250659
Model ind 665 epoch 1033 batch: 600 avg loss -2.840374 avg loss no lamb -2.840374 time 2020-06-26 22:17:54.140265
Model ind 665 epoch 1033 batch: 700 avg loss -2.788536 avg loss no lamb -2.788536 time 2020-06-26 22:18:05.076802
Model ind 665 epoch 1033 batch: 800 avg loss -2.799050 avg loss no lamb -2.799050 time 2020-06-26 22:18:15.773198
last batch sz 10
Pre: time 2020-06-26 22:18:29.632017: 
 	std: 0.0029061355
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9792, 0.9734, 0.9803, 0.9753]
	train_accs: [0.9816, 0.9807, 0.97581667, 0.98155, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97778
	best: 0.9807

Starting e_i: 1034
Model ind 665 epoch 1034 batch: 0 avg loss -2.942846 avg loss no lamb -2.942846 time 2020-06-26 22:18:30.662882
Model ind 665 epoch 1034 batch: 100 avg loss -2.906061 avg loss no lamb -2.906061 time 2020-06-26 22:18:41.429634
Model ind 665 epoch 1034 batch: 200 avg loss -2.858590 avg loss no lamb -2.858590 time 2020-06-26 22:18:52.213422
Model ind 665 epoch 1034 batch: 300 avg loss -2.824853 avg loss no lamb -2.824853 time 2020-06-26 22:19:03.014190
Model ind 665 epoch 1034 batch: 400 avg loss -2.785206 avg loss no lamb -2.785206 time 2020-06-26 22:19:13.765246
Model ind 665 epoch 1034 batch: 500 avg loss -2.849749 avg loss no lamb -2.849749 time 2020-06-26 22:19:24.488575
Model ind 665 epoch 1034 batch: 600 avg loss -2.840861 avg loss no lamb -2.840861 time 2020-06-26 22:19:35.345150
Model ind 665 epoch 1034 batch: 700 avg loss -2.811532 avg loss no lamb -2.811532 time 2020-06-26 22:19:46.141069
Model ind 665 epoch 1034 batch: 800 avg loss -2.857008 avg loss no lamb -2.857008 time 2020-06-26 22:19:56.771438
last batch sz 10
Pre: time 2020-06-26 22:20:10.820439: 
 	std: 0.002906545
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9812, 0.9746, 0.981, 0.9762]
	train_accs: [0.9816333, 0.9810167, 0.9763833, 0.98176664, 0.9773333]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97889996
	best: 0.981

Starting e_i: 1035
Model ind 665 epoch 1035 batch: 0 avg loss -2.982836 avg loss no lamb -2.982836 time 2020-06-26 22:20:12.007213
Model ind 665 epoch 1035 batch: 100 avg loss -2.951521 avg loss no lamb -2.951521 time 2020-06-26 22:20:22.777286
Model ind 665 epoch 1035 batch: 200 avg loss -2.778905 avg loss no lamb -2.778905 time 2020-06-26 22:20:33.558510
Model ind 665 epoch 1035 batch: 300 avg loss -2.855615 avg loss no lamb -2.855615 time 2020-06-26 22:20:44.591831
Model ind 665 epoch 1035 batch: 400 avg loss -2.776522 avg loss no lamb -2.776522 time 2020-06-26 22:20:55.678116
Model ind 665 epoch 1035 batch: 500 avg loss -2.816971 avg loss no lamb -2.816971 time 2020-06-26 22:21:06.522791
Model ind 665 epoch 1035 batch: 600 avg loss -2.911343 avg loss no lamb -2.911343 time 2020-06-26 22:21:17.238553
Model ind 665 epoch 1035 batch: 700 avg loss -2.734187 avg loss no lamb -2.734187 time 2020-06-26 22:21:28.287234
Model ind 665 epoch 1035 batch: 800 avg loss -2.804988 avg loss no lamb -2.804988 time 2020-06-26 22:21:39.317917
last batch sz 10
Pre: time 2020-06-26 22:21:53.595986: 
 	std: 0.0034775843
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9794, 0.9733, 0.9811, 0.9739]
	train_accs: [0.9816667, 0.98055, 0.97578335, 0.9816667, 0.9761]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97778
	best: 0.9812

Starting e_i: 1036
Model ind 665 epoch 1036 batch: 0 avg loss -2.930898 avg loss no lamb -2.930898 time 2020-06-26 22:21:54.723877
Model ind 665 epoch 1036 batch: 100 avg loss -2.876216 avg loss no lamb -2.876216 time 2020-06-26 22:22:05.960805
Model ind 665 epoch 1036 batch: 200 avg loss -2.853449 avg loss no lamb -2.853449 time 2020-06-26 22:22:16.859787
Model ind 665 epoch 1036 batch: 300 avg loss -2.873869 avg loss no lamb -2.873869 time 2020-06-26 22:22:27.961475
Model ind 665 epoch 1036 batch: 400 avg loss -2.772049 avg loss no lamb -2.772049 time 2020-06-26 22:22:38.963953
Model ind 665 epoch 1036 batch: 500 avg loss -2.850227 avg loss no lamb -2.850227 time 2020-06-26 22:22:49.895200
Model ind 665 epoch 1036 batch: 600 avg loss -2.877151 avg loss no lamb -2.877151 time 2020-06-26 22:23:00.879906
Model ind 665 epoch 1036 batch: 700 avg loss -2.778849 avg loss no lamb -2.778849 time 2020-06-26 22:23:11.499042
Model ind 665 epoch 1036 batch: 800 avg loss -2.894943 avg loss no lamb -2.894943 time 2020-06-26 22:23:22.409994
last batch sz 10
Pre: time 2020-06-26 22:23:36.492998: 
 	std: 0.0033588174
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9805, 0.9741, 0.9816, 0.9747]
	train_accs: [0.9823, 0.98111665, 0.9759167, 0.9823833, 0.97676665]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97848
	best: 0.9816

Starting e_i: 1037
Model ind 665 epoch 1037 batch: 0 avg loss -2.947945 avg loss no lamb -2.947945 time 2020-06-26 22:23:37.681901
Model ind 665 epoch 1037 batch: 100 avg loss -2.909880 avg loss no lamb -2.909880 time 2020-06-26 22:23:48.478952
Model ind 665 epoch 1037 batch: 200 avg loss -2.805239 avg loss no lamb -2.805239 time 2020-06-26 22:23:59.352393
Model ind 665 epoch 1037 batch: 300 avg loss -2.806920 avg loss no lamb -2.806920 time 2020-06-26 22:24:10.304128
Model ind 665 epoch 1037 batch: 400 avg loss -2.811871 avg loss no lamb -2.811871 time 2020-06-26 22:24:21.153288
Model ind 665 epoch 1037 batch: 500 avg loss -2.768014 avg loss no lamb -2.768014 time 2020-06-26 22:24:32.374397
Model ind 665 epoch 1037 batch: 600 avg loss -2.845389 avg loss no lamb -2.845389 time 2020-06-26 22:24:43.156783
Model ind 665 epoch 1037 batch: 700 avg loss -2.812588 avg loss no lamb -2.812588 time 2020-06-26 22:24:54.092522
Model ind 665 epoch 1037 batch: 800 avg loss -2.888883 avg loss no lamb -2.888883 time 2020-06-26 22:25:05.098051
last batch sz 10
Pre: time 2020-06-26 22:25:19.182987: 
 	std: 0.0034301218
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9793, 0.9726, 0.9804, 0.9733]
	train_accs: [0.9812, 0.9802167, 0.9751667, 0.9812833, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97712004
	best: 0.9804

Starting e_i: 1038
Model ind 665 epoch 1038 batch: 0 avg loss -2.922009 avg loss no lamb -2.922009 time 2020-06-26 22:25:20.252201
Model ind 665 epoch 1038 batch: 100 avg loss -2.868990 avg loss no lamb -2.868990 time 2020-06-26 22:25:31.178525
Model ind 665 epoch 1038 batch: 200 avg loss -2.880448 avg loss no lamb -2.880448 time 2020-06-26 22:25:42.093425
Model ind 665 epoch 1038 batch: 300 avg loss -2.775178 avg loss no lamb -2.775178 time 2020-06-26 22:25:52.965836
Model ind 665 epoch 1038 batch: 400 avg loss -2.733028 avg loss no lamb -2.733028 time 2020-06-26 22:26:03.738735
Model ind 665 epoch 1038 batch: 500 avg loss -2.827601 avg loss no lamb -2.827601 time 2020-06-26 22:26:14.488245
Model ind 665 epoch 1038 batch: 600 avg loss -2.793844 avg loss no lamb -2.793844 time 2020-06-26 22:26:25.435430
Model ind 665 epoch 1038 batch: 700 avg loss -2.757975 avg loss no lamb -2.757975 time 2020-06-26 22:26:36.307856
Model ind 665 epoch 1038 batch: 800 avg loss -2.858975 avg loss no lamb -2.858975 time 2020-06-26 22:26:46.900466
last batch sz 10
Pre: time 2020-06-26 22:27:01.187436: 
 	std: 0.0028534795
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9805, 0.9743, 0.9806, 0.9755]
	train_accs: [0.9814, 0.98095, 0.9757, 0.9815, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97836
	best: 0.9806

Starting e_i: 1039
Model ind 665 epoch 1039 batch: 0 avg loss -2.910594 avg loss no lamb -2.910594 time 2020-06-26 22:27:02.448983
Model ind 665 epoch 1039 batch: 100 avg loss -2.915630 avg loss no lamb -2.915630 time 2020-06-26 22:27:13.517636
Model ind 665 epoch 1039 batch: 200 avg loss -2.846842 avg loss no lamb -2.846842 time 2020-06-26 22:27:24.382861
Model ind 665 epoch 1039 batch: 300 avg loss -2.888229 avg loss no lamb -2.888229 time 2020-06-26 22:27:35.166633
Model ind 665 epoch 1039 batch: 400 avg loss -2.798934 avg loss no lamb -2.798934 time 2020-06-26 22:27:46.476363
Model ind 665 epoch 1039 batch: 500 avg loss -2.791842 avg loss no lamb -2.791842 time 2020-06-26 22:27:57.462984
Model ind 665 epoch 1039 batch: 600 avg loss -2.895623 avg loss no lamb -2.895623 time 2020-06-26 22:28:08.483746
Model ind 665 epoch 1039 batch: 700 avg loss -2.821394 avg loss no lamb -2.821394 time 2020-06-26 22:28:19.391787
Model ind 665 epoch 1039 batch: 800 avg loss -2.803679 avg loss no lamb -2.803679 time 2020-06-26 22:28:30.089259
last batch sz 10
Pre: time 2020-06-26 22:28:44.258714: 
 	std: 0.002865933
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9791, 0.9734, 0.9799, 0.9742]
	train_accs: [0.98085, 0.9803, 0.97536665, 0.9807, 0.97611666]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97728
	best: 0.9798

Starting e_i: 1040
Model ind 665 epoch 1040 batch: 0 avg loss -2.948971 avg loss no lamb -2.948971 time 2020-06-26 22:28:45.373218
Model ind 665 epoch 1040 batch: 100 avg loss -2.885199 avg loss no lamb -2.885199 time 2020-06-26 22:28:56.100369
Model ind 665 epoch 1040 batch: 200 avg loss -2.805619 avg loss no lamb -2.805619 time 2020-06-26 22:29:06.975990
Model ind 665 epoch 1040 batch: 300 avg loss -2.913049 avg loss no lamb -2.913049 time 2020-06-26 22:29:18.051263
Model ind 665 epoch 1040 batch: 400 avg loss -2.786965 avg loss no lamb -2.786965 time 2020-06-26 22:29:28.841316
Model ind 665 epoch 1040 batch: 500 avg loss -2.843276 avg loss no lamb -2.843276 time 2020-06-26 22:29:39.844316
Model ind 665 epoch 1040 batch: 600 avg loss -2.905837 avg loss no lamb -2.905837 time 2020-06-26 22:29:50.565090
Model ind 665 epoch 1040 batch: 700 avg loss -2.785596 avg loss no lamb -2.785596 time 2020-06-26 22:30:01.366930
Model ind 665 epoch 1040 batch: 800 avg loss -2.873700 avg loss no lamb -2.873700 time 2020-06-26 22:30:12.285365
last batch sz 10
Pre: time 2020-06-26 22:30:26.345419: 
 	std: 0.0038055668
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9787, 0.971, 0.9794, 0.9723]
	train_accs: [0.9809833, 0.97995, 0.97491664, 0.98083335, 0.9755667]
	best_train_sub_head: 0
	worst: 0.971
	avg: 0.97626
	best: 0.9799

Starting e_i: 1041
Model ind 665 epoch 1041 batch: 0 avg loss -2.932674 avg loss no lamb -2.932674 time 2020-06-26 22:30:28.739933
Model ind 665 epoch 1041 batch: 100 avg loss -2.867480 avg loss no lamb -2.867480 time 2020-06-26 22:30:39.665423
Model ind 665 epoch 1041 batch: 200 avg loss -2.868517 avg loss no lamb -2.868517 time 2020-06-26 22:30:50.579372
Model ind 665 epoch 1041 batch: 300 avg loss -2.856662 avg loss no lamb -2.856662 time 2020-06-26 22:31:01.437955
Model ind 665 epoch 1041 batch: 400 avg loss -2.778265 avg loss no lamb -2.778265 time 2020-06-26 22:31:12.489417
Model ind 665 epoch 1041 batch: 500 avg loss -2.863935 avg loss no lamb -2.863935 time 2020-06-26 22:31:23.393005
Model ind 665 epoch 1041 batch: 600 avg loss -2.898384 avg loss no lamb -2.898384 time 2020-06-26 22:31:34.152809
Model ind 665 epoch 1041 batch: 700 avg loss -2.773792 avg loss no lamb -2.773792 time 2020-06-26 22:31:44.963644
Model ind 665 epoch 1041 batch: 800 avg loss -2.806222 avg loss no lamb -2.806222 time 2020-06-26 22:31:55.767250
last batch sz 10
Pre: time 2020-06-26 22:32:09.797833: 
 	std: 0.0029370748
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9795, 0.9733, 0.9799, 0.975]
	train_accs: [0.98105, 0.98036665, 0.97538334, 0.98085, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97766
	best: 0.9806

Starting e_i: 1042
Model ind 665 epoch 1042 batch: 0 avg loss -2.966824 avg loss no lamb -2.966824 time 2020-06-26 22:32:10.858913
Model ind 665 epoch 1042 batch: 100 avg loss -2.910768 avg loss no lamb -2.910768 time 2020-06-26 22:32:21.669631
Model ind 665 epoch 1042 batch: 200 avg loss -2.840297 avg loss no lamb -2.840297 time 2020-06-26 22:32:32.632591
Model ind 665 epoch 1042 batch: 300 avg loss -2.883096 avg loss no lamb -2.883096 time 2020-06-26 22:32:43.340871
Model ind 665 epoch 1042 batch: 400 avg loss -2.781313 avg loss no lamb -2.781313 time 2020-06-26 22:32:54.016760
Model ind 665 epoch 1042 batch: 500 avg loss -2.813916 avg loss no lamb -2.813916 time 2020-06-26 22:33:04.770643
Model ind 665 epoch 1042 batch: 600 avg loss -2.910291 avg loss no lamb -2.910291 time 2020-06-26 22:33:15.698253
Model ind 665 epoch 1042 batch: 700 avg loss -2.741604 avg loss no lamb -2.741604 time 2020-06-26 22:33:26.817490
Model ind 665 epoch 1042 batch: 800 avg loss -2.795328 avg loss no lamb -2.795328 time 2020-06-26 22:33:37.693509
last batch sz 10
Pre: time 2020-06-26 22:33:52.200962: 
 	std: 0.0025381881
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9795, 0.9745, 0.9804, 0.9757]
	train_accs: [0.98151666, 0.98095, 0.9761, 0.9814, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97814
	best: 0.9806

Starting e_i: 1043
Model ind 665 epoch 1043 batch: 0 avg loss -2.950114 avg loss no lamb -2.950114 time 2020-06-26 22:33:53.430607
Model ind 665 epoch 1043 batch: 100 avg loss -2.890321 avg loss no lamb -2.890321 time 2020-06-26 22:34:04.702044
Model ind 665 epoch 1043 batch: 200 avg loss -2.865849 avg loss no lamb -2.865849 time 2020-06-26 22:34:15.752153
Model ind 665 epoch 1043 batch: 300 avg loss -2.823630 avg loss no lamb -2.823630 time 2020-06-26 22:34:26.698806
Model ind 665 epoch 1043 batch: 400 avg loss -2.767606 avg loss no lamb -2.767606 time 2020-06-26 22:34:37.741458
Model ind 665 epoch 1043 batch: 500 avg loss -2.811836 avg loss no lamb -2.811836 time 2020-06-26 22:34:48.658704
Model ind 665 epoch 1043 batch: 600 avg loss -2.859844 avg loss no lamb -2.859844 time 2020-06-26 22:34:59.763071
Model ind 665 epoch 1043 batch: 700 avg loss -2.722879 avg loss no lamb -2.722879 time 2020-06-26 22:35:10.966633
Model ind 665 epoch 1043 batch: 800 avg loss -2.846853 avg loss no lamb -2.846853 time 2020-06-26 22:35:22.079241
last batch sz 10
Pre: time 2020-06-26 22:35:36.517853: 
 	std: 0.0029728056
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9799, 0.9735, 0.98, 0.9742]
	train_accs: [0.9814, 0.98036665, 0.97525, 0.98118335, 0.97583336]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97747993
	best: 0.9798

Starting e_i: 1044
Model ind 665 epoch 1044 batch: 0 avg loss -2.955456 avg loss no lamb -2.955456 time 2020-06-26 22:35:37.602021
Model ind 665 epoch 1044 batch: 100 avg loss -2.882329 avg loss no lamb -2.882329 time 2020-06-26 22:35:48.400983
Model ind 665 epoch 1044 batch: 200 avg loss -2.882520 avg loss no lamb -2.882520 time 2020-06-26 22:35:59.276096
Model ind 665 epoch 1044 batch: 300 avg loss -2.837828 avg loss no lamb -2.837828 time 2020-06-26 22:36:10.134086
Model ind 665 epoch 1044 batch: 400 avg loss -2.783675 avg loss no lamb -2.783675 time 2020-06-26 22:36:20.768600
Model ind 665 epoch 1044 batch: 500 avg loss -2.861473 avg loss no lamb -2.861473 time 2020-06-26 22:36:31.430538
Model ind 665 epoch 1044 batch: 600 avg loss -2.889746 avg loss no lamb -2.889746 time 2020-06-26 22:36:42.248097
Model ind 665 epoch 1044 batch: 700 avg loss -2.761251 avg loss no lamb -2.761251 time 2020-06-26 22:36:53.354787
Model ind 665 epoch 1044 batch: 800 avg loss -2.896998 avg loss no lamb -2.896998 time 2020-06-26 22:37:04.440129
last batch sz 10
Pre: time 2020-06-26 22:37:18.540263: 
 	std: 0.0037780502
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.981, 0.9734, 0.9814, 0.9738]
	train_accs: [0.9819833, 0.9812667, 0.9751667, 0.9820167, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.9782201
	best: 0.9814

Starting e_i: 1045
Model ind 665 epoch 1045 batch: 0 avg loss -2.971322 avg loss no lamb -2.971322 time 2020-06-26 22:37:19.777546
Model ind 665 epoch 1045 batch: 100 avg loss -2.899325 avg loss no lamb -2.899325 time 2020-06-26 22:37:30.771812
Model ind 665 epoch 1045 batch: 200 avg loss -2.922346 avg loss no lamb -2.922346 time 2020-06-26 22:37:41.959745
Model ind 665 epoch 1045 batch: 300 avg loss -2.825785 avg loss no lamb -2.825785 time 2020-06-26 22:37:53.154601
Model ind 665 epoch 1045 batch: 400 avg loss -2.847636 avg loss no lamb -2.847636 time 2020-06-26 22:38:03.828112
Model ind 665 epoch 1045 batch: 500 avg loss -2.856483 avg loss no lamb -2.856483 time 2020-06-26 22:38:14.870644
Model ind 665 epoch 1045 batch: 600 avg loss -2.892911 avg loss no lamb -2.892911 time 2020-06-26 22:38:25.716773
Model ind 665 epoch 1045 batch: 700 avg loss -2.805031 avg loss no lamb -2.805031 time 2020-06-26 22:38:36.768436
Model ind 665 epoch 1045 batch: 800 avg loss -2.864115 avg loss no lamb -2.864115 time 2020-06-26 22:38:47.389584
last batch sz 10
Pre: time 2020-06-26 22:39:01.374288: 
 	std: 0.002955256
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9805, 0.9739, 0.9808, 0.9754]
	train_accs: [0.9816, 0.9809167, 0.9755333, 0.98178333, 0.97651666]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97822
	best: 0.9808

Starting e_i: 1046
Model ind 665 epoch 1046 batch: 0 avg loss -2.909470 avg loss no lamb -2.909470 time 2020-06-26 22:39:02.420350
Model ind 665 epoch 1046 batch: 100 avg loss -2.867834 avg loss no lamb -2.867834 time 2020-06-26 22:39:13.986840
Model ind 665 epoch 1046 batch: 200 avg loss -2.838772 avg loss no lamb -2.838772 time 2020-06-26 22:39:24.609666
Model ind 665 epoch 1046 batch: 300 avg loss -2.856633 avg loss no lamb -2.856633 time 2020-06-26 22:39:35.481669
Model ind 665 epoch 1046 batch: 400 avg loss -2.730303 avg loss no lamb -2.730303 time 2020-06-26 22:39:46.524085
Model ind 665 epoch 1046 batch: 500 avg loss -2.883405 avg loss no lamb -2.883405 time 2020-06-26 22:39:57.177757
Model ind 665 epoch 1046 batch: 600 avg loss -2.892348 avg loss no lamb -2.892348 time 2020-06-26 22:40:08.004342
Model ind 665 epoch 1046 batch: 700 avg loss -2.781911 avg loss no lamb -2.781911 time 2020-06-26 22:40:18.585940
Model ind 665 epoch 1046 batch: 800 avg loss -2.878327 avg loss no lamb -2.878327 time 2020-06-26 22:40:29.485753
last batch sz 10
Pre: time 2020-06-26 22:40:43.660579: 
 	std: 0.0034267248
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9802, 0.9736, 0.9808, 0.9742]
	train_accs: [0.9815, 0.98071665, 0.97501665, 0.9814, 0.97546667]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97805995
	best: 0.9815

Starting e_i: 1047
Model ind 665 epoch 1047 batch: 0 avg loss -2.951514 avg loss no lamb -2.951514 time 2020-06-26 22:40:44.896110
Model ind 665 epoch 1047 batch: 100 avg loss -2.865182 avg loss no lamb -2.865182 time 2020-06-26 22:40:55.673674
Model ind 665 epoch 1047 batch: 200 avg loss -2.875367 avg loss no lamb -2.875367 time 2020-06-26 22:41:06.479451
Model ind 665 epoch 1047 batch: 300 avg loss -2.897323 avg loss no lamb -2.897323 time 2020-06-26 22:41:17.238089
Model ind 665 epoch 1047 batch: 400 avg loss -2.763847 avg loss no lamb -2.763847 time 2020-06-26 22:41:28.381308
Model ind 665 epoch 1047 batch: 500 avg loss -2.811623 avg loss no lamb -2.811623 time 2020-06-26 22:41:39.297463
Model ind 665 epoch 1047 batch: 600 avg loss -2.922277 avg loss no lamb -2.922277 time 2020-06-26 22:41:50.100079
Model ind 665 epoch 1047 batch: 700 avg loss -2.725766 avg loss no lamb -2.725766 time 2020-06-26 22:42:01.139776
Model ind 665 epoch 1047 batch: 800 avg loss -2.836931 avg loss no lamb -2.836931 time 2020-06-26 22:42:11.897971
last batch sz 10
Pre: time 2020-06-26 22:42:25.941023: 
 	std: 0.0030446104
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9796, 0.9731, 0.9804, 0.9759]
	train_accs: [0.9819667, 0.9810333, 0.9757, 0.98183334, 0.97718334]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97802
	best: 0.9811

Starting e_i: 1048
Model ind 665 epoch 1048 batch: 0 avg loss -2.931757 avg loss no lamb -2.931757 time 2020-06-26 22:42:26.958057
Model ind 665 epoch 1048 batch: 100 avg loss -2.899466 avg loss no lamb -2.899466 time 2020-06-26 22:42:37.725008
Model ind 665 epoch 1048 batch: 200 avg loss -2.858702 avg loss no lamb -2.858702 time 2020-06-26 22:42:48.603614
Model ind 665 epoch 1048 batch: 300 avg loss -2.904170 avg loss no lamb -2.904170 time 2020-06-26 22:42:59.666063
Model ind 665 epoch 1048 batch: 400 avg loss -2.820194 avg loss no lamb -2.820194 time 2020-06-26 22:43:10.674999
Model ind 665 epoch 1048 batch: 500 avg loss -2.852856 avg loss no lamb -2.852856 time 2020-06-26 22:43:21.579488
Model ind 665 epoch 1048 batch: 600 avg loss -2.900915 avg loss no lamb -2.900915 time 2020-06-26 22:43:32.392590
Model ind 665 epoch 1048 batch: 700 avg loss -2.779680 avg loss no lamb -2.779680 time 2020-06-26 22:43:43.166432
Model ind 665 epoch 1048 batch: 800 avg loss -2.836426 avg loss no lamb -2.836426 time 2020-06-26 22:43:53.907599
last batch sz 10
Pre: time 2020-06-26 22:44:07.999819: 
 	std: 0.0031656893
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9807, 0.9746, 0.981, 0.9745]
	train_accs: [0.9816167, 0.9810167, 0.9759833, 0.98176664, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97841996
	best: 0.981

Starting e_i: 1049
Model ind 665 epoch 1049 batch: 0 avg loss -2.962324 avg loss no lamb -2.962324 time 2020-06-26 22:44:09.238873
Model ind 665 epoch 1049 batch: 100 avg loss -2.892432 avg loss no lamb -2.892432 time 2020-06-26 22:44:20.063407
Model ind 665 epoch 1049 batch: 200 avg loss -2.862749 avg loss no lamb -2.862749 time 2020-06-26 22:44:30.648778
Model ind 665 epoch 1049 batch: 300 avg loss -2.853687 avg loss no lamb -2.853687 time 2020-06-26 22:44:41.326732
Model ind 665 epoch 1049 batch: 400 avg loss -2.748774 avg loss no lamb -2.748774 time 2020-06-26 22:44:52.244033
Model ind 665 epoch 1049 batch: 500 avg loss -2.882353 avg loss no lamb -2.882353 time 2020-06-26 22:45:03.265125
Model ind 665 epoch 1049 batch: 600 avg loss -2.851332 avg loss no lamb -2.851332 time 2020-06-26 22:45:14.098726
Model ind 665 epoch 1049 batch: 700 avg loss -2.781670 avg loss no lamb -2.781670 time 2020-06-26 22:45:24.902437
Model ind 665 epoch 1049 batch: 800 avg loss -2.884450 avg loss no lamb -2.884450 time 2020-06-26 22:45:35.646563
last batch sz 10
Pre: time 2020-06-26 22:45:50.065132: 
 	std: 0.0023163736
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9788, 0.9731, 0.9783, 0.9751]
	train_accs: [0.98081666, 0.9801667, 0.975, 0.9806, 0.97616667]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97682
	best: 0.9788

Starting e_i: 1050
Model ind 665 epoch 1050 batch: 0 avg loss -2.926028 avg loss no lamb -2.926028 time 2020-06-26 22:45:51.140446
Model ind 665 epoch 1050 batch: 100 avg loss -2.874819 avg loss no lamb -2.874819 time 2020-06-26 22:46:02.089353
Model ind 665 epoch 1050 batch: 200 avg loss -2.839441 avg loss no lamb -2.839441 time 2020-06-26 22:46:12.889113
Model ind 665 epoch 1050 batch: 300 avg loss -2.799462 avg loss no lamb -2.799462 time 2020-06-26 22:46:23.692413
Model ind 665 epoch 1050 batch: 400 avg loss -2.818801 avg loss no lamb -2.818801 time 2020-06-26 22:46:34.616864
Model ind 665 epoch 1050 batch: 500 avg loss -2.859418 avg loss no lamb -2.859418 time 2020-06-26 22:46:45.640735
Model ind 665 epoch 1050 batch: 600 avg loss -2.870486 avg loss no lamb -2.870486 time 2020-06-26 22:46:56.403427
Model ind 665 epoch 1050 batch: 700 avg loss -2.826876 avg loss no lamb -2.826876 time 2020-06-26 22:47:07.489024
Model ind 665 epoch 1050 batch: 800 avg loss -2.845226 avg loss no lamb -2.845226 time 2020-06-26 22:47:18.124475
last batch sz 10
Pre: time 2020-06-26 22:47:32.323746: 
 	std: 0.0035034788
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9801, 0.9734, 0.9819, 0.9749]
	train_accs: [0.98116666, 0.98066664, 0.9755667, 0.9812, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97833997
	best: 0.9819

Starting e_i: 1051
Model ind 665 epoch 1051 batch: 0 avg loss -2.982036 avg loss no lamb -2.982036 time 2020-06-26 22:47:34.724089
Model ind 665 epoch 1051 batch: 100 avg loss -2.896894 avg loss no lamb -2.896894 time 2020-06-26 22:47:45.649878
Model ind 665 epoch 1051 batch: 200 avg loss -2.881198 avg loss no lamb -2.881198 time 2020-06-26 22:47:56.442708
Model ind 665 epoch 1051 batch: 300 avg loss -2.871641 avg loss no lamb -2.871641 time 2020-06-26 22:48:07.186516
Model ind 665 epoch 1051 batch: 400 avg loss -2.792255 avg loss no lamb -2.792255 time 2020-06-26 22:48:18.062450
Model ind 665 epoch 1051 batch: 500 avg loss -2.820006 avg loss no lamb -2.820006 time 2020-06-26 22:48:28.833349
Model ind 665 epoch 1051 batch: 600 avg loss -2.876311 avg loss no lamb -2.876311 time 2020-06-26 22:48:39.741013
Model ind 665 epoch 1051 batch: 700 avg loss -2.817614 avg loss no lamb -2.817614 time 2020-06-26 22:48:50.532524
Model ind 665 epoch 1051 batch: 800 avg loss -2.897938 avg loss no lamb -2.897938 time 2020-06-26 22:49:01.566810
last batch sz 10
Pre: time 2020-06-26 22:49:15.335384: 
 	std: 0.003594658
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9794, 0.9721, 0.9808, 0.9736]
	train_accs: [0.9811, 0.9805833, 0.97473335, 0.98116666, 0.97571665]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97718
	best: 0.9808

Starting e_i: 1052
Model ind 665 epoch 1052 batch: 0 avg loss -3.010923 avg loss no lamb -3.010923 time 2020-06-26 22:49:16.384509
Model ind 665 epoch 1052 batch: 100 avg loss -2.918391 avg loss no lamb -2.918391 time 2020-06-26 22:49:27.060685
Model ind 665 epoch 1052 batch: 200 avg loss -2.873271 avg loss no lamb -2.873271 time 2020-06-26 22:49:37.883697
Model ind 665 epoch 1052 batch: 300 avg loss -2.866226 avg loss no lamb -2.866226 time 2020-06-26 22:49:48.471746
Model ind 665 epoch 1052 batch: 400 avg loss -2.794352 avg loss no lamb -2.794352 time 2020-06-26 22:49:59.048014
Model ind 665 epoch 1052 batch: 500 avg loss -2.824034 avg loss no lamb -2.824034 time 2020-06-26 22:50:09.715857
Model ind 665 epoch 1052 batch: 600 avg loss -2.909193 avg loss no lamb -2.909193 time 2020-06-26 22:50:20.756712
Model ind 665 epoch 1052 batch: 700 avg loss -2.794139 avg loss no lamb -2.794139 time 2020-06-26 22:50:31.651906
Model ind 665 epoch 1052 batch: 800 avg loss -2.882308 avg loss no lamb -2.882308 time 2020-06-26 22:50:42.682394
last batch sz 10
Pre: time 2020-06-26 22:50:56.827844: 
 	std: 0.0032467896
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9801, 0.9734, 0.9811, 0.9748]
	train_accs: [0.9812, 0.98011667, 0.9758, 0.9813667, 0.9766]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97802
	best: 0.9811

Starting e_i: 1053
Model ind 665 epoch 1053 batch: 0 avg loss -2.957232 avg loss no lamb -2.957232 time 2020-06-26 22:50:58.057476
Model ind 665 epoch 1053 batch: 100 avg loss -2.937866 avg loss no lamb -2.937866 time 2020-06-26 22:51:08.746918
Model ind 665 epoch 1053 batch: 200 avg loss -2.875107 avg loss no lamb -2.875107 time 2020-06-26 22:51:19.474471
Model ind 665 epoch 1053 batch: 300 avg loss -2.877317 avg loss no lamb -2.877317 time 2020-06-26 22:51:30.265222
Model ind 665 epoch 1053 batch: 400 avg loss -2.754643 avg loss no lamb -2.754643 time 2020-06-26 22:51:41.250398
Model ind 665 epoch 1053 batch: 500 avg loss -2.868015 avg loss no lamb -2.868015 time 2020-06-26 22:51:52.252412
Model ind 665 epoch 1053 batch: 600 avg loss -2.876981 avg loss no lamb -2.876981 time 2020-06-26 22:52:03.151565
Model ind 665 epoch 1053 batch: 700 avg loss -2.771298 avg loss no lamb -2.771298 time 2020-06-26 22:52:13.960572
Model ind 665 epoch 1053 batch: 800 avg loss -2.827100 avg loss no lamb -2.827100 time 2020-06-26 22:52:24.528396
last batch sz 10
Pre: time 2020-06-26 22:52:38.559703: 
 	std: 0.003802117
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9793, 0.9715, 0.9797, 0.972]
	train_accs: [0.98118335, 0.98046666, 0.97501665, 0.9813833, 0.97545]
	best_train_sub_head: 3
	worst: 0.9715
	avg: 0.9764
	best: 0.9797

Starting e_i: 1054
Model ind 665 epoch 1054 batch: 0 avg loss -2.939917 avg loss no lamb -2.939917 time 2020-06-26 22:52:39.617320
Model ind 665 epoch 1054 batch: 100 avg loss -2.946154 avg loss no lamb -2.946154 time 2020-06-26 22:52:50.640742
Model ind 665 epoch 1054 batch: 200 avg loss -2.882972 avg loss no lamb -2.882972 time 2020-06-26 22:53:01.482118
Model ind 665 epoch 1054 batch: 300 avg loss -2.876153 avg loss no lamb -2.876153 time 2020-06-26 22:53:12.398328
Model ind 665 epoch 1054 batch: 400 avg loss -2.762683 avg loss no lamb -2.762683 time 2020-06-26 22:53:23.166296
Model ind 665 epoch 1054 batch: 500 avg loss -2.872319 avg loss no lamb -2.872319 time 2020-06-26 22:53:34.102105
Model ind 665 epoch 1054 batch: 600 avg loss -2.899626 avg loss no lamb -2.899626 time 2020-06-26 22:53:44.956335
Model ind 665 epoch 1054 batch: 700 avg loss -2.757020 avg loss no lamb -2.757020 time 2020-06-26 22:53:55.773796
Model ind 665 epoch 1054 batch: 800 avg loss -2.809898 avg loss no lamb -2.809898 time 2020-06-26 22:54:06.526484
last batch sz 10
Pre: time 2020-06-26 22:54:20.482666: 
 	std: 0.0038467718
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9796, 0.9719, 0.981, 0.9741]
	train_accs: [0.9817, 0.98055, 0.9745833, 0.98158336, 0.9755167]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97757995
	best: 0.9813

Starting e_i: 1055
Model ind 665 epoch 1055 batch: 0 avg loss -2.907983 avg loss no lamb -2.907983 time 2020-06-26 22:54:21.715254
Model ind 665 epoch 1055 batch: 100 avg loss -2.905080 avg loss no lamb -2.905080 time 2020-06-26 22:54:32.698580
Model ind 665 epoch 1055 batch: 200 avg loss -2.895662 avg loss no lamb -2.895662 time 2020-06-26 22:54:43.531101
Model ind 665 epoch 1055 batch: 300 avg loss -2.835114 avg loss no lamb -2.835114 time 2020-06-26 22:54:54.462140
Model ind 665 epoch 1055 batch: 400 avg loss -2.750499 avg loss no lamb -2.750499 time 2020-06-26 22:55:05.391490
Model ind 665 epoch 1055 batch: 500 avg loss -2.851356 avg loss no lamb -2.851356 time 2020-06-26 22:55:16.325806
Model ind 665 epoch 1055 batch: 600 avg loss -2.904134 avg loss no lamb -2.904134 time 2020-06-26 22:55:27.247990
Model ind 665 epoch 1055 batch: 700 avg loss -2.790176 avg loss no lamb -2.790176 time 2020-06-26 22:55:38.454528
Model ind 665 epoch 1055 batch: 800 avg loss -2.854835 avg loss no lamb -2.854835 time 2020-06-26 22:55:49.478421
last batch sz 10
Pre: time 2020-06-26 22:56:03.454085: 
 	std: 0.0029076503
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9793, 0.9749, 0.9807, 0.9752]
	train_accs: [0.98188335, 0.9808, 0.9762, 0.9815, 0.9766167]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97844
	best: 0.9821

Starting e_i: 1056
Model ind 665 epoch 1056 batch: 0 avg loss -2.903713 avg loss no lamb -2.903713 time 2020-06-26 22:56:04.520810
Model ind 665 epoch 1056 batch: 100 avg loss -2.908584 avg loss no lamb -2.908584 time 2020-06-26 22:56:15.464935
Model ind 665 epoch 1056 batch: 200 avg loss -2.852125 avg loss no lamb -2.852125 time 2020-06-26 22:56:26.355077
Model ind 665 epoch 1056 batch: 300 avg loss -2.887580 avg loss no lamb -2.887580 time 2020-06-26 22:56:37.264447
Model ind 665 epoch 1056 batch: 400 avg loss -2.798009 avg loss no lamb -2.798009 time 2020-06-26 22:56:48.280948
Model ind 665 epoch 1056 batch: 500 avg loss -2.876182 avg loss no lamb -2.876182 time 2020-06-26 22:56:58.875615
Model ind 665 epoch 1056 batch: 600 avg loss -2.889301 avg loss no lamb -2.889301 time 2020-06-26 22:57:09.862663
Model ind 665 epoch 1056 batch: 700 avg loss -2.823126 avg loss no lamb -2.823126 time 2020-06-26 22:57:20.853658
Model ind 665 epoch 1056 batch: 800 avg loss -2.887976 avg loss no lamb -2.887976 time 2020-06-26 22:57:31.932744
last batch sz 10
Pre: time 2020-06-26 22:57:46.094303: 
 	std: 0.0031578434
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.98, 0.9731, 0.9795, 0.9738]
	train_accs: [0.9811, 0.98075, 0.9755167, 0.98105, 0.97611666]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.9773
	best: 0.9801

Starting e_i: 1057
Model ind 665 epoch 1057 batch: 0 avg loss -2.928161 avg loss no lamb -2.928161 time 2020-06-26 22:57:47.319901
Model ind 665 epoch 1057 batch: 100 avg loss -2.947320 avg loss no lamb -2.947320 time 2020-06-26 22:57:58.297194
Model ind 665 epoch 1057 batch: 200 avg loss -2.828780 avg loss no lamb -2.828780 time 2020-06-26 22:58:09.295019
Model ind 665 epoch 1057 batch: 300 avg loss -2.912783 avg loss no lamb -2.912783 time 2020-06-26 22:58:20.009454
Model ind 665 epoch 1057 batch: 400 avg loss -2.820134 avg loss no lamb -2.820134 time 2020-06-26 22:58:30.775368
Model ind 665 epoch 1057 batch: 500 avg loss -2.786880 avg loss no lamb -2.786880 time 2020-06-26 22:58:41.755553
Model ind 665 epoch 1057 batch: 600 avg loss -2.922443 avg loss no lamb -2.922443 time 2020-06-26 22:58:52.868021
Model ind 665 epoch 1057 batch: 700 avg loss -2.807337 avg loss no lamb -2.807337 time 2020-06-26 22:59:03.993995
Model ind 665 epoch 1057 batch: 800 avg loss -2.826035 avg loss no lamb -2.826035 time 2020-06-26 22:59:15.011130
last batch sz 10
Pre: time 2020-06-26 22:59:29.493424: 
 	std: 0.0032327124
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9806, 0.9738, 0.9808, 0.9751]
	train_accs: [0.98191667, 0.98106664, 0.97545, 0.9816167, 0.9763]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97836
	best: 0.9815

Starting e_i: 1058
Model ind 665 epoch 1058 batch: 0 avg loss -2.957679 avg loss no lamb -2.957679 time 2020-06-26 22:59:30.649873
Model ind 665 epoch 1058 batch: 100 avg loss -2.898330 avg loss no lamb -2.898330 time 2020-06-26 22:59:41.633522
Model ind 665 epoch 1058 batch: 200 avg loss -2.860683 avg loss no lamb -2.860683 time 2020-06-26 22:59:52.754777
Model ind 665 epoch 1058 batch: 300 avg loss -2.866789 avg loss no lamb -2.866789 time 2020-06-26 23:00:03.870665
Model ind 665 epoch 1058 batch: 400 avg loss -2.783623 avg loss no lamb -2.783623 time 2020-06-26 23:00:14.604332
Model ind 665 epoch 1058 batch: 500 avg loss -2.837291 avg loss no lamb -2.837291 time 2020-06-26 23:00:25.414570
Model ind 665 epoch 1058 batch: 600 avg loss -2.903445 avg loss no lamb -2.903445 time 2020-06-26 23:00:36.190583
Model ind 665 epoch 1058 batch: 700 avg loss -2.766471 avg loss no lamb -2.766471 time 2020-06-26 23:00:47.301235
Model ind 665 epoch 1058 batch: 800 avg loss -2.898583 avg loss no lamb -2.898583 time 2020-06-26 23:00:58.324941
last batch sz 10
Pre: time 2020-06-26 23:01:12.588251: 
 	std: 0.0032808522
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9799, 0.9735, 0.9814, 0.9753]
	train_accs: [0.98123336, 0.9804, 0.9752167, 0.98123336, 0.9763333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.9783
	best: 0.9814

Starting e_i: 1059
Model ind 665 epoch 1059 batch: 0 avg loss -2.961585 avg loss no lamb -2.961585 time 2020-06-26 23:01:13.809736
Model ind 665 epoch 1059 batch: 100 avg loss -2.846229 avg loss no lamb -2.846229 time 2020-06-26 23:01:24.814442
Model ind 665 epoch 1059 batch: 200 avg loss -2.903549 avg loss no lamb -2.903549 time 2020-06-26 23:01:35.664639
Model ind 665 epoch 1059 batch: 300 avg loss -2.836527 avg loss no lamb -2.836527 time 2020-06-26 23:01:46.386418
Model ind 665 epoch 1059 batch: 400 avg loss -2.749245 avg loss no lamb -2.749245 time 2020-06-26 23:01:57.182056
Model ind 665 epoch 1059 batch: 500 avg loss -2.883415 avg loss no lamb -2.883415 time 2020-06-26 23:02:07.859144
Model ind 665 epoch 1059 batch: 600 avg loss -2.860464 avg loss no lamb -2.860464 time 2020-06-26 23:02:18.752268
Model ind 665 epoch 1059 batch: 700 avg loss -2.798944 avg loss no lamb -2.798944 time 2020-06-26 23:02:29.540661
Model ind 665 epoch 1059 batch: 800 avg loss -2.895731 avg loss no lamb -2.895731 time 2020-06-26 23:02:40.398454
last batch sz 10
Pre: time 2020-06-26 23:02:54.455929: 
 	std: 0.002879318
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9792, 0.9729, 0.9797, 0.9752]
	train_accs: [0.9813833, 0.98066664, 0.9752833, 0.9814, 0.97655]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97744
	best: 0.9797

Starting e_i: 1060
Model ind 665 epoch 1060 batch: 0 avg loss -2.981195 avg loss no lamb -2.981195 time 2020-06-26 23:02:55.517261
Model ind 665 epoch 1060 batch: 100 avg loss -2.926229 avg loss no lamb -2.926229 time 2020-06-26 23:03:06.251766
Model ind 665 epoch 1060 batch: 200 avg loss -2.834507 avg loss no lamb -2.834507 time 2020-06-26 23:03:17.199268
Model ind 665 epoch 1060 batch: 300 avg loss -2.881166 avg loss no lamb -2.881166 time 2020-06-26 23:03:28.099041
Model ind 665 epoch 1060 batch: 400 avg loss -2.770424 avg loss no lamb -2.770424 time 2020-06-26 23:03:38.827298
Model ind 665 epoch 1060 batch: 500 avg loss -2.844550 avg loss no lamb -2.844550 time 2020-06-26 23:03:49.504736
Model ind 665 epoch 1060 batch: 600 avg loss -2.852523 avg loss no lamb -2.852523 time 2020-06-26 23:04:00.359643
Model ind 665 epoch 1060 batch: 700 avg loss -2.813004 avg loss no lamb -2.813004 time 2020-06-26 23:04:11.133192
Model ind 665 epoch 1060 batch: 800 avg loss -2.858444 avg loss no lamb -2.858444 time 2020-06-26 23:04:21.913447
last batch sz 10
Pre: time 2020-06-26 23:04:35.953828: 
 	std: 0.0033386184
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9794, 0.9728, 0.9807, 0.9756]
	train_accs: [0.9816, 0.98041666, 0.97435, 0.9810167, 0.9762]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.9780399
	best: 0.9817

Starting e_i: 1061
Model ind 665 epoch 1061 batch: 0 avg loss -2.950840 avg loss no lamb -2.950840 time 2020-06-26 23:04:38.345548
Model ind 665 epoch 1061 batch: 100 avg loss -2.867828 avg loss no lamb -2.867828 time 2020-06-26 23:04:49.194898
Model ind 665 epoch 1061 batch: 200 avg loss -2.852566 avg loss no lamb -2.852566 time 2020-06-26 23:04:59.992634
Model ind 665 epoch 1061 batch: 300 avg loss -2.877712 avg loss no lamb -2.877712 time 2020-06-26 23:05:10.767859
Model ind 665 epoch 1061 batch: 400 avg loss -2.753703 avg loss no lamb -2.753703 time 2020-06-26 23:05:21.651149
Model ind 665 epoch 1061 batch: 500 avg loss -2.821117 avg loss no lamb -2.821117 time 2020-06-26 23:05:32.672906
Model ind 665 epoch 1061 batch: 600 avg loss -2.885108 avg loss no lamb -2.885108 time 2020-06-26 23:05:43.608660
Model ind 665 epoch 1061 batch: 700 avg loss -2.809995 avg loss no lamb -2.809995 time 2020-06-26 23:05:54.671792
Model ind 665 epoch 1061 batch: 800 avg loss -2.832755 avg loss no lamb -2.832755 time 2020-06-26 23:06:05.564639
last batch sz 10
Pre: time 2020-06-26 23:06:19.736450: 
 	std: 0.002761602
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9806, 0.9751, 0.9817, 0.9769]
	train_accs: [0.98191667, 0.9809333, 0.97651666, 0.98183334, 0.97735]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97925997
	best: 0.982

Starting e_i: 1062
Model ind 665 epoch 1062 batch: 0 avg loss -2.904517 avg loss no lamb -2.904517 time 2020-06-26 23:06:20.811365
Model ind 665 epoch 1062 batch: 100 avg loss -2.892550 avg loss no lamb -2.892550 time 2020-06-26 23:06:31.709392
Model ind 665 epoch 1062 batch: 200 avg loss -2.867619 avg loss no lamb -2.867619 time 2020-06-26 23:06:42.524158
Model ind 665 epoch 1062 batch: 300 avg loss -2.875080 avg loss no lamb -2.875080 time 2020-06-26 23:06:53.499623
Model ind 665 epoch 1062 batch: 400 avg loss -2.778700 avg loss no lamb -2.778700 time 2020-06-26 23:07:04.325927
Model ind 665 epoch 1062 batch: 500 avg loss -2.830024 avg loss no lamb -2.830024 time 2020-06-26 23:07:15.284200
Model ind 665 epoch 1062 batch: 600 avg loss -2.865235 avg loss no lamb -2.865235 time 2020-06-26 23:07:26.362798
Model ind 665 epoch 1062 batch: 700 avg loss -2.770016 avg loss no lamb -2.770016 time 2020-06-26 23:07:37.242364
Model ind 665 epoch 1062 batch: 800 avg loss -2.843880 avg loss no lamb -2.843880 time 2020-06-26 23:07:48.381922
last batch sz 10
Pre: time 2020-06-26 23:08:02.420493: 
 	std: 0.0029736152
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9797, 0.9735, 0.9801, 0.9748]
	train_accs: [0.98106664, 0.98011667, 0.97543335, 0.98075, 0.97583336]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97774
	best: 0.9806

Starting e_i: 1063
Model ind 665 epoch 1063 batch: 0 avg loss -2.965270 avg loss no lamb -2.965270 time 2020-06-26 23:08:03.664917
Model ind 665 epoch 1063 batch: 100 avg loss -2.861556 avg loss no lamb -2.861556 time 2020-06-26 23:08:14.542706
Model ind 665 epoch 1063 batch: 200 avg loss -2.888307 avg loss no lamb -2.888307 time 2020-06-26 23:08:25.352769
Model ind 665 epoch 1063 batch: 300 avg loss -2.823980 avg loss no lamb -2.823980 time 2020-06-26 23:08:36.072058
Model ind 665 epoch 1063 batch: 400 avg loss -2.786208 avg loss no lamb -2.786208 time 2020-06-26 23:08:47.039269
Model ind 665 epoch 1063 batch: 500 avg loss -2.837631 avg loss no lamb -2.837631 time 2020-06-26 23:08:58.189938
Model ind 665 epoch 1063 batch: 600 avg loss -2.867969 avg loss no lamb -2.867969 time 2020-06-26 23:09:09.323180
Model ind 665 epoch 1063 batch: 700 avg loss -2.761387 avg loss no lamb -2.761387 time 2020-06-26 23:09:20.296361
Model ind 665 epoch 1063 batch: 800 avg loss -2.905630 avg loss no lamb -2.905630 time 2020-06-26 23:09:31.028994
last batch sz 10
Pre: time 2020-06-26 23:09:45.396915: 
 	std: 0.003252433
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.979, 0.9714, 0.9782, 0.9731]
	train_accs: [0.98053336, 0.98003334, 0.97415, 0.98, 0.9749333]
	best_train_sub_head: 0
	worst: 0.9714
	avg: 0.97616005
	best: 0.9791

Starting e_i: 1064
Model ind 665 epoch 1064 batch: 0 avg loss -2.908876 avg loss no lamb -2.908876 time 2020-06-26 23:09:46.452774
Model ind 665 epoch 1064 batch: 100 avg loss -2.891802 avg loss no lamb -2.891802 time 2020-06-26 23:09:57.267030
Model ind 665 epoch 1064 batch: 200 avg loss -2.865534 avg loss no lamb -2.865534 time 2020-06-26 23:10:08.041373
Model ind 665 epoch 1064 batch: 300 avg loss -2.906844 avg loss no lamb -2.906844 time 2020-06-26 23:10:18.894540
Model ind 665 epoch 1064 batch: 400 avg loss -2.795954 avg loss no lamb -2.795954 time 2020-06-26 23:10:29.747003
Model ind 665 epoch 1064 batch: 500 avg loss -2.830105 avg loss no lamb -2.830105 time 2020-06-26 23:10:40.784960
Model ind 665 epoch 1064 batch: 600 avg loss -2.831963 avg loss no lamb -2.831963 time 2020-06-26 23:10:51.753414
Model ind 665 epoch 1064 batch: 700 avg loss -2.852784 avg loss no lamb -2.852784 time 2020-06-26 23:11:02.748251
Model ind 665 epoch 1064 batch: 800 avg loss -2.850189 avg loss no lamb -2.850189 time 2020-06-26 23:11:13.506065
last batch sz 10
Pre: time 2020-06-26 23:11:27.748992: 
 	std: 0.0032407413
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9821, 0.9753, 0.9821, 0.9759]
	train_accs: [0.9816167, 0.9812, 0.97568333, 0.9816, 0.97636664]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97956
	best: 0.9824

Starting e_i: 1065
Model ind 665 epoch 1065 batch: 0 avg loss -2.921555 avg loss no lamb -2.921555 time 2020-06-26 23:11:28.969696
Model ind 665 epoch 1065 batch: 100 avg loss -2.856857 avg loss no lamb -2.856857 time 2020-06-26 23:11:39.924939
Model ind 665 epoch 1065 batch: 200 avg loss -2.877966 avg loss no lamb -2.877966 time 2020-06-26 23:11:50.683859
Model ind 665 epoch 1065 batch: 300 avg loss -2.925232 avg loss no lamb -2.925232 time 2020-06-26 23:12:01.571895
Model ind 665 epoch 1065 batch: 400 avg loss -2.737481 avg loss no lamb -2.737481 time 2020-06-26 23:12:12.365024
Model ind 665 epoch 1065 batch: 500 avg loss -2.836964 avg loss no lamb -2.836964 time 2020-06-26 23:12:23.277341
Model ind 665 epoch 1065 batch: 600 avg loss -2.890158 avg loss no lamb -2.890158 time 2020-06-26 23:12:34.317438
Model ind 665 epoch 1065 batch: 700 avg loss -2.802542 avg loss no lamb -2.802542 time 2020-06-26 23:12:45.183850
Model ind 665 epoch 1065 batch: 800 avg loss -2.860641 avg loss no lamb -2.860641 time 2020-06-26 23:12:56.084017
last batch sz 10
Pre: time 2020-06-26 23:13:10.416779: 
 	std: 0.002713386
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9795, 0.9747, 0.9813, 0.9758]
	train_accs: [0.9817167, 0.9805833, 0.9758, 0.9816833, 0.97643334]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97846
	best: 0.981

Starting e_i: 1066
Model ind 665 epoch 1066 batch: 0 avg loss -2.946788 avg loss no lamb -2.946788 time 2020-06-26 23:13:11.484227
Model ind 665 epoch 1066 batch: 100 avg loss -2.917028 avg loss no lamb -2.917028 time 2020-06-26 23:13:22.349336
Model ind 665 epoch 1066 batch: 200 avg loss -2.874066 avg loss no lamb -2.874066 time 2020-06-26 23:13:33.240073
Model ind 665 epoch 1066 batch: 300 avg loss -2.827134 avg loss no lamb -2.827134 time 2020-06-26 23:13:43.952368
Model ind 665 epoch 1066 batch: 400 avg loss -2.815351 avg loss no lamb -2.815351 time 2020-06-26 23:13:54.894034
Model ind 665 epoch 1066 batch: 500 avg loss -2.846484 avg loss no lamb -2.846484 time 2020-06-26 23:14:05.959790
Model ind 665 epoch 1066 batch: 600 avg loss -2.919360 avg loss no lamb -2.919360 time 2020-06-26 23:14:16.974436
Model ind 665 epoch 1066 batch: 700 avg loss -2.788481 avg loss no lamb -2.788481 time 2020-06-26 23:14:27.816516
Model ind 665 epoch 1066 batch: 800 avg loss -2.834816 avg loss no lamb -2.834816 time 2020-06-26 23:14:38.467044
last batch sz 10
Pre: time 2020-06-26 23:14:52.450895: 
 	std: 0.002253351
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9804, 0.976, 0.9813, 0.9773]
	train_accs: [0.9819667, 0.98108333, 0.9772, 0.98175, 0.9774167]
	best_train_sub_head: 0
	worst: 0.976
	avg: 0.97932005
	best: 0.9816

Starting e_i: 1067
Model ind 665 epoch 1067 batch: 0 avg loss -2.924058 avg loss no lamb -2.924058 time 2020-06-26 23:14:53.668479
Model ind 665 epoch 1067 batch: 100 avg loss -2.850724 avg loss no lamb -2.850724 time 2020-06-26 23:15:04.691225
Model ind 665 epoch 1067 batch: 200 avg loss -2.819360 avg loss no lamb -2.819360 time 2020-06-26 23:15:15.663195
Model ind 665 epoch 1067 batch: 300 avg loss -2.876875 avg loss no lamb -2.876875 time 2020-06-26 23:15:26.756287
Model ind 665 epoch 1067 batch: 400 avg loss -2.751006 avg loss no lamb -2.751006 time 2020-06-26 23:15:37.728109
Model ind 665 epoch 1067 batch: 500 avg loss -2.859367 avg loss no lamb -2.859367 time 2020-06-26 23:15:48.561177
Model ind 665 epoch 1067 batch: 600 avg loss -2.797739 avg loss no lamb -2.797739 time 2020-06-26 23:15:59.392637
Model ind 665 epoch 1067 batch: 700 avg loss -2.802766 avg loss no lamb -2.802766 time 2020-06-26 23:16:10.344201
Model ind 665 epoch 1067 batch: 800 avg loss -2.827866 avg loss no lamb -2.827866 time 2020-06-26 23:16:21.406723
last batch sz 10
Pre: time 2020-06-26 23:16:35.598917: 
 	std: 0.0032003857
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9807, 0.9746, 0.9818, 0.9751]
	train_accs: [0.98185, 0.98106664, 0.9755333, 0.98156667, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97874004
	best: 0.9815

Starting e_i: 1068
Model ind 665 epoch 1068 batch: 0 avg loss -2.951304 avg loss no lamb -2.951304 time 2020-06-26 23:16:36.682754
Model ind 665 epoch 1068 batch: 100 avg loss -2.873308 avg loss no lamb -2.873308 time 2020-06-26 23:16:47.628899
Model ind 665 epoch 1068 batch: 200 avg loss -2.901019 avg loss no lamb -2.901019 time 2020-06-26 23:16:58.557801
Model ind 665 epoch 1068 batch: 300 avg loss -2.855637 avg loss no lamb -2.855637 time 2020-06-26 23:17:09.595392
Model ind 665 epoch 1068 batch: 400 avg loss -2.776775 avg loss no lamb -2.776775 time 2020-06-26 23:17:20.440386
Model ind 665 epoch 1068 batch: 500 avg loss -2.813751 avg loss no lamb -2.813751 time 2020-06-26 23:17:31.256154
Model ind 665 epoch 1068 batch: 600 avg loss -2.880589 avg loss no lamb -2.880589 time 2020-06-26 23:17:42.293349
Model ind 665 epoch 1068 batch: 700 avg loss -2.771363 avg loss no lamb -2.771363 time 2020-06-26 23:17:53.397406
Model ind 665 epoch 1068 batch: 800 avg loss -2.815228 avg loss no lamb -2.815228 time 2020-06-26 23:18:04.293418
last batch sz 10
Pre: time 2020-06-26 23:18:18.330520: 
 	std: 0.002609207
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9804, 0.975, 0.9814, 0.9771]
	train_accs: [0.9816, 0.98113334, 0.976, 0.9814, 0.97705]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97910005
	best: 0.9816

Starting e_i: 1069
Model ind 665 epoch 1069 batch: 0 avg loss -2.995981 avg loss no lamb -2.995981 time 2020-06-26 23:18:19.528585
Model ind 665 epoch 1069 batch: 100 avg loss -2.850516 avg loss no lamb -2.850516 time 2020-06-26 23:18:30.706475
Model ind 665 epoch 1069 batch: 200 avg loss -2.844124 avg loss no lamb -2.844124 time 2020-06-26 23:18:41.717101
Model ind 665 epoch 1069 batch: 300 avg loss -2.851439 avg loss no lamb -2.851439 time 2020-06-26 23:18:52.504474
Model ind 665 epoch 1069 batch: 400 avg loss -2.762209 avg loss no lamb -2.762209 time 2020-06-26 23:19:03.445141
Model ind 665 epoch 1069 batch: 500 avg loss -2.789910 avg loss no lamb -2.789910 time 2020-06-26 23:19:14.468106
Model ind 665 epoch 1069 batch: 600 avg loss -2.882241 avg loss no lamb -2.882241 time 2020-06-26 23:19:25.443857
Model ind 665 epoch 1069 batch: 700 avg loss -2.763464 avg loss no lamb -2.763464 time 2020-06-26 23:19:36.356590
Model ind 665 epoch 1069 batch: 800 avg loss -2.840381 avg loss no lamb -2.840381 time 2020-06-26 23:19:47.091324
last batch sz 10
Pre: time 2020-06-26 23:20:01.349588: 
 	std: 0.0026310415
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9795, 0.9741, 0.9808, 0.9763]
	train_accs: [0.98123336, 0.9805833, 0.97583336, 0.98123336, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.9782599
	best: 0.9806

Starting e_i: 1070
Model ind 665 epoch 1070 batch: 0 avg loss -2.962797 avg loss no lamb -2.962797 time 2020-06-26 23:20:02.445761
Model ind 665 epoch 1070 batch: 100 avg loss -2.864088 avg loss no lamb -2.864088 time 2020-06-26 23:20:13.399027
Model ind 665 epoch 1070 batch: 200 avg loss -2.873555 avg loss no lamb -2.873555 time 2020-06-26 23:20:24.130196
Model ind 665 epoch 1070 batch: 300 avg loss -2.843355 avg loss no lamb -2.843355 time 2020-06-26 23:20:34.962813
Model ind 665 epoch 1070 batch: 400 avg loss -2.847573 avg loss no lamb -2.847573 time 2020-06-26 23:20:45.848613
Model ind 665 epoch 1070 batch: 500 avg loss -2.824220 avg loss no lamb -2.824220 time 2020-06-26 23:20:56.629090
Model ind 665 epoch 1070 batch: 600 avg loss -2.870994 avg loss no lamb -2.870994 time 2020-06-26 23:21:07.645649
Model ind 665 epoch 1070 batch: 700 avg loss -2.750661 avg loss no lamb -2.750661 time 2020-06-26 23:21:18.347336
Model ind 665 epoch 1070 batch: 800 avg loss -2.837176 avg loss no lamb -2.837176 time 2020-06-26 23:21:29.057018
last batch sz 10
Pre: time 2020-06-26 23:21:42.870278: 
 	std: 0.0030155764
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9807, 0.9751, 0.9822, 0.9766]
	train_accs: [0.9821, 0.9810167, 0.97665, 0.9821333, 0.97695]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97942007
	best: 0.9822

Starting e_i: 1071
Model ind 665 epoch 1071 batch: 0 avg loss -2.997655 avg loss no lamb -2.997655 time 2020-06-26 23:21:45.280686
Model ind 665 epoch 1071 batch: 100 avg loss -2.896834 avg loss no lamb -2.896834 time 2020-06-26 23:21:56.231168
Model ind 665 epoch 1071 batch: 200 avg loss -2.852684 avg loss no lamb -2.852684 time 2020-06-26 23:22:07.211504
Model ind 665 epoch 1071 batch: 300 avg loss -2.834019 avg loss no lamb -2.834019 time 2020-06-26 23:22:18.116123
Model ind 665 epoch 1071 batch: 400 avg loss -2.764571 avg loss no lamb -2.764571 time 2020-06-26 23:22:29.083691
Model ind 665 epoch 1071 batch: 500 avg loss -2.858194 avg loss no lamb -2.858194 time 2020-06-26 23:22:39.764612
Model ind 665 epoch 1071 batch: 600 avg loss -2.860798 avg loss no lamb -2.860798 time 2020-06-26 23:22:50.691795
Model ind 665 epoch 1071 batch: 700 avg loss -2.774705 avg loss no lamb -2.774705 time 2020-06-26 23:23:01.586058
Model ind 665 epoch 1071 batch: 800 avg loss -2.897251 avg loss no lamb -2.897251 time 2020-06-26 23:23:12.331354
last batch sz 10
Pre: time 2020-06-26 23:23:26.141590: 
 	std: 0.0026820824
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9799, 0.9753, 0.9818, 0.9767]
	train_accs: [0.98176664, 0.9805667, 0.9761, 0.9818, 0.9769]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97911996
	best: 0.9818

Starting e_i: 1072
Model ind 665 epoch 1072 batch: 0 avg loss -2.954651 avg loss no lamb -2.954651 time 2020-06-26 23:23:27.194291
Model ind 665 epoch 1072 batch: 100 avg loss -2.874419 avg loss no lamb -2.874419 time 2020-06-26 23:23:37.965766
Model ind 665 epoch 1072 batch: 200 avg loss -2.857983 avg loss no lamb -2.857983 time 2020-06-26 23:23:49.065838
Model ind 665 epoch 1072 batch: 300 avg loss -2.840471 avg loss no lamb -2.840471 time 2020-06-26 23:23:59.853630
Model ind 665 epoch 1072 batch: 400 avg loss -2.726062 avg loss no lamb -2.726062 time 2020-06-26 23:24:10.490404
Model ind 665 epoch 1072 batch: 500 avg loss -2.836723 avg loss no lamb -2.836723 time 2020-06-26 23:24:21.499437
Model ind 665 epoch 1072 batch: 600 avg loss -2.855183 avg loss no lamb -2.855183 time 2020-06-26 23:24:32.530322
Model ind 665 epoch 1072 batch: 700 avg loss -2.794747 avg loss no lamb -2.794747 time 2020-06-26 23:24:43.445386
Model ind 665 epoch 1072 batch: 800 avg loss -2.859693 avg loss no lamb -2.859693 time 2020-06-26 23:24:54.105944
last batch sz 10
Pre: time 2020-06-26 23:25:08.037743: 
 	std: 0.003194011
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9802, 0.9736, 0.981, 0.9754]
	train_accs: [0.98145, 0.9806, 0.9748333, 0.98121667, 0.97568333]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97831994
	best: 0.9814

Starting e_i: 1073
Model ind 665 epoch 1073 batch: 0 avg loss -2.956573 avg loss no lamb -2.956573 time 2020-06-26 23:25:09.295816
Model ind 665 epoch 1073 batch: 100 avg loss -2.904120 avg loss no lamb -2.904120 time 2020-06-26 23:25:20.291988
Model ind 665 epoch 1073 batch: 200 avg loss -2.853605 avg loss no lamb -2.853605 time 2020-06-26 23:25:31.175453
Model ind 665 epoch 1073 batch: 300 avg loss -2.843622 avg loss no lamb -2.843622 time 2020-06-26 23:25:42.000520
Model ind 665 epoch 1073 batch: 400 avg loss -2.788833 avg loss no lamb -2.788833 time 2020-06-26 23:25:52.762567
Model ind 665 epoch 1073 batch: 500 avg loss -2.877948 avg loss no lamb -2.877948 time 2020-06-26 23:26:03.680098
Model ind 665 epoch 1073 batch: 600 avg loss -2.935049 avg loss no lamb -2.935049 time 2020-06-26 23:26:14.545920
Model ind 665 epoch 1073 batch: 700 avg loss -2.731590 avg loss no lamb -2.731590 time 2020-06-26 23:26:25.459934
Model ind 665 epoch 1073 batch: 800 avg loss -2.829884 avg loss no lamb -2.829884 time 2020-06-26 23:26:36.379661
last batch sz 10
Pre: time 2020-06-26 23:26:50.289688: 
 	std: 0.0029763186
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9805, 0.9736, 0.9804, 0.9756]
	train_accs: [0.98148334, 0.98073334, 0.97505, 0.9813667, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97816
	best: 0.9807

Starting e_i: 1074
Model ind 665 epoch 1074 batch: 0 avg loss -2.960284 avg loss no lamb -2.960284 time 2020-06-26 23:26:51.392510
Model ind 665 epoch 1074 batch: 100 avg loss -2.911022 avg loss no lamb -2.911022 time 2020-06-26 23:27:01.951480
Model ind 665 epoch 1074 batch: 200 avg loss -2.893307 avg loss no lamb -2.893307 time 2020-06-26 23:27:12.708202
Model ind 665 epoch 1074 batch: 300 avg loss -2.875129 avg loss no lamb -2.875129 time 2020-06-26 23:27:23.661081
Model ind 665 epoch 1074 batch: 400 avg loss -2.764703 avg loss no lamb -2.764703 time 2020-06-26 23:27:34.575208
Model ind 665 epoch 1074 batch: 500 avg loss -2.812910 avg loss no lamb -2.812910 time 2020-06-26 23:27:45.304187
Model ind 665 epoch 1074 batch: 600 avg loss -2.849385 avg loss no lamb -2.849385 time 2020-06-26 23:27:55.974366
Model ind 665 epoch 1074 batch: 700 avg loss -2.780094 avg loss no lamb -2.780094 time 2020-06-26 23:28:06.961241
Model ind 665 epoch 1074 batch: 800 avg loss -2.857770 avg loss no lamb -2.857770 time 2020-06-26 23:28:17.759558
last batch sz 10
Pre: time 2020-06-26 23:28:31.719191: 
 	std: 0.0032682712
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.979, 0.9723, 0.9806, 0.9748]
	train_accs: [0.98146665, 0.9802333, 0.9741333, 0.98155, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97738
	best: 0.9806

Starting e_i: 1075
Model ind 665 epoch 1075 batch: 0 avg loss -2.977118 avg loss no lamb -2.977118 time 2020-06-26 23:28:32.956462
Model ind 665 epoch 1075 batch: 100 avg loss -2.900930 avg loss no lamb -2.900930 time 2020-06-26 23:28:43.657524
Model ind 665 epoch 1075 batch: 200 avg loss -2.858712 avg loss no lamb -2.858712 time 2020-06-26 23:28:54.551190
Model ind 665 epoch 1075 batch: 300 avg loss -2.875866 avg loss no lamb -2.875866 time 2020-06-26 23:29:05.523267
Model ind 665 epoch 1075 batch: 400 avg loss -2.791339 avg loss no lamb -2.791339 time 2020-06-26 23:29:16.400753
Model ind 665 epoch 1075 batch: 500 avg loss -2.834275 avg loss no lamb -2.834275 time 2020-06-26 23:29:27.162570
Model ind 665 epoch 1075 batch: 600 avg loss -2.821846 avg loss no lamb -2.821846 time 2020-06-26 23:29:38.247071
Model ind 665 epoch 1075 batch: 700 avg loss -2.789322 avg loss no lamb -2.789322 time 2020-06-26 23:29:49.342177
Model ind 665 epoch 1075 batch: 800 avg loss -2.829387 avg loss no lamb -2.829387 time 2020-06-26 23:30:00.379051
last batch sz 10
Pre: time 2020-06-26 23:30:14.684015: 
 	std: 0.002798146
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9798, 0.9745, 0.9814, 0.9758]
	train_accs: [0.98108333, 0.9802167, 0.9755167, 0.98153335, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97848
	best: 0.9814

Starting e_i: 1076
Model ind 665 epoch 1076 batch: 0 avg loss -2.968451 avg loss no lamb -2.968451 time 2020-06-26 23:30:15.768420
Model ind 665 epoch 1076 batch: 100 avg loss -2.872640 avg loss no lamb -2.872640 time 2020-06-26 23:30:26.754998
Model ind 665 epoch 1076 batch: 200 avg loss -2.896296 avg loss no lamb -2.896296 time 2020-06-26 23:30:37.891120
Model ind 665 epoch 1076 batch: 300 avg loss -2.826138 avg loss no lamb -2.826138 time 2020-06-26 23:30:48.997719
Model ind 665 epoch 1076 batch: 400 avg loss -2.810523 avg loss no lamb -2.810523 time 2020-06-26 23:31:00.049289
Model ind 665 epoch 1076 batch: 500 avg loss -2.775923 avg loss no lamb -2.775923 time 2020-06-26 23:31:10.847910
Model ind 665 epoch 1076 batch: 600 avg loss -2.905894 avg loss no lamb -2.905894 time 2020-06-26 23:31:21.364462
Model ind 665 epoch 1076 batch: 700 avg loss -2.811963 avg loss no lamb -2.811963 time 2020-06-26 23:31:32.458430
Model ind 665 epoch 1076 batch: 800 avg loss -2.855505 avg loss no lamb -2.855505 time 2020-06-26 23:31:43.097887
last batch sz 10
Pre: time 2020-06-26 23:31:57.264102: 
 	std: 0.0027665186
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9793, 0.9738, 0.981, 0.9763]
	train_accs: [0.9813333, 0.9806833, 0.9756, 0.9816167, 0.9765667]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97822
	best: 0.981

Starting e_i: 1077
Model ind 665 epoch 1077 batch: 0 avg loss -2.957858 avg loss no lamb -2.957858 time 2020-06-26 23:31:58.485420
Model ind 665 epoch 1077 batch: 100 avg loss -2.804654 avg loss no lamb -2.804654 time 2020-06-26 23:32:09.181546
Model ind 665 epoch 1077 batch: 200 avg loss -2.879715 avg loss no lamb -2.879715 time 2020-06-26 23:32:19.895484
Model ind 665 epoch 1077 batch: 300 avg loss -2.868673 avg loss no lamb -2.868673 time 2020-06-26 23:32:30.559471
Model ind 665 epoch 1077 batch: 400 avg loss -2.808232 avg loss no lamb -2.808232 time 2020-06-26 23:32:41.692861
Model ind 665 epoch 1077 batch: 500 avg loss -2.826046 avg loss no lamb -2.826046 time 2020-06-26 23:32:52.514041
Model ind 665 epoch 1077 batch: 600 avg loss -2.868295 avg loss no lamb -2.868295 time 2020-06-26 23:33:03.319088
Model ind 665 epoch 1077 batch: 700 avg loss -2.799843 avg loss no lamb -2.799843 time 2020-06-26 23:33:14.336551
Model ind 665 epoch 1077 batch: 800 avg loss -2.912175 avg loss no lamb -2.912175 time 2020-06-26 23:33:25.192317
last batch sz 10
Pre: time 2020-06-26 23:33:39.532945: 
 	std: 0.0031984993
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9827, 0.9807, 0.9745, 0.9821, 0.9767]
	train_accs: [0.98188335, 0.9809, 0.9755833, 0.98205, 0.97675]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97933996
	best: 0.9821

Starting e_i: 1078
Model ind 665 epoch 1078 batch: 0 avg loss -2.922221 avg loss no lamb -2.922221 time 2020-06-26 23:33:40.638202
Model ind 665 epoch 1078 batch: 100 avg loss -2.966067 avg loss no lamb -2.966067 time 2020-06-26 23:33:51.447031
Model ind 665 epoch 1078 batch: 200 avg loss -2.794360 avg loss no lamb -2.794360 time 2020-06-26 23:34:02.296197
Model ind 665 epoch 1078 batch: 300 avg loss -2.865089 avg loss no lamb -2.865089 time 2020-06-26 23:34:13.291310
Model ind 665 epoch 1078 batch: 400 avg loss -2.824939 avg loss no lamb -2.824939 time 2020-06-26 23:34:24.439486
Model ind 665 epoch 1078 batch: 500 avg loss -2.831970 avg loss no lamb -2.831970 time 2020-06-26 23:34:35.328915
Model ind 665 epoch 1078 batch: 600 avg loss -2.922654 avg loss no lamb -2.922654 time 2020-06-26 23:34:46.290745
Model ind 665 epoch 1078 batch: 700 avg loss -2.772683 avg loss no lamb -2.772683 time 2020-06-26 23:34:57.252679
Model ind 665 epoch 1078 batch: 800 avg loss -2.906686 avg loss no lamb -2.906686 time 2020-06-26 23:35:08.353385
last batch sz 10
Pre: time 2020-06-26 23:35:22.289175: 
 	std: 0.002646212
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9802, 0.9747, 0.9809, 0.9756]
	train_accs: [0.9809667, 0.98095, 0.97581667, 0.9815, 0.9763333]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97836
	best: 0.9809

Starting e_i: 1079
Model ind 665 epoch 1079 batch: 0 avg loss -2.881500 avg loss no lamb -2.881500 time 2020-06-26 23:35:23.529754
Model ind 665 epoch 1079 batch: 100 avg loss -2.878038 avg loss no lamb -2.878038 time 2020-06-26 23:35:34.760035
Model ind 665 epoch 1079 batch: 200 avg loss -2.885230 avg loss no lamb -2.885230 time 2020-06-26 23:35:45.559583
Model ind 665 epoch 1079 batch: 300 avg loss -2.856867 avg loss no lamb -2.856867 time 2020-06-26 23:35:56.476908
Model ind 665 epoch 1079 batch: 400 avg loss -2.787918 avg loss no lamb -2.787918 time 2020-06-26 23:36:07.481157
Model ind 665 epoch 1079 batch: 500 avg loss -2.870024 avg loss no lamb -2.870024 time 2020-06-26 23:36:18.208412
Model ind 665 epoch 1079 batch: 600 avg loss -2.911929 avg loss no lamb -2.911929 time 2020-06-26 23:36:29.045546
Model ind 665 epoch 1079 batch: 700 avg loss -2.700380 avg loss no lamb -2.700380 time 2020-06-26 23:36:39.910392
Model ind 665 epoch 1079 batch: 800 avg loss -2.826577 avg loss no lamb -2.826577 time 2020-06-26 23:36:50.829136
last batch sz 10
Pre: time 2020-06-26 23:37:05.345412: 
 	std: 0.0030407915
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9799, 0.9747, 0.9819, 0.9758]
	train_accs: [0.9816667, 0.98035, 0.97536665, 0.98155, 0.9763333]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97884
	best: 0.9819

Starting e_i: 1080
Model ind 665 epoch 1080 batch: 0 avg loss -2.928463 avg loss no lamb -2.928463 time 2020-06-26 23:37:06.412775
Model ind 665 epoch 1080 batch: 100 avg loss -2.910030 avg loss no lamb -2.910030 time 2020-06-26 23:37:17.398767
Model ind 665 epoch 1080 batch: 200 avg loss -2.854293 avg loss no lamb -2.854293 time 2020-06-26 23:37:28.189199
Model ind 665 epoch 1080 batch: 300 avg loss -2.862690 avg loss no lamb -2.862690 time 2020-06-26 23:37:38.904429
Model ind 665 epoch 1080 batch: 400 avg loss -2.798062 avg loss no lamb -2.798062 time 2020-06-26 23:37:49.832296
Model ind 665 epoch 1080 batch: 500 avg loss -2.819607 avg loss no lamb -2.819607 time 2020-06-26 23:38:00.986786
Model ind 665 epoch 1080 batch: 600 avg loss -2.912978 avg loss no lamb -2.912978 time 2020-06-26 23:38:11.913263
Model ind 665 epoch 1080 batch: 700 avg loss -2.786638 avg loss no lamb -2.786638 time 2020-06-26 23:38:22.853309
Model ind 665 epoch 1080 batch: 800 avg loss -2.821414 avg loss no lamb -2.821414 time 2020-06-26 23:38:33.851984
last batch sz 10
Pre: time 2020-06-26 23:38:48.062604: 
 	std: 0.0026356075
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9804, 0.9752, 0.9813, 0.9766]
	train_accs: [0.98158336, 0.9810333, 0.9759333, 0.9816167, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97904
	best: 0.9813

Starting e_i: 1081
Model ind 665 epoch 1081 batch: 0 avg loss -2.923522 avg loss no lamb -2.923522 time 2020-06-26 23:38:50.454561
Model ind 665 epoch 1081 batch: 100 avg loss -2.890180 avg loss no lamb -2.890180 time 2020-06-26 23:39:01.268070
Model ind 665 epoch 1081 batch: 200 avg loss -2.864499 avg loss no lamb -2.864499 time 2020-06-26 23:39:12.059510
Model ind 665 epoch 1081 batch: 300 avg loss -2.824160 avg loss no lamb -2.824160 time 2020-06-26 23:39:22.988404
Model ind 665 epoch 1081 batch: 400 avg loss -2.798224 avg loss no lamb -2.798224 time 2020-06-26 23:39:33.940437
Model ind 665 epoch 1081 batch: 500 avg loss -2.851742 avg loss no lamb -2.851742 time 2020-06-26 23:39:44.629977
Model ind 665 epoch 1081 batch: 600 avg loss -2.887501 avg loss no lamb -2.887501 time 2020-06-26 23:39:55.551852
Model ind 665 epoch 1081 batch: 700 avg loss -2.767510 avg loss no lamb -2.767510 time 2020-06-26 23:40:06.192548
Model ind 665 epoch 1081 batch: 800 avg loss -2.834381 avg loss no lamb -2.834381 time 2020-06-26 23:40:16.994917
last batch sz 10
Pre: time 2020-06-26 23:40:31.023428: 
 	std: 0.0028135425
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.98, 0.9745, 0.9815, 0.9768]
	train_accs: [0.98153335, 0.9806333, 0.9753, 0.98158336, 0.977]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97889996
	best: 0.9815

Starting e_i: 1082
Model ind 665 epoch 1082 batch: 0 avg loss -2.976495 avg loss no lamb -2.976495 time 2020-06-26 23:40:32.257376
Model ind 665 epoch 1082 batch: 100 avg loss -2.923747 avg loss no lamb -2.923747 time 2020-06-26 23:40:43.016349
Model ind 665 epoch 1082 batch: 200 avg loss -2.795111 avg loss no lamb -2.795111 time 2020-06-26 23:40:53.810994
Model ind 665 epoch 1082 batch: 300 avg loss -2.840506 avg loss no lamb -2.840506 time 2020-06-26 23:41:04.704267
Model ind 665 epoch 1082 batch: 400 avg loss -2.818795 avg loss no lamb -2.818795 time 2020-06-26 23:41:15.807187
Model ind 665 epoch 1082 batch: 500 avg loss -2.840302 avg loss no lamb -2.840302 time 2020-06-26 23:41:26.668828
Model ind 665 epoch 1082 batch: 600 avg loss -2.843245 avg loss no lamb -2.843245 time 2020-06-26 23:41:37.499314
Model ind 665 epoch 1082 batch: 700 avg loss -2.819640 avg loss no lamb -2.819640 time 2020-06-26 23:41:48.096499
Model ind 665 epoch 1082 batch: 800 avg loss -2.863349 avg loss no lamb -2.863349 time 2020-06-26 23:41:58.658462
last batch sz 10
Pre: time 2020-06-26 23:42:12.786231: 
 	std: 0.0024108149
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9788, 0.974, 0.9796, 0.9755]
	train_accs: [0.98105, 0.98011667, 0.9747, 0.98113334, 0.97653335]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.9776
	best: 0.9796

Starting e_i: 1083
Model ind 665 epoch 1083 batch: 0 avg loss -2.933157 avg loss no lamb -2.933157 time 2020-06-26 23:42:14.023242
Model ind 665 epoch 1083 batch: 100 avg loss -2.951513 avg loss no lamb -2.951513 time 2020-06-26 23:42:24.893374
Model ind 665 epoch 1083 batch: 200 avg loss -2.852506 avg loss no lamb -2.852506 time 2020-06-26 23:42:35.679136
Model ind 665 epoch 1083 batch: 300 avg loss -2.896623 avg loss no lamb -2.896623 time 2020-06-26 23:42:46.362241
Model ind 665 epoch 1083 batch: 400 avg loss -2.767378 avg loss no lamb -2.767378 time 2020-06-26 23:42:57.176433
Model ind 665 epoch 1083 batch: 500 avg loss -2.838873 avg loss no lamb -2.838873 time 2020-06-26 23:43:07.924654
Model ind 665 epoch 1083 batch: 600 avg loss -2.896735 avg loss no lamb -2.896735 time 2020-06-26 23:43:18.949649
Model ind 665 epoch 1083 batch: 700 avg loss -2.786141 avg loss no lamb -2.786141 time 2020-06-26 23:43:29.756910
Model ind 665 epoch 1083 batch: 800 avg loss -2.854535 avg loss no lamb -2.854535 time 2020-06-26 23:43:40.506630
last batch sz 10
Pre: time 2020-06-26 23:43:54.562599: 
 	std: 0.0025023161
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9812, 0.9753, 0.9813, 0.978]
	train_accs: [0.98145, 0.98111665, 0.9758833, 0.98165, 0.97745]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97952
	best: 0.9813

Starting e_i: 1084
Model ind 665 epoch 1084 batch: 0 avg loss -2.941514 avg loss no lamb -2.941514 time 2020-06-26 23:43:55.649143
Model ind 665 epoch 1084 batch: 100 avg loss -2.980855 avg loss no lamb -2.980855 time 2020-06-26 23:44:06.388822
Model ind 665 epoch 1084 batch: 200 avg loss -2.859303 avg loss no lamb -2.859303 time 2020-06-26 23:44:17.264148
Model ind 665 epoch 1084 batch: 300 avg loss -2.875721 avg loss no lamb -2.875721 time 2020-06-26 23:44:27.876392
Model ind 665 epoch 1084 batch: 400 avg loss -2.758943 avg loss no lamb -2.758943 time 2020-06-26 23:44:38.900081
Model ind 665 epoch 1084 batch: 500 avg loss -2.828958 avg loss no lamb -2.828958 time 2020-06-26 23:44:49.883915
Model ind 665 epoch 1084 batch: 600 avg loss -2.876175 avg loss no lamb -2.876175 time 2020-06-26 23:45:00.775961
Model ind 665 epoch 1084 batch: 700 avg loss -2.785917 avg loss no lamb -2.785917 time 2020-06-26 23:45:11.629078
Model ind 665 epoch 1084 batch: 800 avg loss -2.836652 avg loss no lamb -2.836652 time 2020-06-26 23:45:22.557510
last batch sz 10
Pre: time 2020-06-26 23:45:36.461841: 
 	std: 0.003004267
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9802, 0.9743, 0.9815, 0.9759]
	train_accs: [0.98143333, 0.9807, 0.97503334, 0.98146665, 0.9761]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97868
	best: 0.9815

Starting e_i: 1085
Model ind 665 epoch 1085 batch: 0 avg loss -2.948651 avg loss no lamb -2.948651 time 2020-06-26 23:45:37.588446
Model ind 665 epoch 1085 batch: 100 avg loss -2.913069 avg loss no lamb -2.913069 time 2020-06-26 23:45:48.391649
Model ind 665 epoch 1085 batch: 200 avg loss -2.790711 avg loss no lamb -2.790711 time 2020-06-26 23:45:59.522135
Model ind 665 epoch 1085 batch: 300 avg loss -2.854038 avg loss no lamb -2.854038 time 2020-06-26 23:46:10.713968
Model ind 665 epoch 1085 batch: 400 avg loss -2.766879 avg loss no lamb -2.766879 time 2020-06-26 23:46:21.987622
Model ind 665 epoch 1085 batch: 500 avg loss -2.899759 avg loss no lamb -2.899759 time 2020-06-26 23:46:32.718345
Model ind 665 epoch 1085 batch: 600 avg loss -2.933749 avg loss no lamb -2.933749 time 2020-06-26 23:46:43.787018
Model ind 665 epoch 1085 batch: 700 avg loss -2.750773 avg loss no lamb -2.750773 time 2020-06-26 23:46:54.758386
Model ind 665 epoch 1085 batch: 800 avg loss -2.881858 avg loss no lamb -2.881858 time 2020-06-26 23:47:05.763961
last batch sz 10
Pre: time 2020-06-26 23:47:20.021657: 
 	std: 0.0031639226
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9797, 0.9739, 0.9808, 0.9751]
	train_accs: [0.98158336, 0.98088336, 0.97511667, 0.98123336, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.9782599
	best: 0.9818

Starting e_i: 1086
Model ind 665 epoch 1086 batch: 0 avg loss -2.968647 avg loss no lamb -2.968647 time 2020-06-26 23:47:21.134414
Model ind 665 epoch 1086 batch: 100 avg loss -2.899841 avg loss no lamb -2.899841 time 2020-06-26 23:47:32.005597
Model ind 665 epoch 1086 batch: 200 avg loss -2.877267 avg loss no lamb -2.877267 time 2020-06-26 23:47:43.252172
Model ind 665 epoch 1086 batch: 300 avg loss -2.838973 avg loss no lamb -2.838973 time 2020-06-26 23:47:54.249663
Model ind 665 epoch 1086 batch: 400 avg loss -2.787022 avg loss no lamb -2.787022 time 2020-06-26 23:48:05.467567
Model ind 665 epoch 1086 batch: 500 avg loss -2.863984 avg loss no lamb -2.863984 time 2020-06-26 23:48:16.297604
Model ind 665 epoch 1086 batch: 600 avg loss -2.852326 avg loss no lamb -2.852326 time 2020-06-26 23:48:27.075140
Model ind 665 epoch 1086 batch: 700 avg loss -2.761097 avg loss no lamb -2.761097 time 2020-06-26 23:48:38.045078
Model ind 665 epoch 1086 batch: 800 avg loss -2.837240 avg loss no lamb -2.837240 time 2020-06-26 23:48:49.046683
last batch sz 10
Pre: time 2020-06-26 23:49:03.227806: 
 	std: 0.0025086936
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9795, 0.9745, 0.9808, 0.9771]
	train_accs: [0.9819, 0.9809833, 0.9767333, 0.98186666, 0.9774333]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97861993
	best: 0.9812

Starting e_i: 1087
Model ind 665 epoch 1087 batch: 0 avg loss -2.932214 avg loss no lamb -2.932214 time 2020-06-26 23:49:04.359156
Model ind 665 epoch 1087 batch: 100 avg loss -2.905799 avg loss no lamb -2.905799 time 2020-06-26 23:49:15.347863
Model ind 665 epoch 1087 batch: 200 avg loss -2.879994 avg loss no lamb -2.879994 time 2020-06-26 23:49:25.994298
Model ind 665 epoch 1087 batch: 300 avg loss -2.941153 avg loss no lamb -2.941153 time 2020-06-26 23:49:37.016978
Model ind 665 epoch 1087 batch: 400 avg loss -2.743036 avg loss no lamb -2.743036 time 2020-06-26 23:49:48.045342
Model ind 665 epoch 1087 batch: 500 avg loss -2.813793 avg loss no lamb -2.813793 time 2020-06-26 23:49:58.696798
Model ind 665 epoch 1087 batch: 600 avg loss -2.877401 avg loss no lamb -2.877401 time 2020-06-26 23:50:09.665698
Model ind 665 epoch 1087 batch: 700 avg loss -2.792099 avg loss no lamb -2.792099 time 2020-06-26 23:50:20.385750
Model ind 665 epoch 1087 batch: 800 avg loss -2.909909 avg loss no lamb -2.909909 time 2020-06-26 23:50:31.504321
last batch sz 10
Pre: time 2020-06-26 23:50:46.019612: 
 	std: 0.0038311284
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9829, 0.9814, 0.9739, 0.9826, 0.9753]
	train_accs: [0.98223335, 0.9810167, 0.97541666, 0.9823, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97922003
	best: 0.9826

Starting e_i: 1088
Model ind 665 epoch 1088 batch: 0 avg loss -2.969030 avg loss no lamb -2.969030 time 2020-06-26 23:50:47.086906
Model ind 665 epoch 1088 batch: 100 avg loss -2.885820 avg loss no lamb -2.885820 time 2020-06-26 23:50:57.989690
Model ind 665 epoch 1088 batch: 200 avg loss -2.876148 avg loss no lamb -2.876148 time 2020-06-26 23:51:08.828650
Model ind 665 epoch 1088 batch: 300 avg loss -2.828152 avg loss no lamb -2.828152 time 2020-06-26 23:51:19.764846
Model ind 665 epoch 1088 batch: 400 avg loss -2.806470 avg loss no lamb -2.806470 time 2020-06-26 23:51:30.658364
Model ind 665 epoch 1088 batch: 500 avg loss -2.864595 avg loss no lamb -2.864595 time 2020-06-26 23:51:41.573691
Model ind 665 epoch 1088 batch: 600 avg loss -2.867074 avg loss no lamb -2.867074 time 2020-06-26 23:51:52.464228
Model ind 665 epoch 1088 batch: 700 avg loss -2.767880 avg loss no lamb -2.767880 time 2020-06-26 23:52:03.439235
Model ind 665 epoch 1088 batch: 800 avg loss -2.854148 avg loss no lamb -2.854148 time 2020-06-26 23:52:14.369722
last batch sz 10
Pre: time 2020-06-26 23:52:28.319571: 
 	std: 0.003291565
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9799, 0.9734, 0.9811, 0.975]
	train_accs: [0.98178333, 0.9809333, 0.97525, 0.98183334, 0.976]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97814
	best: 0.9811

Starting e_i: 1089
Model ind 665 epoch 1089 batch: 0 avg loss -2.915007 avg loss no lamb -2.915007 time 2020-06-26 23:52:29.417739
Model ind 665 epoch 1089 batch: 100 avg loss -2.909581 avg loss no lamb -2.909581 time 2020-06-26 23:52:39.954406
Model ind 665 epoch 1089 batch: 200 avg loss -2.865671 avg loss no lamb -2.865671 time 2020-06-26 23:52:51.035874
Model ind 665 epoch 1089 batch: 300 avg loss -2.911054 avg loss no lamb -2.911054 time 2020-06-26 23:53:02.039791
Model ind 665 epoch 1089 batch: 400 avg loss -2.757132 avg loss no lamb -2.757132 time 2020-06-26 23:53:13.049671
Model ind 665 epoch 1089 batch: 500 avg loss -2.802987 avg loss no lamb -2.802987 time 2020-06-26 23:53:23.982763
Model ind 665 epoch 1089 batch: 600 avg loss -2.885557 avg loss no lamb -2.885557 time 2020-06-26 23:53:34.899286
Model ind 665 epoch 1089 batch: 700 avg loss -2.747884 avg loss no lamb -2.747884 time 2020-06-26 23:53:46.165957
Model ind 665 epoch 1089 batch: 800 avg loss -2.858326 avg loss no lamb -2.858326 time 2020-06-26 23:53:57.241280
last batch sz 10
Pre: time 2020-06-26 23:54:11.828450: 
 	std: 0.002794709
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9794, 0.9733, 0.9799, 0.9749]
	train_accs: [0.98115, 0.9802667, 0.97538334, 0.9811, 0.9759167]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.9774599
	best: 0.9798

Starting e_i: 1090
Model ind 665 epoch 1090 batch: 0 avg loss -2.949697 avg loss no lamb -2.949697 time 2020-06-26 23:54:12.940118
Model ind 665 epoch 1090 batch: 100 avg loss -2.923663 avg loss no lamb -2.923663 time 2020-06-26 23:54:23.739355
Model ind 665 epoch 1090 batch: 200 avg loss -2.839719 avg loss no lamb -2.839719 time 2020-06-26 23:54:34.597445
Model ind 665 epoch 1090 batch: 300 avg loss -2.867582 avg loss no lamb -2.867582 time 2020-06-26 23:54:45.566807
Model ind 665 epoch 1090 batch: 400 avg loss -2.829096 avg loss no lamb -2.829096 time 2020-06-26 23:54:56.491321
Model ind 665 epoch 1090 batch: 500 avg loss -2.918075 avg loss no lamb -2.918075 time 2020-06-26 23:55:07.465712
Model ind 665 epoch 1090 batch: 600 avg loss -2.864376 avg loss no lamb -2.864376 time 2020-06-26 23:55:18.399100
Model ind 665 epoch 1090 batch: 700 avg loss -2.754332 avg loss no lamb -2.754332 time 2020-06-26 23:55:29.580289
Model ind 665 epoch 1090 batch: 800 avg loss -2.825591 avg loss no lamb -2.825591 time 2020-06-26 23:55:40.467640
last batch sz 10
Pre: time 2020-06-26 23:55:54.776490: 
 	std: 0.0033438948
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9796, 0.9723, 0.9798, 0.9745]
	train_accs: [0.98105, 0.98043334, 0.9749333, 0.98066664, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97738
	best: 0.9807

Starting e_i: 1091
Model ind 665 epoch 1091 batch: 0 avg loss -2.971241 avg loss no lamb -2.971241 time 2020-06-26 23:55:57.056065
Model ind 665 epoch 1091 batch: 100 avg loss -2.914393 avg loss no lamb -2.914393 time 2020-06-26 23:56:07.808530
Model ind 665 epoch 1091 batch: 200 avg loss -2.840085 avg loss no lamb -2.840085 time 2020-06-26 23:56:18.534077
Model ind 665 epoch 1091 batch: 300 avg loss -2.774281 avg loss no lamb -2.774281 time 2020-06-26 23:56:29.763716
Model ind 665 epoch 1091 batch: 400 avg loss -2.786719 avg loss no lamb -2.786719 time 2020-06-26 23:56:40.632759
Model ind 665 epoch 1091 batch: 500 avg loss -2.809863 avg loss no lamb -2.809863 time 2020-06-26 23:56:51.511476
Model ind 665 epoch 1091 batch: 600 avg loss -2.859523 avg loss no lamb -2.859523 time 2020-06-26 23:57:02.632615
Model ind 665 epoch 1091 batch: 700 avg loss -2.740750 avg loss no lamb -2.740750 time 2020-06-26 23:57:13.362275
Model ind 665 epoch 1091 batch: 800 avg loss -2.915556 avg loss no lamb -2.915556 time 2020-06-26 23:57:24.490669
last batch sz 10
Pre: time 2020-06-26 23:57:38.928396: 
 	std: 0.0031187318
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9801, 0.9733, 0.9815, 0.9762]
	train_accs: [0.98148334, 0.98108333, 0.97535, 0.98148334, 0.9769167]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97836
	best: 0.9807

Starting e_i: 1092
Model ind 665 epoch 1092 batch: 0 avg loss -2.943219 avg loss no lamb -2.943219 time 2020-06-26 23:57:39.984912
Model ind 665 epoch 1092 batch: 100 avg loss -2.897121 avg loss no lamb -2.897121 time 2020-06-26 23:57:51.189340
Model ind 665 epoch 1092 batch: 200 avg loss -2.927334 avg loss no lamb -2.927334 time 2020-06-26 23:58:02.214199
Model ind 665 epoch 1092 batch: 300 avg loss -2.915077 avg loss no lamb -2.915077 time 2020-06-26 23:58:13.147778
Model ind 665 epoch 1092 batch: 400 avg loss -2.793465 avg loss no lamb -2.793465 time 2020-06-26 23:58:24.026803
Model ind 665 epoch 1092 batch: 500 avg loss -2.847613 avg loss no lamb -2.847613 time 2020-06-26 23:58:34.889254
Model ind 665 epoch 1092 batch: 600 avg loss -2.886805 avg loss no lamb -2.886805 time 2020-06-26 23:58:45.811702
Model ind 665 epoch 1092 batch: 700 avg loss -2.813856 avg loss no lamb -2.813856 time 2020-06-26 23:58:56.642719
Model ind 665 epoch 1092 batch: 800 avg loss -2.881178 avg loss no lamb -2.881178 time 2020-06-26 23:59:07.756848
last batch sz 10
Pre: time 2020-06-26 23:59:21.993774: 
 	std: 0.0033352613
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9807, 0.9732, 0.9806, 0.9752]
	train_accs: [0.98121667, 0.98048335, 0.97508335, 0.98076665, 0.97585]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.9782001
	best: 0.9813

Starting e_i: 1093
Model ind 665 epoch 1093 batch: 0 avg loss -2.981764 avg loss no lamb -2.981764 time 2020-06-26 23:59:23.188917
Model ind 665 epoch 1093 batch: 100 avg loss -2.903714 avg loss no lamb -2.903714 time 2020-06-26 23:59:34.082174
Model ind 665 epoch 1093 batch: 200 avg loss -2.828783 avg loss no lamb -2.828783 time 2020-06-26 23:59:45.314528
Model ind 665 epoch 1093 batch: 300 avg loss -2.855805 avg loss no lamb -2.855805 time 2020-06-26 23:59:56.205047
Model ind 665 epoch 1093 batch: 400 avg loss -2.764912 avg loss no lamb -2.764912 time 2020-06-27 00:00:07.081548
Model ind 665 epoch 1093 batch: 500 avg loss -2.837555 avg loss no lamb -2.837555 time 2020-06-27 00:00:18.008168
Model ind 665 epoch 1093 batch: 600 avg loss -2.876707 avg loss no lamb -2.876707 time 2020-06-27 00:00:28.893555
Model ind 665 epoch 1093 batch: 700 avg loss -2.814182 avg loss no lamb -2.814182 time 2020-06-27 00:00:39.758980
Model ind 665 epoch 1093 batch: 800 avg loss -2.869708 avg loss no lamb -2.869708 time 2020-06-27 00:00:50.879693
last batch sz 10
Pre: time 2020-06-27 00:01:05.239122: 
 	std: 0.003044071
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9812, 0.9744, 0.9816, 0.9767]
	train_accs: [0.9816, 0.98111665, 0.976, 0.9814, 0.97693336]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97916
	best: 0.9819

Starting e_i: 1094
Model ind 665 epoch 1094 batch: 0 avg loss -2.958379 avg loss no lamb -2.958379 time 2020-06-27 00:01:06.319762
Model ind 665 epoch 1094 batch: 100 avg loss -2.895243 avg loss no lamb -2.895243 time 2020-06-27 00:01:17.164158
Model ind 665 epoch 1094 batch: 200 avg loss -2.862305 avg loss no lamb -2.862305 time 2020-06-27 00:01:27.912165
Model ind 665 epoch 1094 batch: 300 avg loss -2.859962 avg loss no lamb -2.859962 time 2020-06-27 00:01:38.892069
Model ind 665 epoch 1094 batch: 400 avg loss -2.841396 avg loss no lamb -2.841396 time 2020-06-27 00:01:49.595964
Model ind 665 epoch 1094 batch: 500 avg loss -2.843243 avg loss no lamb -2.843243 time 2020-06-27 00:02:00.576596
Model ind 665 epoch 1094 batch: 600 avg loss -2.879201 avg loss no lamb -2.879201 time 2020-06-27 00:02:11.529496
Model ind 665 epoch 1094 batch: 700 avg loss -2.790273 avg loss no lamb -2.790273 time 2020-06-27 00:02:22.437823
Model ind 665 epoch 1094 batch: 800 avg loss -2.856396 avg loss no lamb -2.856396 time 2020-06-27 00:02:33.277109
last batch sz 10
Pre: time 2020-06-27 00:02:47.692025: 
 	std: 0.003144901
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9804, 0.9735, 0.9807, 0.9755]
	train_accs: [0.98156667, 0.98066664, 0.9754, 0.98125, 0.97635]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97826004
	best: 0.9812

Starting e_i: 1095
Model ind 665 epoch 1095 batch: 0 avg loss -2.925673 avg loss no lamb -2.925673 time 2020-06-27 00:02:48.797318
Model ind 665 epoch 1095 batch: 100 avg loss -2.925517 avg loss no lamb -2.925517 time 2020-06-27 00:02:59.551772
Model ind 665 epoch 1095 batch: 200 avg loss -2.879974 avg loss no lamb -2.879974 time 2020-06-27 00:03:10.479692
Model ind 665 epoch 1095 batch: 300 avg loss -2.896724 avg loss no lamb -2.896724 time 2020-06-27 00:03:21.262325
Model ind 665 epoch 1095 batch: 400 avg loss -2.824584 avg loss no lamb -2.824584 time 2020-06-27 00:03:32.157488
Model ind 665 epoch 1095 batch: 500 avg loss -2.875296 avg loss no lamb -2.875296 time 2020-06-27 00:03:43.246261
Model ind 665 epoch 1095 batch: 600 avg loss -2.830693 avg loss no lamb -2.830693 time 2020-06-27 00:03:54.195873
Model ind 665 epoch 1095 batch: 700 avg loss -2.815193 avg loss no lamb -2.815193 time 2020-06-27 00:04:05.093807
Model ind 665 epoch 1095 batch: 800 avg loss -2.808429 avg loss no lamb -2.808429 time 2020-06-27 00:04:16.238262
last batch sz 10
Pre: time 2020-06-27 00:04:30.373093: 
 	std: 0.003222171
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9794, 0.9732, 0.981, 0.9756]
	train_accs: [0.9814, 0.9799167, 0.9748, 0.98145, 0.97645]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97814006
	best: 0.981

Starting e_i: 1096
Model ind 665 epoch 1096 batch: 0 avg loss -2.964970 avg loss no lamb -2.964970 time 2020-06-27 00:04:31.493839
Model ind 665 epoch 1096 batch: 100 avg loss -2.873696 avg loss no lamb -2.873696 time 2020-06-27 00:04:42.184721
Model ind 665 epoch 1096 batch: 200 avg loss -2.829591 avg loss no lamb -2.829591 time 2020-06-27 00:04:52.863953
Model ind 665 epoch 1096 batch: 300 avg loss -2.839891 avg loss no lamb -2.839891 time 2020-06-27 00:05:03.674035
Model ind 665 epoch 1096 batch: 400 avg loss -2.875541 avg loss no lamb -2.875541 time 2020-06-27 00:05:14.771360
Model ind 665 epoch 1096 batch: 500 avg loss -2.872535 avg loss no lamb -2.872535 time 2020-06-27 00:05:25.978741
Model ind 665 epoch 1096 batch: 600 avg loss -2.864040 avg loss no lamb -2.864040 time 2020-06-27 00:05:36.802443
Model ind 665 epoch 1096 batch: 700 avg loss -2.827466 avg loss no lamb -2.827466 time 2020-06-27 00:05:47.633251
Model ind 665 epoch 1096 batch: 800 avg loss -2.871900 avg loss no lamb -2.871900 time 2020-06-27 00:05:58.530010
last batch sz 10
Pre: time 2020-06-27 00:06:12.811291: 
 	std: 0.0032058845
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9824, 0.9811, 0.9747, 0.9824, 0.9765]
	train_accs: [0.98175, 0.9809667, 0.97546667, 0.9819, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97942
	best: 0.9824

Starting e_i: 1097
Model ind 665 epoch 1097 batch: 0 avg loss -2.983068 avg loss no lamb -2.983068 time 2020-06-27 00:06:13.958528
Model ind 665 epoch 1097 batch: 100 avg loss -2.882347 avg loss no lamb -2.882347 time 2020-06-27 00:06:24.960480
Model ind 665 epoch 1097 batch: 200 avg loss -2.886914 avg loss no lamb -2.886914 time 2020-06-27 00:06:35.840785
Model ind 665 epoch 1097 batch: 300 avg loss -2.777019 avg loss no lamb -2.777019 time 2020-06-27 00:06:46.773862
Model ind 665 epoch 1097 batch: 400 avg loss -2.714304 avg loss no lamb -2.714304 time 2020-06-27 00:06:57.761664
Model ind 665 epoch 1097 batch: 500 avg loss -2.832738 avg loss no lamb -2.832738 time 2020-06-27 00:07:08.726043
Model ind 665 epoch 1097 batch: 600 avg loss -2.862036 avg loss no lamb -2.862036 time 2020-06-27 00:07:19.552575
Model ind 665 epoch 1097 batch: 700 avg loss -2.747775 avg loss no lamb -2.747775 time 2020-06-27 00:07:30.605837
Model ind 665 epoch 1097 batch: 800 avg loss -2.902740 avg loss no lamb -2.902740 time 2020-06-27 00:07:41.553986
last batch sz 10
Pre: time 2020-06-27 00:07:55.995799: 
 	std: 0.0026346908
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9809, 0.9757, 0.9822, 0.9772]
	train_accs: [0.9816833, 0.9810333, 0.97658336, 0.98158336, 0.9770167]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97958004
	best: 0.9819

Starting e_i: 1098
Model ind 665 epoch 1098 batch: 0 avg loss -2.928908 avg loss no lamb -2.928908 time 2020-06-27 00:07:57.086958
Model ind 665 epoch 1098 batch: 100 avg loss -2.862385 avg loss no lamb -2.862385 time 2020-06-27 00:08:08.182196
Model ind 665 epoch 1098 batch: 200 avg loss -2.903844 avg loss no lamb -2.903844 time 2020-06-27 00:08:18.983951
Model ind 665 epoch 1098 batch: 300 avg loss -2.844644 avg loss no lamb -2.844644 time 2020-06-27 00:08:29.993588
Model ind 665 epoch 1098 batch: 400 avg loss -2.798165 avg loss no lamb -2.798165 time 2020-06-27 00:08:40.972889
Model ind 665 epoch 1098 batch: 500 avg loss -2.802987 avg loss no lamb -2.802987 time 2020-06-27 00:08:51.683643
Model ind 665 epoch 1098 batch: 600 avg loss -2.921230 avg loss no lamb -2.921230 time 2020-06-27 00:09:02.490054
Model ind 665 epoch 1098 batch: 700 avg loss -2.746404 avg loss no lamb -2.746404 time 2020-06-27 00:09:13.299437
Model ind 665 epoch 1098 batch: 800 avg loss -2.912649 avg loss no lamb -2.912649 time 2020-06-27 00:09:24.243528
last batch sz 10
Pre: time 2020-06-27 00:09:38.356579: 
 	std: 0.0031347035
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9828, 0.9808, 0.9751, 0.9819, 0.9761]
	train_accs: [0.98186666, 0.9809833, 0.97605, 0.9816833, 0.97665]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97933996
	best: 0.9828

Starting e_i: 1099
Model ind 665 epoch 1099 batch: 0 avg loss -2.897773 avg loss no lamb -2.897773 time 2020-06-27 00:09:39.432429
Model ind 665 epoch 1099 batch: 100 avg loss -2.896665 avg loss no lamb -2.896665 time 2020-06-27 00:09:50.363740
Model ind 665 epoch 1099 batch: 200 avg loss -2.861723 avg loss no lamb -2.861723 time 2020-06-27 00:10:01.192924
Model ind 665 epoch 1099 batch: 300 avg loss -2.850808 avg loss no lamb -2.850808 time 2020-06-27 00:10:12.032246
Model ind 665 epoch 1099 batch: 400 avg loss -2.790441 avg loss no lamb -2.790441 time 2020-06-27 00:10:22.986427
Model ind 665 epoch 1099 batch: 500 avg loss -2.880304 avg loss no lamb -2.880304 time 2020-06-27 00:10:33.833083
Model ind 665 epoch 1099 batch: 600 avg loss -2.829994 avg loss no lamb -2.829994 time 2020-06-27 00:10:44.744360
Model ind 665 epoch 1099 batch: 700 avg loss -2.814367 avg loss no lamb -2.814367 time 2020-06-27 00:10:55.554089
Model ind 665 epoch 1099 batch: 800 avg loss -2.890223 avg loss no lamb -2.890223 time 2020-06-27 00:11:06.556518
last batch sz 10
Pre: time 2020-06-27 00:11:20.728420: 
 	std: 0.0032536732
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9786, 0.9724, 0.9801, 0.9745]
	train_accs: [0.9812667, 0.9802833, 0.9745333, 0.9810333, 0.9756167]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97726
	best: 0.9807

Starting e_i: 1100
Model ind 665 epoch 1100 batch: 0 avg loss -2.984472 avg loss no lamb -2.984472 time 2020-06-27 00:11:21.843505
Model ind 665 epoch 1100 batch: 100 avg loss -2.930885 avg loss no lamb -2.930885 time 2020-06-27 00:11:32.729816
Model ind 665 epoch 1100 batch: 200 avg loss -2.904531 avg loss no lamb -2.904531 time 2020-06-27 00:11:43.704558
Model ind 665 epoch 1100 batch: 300 avg loss -2.804786 avg loss no lamb -2.804786 time 2020-06-27 00:11:54.393242
Model ind 665 epoch 1100 batch: 400 avg loss -2.801171 avg loss no lamb -2.801171 time 2020-06-27 00:12:05.287690
Model ind 665 epoch 1100 batch: 500 avg loss -2.806848 avg loss no lamb -2.806848 time 2020-06-27 00:12:16.372939
Model ind 665 epoch 1100 batch: 600 avg loss -2.823231 avg loss no lamb -2.823231 time 2020-06-27 00:12:27.156519
Model ind 665 epoch 1100 batch: 700 avg loss -2.782483 avg loss no lamb -2.782483 time 2020-06-27 00:12:37.964222
Model ind 665 epoch 1100 batch: 800 avg loss -2.859692 avg loss no lamb -2.859692 time 2020-06-27 00:12:48.684598
last batch sz 10
Pre: time 2020-06-27 00:13:02.735086: 
 	std: 0.0028188063
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9792, 0.9741, 0.9807, 0.9755]
	train_accs: [0.98158336, 0.98013335, 0.9751, 0.9816667, 0.9763833]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97812
	best: 0.9807

Starting e_i: 1101
Model ind 665 epoch 1101 batch: 0 avg loss -2.913481 avg loss no lamb -2.913481 time 2020-06-27 00:13:05.036467
Model ind 665 epoch 1101 batch: 100 avg loss -2.939683 avg loss no lamb -2.939683 time 2020-06-27 00:13:16.352343
Model ind 665 epoch 1101 batch: 200 avg loss -2.877261 avg loss no lamb -2.877261 time 2020-06-27 00:13:27.095219
Model ind 665 epoch 1101 batch: 300 avg loss -2.901780 avg loss no lamb -2.901780 time 2020-06-27 00:13:37.887429
Model ind 665 epoch 1101 batch: 400 avg loss -2.797169 avg loss no lamb -2.797169 time 2020-06-27 00:13:48.687746
Model ind 665 epoch 1101 batch: 500 avg loss -2.855689 avg loss no lamb -2.855689 time 2020-06-27 00:13:59.232098
Model ind 665 epoch 1101 batch: 600 avg loss -2.844291 avg loss no lamb -2.844291 time 2020-06-27 00:14:10.498212
Model ind 665 epoch 1101 batch: 700 avg loss -2.686116 avg loss no lamb -2.686116 time 2020-06-27 00:14:21.583188
Model ind 665 epoch 1101 batch: 800 avg loss -2.819919 avg loss no lamb -2.819919 time 2020-06-27 00:14:32.547084
last batch sz 10
Pre: time 2020-06-27 00:14:47.004823: 
 	std: 0.0036197172
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9802, 0.9732, 0.9812, 0.9737]
	train_accs: [0.9813667, 0.9812, 0.9748333, 0.98151666, 0.97571665]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97786
	best: 0.9812

Starting e_i: 1102
Model ind 665 epoch 1102 batch: 0 avg loss -2.842238 avg loss no lamb -2.842238 time 2020-06-27 00:14:48.060804
Model ind 665 epoch 1102 batch: 100 avg loss -2.910060 avg loss no lamb -2.910060 time 2020-06-27 00:14:59.081682
Model ind 665 epoch 1102 batch: 200 avg loss -2.837113 avg loss no lamb -2.837113 time 2020-06-27 00:15:10.013875
Model ind 665 epoch 1102 batch: 300 avg loss -2.871864 avg loss no lamb -2.871864 time 2020-06-27 00:15:20.786469
Model ind 665 epoch 1102 batch: 400 avg loss -2.799892 avg loss no lamb -2.799892 time 2020-06-27 00:15:31.791424
Model ind 665 epoch 1102 batch: 500 avg loss -2.749353 avg loss no lamb -2.749353 time 2020-06-27 00:15:42.704484
Model ind 665 epoch 1102 batch: 600 avg loss -2.869350 avg loss no lamb -2.869350 time 2020-06-27 00:15:53.575726
Model ind 665 epoch 1102 batch: 700 avg loss -2.784113 avg loss no lamb -2.784113 time 2020-06-27 00:16:04.520505
Model ind 665 epoch 1102 batch: 800 avg loss -2.844476 avg loss no lamb -2.844476 time 2020-06-27 00:16:15.233007
last batch sz 10
Pre: time 2020-06-27 00:16:29.419389: 
 	std: 0.0035840718
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9805, 0.9732, 0.9816, 0.9748]
	train_accs: [0.9818, 0.98088336, 0.97445, 0.9818, 0.97643334]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97832
	best: 0.9815

Starting e_i: 1103
Model ind 665 epoch 1103 batch: 0 avg loss -2.969814 avg loss no lamb -2.969814 time 2020-06-27 00:16:30.504847
Model ind 665 epoch 1103 batch: 100 avg loss -2.927667 avg loss no lamb -2.927667 time 2020-06-27 00:16:41.228772
Model ind 665 epoch 1103 batch: 200 avg loss -2.841838 avg loss no lamb -2.841838 time 2020-06-27 00:16:52.210645
Model ind 665 epoch 1103 batch: 300 avg loss -2.910958 avg loss no lamb -2.910958 time 2020-06-27 00:17:03.659494
Model ind 665 epoch 1103 batch: 400 avg loss -2.758338 avg loss no lamb -2.758338 time 2020-06-27 00:17:14.658465
Model ind 665 epoch 1103 batch: 500 avg loss -2.860932 avg loss no lamb -2.860932 time 2020-06-27 00:17:25.457705
Model ind 665 epoch 1103 batch: 600 avg loss -2.917854 avg loss no lamb -2.917854 time 2020-06-27 00:17:36.243461
Model ind 665 epoch 1103 batch: 700 avg loss -2.694601 avg loss no lamb -2.694601 time 2020-06-27 00:17:47.129200
Model ind 665 epoch 1103 batch: 800 avg loss -2.892297 avg loss no lamb -2.892297 time 2020-06-27 00:17:57.952134
last batch sz 10
Pre: time 2020-06-27 00:18:12.196021: 
 	std: 0.0037477405
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9797, 0.9731, 0.982, 0.9742]
	train_accs: [0.98148334, 0.9805, 0.9748333, 0.9816667, 0.97578335]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97812
	best: 0.982

Starting e_i: 1104
Model ind 665 epoch 1104 batch: 0 avg loss -2.956736 avg loss no lamb -2.956736 time 2020-06-27 00:18:13.266550
Model ind 665 epoch 1104 batch: 100 avg loss -2.897228 avg loss no lamb -2.897228 time 2020-06-27 00:18:23.979756
Model ind 665 epoch 1104 batch: 200 avg loss -2.823683 avg loss no lamb -2.823683 time 2020-06-27 00:18:34.939299
Model ind 665 epoch 1104 batch: 300 avg loss -2.834745 avg loss no lamb -2.834745 time 2020-06-27 00:18:45.890085
Model ind 665 epoch 1104 batch: 400 avg loss -2.787755 avg loss no lamb -2.787755 time 2020-06-27 00:18:56.473747
Model ind 665 epoch 1104 batch: 500 avg loss -2.839028 avg loss no lamb -2.839028 time 2020-06-27 00:19:07.397795
Model ind 665 epoch 1104 batch: 600 avg loss -2.904709 avg loss no lamb -2.904709 time 2020-06-27 00:19:18.540902
Model ind 665 epoch 1104 batch: 700 avg loss -2.711581 avg loss no lamb -2.711581 time 2020-06-27 00:19:29.457856
Model ind 665 epoch 1104 batch: 800 avg loss -2.847599 avg loss no lamb -2.847599 time 2020-06-27 00:19:40.347717
last batch sz 10
Pre: time 2020-06-27 00:19:54.684269: 
 	std: 0.0035535854
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9824, 0.9749, 0.9821, 0.975]
	train_accs: [0.9819, 0.9816167, 0.9759333, 0.982, 0.9767333]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.9793
	best: 0.9821

Starting e_i: 1105
Model ind 665 epoch 1105 batch: 0 avg loss -2.945186 avg loss no lamb -2.945186 time 2020-06-27 00:19:55.791506
Model ind 665 epoch 1105 batch: 100 avg loss -2.878076 avg loss no lamb -2.878076 time 2020-06-27 00:20:06.662089
Model ind 665 epoch 1105 batch: 200 avg loss -2.876432 avg loss no lamb -2.876432 time 2020-06-27 00:20:17.642741
Model ind 665 epoch 1105 batch: 300 avg loss -2.840298 avg loss no lamb -2.840298 time 2020-06-27 00:20:28.650754
Model ind 665 epoch 1105 batch: 400 avg loss -2.793130 avg loss no lamb -2.793130 time 2020-06-27 00:20:39.582841
Model ind 665 epoch 1105 batch: 500 avg loss -2.890531 avg loss no lamb -2.890531 time 2020-06-27 00:20:50.767826
Model ind 665 epoch 1105 batch: 600 avg loss -2.867505 avg loss no lamb -2.867505 time 2020-06-27 00:21:01.558426
Model ind 665 epoch 1105 batch: 700 avg loss -2.761803 avg loss no lamb -2.761803 time 2020-06-27 00:21:12.384505
Model ind 665 epoch 1105 batch: 800 avg loss -2.817794 avg loss no lamb -2.817794 time 2020-06-27 00:21:23.213063
last batch sz 10
Pre: time 2020-06-27 00:21:37.393239: 
 	std: 0.003229489
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9802, 0.9744, 0.9818, 0.9757]
	train_accs: [0.9817167, 0.9808, 0.97606665, 0.9818, 0.97695]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97888005
	best: 0.9818

Starting e_i: 1106
Model ind 665 epoch 1106 batch: 0 avg loss -2.900660 avg loss no lamb -2.900660 time 2020-06-27 00:21:38.533286
Model ind 665 epoch 1106 batch: 100 avg loss -2.900095 avg loss no lamb -2.900095 time 2020-06-27 00:21:49.497571
Model ind 665 epoch 1106 batch: 200 avg loss -2.857089 avg loss no lamb -2.857089 time 2020-06-27 00:22:00.531146
Model ind 665 epoch 1106 batch: 300 avg loss -2.886004 avg loss no lamb -2.886004 time 2020-06-27 00:22:11.375044
Model ind 665 epoch 1106 batch: 400 avg loss -2.777657 avg loss no lamb -2.777657 time 2020-06-27 00:22:22.006754
Model ind 665 epoch 1106 batch: 500 avg loss -2.899337 avg loss no lamb -2.899337 time 2020-06-27 00:22:32.840700
Model ind 665 epoch 1106 batch: 600 avg loss -2.880055 avg loss no lamb -2.880055 time 2020-06-27 00:22:43.842397
Model ind 665 epoch 1106 batch: 700 avg loss -2.744495 avg loss no lamb -2.744495 time 2020-06-27 00:22:54.672226
Model ind 665 epoch 1106 batch: 800 avg loss -2.847775 avg loss no lamb -2.847775 time 2020-06-27 00:23:05.756306
last batch sz 10
Pre: time 2020-06-27 00:23:19.816231: 
 	std: 0.003087647
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9801, 0.9742, 0.9812, 0.9751]
	train_accs: [0.98116666, 0.98041666, 0.97508335, 0.98148334, 0.9764]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97838
	best: 0.9812

Starting e_i: 1107
Model ind 665 epoch 1107 batch: 0 avg loss -2.963102 avg loss no lamb -2.963102 time 2020-06-27 00:23:20.899488
Model ind 665 epoch 1107 batch: 100 avg loss -2.921981 avg loss no lamb -2.921981 time 2020-06-27 00:23:31.707797
Model ind 665 epoch 1107 batch: 200 avg loss -2.825545 avg loss no lamb -2.825545 time 2020-06-27 00:23:42.510464
Model ind 665 epoch 1107 batch: 300 avg loss -2.864996 avg loss no lamb -2.864996 time 2020-06-27 00:23:53.828516
Model ind 665 epoch 1107 batch: 400 avg loss -2.796175 avg loss no lamb -2.796175 time 2020-06-27 00:24:04.792391
Model ind 665 epoch 1107 batch: 500 avg loss -2.816451 avg loss no lamb -2.816451 time 2020-06-27 00:24:15.769048
Model ind 665 epoch 1107 batch: 600 avg loss -2.906628 avg loss no lamb -2.906628 time 2020-06-27 00:24:26.900595
Model ind 665 epoch 1107 batch: 700 avg loss -2.758029 avg loss no lamb -2.758029 time 2020-06-27 00:24:37.735579
Model ind 665 epoch 1107 batch: 800 avg loss -2.885677 avg loss no lamb -2.885677 time 2020-06-27 00:24:48.445615
last batch sz 10
Pre: time 2020-06-27 00:25:02.607622: 
 	std: 0.0029247897
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9805, 0.9746, 0.981, 0.9756]
	train_accs: [0.98143333, 0.98095, 0.97595, 0.98145, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97863996
	best: 0.981

Starting e_i: 1108
Model ind 665 epoch 1108 batch: 0 avg loss -2.967559 avg loss no lamb -2.967559 time 2020-06-27 00:25:03.689092
Model ind 665 epoch 1108 batch: 100 avg loss -2.906507 avg loss no lamb -2.906507 time 2020-06-27 00:25:14.360985
Model ind 665 epoch 1108 batch: 200 avg loss -2.904135 avg loss no lamb -2.904135 time 2020-06-27 00:25:25.204113
Model ind 665 epoch 1108 batch: 300 avg loss -2.868162 avg loss no lamb -2.868162 time 2020-06-27 00:25:36.067518
Model ind 665 epoch 1108 batch: 400 avg loss -2.786865 avg loss no lamb -2.786865 time 2020-06-27 00:25:47.061494
Model ind 665 epoch 1108 batch: 500 avg loss -2.842102 avg loss no lamb -2.842102 time 2020-06-27 00:25:57.958954
Model ind 665 epoch 1108 batch: 600 avg loss -2.876898 avg loss no lamb -2.876898 time 2020-06-27 00:26:08.789821
Model ind 665 epoch 1108 batch: 700 avg loss -2.812626 avg loss no lamb -2.812626 time 2020-06-27 00:26:19.783830
Model ind 665 epoch 1108 batch: 800 avg loss -2.862036 avg loss no lamb -2.862036 time 2020-06-27 00:26:30.562974
last batch sz 10
Pre: time 2020-06-27 00:26:44.887607: 
 	std: 0.0028975813
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9809, 0.975, 0.9817, 0.9762]
	train_accs: [0.9816333, 0.98085, 0.97576666, 0.98143333, 0.97713333]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97910005
	best: 0.9817

Starting e_i: 1109
Model ind 665 epoch 1109 batch: 0 avg loss -2.966239 avg loss no lamb -2.966239 time 2020-06-27 00:26:45.957854
Model ind 665 epoch 1109 batch: 100 avg loss -2.886410 avg loss no lamb -2.886410 time 2020-06-27 00:26:56.906911
Model ind 665 epoch 1109 batch: 200 avg loss -2.877567 avg loss no lamb -2.877567 time 2020-06-27 00:27:07.854526
Model ind 665 epoch 1109 batch: 300 avg loss -2.855449 avg loss no lamb -2.855449 time 2020-06-27 00:27:18.672818
Model ind 665 epoch 1109 batch: 400 avg loss -2.764794 avg loss no lamb -2.764794 time 2020-06-27 00:27:29.630515
Model ind 665 epoch 1109 batch: 500 avg loss -2.853270 avg loss no lamb -2.853270 time 2020-06-27 00:27:40.670190
Model ind 665 epoch 1109 batch: 600 avg loss -2.868281 avg loss no lamb -2.868281 time 2020-06-27 00:27:51.631986
Model ind 665 epoch 1109 batch: 700 avg loss -2.759929 avg loss no lamb -2.759929 time 2020-06-27 00:28:02.525462
Model ind 665 epoch 1109 batch: 800 avg loss -2.886678 avg loss no lamb -2.886678 time 2020-06-27 00:28:13.446454
last batch sz 10
Pre: time 2020-06-27 00:28:27.755268: 
 	std: 0.0028667727
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9825, 0.9809, 0.9753, 0.982, 0.977]
	train_accs: [0.98226666, 0.98135, 0.976, 0.98215, 0.977]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97954005
	best: 0.9825

Starting e_i: 1110
Model ind 665 epoch 1110 batch: 0 avg loss -2.940348 avg loss no lamb -2.940348 time 2020-06-27 00:28:28.848509
Model ind 665 epoch 1110 batch: 100 avg loss -2.893955 avg loss no lamb -2.893955 time 2020-06-27 00:28:39.627520
Model ind 665 epoch 1110 batch: 200 avg loss -2.852437 avg loss no lamb -2.852437 time 2020-06-27 00:28:50.513625
Model ind 665 epoch 1110 batch: 300 avg loss -2.877689 avg loss no lamb -2.877689 time 2020-06-27 00:29:01.679539
Model ind 665 epoch 1110 batch: 400 avg loss -2.811046 avg loss no lamb -2.811046 time 2020-06-27 00:29:12.478939
Model ind 665 epoch 1110 batch: 500 avg loss -2.809586 avg loss no lamb -2.809586 time 2020-06-27 00:29:23.204848
Model ind 665 epoch 1110 batch: 600 avg loss -2.870967 avg loss no lamb -2.870967 time 2020-06-27 00:29:33.636630
Model ind 665 epoch 1110 batch: 700 avg loss -2.797640 avg loss no lamb -2.797640 time 2020-06-27 00:29:44.512907
Model ind 665 epoch 1110 batch: 800 avg loss -2.820712 avg loss no lamb -2.820712 time 2020-06-27 00:29:55.401379
last batch sz 10
Pre: time 2020-06-27 00:30:09.493950: 
 	std: 0.0035651706
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9814, 0.9734, 0.9818, 0.976]
	train_accs: [0.9817333, 0.9808, 0.97536665, 0.98141664, 0.9767]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97894
	best: 0.9821

Starting e_i: 1111
Model ind 665 epoch 1111 batch: 0 avg loss -2.924787 avg loss no lamb -2.924787 time 2020-06-27 00:30:11.754564
Model ind 665 epoch 1111 batch: 100 avg loss -2.876633 avg loss no lamb -2.876633 time 2020-06-27 00:30:22.479953
Model ind 665 epoch 1111 batch: 200 avg loss -2.902550 avg loss no lamb -2.902550 time 2020-06-27 00:30:33.195274
Model ind 665 epoch 1111 batch: 300 avg loss -2.852362 avg loss no lamb -2.852362 time 2020-06-27 00:30:44.052213
Model ind 665 epoch 1111 batch: 400 avg loss -2.768201 avg loss no lamb -2.768201 time 2020-06-27 00:30:54.975961
Model ind 665 epoch 1111 batch: 500 avg loss -2.833866 avg loss no lamb -2.833866 time 2020-06-27 00:31:05.892736
Model ind 665 epoch 1111 batch: 600 avg loss -2.898778 avg loss no lamb -2.898778 time 2020-06-27 00:31:16.591619
Model ind 665 epoch 1111 batch: 700 avg loss -2.826838 avg loss no lamb -2.826838 time 2020-06-27 00:31:27.668086
Model ind 665 epoch 1111 batch: 800 avg loss -2.859938 avg loss no lamb -2.859938 time 2020-06-27 00:31:38.630904
last batch sz 10
Pre: time 2020-06-27 00:31:52.904987: 
 	std: 0.003600761
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.981, 0.9725, 0.9808, 0.9746]
	train_accs: [0.9811, 0.9809833, 0.9743, 0.98146665, 0.97605]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97788
	best: 0.9808

Starting e_i: 1112
Model ind 665 epoch 1112 batch: 0 avg loss -2.921880 avg loss no lamb -2.921880 time 2020-06-27 00:31:53.975664
Model ind 665 epoch 1112 batch: 100 avg loss -2.863935 avg loss no lamb -2.863935 time 2020-06-27 00:32:04.762333
Model ind 665 epoch 1112 batch: 200 avg loss -2.834728 avg loss no lamb -2.834728 time 2020-06-27 00:32:15.790122
Model ind 665 epoch 1112 batch: 300 avg loss -2.901744 avg loss no lamb -2.901744 time 2020-06-27 00:32:26.590256
Model ind 665 epoch 1112 batch: 400 avg loss -2.787315 avg loss no lamb -2.787315 time 2020-06-27 00:32:37.322634
Model ind 665 epoch 1112 batch: 500 avg loss -2.840770 avg loss no lamb -2.840770 time 2020-06-27 00:32:48.039707
Model ind 665 epoch 1112 batch: 600 avg loss -2.840486 avg loss no lamb -2.840486 time 2020-06-27 00:32:58.776969
Model ind 665 epoch 1112 batch: 700 avg loss -2.736857 avg loss no lamb -2.736857 time 2020-06-27 00:33:09.681417
Model ind 665 epoch 1112 batch: 800 avg loss -2.874677 avg loss no lamb -2.874677 time 2020-06-27 00:33:20.604351
last batch sz 10
Pre: time 2020-06-27 00:33:34.633785: 
 	std: 0.003085832
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.98, 0.9739, 0.9811, 0.9757]
	train_accs: [0.9821333, 0.98078334, 0.97538334, 0.9820333, 0.9766167]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97846
	best: 0.9816

Starting e_i: 1113
Model ind 665 epoch 1113 batch: 0 avg loss -2.891507 avg loss no lamb -2.891507 time 2020-06-27 00:33:35.705232
Model ind 665 epoch 1113 batch: 100 avg loss -2.934616 avg loss no lamb -2.934616 time 2020-06-27 00:33:46.534867
Model ind 665 epoch 1113 batch: 200 avg loss -2.832745 avg loss no lamb -2.832745 time 2020-06-27 00:33:57.512988
Model ind 665 epoch 1113 batch: 300 avg loss -2.898353 avg loss no lamb -2.898353 time 2020-06-27 00:34:08.450407
Model ind 665 epoch 1113 batch: 400 avg loss -2.784563 avg loss no lamb -2.784563 time 2020-06-27 00:34:19.376744
Model ind 665 epoch 1113 batch: 500 avg loss -2.894639 avg loss no lamb -2.894639 time 2020-06-27 00:34:30.489766
Model ind 665 epoch 1113 batch: 600 avg loss -2.891209 avg loss no lamb -2.891209 time 2020-06-27 00:34:41.246489
Model ind 665 epoch 1113 batch: 700 avg loss -2.780848 avg loss no lamb -2.780848 time 2020-06-27 00:34:52.039791
Model ind 665 epoch 1113 batch: 800 avg loss -2.838012 avg loss no lamb -2.838012 time 2020-06-27 00:35:02.942925
last batch sz 10
Pre: time 2020-06-27 00:35:17.267232: 
 	std: 0.0032737716
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9798, 0.9733, 0.9812, 0.9749]
	train_accs: [0.98116666, 0.98015, 0.9748833, 0.9813167, 0.9759167]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97802
	best: 0.9812

Starting e_i: 1114
Model ind 665 epoch 1114 batch: 0 avg loss -2.945220 avg loss no lamb -2.945220 time 2020-06-27 00:35:18.366029
Model ind 665 epoch 1114 batch: 100 avg loss -2.854861 avg loss no lamb -2.854861 time 2020-06-27 00:35:29.117351
Model ind 665 epoch 1114 batch: 200 avg loss -2.880101 avg loss no lamb -2.880101 time 2020-06-27 00:35:39.833529
Model ind 665 epoch 1114 batch: 300 avg loss -2.797239 avg loss no lamb -2.797239 time 2020-06-27 00:35:50.656503
Model ind 665 epoch 1114 batch: 400 avg loss -2.765549 avg loss no lamb -2.765549 time 2020-06-27 00:36:01.501279
Model ind 665 epoch 1114 batch: 500 avg loss -2.853927 avg loss no lamb -2.853927 time 2020-06-27 00:36:12.475146
Model ind 665 epoch 1114 batch: 600 avg loss -2.849451 avg loss no lamb -2.849451 time 2020-06-27 00:36:23.402433
Model ind 665 epoch 1114 batch: 700 avg loss -2.770989 avg loss no lamb -2.770989 time 2020-06-27 00:36:34.278021
Model ind 665 epoch 1114 batch: 800 avg loss -2.874827 avg loss no lamb -2.874827 time 2020-06-27 00:36:45.017554
last batch sz 10
Pre: time 2020-06-27 00:36:58.818577: 
 	std: 0.0033075684
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9788, 0.973, 0.9806, 0.9739]
	train_accs: [0.9814, 0.98045, 0.97465, 0.98125, 0.97603333]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.9774
	best: 0.9807

Starting e_i: 1115
Model ind 665 epoch 1115 batch: 0 avg loss -2.936497 avg loss no lamb -2.936497 time 2020-06-27 00:36:59.920302
Model ind 665 epoch 1115 batch: 100 avg loss -2.906908 avg loss no lamb -2.906908 time 2020-06-27 00:37:10.777254
Model ind 665 epoch 1115 batch: 200 avg loss -2.885029 avg loss no lamb -2.885029 time 2020-06-27 00:37:21.605668
Model ind 665 epoch 1115 batch: 300 avg loss -2.840752 avg loss no lamb -2.840752 time 2020-06-27 00:37:32.407485
Model ind 665 epoch 1115 batch: 400 avg loss -2.780966 avg loss no lamb -2.780966 time 2020-06-27 00:37:43.081232
Model ind 665 epoch 1115 batch: 500 avg loss -2.857882 avg loss no lamb -2.857882 time 2020-06-27 00:37:53.901482
Model ind 665 epoch 1115 batch: 600 avg loss -2.868227 avg loss no lamb -2.868227 time 2020-06-27 00:38:04.961125
Model ind 665 epoch 1115 batch: 700 avg loss -2.687302 avg loss no lamb -2.687302 time 2020-06-27 00:38:15.593094
Model ind 665 epoch 1115 batch: 800 avg loss -2.877581 avg loss no lamb -2.877581 time 2020-06-27 00:38:26.117146
last batch sz 10
Pre: time 2020-06-27 00:38:40.229096: 
 	std: 0.002643486
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9803, 0.9746, 0.9806, 0.9758]
	train_accs: [0.9813667, 0.98053336, 0.9755667, 0.98118335, 0.97683334]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97840005
	best: 0.9807

Starting e_i: 1116
Model ind 665 epoch 1116 batch: 0 avg loss -2.995817 avg loss no lamb -2.995817 time 2020-06-27 00:38:41.346314
Model ind 665 epoch 1116 batch: 100 avg loss -2.895253 avg loss no lamb -2.895253 time 2020-06-27 00:38:52.384485
Model ind 665 epoch 1116 batch: 200 avg loss -2.858005 avg loss no lamb -2.858005 time 2020-06-27 00:39:03.322765
Model ind 665 epoch 1116 batch: 300 avg loss -2.870876 avg loss no lamb -2.870876 time 2020-06-27 00:39:14.095526
Model ind 665 epoch 1116 batch: 400 avg loss -2.798507 avg loss no lamb -2.798507 time 2020-06-27 00:39:24.729549
Model ind 665 epoch 1116 batch: 500 avg loss -2.839962 avg loss no lamb -2.839962 time 2020-06-27 00:39:35.658943
Model ind 665 epoch 1116 batch: 600 avg loss -2.903841 avg loss no lamb -2.903841 time 2020-06-27 00:39:46.877564
Model ind 665 epoch 1116 batch: 700 avg loss -2.822799 avg loss no lamb -2.822799 time 2020-06-27 00:39:57.791062
Model ind 665 epoch 1116 batch: 800 avg loss -2.880159 avg loss no lamb -2.880159 time 2020-06-27 00:40:08.682072
last batch sz 10
Pre: time 2020-06-27 00:40:22.824840: 
 	std: 0.0035860792
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9799, 0.9731, 0.9812, 0.9738]
	train_accs: [0.98153335, 0.98066664, 0.9747667, 0.9818, 0.97603333]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.9778
	best: 0.9812

Starting e_i: 1117
Model ind 665 epoch 1117 batch: 0 avg loss -2.928674 avg loss no lamb -2.928674 time 2020-06-27 00:40:24.007000
Model ind 665 epoch 1117 batch: 100 avg loss -2.941126 avg loss no lamb -2.941126 time 2020-06-27 00:40:35.051877
Model ind 665 epoch 1117 batch: 200 avg loss -2.879240 avg loss no lamb -2.879240 time 2020-06-27 00:40:45.910461
Model ind 665 epoch 1117 batch: 300 avg loss -2.882321 avg loss no lamb -2.882321 time 2020-06-27 00:40:56.716667
Model ind 665 epoch 1117 batch: 400 avg loss -2.777050 avg loss no lamb -2.777050 time 2020-06-27 00:41:07.602808
Model ind 665 epoch 1117 batch: 500 avg loss -2.887466 avg loss no lamb -2.887466 time 2020-06-27 00:41:18.502570
Model ind 665 epoch 1117 batch: 600 avg loss -2.864794 avg loss no lamb -2.864794 time 2020-06-27 00:41:29.447827
Model ind 665 epoch 1117 batch: 700 avg loss -2.683628 avg loss no lamb -2.683628 time 2020-06-27 00:41:40.055328
Model ind 665 epoch 1117 batch: 800 avg loss -2.894052 avg loss no lamb -2.894052 time 2020-06-27 00:41:50.971774
last batch sz 10
Pre: time 2020-06-27 00:42:05.204863: 
 	std: 0.0032369238
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9788, 0.9722, 0.9795, 0.9734]
	train_accs: [0.98088336, 0.9795667, 0.9745833, 0.98081666, 0.9755667]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97672004
	best: 0.9797

Starting e_i: 1118
Model ind 665 epoch 1118 batch: 0 avg loss -2.938657 avg loss no lamb -2.938657 time 2020-06-27 00:42:06.385897
Model ind 665 epoch 1118 batch: 100 avg loss -2.885408 avg loss no lamb -2.885408 time 2020-06-27 00:42:17.276271
Model ind 665 epoch 1118 batch: 200 avg loss -2.842834 avg loss no lamb -2.842834 time 2020-06-27 00:42:28.077552
Model ind 665 epoch 1118 batch: 300 avg loss -2.841776 avg loss no lamb -2.841776 time 2020-06-27 00:42:38.771223
Model ind 665 epoch 1118 batch: 400 avg loss -2.776849 avg loss no lamb -2.776849 time 2020-06-27 00:42:49.430276
Model ind 665 epoch 1118 batch: 500 avg loss -2.822490 avg loss no lamb -2.822490 time 2020-06-27 00:42:59.947911
Model ind 665 epoch 1118 batch: 600 avg loss -2.929998 avg loss no lamb -2.929998 time 2020-06-27 00:43:10.902563
Model ind 665 epoch 1118 batch: 700 avg loss -2.671621 avg loss no lamb -2.671621 time 2020-06-27 00:43:21.768473
Model ind 665 epoch 1118 batch: 800 avg loss -2.854412 avg loss no lamb -2.854412 time 2020-06-27 00:43:32.588731
last batch sz 10
Pre: time 2020-06-27 00:43:46.929017: 
 	std: 0.0033784017
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9789, 0.9717, 0.9798, 0.9734]
	train_accs: [0.9805833, 0.97996664, 0.97463334, 0.98085, 0.97568333]
	best_train_sub_head: 3
	worst: 0.9717
	avg: 0.97662
	best: 0.9798

Starting e_i: 1119
Model ind 665 epoch 1119 batch: 0 avg loss -2.920204 avg loss no lamb -2.920204 time 2020-06-27 00:43:48.047361
Model ind 665 epoch 1119 batch: 100 avg loss -2.921616 avg loss no lamb -2.921616 time 2020-06-27 00:43:59.044031
Model ind 665 epoch 1119 batch: 200 avg loss -2.890509 avg loss no lamb -2.890509 time 2020-06-27 00:44:10.127618
Model ind 665 epoch 1119 batch: 300 avg loss -2.843744 avg loss no lamb -2.843744 time 2020-06-27 00:44:20.989187
Model ind 665 epoch 1119 batch: 400 avg loss -2.869698 avg loss no lamb -2.869698 time 2020-06-27 00:44:31.666358
Model ind 665 epoch 1119 batch: 500 avg loss -2.873006 avg loss no lamb -2.873006 time 2020-06-27 00:44:42.551322
Model ind 665 epoch 1119 batch: 600 avg loss -2.915815 avg loss no lamb -2.915815 time 2020-06-27 00:44:53.849536
Model ind 665 epoch 1119 batch: 700 avg loss -2.816073 avg loss no lamb -2.816073 time 2020-06-27 00:45:04.848373
Model ind 665 epoch 1119 batch: 800 avg loss -2.811398 avg loss no lamb -2.811398 time 2020-06-27 00:45:15.710171
last batch sz 10
Pre: time 2020-06-27 00:45:30.036703: 
 	std: 0.0029102543
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9798, 0.9739, 0.9802, 0.9748]
	train_accs: [0.9813667, 0.98043334, 0.97603333, 0.9813, 0.97675]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97788
	best: 0.9807

Starting e_i: 1120
Model ind 665 epoch 1120 batch: 0 avg loss -2.944182 avg loss no lamb -2.944182 time 2020-06-27 00:45:31.152289
Model ind 665 epoch 1120 batch: 100 avg loss -2.899775 avg loss no lamb -2.899775 time 2020-06-27 00:45:41.972911
Model ind 665 epoch 1120 batch: 200 avg loss -2.867941 avg loss no lamb -2.867941 time 2020-06-27 00:45:52.850611
Model ind 665 epoch 1120 batch: 300 avg loss -2.894532 avg loss no lamb -2.894532 time 2020-06-27 00:46:03.786806
Model ind 665 epoch 1120 batch: 400 avg loss -2.742949 avg loss no lamb -2.742949 time 2020-06-27 00:46:14.717091
Model ind 665 epoch 1120 batch: 500 avg loss -2.833354 avg loss no lamb -2.833354 time 2020-06-27 00:46:25.390723
Model ind 665 epoch 1120 batch: 600 avg loss -2.842525 avg loss no lamb -2.842525 time 2020-06-27 00:46:36.299379
Model ind 665 epoch 1120 batch: 700 avg loss -2.779068 avg loss no lamb -2.779068 time 2020-06-27 00:46:47.250428
Model ind 665 epoch 1120 batch: 800 avg loss -2.805135 avg loss no lamb -2.805135 time 2020-06-27 00:46:58.028123
last batch sz 10
Pre: time 2020-06-27 00:47:12.190070: 
 	std: 0.0029823538
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.981, 0.9747, 0.982, 0.976]
	train_accs: [0.98185, 0.98108333, 0.97625, 0.9819833, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.9789399
	best: 0.982

Starting e_i: 1121
Model ind 665 epoch 1121 batch: 0 avg loss -2.952745 avg loss no lamb -2.952745 time 2020-06-27 00:47:14.562115
Model ind 665 epoch 1121 batch: 100 avg loss -2.878602 avg loss no lamb -2.878602 time 2020-06-27 00:47:25.258152
Model ind 665 epoch 1121 batch: 200 avg loss -2.863989 avg loss no lamb -2.863989 time 2020-06-27 00:47:36.287135
Model ind 665 epoch 1121 batch: 300 avg loss -2.857275 avg loss no lamb -2.857275 time 2020-06-27 00:47:47.253609
Model ind 665 epoch 1121 batch: 400 avg loss -2.808343 avg loss no lamb -2.808343 time 2020-06-27 00:47:58.340794
Model ind 665 epoch 1121 batch: 500 avg loss -2.895107 avg loss no lamb -2.895107 time 2020-06-27 00:48:08.955391
Model ind 665 epoch 1121 batch: 600 avg loss -2.903886 avg loss no lamb -2.903886 time 2020-06-27 00:48:19.756872
Model ind 665 epoch 1121 batch: 700 avg loss -2.773244 avg loss no lamb -2.773244 time 2020-06-27 00:48:30.597150
Model ind 665 epoch 1121 batch: 800 avg loss -2.791436 avg loss no lamb -2.791436 time 2020-06-27 00:48:41.448338
last batch sz 10
Pre: time 2020-06-27 00:48:55.807713: 
 	std: 0.0030929598
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9802, 0.9737, 0.9812, 0.9754]
	train_accs: [0.9816333, 0.981, 0.9751833, 0.9816667, 0.97665]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97826004
	best: 0.9812

Starting e_i: 1122
Model ind 665 epoch 1122 batch: 0 avg loss -2.956125 avg loss no lamb -2.956125 time 2020-06-27 00:48:56.875556
Model ind 665 epoch 1122 batch: 100 avg loss -2.901818 avg loss no lamb -2.901818 time 2020-06-27 00:49:07.499796
Model ind 665 epoch 1122 batch: 200 avg loss -2.891021 avg loss no lamb -2.891021 time 2020-06-27 00:49:18.479284
Model ind 665 epoch 1122 batch: 300 avg loss -2.810232 avg loss no lamb -2.810232 time 2020-06-27 00:49:29.348196
Model ind 665 epoch 1122 batch: 400 avg loss -2.778878 avg loss no lamb -2.778878 time 2020-06-27 00:49:40.158567
Model ind 665 epoch 1122 batch: 500 avg loss -2.888400 avg loss no lamb -2.888400 time 2020-06-27 00:49:50.921544
Model ind 665 epoch 1122 batch: 600 avg loss -2.857480 avg loss no lamb -2.857480 time 2020-06-27 00:50:01.858794
Model ind 665 epoch 1122 batch: 700 avg loss -2.778878 avg loss no lamb -2.778878 time 2020-06-27 00:50:12.699870
Model ind 665 epoch 1122 batch: 800 avg loss -2.869835 avg loss no lamb -2.869835 time 2020-06-27 00:50:23.531496
last batch sz 10
Pre: time 2020-06-27 00:50:37.589837: 
 	std: 0.0030017372
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9806, 0.9749, 0.9814, 0.9751]
	train_accs: [0.98153335, 0.98081666, 0.97545, 0.98135, 0.97636664]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97866
	best: 0.9813

Starting e_i: 1123
Model ind 665 epoch 1123 batch: 0 avg loss -2.946678 avg loss no lamb -2.946678 time 2020-06-27 00:50:38.783156
Model ind 665 epoch 1123 batch: 100 avg loss -2.947757 avg loss no lamb -2.947757 time 2020-06-27 00:50:49.751234
Model ind 665 epoch 1123 batch: 200 avg loss -2.863114 avg loss no lamb -2.863114 time 2020-06-27 00:51:00.568047
Model ind 665 epoch 1123 batch: 300 avg loss -2.910219 avg loss no lamb -2.910219 time 2020-06-27 00:51:11.291278
Model ind 665 epoch 1123 batch: 400 avg loss -2.817752 avg loss no lamb -2.817752 time 2020-06-27 00:51:21.869202
Model ind 665 epoch 1123 batch: 500 avg loss -2.834253 avg loss no lamb -2.834253 time 2020-06-27 00:51:32.937304
Model ind 665 epoch 1123 batch: 600 avg loss -2.894852 avg loss no lamb -2.894852 time 2020-06-27 00:51:43.933403
Model ind 665 epoch 1123 batch: 700 avg loss -2.793249 avg loss no lamb -2.793249 time 2020-06-27 00:51:54.598820
Model ind 665 epoch 1123 batch: 800 avg loss -2.909950 avg loss no lamb -2.909950 time 2020-06-27 00:52:05.307004
last batch sz 10
Pre: time 2020-06-27 00:52:19.864329: 
 	std: 0.003634494
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9797, 0.9718, 0.9809, 0.9751]
	train_accs: [0.98188335, 0.98076665, 0.9749, 0.9819667, 0.9766]
	best_train_sub_head: 3
	worst: 0.9718
	avg: 0.97768
	best: 0.9809

Starting e_i: 1124
Model ind 665 epoch 1124 batch: 0 avg loss -2.961068 avg loss no lamb -2.961068 time 2020-06-27 00:52:21.053513
Model ind 665 epoch 1124 batch: 100 avg loss -2.932288 avg loss no lamb -2.932288 time 2020-06-27 00:52:32.125760
Model ind 665 epoch 1124 batch: 200 avg loss -2.867066 avg loss no lamb -2.867066 time 2020-06-27 00:52:43.182891
Model ind 665 epoch 1124 batch: 300 avg loss -2.839900 avg loss no lamb -2.839900 time 2020-06-27 00:52:54.074789
Model ind 665 epoch 1124 batch: 400 avg loss -2.828530 avg loss no lamb -2.828530 time 2020-06-27 00:53:04.945235
Model ind 665 epoch 1124 batch: 500 avg loss -2.892586 avg loss no lamb -2.892586 time 2020-06-27 00:53:15.725118
Model ind 665 epoch 1124 batch: 600 avg loss -2.851700 avg loss no lamb -2.851700 time 2020-06-27 00:53:26.454510
Model ind 665 epoch 1124 batch: 700 avg loss -2.797116 avg loss no lamb -2.797116 time 2020-06-27 00:53:37.250632
Model ind 665 epoch 1124 batch: 800 avg loss -2.897052 avg loss no lamb -2.897052 time 2020-06-27 00:53:48.139521
last batch sz 10
Pre: time 2020-06-27 00:54:02.382755: 
 	std: 0.0033864423
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9804, 0.9734, 0.9812, 0.9753]
	train_accs: [0.98205, 0.98106664, 0.97571665, 0.9813833, 0.9764]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97840005
	best: 0.9817

Starting e_i: 1125
Model ind 665 epoch 1125 batch: 0 avg loss -2.958015 avg loss no lamb -2.958015 time 2020-06-27 00:54:03.474717
Model ind 665 epoch 1125 batch: 100 avg loss -2.944227 avg loss no lamb -2.944227 time 2020-06-27 00:54:14.324921
Model ind 665 epoch 1125 batch: 200 avg loss -2.897860 avg loss no lamb -2.897860 time 2020-06-27 00:54:25.250976
Model ind 665 epoch 1125 batch: 300 avg loss -2.861960 avg loss no lamb -2.861960 time 2020-06-27 00:54:36.251721
Model ind 665 epoch 1125 batch: 400 avg loss -2.812393 avg loss no lamb -2.812393 time 2020-06-27 00:54:47.264149
Model ind 665 epoch 1125 batch: 500 avg loss -2.855871 avg loss no lamb -2.855871 time 2020-06-27 00:54:58.078700
Model ind 665 epoch 1125 batch: 600 avg loss -2.843298 avg loss no lamb -2.843298 time 2020-06-27 00:55:09.194883
Model ind 665 epoch 1125 batch: 700 avg loss -2.749182 avg loss no lamb -2.749182 time 2020-06-27 00:55:20.119628
Model ind 665 epoch 1125 batch: 800 avg loss -2.845592 avg loss no lamb -2.845592 time 2020-06-27 00:55:31.135049
last batch sz 10
Pre: time 2020-06-27 00:55:45.550461: 
 	std: 0.0035062241
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9804, 0.9737, 0.9816, 0.9749]
	train_accs: [0.982, 0.9812, 0.97578335, 0.98181665, 0.9767333]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97852004
	best: 0.982

Starting e_i: 1126
Model ind 665 epoch 1126 batch: 0 avg loss -2.915599 avg loss no lamb -2.915599 time 2020-06-27 00:55:46.665127
Model ind 665 epoch 1126 batch: 100 avg loss -2.901134 avg loss no lamb -2.901134 time 2020-06-27 00:55:57.301392
Model ind 665 epoch 1126 batch: 200 avg loss -2.877268 avg loss no lamb -2.877268 time 2020-06-27 00:56:08.317458
Model ind 665 epoch 1126 batch: 300 avg loss -2.883123 avg loss no lamb -2.883123 time 2020-06-27 00:56:19.184182
Model ind 665 epoch 1126 batch: 400 avg loss -2.825027 avg loss no lamb -2.825027 time 2020-06-27 00:56:29.997806
Model ind 665 epoch 1126 batch: 500 avg loss -2.826270 avg loss no lamb -2.826270 time 2020-06-27 00:56:40.678336
Model ind 665 epoch 1126 batch: 600 avg loss -2.849128 avg loss no lamb -2.849128 time 2020-06-27 00:56:51.309249
Model ind 665 epoch 1126 batch: 700 avg loss -2.684847 avg loss no lamb -2.684847 time 2020-06-27 00:57:02.283856
Model ind 665 epoch 1126 batch: 800 avg loss -2.881831 avg loss no lamb -2.881831 time 2020-06-27 00:57:13.197580
last batch sz 10
Pre: time 2020-06-27 00:57:27.314886: 
 	std: 0.003153142
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9802, 0.9738, 0.9808, 0.9746]
	train_accs: [0.9816833, 0.9808667, 0.9752, 0.9813833, 0.97645]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.9780399
	best: 0.9808

Starting e_i: 1127
Model ind 665 epoch 1127 batch: 0 avg loss -2.960349 avg loss no lamb -2.960349 time 2020-06-27 00:57:28.410518
Model ind 665 epoch 1127 batch: 100 avg loss -2.966589 avg loss no lamb -2.966589 time 2020-06-27 00:57:39.446671
Model ind 665 epoch 1127 batch: 200 avg loss -2.823308 avg loss no lamb -2.823308 time 2020-06-27 00:57:50.353067
Model ind 665 epoch 1127 batch: 300 avg loss -2.838568 avg loss no lamb -2.838568 time 2020-06-27 00:58:01.465207
Model ind 665 epoch 1127 batch: 400 avg loss -2.771254 avg loss no lamb -2.771254 time 2020-06-27 00:58:12.060654
Model ind 665 epoch 1127 batch: 500 avg loss -2.822251 avg loss no lamb -2.822251 time 2020-06-27 00:58:22.911524
Model ind 665 epoch 1127 batch: 600 avg loss -2.882931 avg loss no lamb -2.882931 time 2020-06-27 00:58:33.815925
Model ind 665 epoch 1127 batch: 700 avg loss -2.705718 avg loss no lamb -2.705718 time 2020-06-27 00:58:44.765933
Model ind 665 epoch 1127 batch: 800 avg loss -2.862835 avg loss no lamb -2.862835 time 2020-06-27 00:58:55.688706
last batch sz 10
Pre: time 2020-06-27 00:59:09.952869: 
 	std: 0.0036432892
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9791, 0.9718, 0.9806, 0.9734]
	train_accs: [0.98145, 0.98031664, 0.97466666, 0.98146665, 0.9758667]
	best_train_sub_head: 3
	worst: 0.9718
	avg: 0.97698003
	best: 0.9806

Starting e_i: 1128
Model ind 665 epoch 1128 batch: 0 avg loss -2.920271 avg loss no lamb -2.920271 time 2020-06-27 00:59:11.066016
Model ind 665 epoch 1128 batch: 100 avg loss -2.813975 avg loss no lamb -2.813975 time 2020-06-27 00:59:22.147757
Model ind 665 epoch 1128 batch: 200 avg loss -2.842616 avg loss no lamb -2.842616 time 2020-06-27 00:59:33.045879
Model ind 665 epoch 1128 batch: 300 avg loss -2.929256 avg loss no lamb -2.929256 time 2020-06-27 00:59:43.979708
Model ind 665 epoch 1128 batch: 400 avg loss -2.790394 avg loss no lamb -2.790394 time 2020-06-27 00:59:54.776773
Model ind 665 epoch 1128 batch: 500 avg loss -2.838320 avg loss no lamb -2.838320 time 2020-06-27 01:00:05.640531
Model ind 665 epoch 1128 batch: 600 avg loss -2.848695 avg loss no lamb -2.848695 time 2020-06-27 01:00:16.363106
Model ind 665 epoch 1128 batch: 700 avg loss -2.756873 avg loss no lamb -2.756873 time 2020-06-27 01:00:27.069548
Model ind 665 epoch 1128 batch: 800 avg loss -2.854643 avg loss no lamb -2.854643 time 2020-06-27 01:00:37.615754
last batch sz 10
Pre: time 2020-06-27 01:00:51.838964: 
 	std: 0.0031396924
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9791, 0.9736, 0.9808, 0.974]
	train_accs: [0.98106664, 0.9801, 0.975, 0.98113334, 0.9757]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97757995
	best: 0.9808

Starting e_i: 1129
Model ind 665 epoch 1129 batch: 0 avg loss -2.909166 avg loss no lamb -2.909166 time 2020-06-27 01:00:52.948899
Model ind 665 epoch 1129 batch: 100 avg loss -2.801570 avg loss no lamb -2.801570 time 2020-06-27 01:01:03.789489
Model ind 665 epoch 1129 batch: 200 avg loss -2.882425 avg loss no lamb -2.882425 time 2020-06-27 01:01:14.473666
Model ind 665 epoch 1129 batch: 300 avg loss -2.888326 avg loss no lamb -2.888326 time 2020-06-27 01:01:25.269711
Model ind 665 epoch 1129 batch: 400 avg loss -2.810851 avg loss no lamb -2.810851 time 2020-06-27 01:01:36.367853
Model ind 665 epoch 1129 batch: 500 avg loss -2.876395 avg loss no lamb -2.876395 time 2020-06-27 01:01:47.351713
Model ind 665 epoch 1129 batch: 600 avg loss -2.904970 avg loss no lamb -2.904970 time 2020-06-27 01:01:58.349258
Model ind 665 epoch 1129 batch: 700 avg loss -2.787908 avg loss no lamb -2.787908 time 2020-06-27 01:02:09.257238
Model ind 665 epoch 1129 batch: 800 avg loss -2.854313 avg loss no lamb -2.854313 time 2020-06-27 01:02:20.077630
last batch sz 10
Pre: time 2020-06-27 01:02:34.504989: 
 	std: 0.0030616426
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9809, 0.9747, 0.9812, 0.9755]
	train_accs: [0.9817, 0.9809667, 0.97573334, 0.98148334, 0.9766667]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97881997
	best: 0.9818

Starting e_i: 1130
Model ind 665 epoch 1130 batch: 0 avg loss -2.969696 avg loss no lamb -2.969696 time 2020-06-27 01:02:35.678565
Model ind 665 epoch 1130 batch: 100 avg loss -2.927677 avg loss no lamb -2.927677 time 2020-06-27 01:02:46.581778
Model ind 665 epoch 1130 batch: 200 avg loss -2.907069 avg loss no lamb -2.907069 time 2020-06-27 01:02:57.575173
Model ind 665 epoch 1130 batch: 300 avg loss -2.843344 avg loss no lamb -2.843344 time 2020-06-27 01:03:08.384655
Model ind 665 epoch 1130 batch: 400 avg loss -2.798814 avg loss no lamb -2.798814 time 2020-06-27 01:03:19.373192
Model ind 665 epoch 1130 batch: 500 avg loss -2.776313 avg loss no lamb -2.776313 time 2020-06-27 01:03:30.038809
Model ind 665 epoch 1130 batch: 600 avg loss -2.885979 avg loss no lamb -2.885979 time 2020-06-27 01:03:41.119249
Model ind 665 epoch 1130 batch: 700 avg loss -2.743520 avg loss no lamb -2.743520 time 2020-06-27 01:03:52.281365
Model ind 665 epoch 1130 batch: 800 avg loss -2.904367 avg loss no lamb -2.904367 time 2020-06-27 01:04:03.427401
last batch sz 10
Pre: time 2020-06-27 01:04:17.274508: 
 	std: 0.0028499877
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.98, 0.9741, 0.9806, 0.9751]
	train_accs: [0.9816667, 0.9809833, 0.97608334, 0.98165, 0.9769167]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97805995
	best: 0.9805

Starting e_i: 1131
Model ind 665 epoch 1131 batch: 0 avg loss -2.952154 avg loss no lamb -2.952154 time 2020-06-27 01:04:19.484369
Model ind 665 epoch 1131 batch: 100 avg loss -2.921674 avg loss no lamb -2.921674 time 2020-06-27 01:04:30.358558
Model ind 665 epoch 1131 batch: 200 avg loss -2.854401 avg loss no lamb -2.854401 time 2020-06-27 01:04:41.294691
Model ind 665 epoch 1131 batch: 300 avg loss -2.833351 avg loss no lamb -2.833351 time 2020-06-27 01:04:52.050208
Model ind 665 epoch 1131 batch: 400 avg loss -2.790013 avg loss no lamb -2.790013 time 2020-06-27 01:05:03.076004
Model ind 665 epoch 1131 batch: 500 avg loss -2.861001 avg loss no lamb -2.861001 time 2020-06-27 01:05:14.034809
Model ind 665 epoch 1131 batch: 600 avg loss -2.900066 avg loss no lamb -2.900066 time 2020-06-27 01:05:25.071902
Model ind 665 epoch 1131 batch: 700 avg loss -2.755718 avg loss no lamb -2.755718 time 2020-06-27 01:05:35.928308
Model ind 665 epoch 1131 batch: 800 avg loss -2.922995 avg loss no lamb -2.922995 time 2020-06-27 01:05:46.784447
last batch sz 10
Pre: time 2020-06-27 01:06:01.007911: 
 	std: 0.002877919
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9791, 0.9736, 0.9805, 0.9746]
	train_accs: [0.9817333, 0.98121667, 0.9759667, 0.9816667, 0.9767]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.9775599
	best: 0.98

Starting e_i: 1132
Model ind 665 epoch 1132 batch: 0 avg loss -2.923230 avg loss no lamb -2.923230 time 2020-06-27 01:06:02.096642
Model ind 665 epoch 1132 batch: 100 avg loss -2.917074 avg loss no lamb -2.917074 time 2020-06-27 01:06:13.126049
Model ind 665 epoch 1132 batch: 200 avg loss -2.924463 avg loss no lamb -2.924463 time 2020-06-27 01:06:23.988312
Model ind 665 epoch 1132 batch: 300 avg loss -2.881187 avg loss no lamb -2.881187 time 2020-06-27 01:06:34.775296
Model ind 665 epoch 1132 batch: 400 avg loss -2.810149 avg loss no lamb -2.810149 time 2020-06-27 01:06:45.867823
Model ind 665 epoch 1132 batch: 500 avg loss -2.876874 avg loss no lamb -2.876874 time 2020-06-27 01:06:57.054921
Model ind 665 epoch 1132 batch: 600 avg loss -2.872226 avg loss no lamb -2.872226 time 2020-06-27 01:07:08.306497
Model ind 665 epoch 1132 batch: 700 avg loss -2.772146 avg loss no lamb -2.772146 time 2020-06-27 01:07:19.313995
Model ind 665 epoch 1132 batch: 800 avg loss -2.883435 avg loss no lamb -2.883435 time 2020-06-27 01:07:30.186504
last batch sz 10
Pre: time 2020-06-27 01:07:44.690747: 
 	std: 0.002802573
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9776, 0.973, 0.9796, 0.9742]
	train_accs: [0.9812667, 0.98018336, 0.9746, 0.98143333, 0.97605]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97686005
	best: 0.9796

Starting e_i: 1133
Model ind 665 epoch 1133 batch: 0 avg loss -2.977523 avg loss no lamb -2.977523 time 2020-06-27 01:07:45.810698
Model ind 665 epoch 1133 batch: 100 avg loss -2.933091 avg loss no lamb -2.933091 time 2020-06-27 01:07:56.634203
Model ind 665 epoch 1133 batch: 200 avg loss -2.902815 avg loss no lamb -2.902815 time 2020-06-27 01:08:07.762677
Model ind 665 epoch 1133 batch: 300 avg loss -2.821969 avg loss no lamb -2.821969 time 2020-06-27 01:08:18.564700
Model ind 665 epoch 1133 batch: 400 avg loss -2.717146 avg loss no lamb -2.717146 time 2020-06-27 01:08:29.578479
Model ind 665 epoch 1133 batch: 500 avg loss -2.813972 avg loss no lamb -2.813972 time 2020-06-27 01:08:40.608411
Model ind 665 epoch 1133 batch: 600 avg loss -2.819267 avg loss no lamb -2.819267 time 2020-06-27 01:08:51.481257
Model ind 665 epoch 1133 batch: 700 avg loss -2.830854 avg loss no lamb -2.830854 time 2020-06-27 01:09:02.452598
Model ind 665 epoch 1133 batch: 800 avg loss -2.840430 avg loss no lamb -2.840430 time 2020-06-27 01:09:13.177916
last batch sz 10
Pre: time 2020-06-27 01:09:27.691566: 
 	std: 0.0028798454
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9813, 0.975, 0.9819, 0.9774]
	train_accs: [0.98225, 0.9812, 0.97578335, 0.9821333, 0.97715]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97958004
	best: 0.9823

Starting e_i: 1134
Model ind 665 epoch 1134 batch: 0 avg loss -2.929656 avg loss no lamb -2.929656 time 2020-06-27 01:09:28.859699
Model ind 665 epoch 1134 batch: 100 avg loss -2.924627 avg loss no lamb -2.924627 time 2020-06-27 01:09:39.881967
Model ind 665 epoch 1134 batch: 200 avg loss -2.865535 avg loss no lamb -2.865535 time 2020-06-27 01:09:50.957329
Model ind 665 epoch 1134 batch: 300 avg loss -2.877010 avg loss no lamb -2.877010 time 2020-06-27 01:10:01.885583
Model ind 665 epoch 1134 batch: 400 avg loss -2.773359 avg loss no lamb -2.773359 time 2020-06-27 01:10:12.854778
Model ind 665 epoch 1134 batch: 500 avg loss -2.889829 avg loss no lamb -2.889829 time 2020-06-27 01:10:24.072574
Model ind 665 epoch 1134 batch: 600 avg loss -2.889396 avg loss no lamb -2.889396 time 2020-06-27 01:10:34.988129
Model ind 665 epoch 1134 batch: 700 avg loss -2.791332 avg loss no lamb -2.791332 time 2020-06-27 01:10:45.987880
Model ind 665 epoch 1134 batch: 800 avg loss -2.891690 avg loss no lamb -2.891690 time 2020-06-27 01:10:56.852332
last batch sz 10
Pre: time 2020-06-27 01:11:10.943384: 
 	std: 0.0028611845
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.981, 0.975, 0.981, 0.9749]
	train_accs: [0.9812, 0.98095, 0.97608334, 0.98151666, 0.97616667]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97844
	best: 0.981

Starting e_i: 1135
Model ind 665 epoch 1135 batch: 0 avg loss -2.976336 avg loss no lamb -2.976336 time 2020-06-27 01:11:11.995511
Model ind 665 epoch 1135 batch: 100 avg loss -2.908031 avg loss no lamb -2.908031 time 2020-06-27 01:11:22.741965
Model ind 665 epoch 1135 batch: 200 avg loss -2.810854 avg loss no lamb -2.810854 time 2020-06-27 01:11:33.555042
Model ind 665 epoch 1135 batch: 300 avg loss -2.867963 avg loss no lamb -2.867963 time 2020-06-27 01:11:44.497109
Model ind 665 epoch 1135 batch: 400 avg loss -2.752253 avg loss no lamb -2.752253 time 2020-06-27 01:11:55.250675
Model ind 665 epoch 1135 batch: 500 avg loss -2.818729 avg loss no lamb -2.818729 time 2020-06-27 01:12:06.320012
Model ind 665 epoch 1135 batch: 600 avg loss -2.871074 avg loss no lamb -2.871074 time 2020-06-27 01:12:17.323928
Model ind 665 epoch 1135 batch: 700 avg loss -2.768265 avg loss no lamb -2.768265 time 2020-06-27 01:12:28.143400
Model ind 665 epoch 1135 batch: 800 avg loss -2.887188 avg loss no lamb -2.887188 time 2020-06-27 01:12:39.000052
last batch sz 10
Pre: time 2020-06-27 01:12:53.464144: 
 	std: 0.0032854793
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9804, 0.9739, 0.9818, 0.9763]
	train_accs: [0.98178333, 0.9803, 0.9748833, 0.98165, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97894
	best: 0.9823

Starting e_i: 1136
Model ind 665 epoch 1136 batch: 0 avg loss -2.944877 avg loss no lamb -2.944877 time 2020-06-27 01:12:54.546414
Model ind 665 epoch 1136 batch: 100 avg loss -2.877498 avg loss no lamb -2.877498 time 2020-06-27 01:13:05.266576
Model ind 665 epoch 1136 batch: 200 avg loss -2.826167 avg loss no lamb -2.826167 time 2020-06-27 01:13:15.880806
Model ind 665 epoch 1136 batch: 300 avg loss -2.846399 avg loss no lamb -2.846399 time 2020-06-27 01:13:26.594324
Model ind 665 epoch 1136 batch: 400 avg loss -2.793838 avg loss no lamb -2.793838 time 2020-06-27 01:13:37.349936
Model ind 665 epoch 1136 batch: 500 avg loss -2.864762 avg loss no lamb -2.864762 time 2020-06-27 01:13:48.283708
Model ind 665 epoch 1136 batch: 600 avg loss -2.929373 avg loss no lamb -2.929373 time 2020-06-27 01:13:59.009902
Model ind 665 epoch 1136 batch: 700 avg loss -2.800014 avg loss no lamb -2.800014 time 2020-06-27 01:14:09.693147
Model ind 665 epoch 1136 batch: 800 avg loss -2.864946 avg loss no lamb -2.864946 time 2020-06-27 01:14:20.946229
last batch sz 10
Pre: time 2020-06-27 01:14:35.155168: 
 	std: 0.0035566238
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9811, 0.9739, 0.9819, 0.9748]
	train_accs: [0.9819, 0.98135, 0.9753, 0.9819, 0.97625]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97868
	best: 0.9817

Starting e_i: 1137
Model ind 665 epoch 1137 batch: 0 avg loss -2.930009 avg loss no lamb -2.930009 time 2020-06-27 01:14:36.276860
Model ind 665 epoch 1137 batch: 100 avg loss -2.937163 avg loss no lamb -2.937163 time 2020-06-27 01:14:47.126184
Model ind 665 epoch 1137 batch: 200 avg loss -2.875599 avg loss no lamb -2.875599 time 2020-06-27 01:14:57.967534
Model ind 665 epoch 1137 batch: 300 avg loss -2.859336 avg loss no lamb -2.859336 time 2020-06-27 01:15:08.757285
Model ind 665 epoch 1137 batch: 400 avg loss -2.809316 avg loss no lamb -2.809316 time 2020-06-27 01:15:19.736378
Model ind 665 epoch 1137 batch: 500 avg loss -2.767491 avg loss no lamb -2.767491 time 2020-06-27 01:15:30.765746
Model ind 665 epoch 1137 batch: 600 avg loss -2.891747 avg loss no lamb -2.891747 time 2020-06-27 01:15:41.777325
Model ind 665 epoch 1137 batch: 700 avg loss -2.767760 avg loss no lamb -2.767760 time 2020-06-27 01:15:52.692987
Model ind 665 epoch 1137 batch: 800 avg loss -2.850377 avg loss no lamb -2.850377 time 2020-06-27 01:16:03.617778
last batch sz 10
Pre: time 2020-06-27 01:16:18.393004: 
 	std: 0.0031891037
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9823, 0.9746, 0.9814, 0.9762]
	train_accs: [0.9817167, 0.9815, 0.9762833, 0.98181665, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97924006
	best: 0.9814

Starting e_i: 1138
Model ind 665 epoch 1138 batch: 0 avg loss -2.950005 avg loss no lamb -2.950005 time 2020-06-27 01:16:19.503907
Model ind 665 epoch 1138 batch: 100 avg loss -2.863184 avg loss no lamb -2.863184 time 2020-06-27 01:16:30.323993
Model ind 665 epoch 1138 batch: 200 avg loss -2.870192 avg loss no lamb -2.870192 time 2020-06-27 01:16:41.170269
Model ind 665 epoch 1138 batch: 300 avg loss -2.836638 avg loss no lamb -2.836638 time 2020-06-27 01:16:52.048668
Model ind 665 epoch 1138 batch: 400 avg loss -2.809100 avg loss no lamb -2.809100 time 2020-06-27 01:17:02.958491
Model ind 665 epoch 1138 batch: 500 avg loss -2.888602 avg loss no lamb -2.888602 time 2020-06-27 01:17:14.099297
Model ind 665 epoch 1138 batch: 600 avg loss -2.875048 avg loss no lamb -2.875048 time 2020-06-27 01:17:24.930829
Model ind 665 epoch 1138 batch: 700 avg loss -2.770027 avg loss no lamb -2.770027 time 2020-06-27 01:17:35.881213
Model ind 665 epoch 1138 batch: 800 avg loss -2.866395 avg loss no lamb -2.866395 time 2020-06-27 01:17:46.662792
last batch sz 10
Pre: time 2020-06-27 01:18:01.097838: 
 	std: 0.0029047588
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9801, 0.9745, 0.9818, 0.9762]
	train_accs: [0.9813, 0.9805167, 0.97578335, 0.9815, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97878
	best: 0.9818

Starting e_i: 1139
Model ind 665 epoch 1139 batch: 0 avg loss -2.947380 avg loss no lamb -2.947380 time 2020-06-27 01:18:02.234335
Model ind 665 epoch 1139 batch: 100 avg loss -2.877100 avg loss no lamb -2.877100 time 2020-06-27 01:18:13.110945
Model ind 665 epoch 1139 batch: 200 avg loss -2.863156 avg loss no lamb -2.863156 time 2020-06-27 01:18:23.890700
Model ind 665 epoch 1139 batch: 300 avg loss -2.891422 avg loss no lamb -2.891422 time 2020-06-27 01:18:34.754575
Model ind 665 epoch 1139 batch: 400 avg loss -2.795786 avg loss no lamb -2.795786 time 2020-06-27 01:18:45.576474
Model ind 665 epoch 1139 batch: 500 avg loss -2.819610 avg loss no lamb -2.819610 time 2020-06-27 01:18:56.516606
Model ind 665 epoch 1139 batch: 600 avg loss -2.891433 avg loss no lamb -2.891433 time 2020-06-27 01:19:07.530085
Model ind 665 epoch 1139 batch: 700 avg loss -2.793952 avg loss no lamb -2.793952 time 2020-06-27 01:19:18.329144
Model ind 665 epoch 1139 batch: 800 avg loss -2.780323 avg loss no lamb -2.780323 time 2020-06-27 01:19:29.287813
last batch sz 10
Pre: time 2020-06-27 01:19:43.414856: 
 	std: 0.0024771062
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9799, 0.9747, 0.981, 0.9767]
	train_accs: [0.9812833, 0.98045, 0.97578335, 0.98116666, 0.97651666]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.9785999
	best: 0.9807

Starting e_i: 1140
Model ind 665 epoch 1140 batch: 0 avg loss -2.978032 avg loss no lamb -2.978032 time 2020-06-27 01:19:44.537571
Model ind 665 epoch 1140 batch: 100 avg loss -2.920952 avg loss no lamb -2.920952 time 2020-06-27 01:19:55.536035
Model ind 665 epoch 1140 batch: 200 avg loss -2.897527 avg loss no lamb -2.897527 time 2020-06-27 01:20:06.301824
Model ind 665 epoch 1140 batch: 300 avg loss -2.895236 avg loss no lamb -2.895236 time 2020-06-27 01:20:17.078259
Model ind 665 epoch 1140 batch: 400 avg loss -2.797684 avg loss no lamb -2.797684 time 2020-06-27 01:20:27.877368
Model ind 665 epoch 1140 batch: 500 avg loss -2.908735 avg loss no lamb -2.908735 time 2020-06-27 01:20:38.922156
Model ind 665 epoch 1140 batch: 600 avg loss -2.872510 avg loss no lamb -2.872510 time 2020-06-27 01:20:49.844854
Model ind 665 epoch 1140 batch: 700 avg loss -2.806119 avg loss no lamb -2.806119 time 2020-06-27 01:21:00.780239
Model ind 665 epoch 1140 batch: 800 avg loss -2.899817 avg loss no lamb -2.899817 time 2020-06-27 01:21:11.674565
last batch sz 10
Pre: time 2020-06-27 01:21:25.654955: 
 	std: 0.0025840986
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9804, 0.9745, 0.9808, 0.9768]
	train_accs: [0.98153335, 0.9809333, 0.9758667, 0.98111665, 0.9769833]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97868
	best: 0.9809

Starting e_i: 1141
Model ind 665 epoch 1141 batch: 0 avg loss -2.915912 avg loss no lamb -2.915912 time 2020-06-27 01:21:27.956831
Model ind 665 epoch 1141 batch: 100 avg loss -2.907836 avg loss no lamb -2.907836 time 2020-06-27 01:21:39.085199
Model ind 665 epoch 1141 batch: 200 avg loss -2.807930 avg loss no lamb -2.807930 time 2020-06-27 01:21:49.485503
Model ind 665 epoch 1141 batch: 300 avg loss -2.875732 avg loss no lamb -2.875732 time 2020-06-27 01:22:00.458877
Model ind 665 epoch 1141 batch: 400 avg loss -2.767359 avg loss no lamb -2.767359 time 2020-06-27 01:22:11.345551
Model ind 665 epoch 1141 batch: 500 avg loss -2.849137 avg loss no lamb -2.849137 time 2020-06-27 01:22:22.489387
Model ind 665 epoch 1141 batch: 600 avg loss -2.904562 avg loss no lamb -2.904562 time 2020-06-27 01:22:33.586447
Model ind 665 epoch 1141 batch: 700 avg loss -2.807117 avg loss no lamb -2.807117 time 2020-06-27 01:22:44.577683
Model ind 665 epoch 1141 batch: 800 avg loss -2.891534 avg loss no lamb -2.891534 time 2020-06-27 01:22:55.292572
last batch sz 10
Pre: time 2020-06-27 01:23:09.682631: 
 	std: 0.0028948253
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9826, 0.9814, 0.9755, 0.9823, 0.9772]
	train_accs: [0.9820667, 0.98118335, 0.97645, 0.98218334, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.9797999
	best: 0.9823

Starting e_i: 1142
Model ind 665 epoch 1142 batch: 0 avg loss -2.963465 avg loss no lamb -2.963465 time 2020-06-27 01:23:10.807893
Model ind 665 epoch 1142 batch: 100 avg loss -2.934175 avg loss no lamb -2.934175 time 2020-06-27 01:23:21.750110
Model ind 665 epoch 1142 batch: 200 avg loss -2.885645 avg loss no lamb -2.885645 time 2020-06-27 01:23:32.651990
Model ind 665 epoch 1142 batch: 300 avg loss -2.853790 avg loss no lamb -2.853790 time 2020-06-27 01:23:43.450429
Model ind 665 epoch 1142 batch: 400 avg loss -2.825122 avg loss no lamb -2.825122 time 2020-06-27 01:23:54.506261
Model ind 665 epoch 1142 batch: 500 avg loss -2.806854 avg loss no lamb -2.806854 time 2020-06-27 01:24:05.390848
Model ind 665 epoch 1142 batch: 600 avg loss -2.877758 avg loss no lamb -2.877758 time 2020-06-27 01:24:16.417133
Model ind 665 epoch 1142 batch: 700 avg loss -2.769674 avg loss no lamb -2.769674 time 2020-06-27 01:24:27.410236
Model ind 665 epoch 1142 batch: 800 avg loss -2.867627 avg loss no lamb -2.867627 time 2020-06-27 01:24:38.191348
last batch sz 10
Pre: time 2020-06-27 01:24:52.306979: 
 	std: 0.0030252272
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9818, 0.9756, 0.9821, 0.976]
	train_accs: [0.98188335, 0.98125, 0.97625, 0.98218334, 0.97686666]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.9795
	best: 0.9821

Starting e_i: 1143
Model ind 665 epoch 1143 batch: 0 avg loss -2.964602 avg loss no lamb -2.964602 time 2020-06-27 01:24:53.401999
Model ind 665 epoch 1143 batch: 100 avg loss -2.877369 avg loss no lamb -2.877369 time 2020-06-27 01:25:04.382642
Model ind 665 epoch 1143 batch: 200 avg loss -2.894010 avg loss no lamb -2.894010 time 2020-06-27 01:25:15.281538
Model ind 665 epoch 1143 batch: 300 avg loss -2.870258 avg loss no lamb -2.870258 time 2020-06-27 01:25:26.313129
Model ind 665 epoch 1143 batch: 400 avg loss -2.789360 avg loss no lamb -2.789360 time 2020-06-27 01:25:36.923994
Model ind 665 epoch 1143 batch: 500 avg loss -2.828360 avg loss no lamb -2.828360 time 2020-06-27 01:25:47.989552
Model ind 665 epoch 1143 batch: 600 avg loss -2.880448 avg loss no lamb -2.880448 time 2020-06-27 01:25:59.036399
Model ind 665 epoch 1143 batch: 700 avg loss -2.771411 avg loss no lamb -2.771411 time 2020-06-27 01:26:09.884568
Model ind 665 epoch 1143 batch: 800 avg loss -2.837577 avg loss no lamb -2.837577 time 2020-06-27 01:26:20.477347
last batch sz 10
Pre: time 2020-06-27 01:26:34.874042: 
 	std: 0.003070905
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9795, 0.9737, 0.9811, 0.9751]
	train_accs: [0.98116666, 0.98035, 0.97535, 0.9813167, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97805995
	best: 0.9811

Starting e_i: 1144
Model ind 665 epoch 1144 batch: 0 avg loss -2.907818 avg loss no lamb -2.907818 time 2020-06-27 01:26:36.019535
Model ind 665 epoch 1144 batch: 100 avg loss -2.867485 avg loss no lamb -2.867485 time 2020-06-27 01:26:47.113535
Model ind 665 epoch 1144 batch: 200 avg loss -2.854484 avg loss no lamb -2.854484 time 2020-06-27 01:26:57.982475
Model ind 665 epoch 1144 batch: 300 avg loss -2.843972 avg loss no lamb -2.843972 time 2020-06-27 01:27:08.846846
Model ind 665 epoch 1144 batch: 400 avg loss -2.822598 avg loss no lamb -2.822598 time 2020-06-27 01:27:19.678405
Model ind 665 epoch 1144 batch: 500 avg loss -2.837572 avg loss no lamb -2.837572 time 2020-06-27 01:27:30.500621
Model ind 665 epoch 1144 batch: 600 avg loss -2.912247 avg loss no lamb -2.912247 time 2020-06-27 01:27:41.364029
Model ind 665 epoch 1144 batch: 700 avg loss -2.850286 avg loss no lamb -2.850286 time 2020-06-27 01:27:52.257280
Model ind 665 epoch 1144 batch: 800 avg loss -2.876653 avg loss no lamb -2.876653 time 2020-06-27 01:28:03.209228
last batch sz 10
Pre: time 2020-06-27 01:28:17.231014: 
 	std: 0.002901435
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9799, 0.9739, 0.9808, 0.9746]
	train_accs: [0.98095, 0.98078334, 0.97566664, 0.9813167, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97776
	best: 0.9808

Starting e_i: 1145
Model ind 665 epoch 1145 batch: 0 avg loss -2.956270 avg loss no lamb -2.956270 time 2020-06-27 01:28:18.371333
Model ind 665 epoch 1145 batch: 100 avg loss -2.913968 avg loss no lamb -2.913968 time 2020-06-27 01:28:29.388547
Model ind 665 epoch 1145 batch: 200 avg loss -2.883421 avg loss no lamb -2.883421 time 2020-06-27 01:28:40.311201
Model ind 665 epoch 1145 batch: 300 avg loss -2.857039 avg loss no lamb -2.857039 time 2020-06-27 01:28:51.341629
Model ind 665 epoch 1145 batch: 400 avg loss -2.755092 avg loss no lamb -2.755092 time 2020-06-27 01:29:02.458106
Model ind 665 epoch 1145 batch: 500 avg loss -2.822014 avg loss no lamb -2.822014 time 2020-06-27 01:29:13.286869
Model ind 665 epoch 1145 batch: 600 avg loss -2.892596 avg loss no lamb -2.892596 time 2020-06-27 01:29:24.163825
Model ind 665 epoch 1145 batch: 700 avg loss -2.790551 avg loss no lamb -2.790551 time 2020-06-27 01:29:35.107441
Model ind 665 epoch 1145 batch: 800 avg loss -2.876042 avg loss no lamb -2.876042 time 2020-06-27 01:29:46.234087
last batch sz 10
Pre: time 2020-06-27 01:30:00.799952: 
 	std: 0.002407808
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9809, 0.9761, 0.9816, 0.9766]
	train_accs: [0.98175, 0.9813, 0.97675, 0.98225, 0.97735]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.97928
	best: 0.9816

Starting e_i: 1146
Model ind 665 epoch 1146 batch: 0 avg loss -2.982553 avg loss no lamb -2.982553 time 2020-06-27 01:30:01.916446
Model ind 665 epoch 1146 batch: 100 avg loss -2.887145 avg loss no lamb -2.887145 time 2020-06-27 01:30:13.059504
Model ind 665 epoch 1146 batch: 200 avg loss -2.845320 avg loss no lamb -2.845320 time 2020-06-27 01:30:24.119262
Model ind 665 epoch 1146 batch: 300 avg loss -2.878935 avg loss no lamb -2.878935 time 2020-06-27 01:30:35.045591
Model ind 665 epoch 1146 batch: 400 avg loss -2.728913 avg loss no lamb -2.728913 time 2020-06-27 01:30:46.180836
Model ind 665 epoch 1146 batch: 500 avg loss -2.867374 avg loss no lamb -2.867374 time 2020-06-27 01:30:57.075198
Model ind 665 epoch 1146 batch: 600 avg loss -2.897753 avg loss no lamb -2.897753 time 2020-06-27 01:31:07.807987
Model ind 665 epoch 1146 batch: 700 avg loss -2.757382 avg loss no lamb -2.757382 time 2020-06-27 01:31:18.843756
Model ind 665 epoch 1146 batch: 800 avg loss -2.931916 avg loss no lamb -2.931916 time 2020-06-27 01:31:29.747032
last batch sz 10
Pre: time 2020-06-27 01:31:44.162347: 
 	std: 0.0029567478
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9809, 0.9745, 0.9817, 0.9757]
	train_accs: [0.98143333, 0.98113334, 0.97618335, 0.9818, 0.97645]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97866
	best: 0.9817

Starting e_i: 1147
Model ind 665 epoch 1147 batch: 0 avg loss -3.001087 avg loss no lamb -3.001087 time 2020-06-27 01:31:45.302561
Model ind 665 epoch 1147 batch: 100 avg loss -2.825248 avg loss no lamb -2.825248 time 2020-06-27 01:31:56.100491
Model ind 665 epoch 1147 batch: 200 avg loss -2.897266 avg loss no lamb -2.897266 time 2020-06-27 01:32:07.062337
Model ind 665 epoch 1147 batch: 300 avg loss -2.874573 avg loss no lamb -2.874573 time 2020-06-27 01:32:18.210266
Model ind 665 epoch 1147 batch: 400 avg loss -2.797966 avg loss no lamb -2.797966 time 2020-06-27 01:32:29.175618
Model ind 665 epoch 1147 batch: 500 avg loss -2.828352 avg loss no lamb -2.828352 time 2020-06-27 01:32:40.287151
Model ind 665 epoch 1147 batch: 600 avg loss -2.841089 avg loss no lamb -2.841089 time 2020-06-27 01:32:51.217562
Model ind 665 epoch 1147 batch: 700 avg loss -2.734432 avg loss no lamb -2.734432 time 2020-06-27 01:33:02.279149
Model ind 665 epoch 1147 batch: 800 avg loss -2.880185 avg loss no lamb -2.880185 time 2020-06-27 01:33:13.294515
last batch sz 10
Pre: time 2020-06-27 01:33:27.743838: 
 	std: 0.0030685419
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.981, 0.9748, 0.9806, 0.9743]
	train_accs: [0.9813333, 0.98113334, 0.97618335, 0.98153335, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.9783
	best: 0.9806

Starting e_i: 1148
Model ind 665 epoch 1148 batch: 0 avg loss -2.966941 avg loss no lamb -2.966941 time 2020-06-27 01:33:28.856141
Model ind 665 epoch 1148 batch: 100 avg loss -2.963692 avg loss no lamb -2.963692 time 2020-06-27 01:33:39.805356
Model ind 665 epoch 1148 batch: 200 avg loss -2.831508 avg loss no lamb -2.831508 time 2020-06-27 01:33:50.709543
Model ind 665 epoch 1148 batch: 300 avg loss -2.909280 avg loss no lamb -2.909280 time 2020-06-27 01:34:01.794166
Model ind 665 epoch 1148 batch: 400 avg loss -2.805140 avg loss no lamb -2.805140 time 2020-06-27 01:34:12.665080
Model ind 665 epoch 1148 batch: 500 avg loss -2.840855 avg loss no lamb -2.840855 time 2020-06-27 01:34:23.157670
Model ind 665 epoch 1148 batch: 600 avg loss -2.908468 avg loss no lamb -2.908468 time 2020-06-27 01:34:33.999854
Model ind 665 epoch 1148 batch: 700 avg loss -2.741285 avg loss no lamb -2.741285 time 2020-06-27 01:34:44.883556
Model ind 665 epoch 1148 batch: 800 avg loss -2.883047 avg loss no lamb -2.883047 time 2020-06-27 01:34:56.065598
last batch sz 10
Pre: time 2020-06-27 01:35:10.381916: 
 	std: 0.0029247832
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9807, 0.9742, 0.9808, 0.9756]
	train_accs: [0.9812833, 0.98115, 0.97595, 0.9816, 0.9763833]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97844
	best: 0.9808

Starting e_i: 1149
Model ind 665 epoch 1149 batch: 0 avg loss -2.987800 avg loss no lamb -2.987800 time 2020-06-27 01:35:11.509335
Model ind 665 epoch 1149 batch: 100 avg loss -2.915423 avg loss no lamb -2.915423 time 2020-06-27 01:35:22.430808
Model ind 665 epoch 1149 batch: 200 avg loss -2.925249 avg loss no lamb -2.925249 time 2020-06-27 01:35:33.459804
Model ind 665 epoch 1149 batch: 300 avg loss -2.907092 avg loss no lamb -2.907092 time 2020-06-27 01:35:44.529072
Model ind 665 epoch 1149 batch: 400 avg loss -2.770423 avg loss no lamb -2.770423 time 2020-06-27 01:35:55.407596
Model ind 665 epoch 1149 batch: 500 avg loss -2.895753 avg loss no lamb -2.895753 time 2020-06-27 01:36:06.399172
Model ind 665 epoch 1149 batch: 600 avg loss -2.864075 avg loss no lamb -2.864075 time 2020-06-27 01:36:17.230803
Model ind 665 epoch 1149 batch: 700 avg loss -2.821896 avg loss no lamb -2.821896 time 2020-06-27 01:36:28.307222
Model ind 665 epoch 1149 batch: 800 avg loss -2.901150 avg loss no lamb -2.901150 time 2020-06-27 01:36:39.285575
last batch sz 10
Pre: time 2020-06-27 01:36:53.858581: 
 	std: 0.0032239056
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.981, 0.9738, 0.9812, 0.9762]
	train_accs: [0.9817, 0.98116666, 0.9755167, 0.9816667, 0.97683334]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97881997
	best: 0.9819

Starting e_i: 1150
Model ind 665 epoch 1150 batch: 0 avg loss -2.978908 avg loss no lamb -2.978908 time 2020-06-27 01:36:55.001519
Model ind 665 epoch 1150 batch: 100 avg loss -2.823801 avg loss no lamb -2.823801 time 2020-06-27 01:37:06.040972
Model ind 665 epoch 1150 batch: 200 avg loss -2.870436 avg loss no lamb -2.870436 time 2020-06-27 01:37:16.855384
Model ind 665 epoch 1150 batch: 300 avg loss -2.871159 avg loss no lamb -2.871159 time 2020-06-27 01:37:27.642593
Model ind 665 epoch 1150 batch: 400 avg loss -2.804720 avg loss no lamb -2.804720 time 2020-06-27 01:37:38.388298
Model ind 665 epoch 1150 batch: 500 avg loss -2.900494 avg loss no lamb -2.900494 time 2020-06-27 01:37:49.367903
Model ind 665 epoch 1150 batch: 600 avg loss -2.887434 avg loss no lamb -2.887434 time 2020-06-27 01:38:00.205933
Model ind 665 epoch 1150 batch: 700 avg loss -2.743903 avg loss no lamb -2.743903 time 2020-06-27 01:38:11.083219
Model ind 665 epoch 1150 batch: 800 avg loss -2.884008 avg loss no lamb -2.884008 time 2020-06-27 01:38:22.115693
last batch sz 10
Pre: time 2020-06-27 01:38:36.514836: 
 	std: 0.0030208598
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9814, 0.9742, 0.9812, 0.9759]
	train_accs: [0.9818, 0.9813, 0.9762, 0.98175, 0.9765667]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97868
	best: 0.9807

Starting e_i: 1151
Model ind 665 epoch 1151 batch: 0 avg loss -2.967221 avg loss no lamb -2.967221 time 2020-06-27 01:38:38.878390
Model ind 665 epoch 1151 batch: 100 avg loss -2.918833 avg loss no lamb -2.918833 time 2020-06-27 01:38:50.131063
Model ind 665 epoch 1151 batch: 200 avg loss -2.823312 avg loss no lamb -2.823312 time 2020-06-27 01:39:01.288115
Model ind 665 epoch 1151 batch: 300 avg loss -2.844057 avg loss no lamb -2.844057 time 2020-06-27 01:39:12.127328
Model ind 665 epoch 1151 batch: 400 avg loss -2.791208 avg loss no lamb -2.791208 time 2020-06-27 01:39:23.231289
Model ind 665 epoch 1151 batch: 500 avg loss -2.887450 avg loss no lamb -2.887450 time 2020-06-27 01:39:34.047511
Model ind 665 epoch 1151 batch: 600 avg loss -2.869816 avg loss no lamb -2.869816 time 2020-06-27 01:39:45.110182
Model ind 665 epoch 1151 batch: 700 avg loss -2.824246 avg loss no lamb -2.824246 time 2020-06-27 01:39:56.049372
Model ind 665 epoch 1151 batch: 800 avg loss -2.850778 avg loss no lamb -2.850778 time 2020-06-27 01:40:07.034943
last batch sz 10
Pre: time 2020-06-27 01:40:21.608036: 
 	std: 0.00330817
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9809, 0.9733, 0.9809, 0.9749]
	train_accs: [0.9811, 0.98035, 0.97543335, 0.9814, 0.9762333]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9781
	best: 0.9809

Starting e_i: 1152
Model ind 665 epoch 1152 batch: 0 avg loss -2.948610 avg loss no lamb -2.948610 time 2020-06-27 01:40:22.676644
Model ind 665 epoch 1152 batch: 100 avg loss -2.933040 avg loss no lamb -2.933040 time 2020-06-27 01:40:33.446459
Model ind 665 epoch 1152 batch: 200 avg loss -2.859741 avg loss no lamb -2.859741 time 2020-06-27 01:40:44.489898
Model ind 665 epoch 1152 batch: 300 avg loss -2.894681 avg loss no lamb -2.894681 time 2020-06-27 01:40:55.321498
Model ind 665 epoch 1152 batch: 400 avg loss -2.815257 avg loss no lamb -2.815257 time 2020-06-27 01:41:06.186523
Model ind 665 epoch 1152 batch: 500 avg loss -2.825715 avg loss no lamb -2.825715 time 2020-06-27 01:41:16.762937
Model ind 665 epoch 1152 batch: 600 avg loss -2.875824 avg loss no lamb -2.875824 time 2020-06-27 01:41:27.467277
Model ind 665 epoch 1152 batch: 700 avg loss -2.820392 avg loss no lamb -2.820392 time 2020-06-27 01:41:38.353277
Model ind 665 epoch 1152 batch: 800 avg loss -2.860795 avg loss no lamb -2.860795 time 2020-06-27 01:41:49.250506
last batch sz 10
Pre: time 2020-06-27 01:42:03.562137: 
 	std: 0.0031352271
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9799, 0.974, 0.9825, 0.9775]
	train_accs: [0.9815, 0.9805667, 0.97581667, 0.98195, 0.97758335]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97918004
	best: 0.9825

Starting e_i: 1153
Model ind 665 epoch 1153 batch: 0 avg loss -2.925810 avg loss no lamb -2.925810 time 2020-06-27 01:42:04.695975
Model ind 665 epoch 1153 batch: 100 avg loss -2.953074 avg loss no lamb -2.953074 time 2020-06-27 01:42:15.405793
Model ind 665 epoch 1153 batch: 200 avg loss -2.821031 avg loss no lamb -2.821031 time 2020-06-27 01:42:26.144784
Model ind 665 epoch 1153 batch: 300 avg loss -2.858484 avg loss no lamb -2.858484 time 2020-06-27 01:42:37.139343
Model ind 665 epoch 1153 batch: 400 avg loss -2.805269 avg loss no lamb -2.805269 time 2020-06-27 01:42:47.931703
Model ind 665 epoch 1153 batch: 500 avg loss -2.806094 avg loss no lamb -2.806094 time 2020-06-27 01:42:58.476937
Model ind 665 epoch 1153 batch: 600 avg loss -2.858861 avg loss no lamb -2.858861 time 2020-06-27 01:43:09.332075
Model ind 665 epoch 1153 batch: 700 avg loss -2.793742 avg loss no lamb -2.793742 time 2020-06-27 01:43:20.354041
Model ind 665 epoch 1153 batch: 800 avg loss -2.902379 avg loss no lamb -2.902379 time 2020-06-27 01:43:31.290930
last batch sz 10
Pre: time 2020-06-27 01:43:45.905811: 
 	std: 0.003955754
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.9792, 0.9718, 0.9807, 0.9724]
	train_accs: [0.9813, 0.98003334, 0.97393334, 0.98118335, 0.97503334]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.9769
	best: 0.9804

Starting e_i: 1154
Model ind 665 epoch 1154 batch: 0 avg loss -2.937537 avg loss no lamb -2.937537 time 2020-06-27 01:43:47.037037
Model ind 665 epoch 1154 batch: 100 avg loss -2.833889 avg loss no lamb -2.833889 time 2020-06-27 01:43:58.115612
Model ind 665 epoch 1154 batch: 200 avg loss -2.844841 avg loss no lamb -2.844841 time 2020-06-27 01:44:09.074934
Model ind 665 epoch 1154 batch: 300 avg loss -2.894940 avg loss no lamb -2.894940 time 2020-06-27 01:44:20.032101
Model ind 665 epoch 1154 batch: 400 avg loss -2.800208 avg loss no lamb -2.800208 time 2020-06-27 01:44:30.756534
Model ind 665 epoch 1154 batch: 500 avg loss -2.855106 avg loss no lamb -2.855106 time 2020-06-27 01:44:41.426848
Model ind 665 epoch 1154 batch: 600 avg loss -2.897863 avg loss no lamb -2.897863 time 2020-06-27 01:44:52.311326
Model ind 665 epoch 1154 batch: 700 avg loss -2.759109 avg loss no lamb -2.759109 time 2020-06-27 01:45:03.341963
Model ind 665 epoch 1154 batch: 800 avg loss -2.906772 avg loss no lamb -2.906772 time 2020-06-27 01:45:14.075380
last batch sz 10
Pre: time 2020-06-27 01:45:28.345626: 
 	std: 0.0028288541
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9797, 0.9738, 0.9801, 0.9751]
	train_accs: [0.9808667, 0.9802333, 0.9755833, 0.98106664, 0.9766]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97786
	best: 0.9801

Starting e_i: 1155
Model ind 665 epoch 1155 batch: 0 avg loss -2.975153 avg loss no lamb -2.975153 time 2020-06-27 01:45:29.414609
Model ind 665 epoch 1155 batch: 100 avg loss -2.844155 avg loss no lamb -2.844155 time 2020-06-27 01:45:40.241345
Model ind 665 epoch 1155 batch: 200 avg loss -2.877908 avg loss no lamb -2.877908 time 2020-06-27 01:45:51.449524
Model ind 665 epoch 1155 batch: 300 avg loss -2.888195 avg loss no lamb -2.888195 time 2020-06-27 01:46:02.559247
Model ind 665 epoch 1155 batch: 400 avg loss -2.850558 avg loss no lamb -2.850558 time 2020-06-27 01:46:13.558337
Model ind 665 epoch 1155 batch: 500 avg loss -2.848099 avg loss no lamb -2.848099 time 2020-06-27 01:46:24.663518
Model ind 665 epoch 1155 batch: 600 avg loss -2.886111 avg loss no lamb -2.886111 time 2020-06-27 01:46:35.769202
Model ind 665 epoch 1155 batch: 700 avg loss -2.774250 avg loss no lamb -2.774250 time 2020-06-27 01:46:46.786607
Model ind 665 epoch 1155 batch: 800 avg loss -2.928044 avg loss no lamb -2.928044 time 2020-06-27 01:46:57.860930
last batch sz 10
Pre: time 2020-06-27 01:47:12.282149: 
 	std: 0.0033783994
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9803, 0.9735, 0.9816, 0.9754]
	train_accs: [0.98216665, 0.98108333, 0.97543335, 0.9821333, 0.97675]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97848
	best: 0.9816

Starting e_i: 1156
Model ind 665 epoch 1156 batch: 0 avg loss -2.917742 avg loss no lamb -2.917742 time 2020-06-27 01:47:13.420810
Model ind 665 epoch 1156 batch: 100 avg loss -2.849108 avg loss no lamb -2.849108 time 2020-06-27 01:47:24.150807
Model ind 665 epoch 1156 batch: 200 avg loss -2.821356 avg loss no lamb -2.821356 time 2020-06-27 01:47:35.096516
Model ind 665 epoch 1156 batch: 300 avg loss -2.828198 avg loss no lamb -2.828198 time 2020-06-27 01:47:46.061504
Model ind 665 epoch 1156 batch: 400 avg loss -2.779168 avg loss no lamb -2.779168 time 2020-06-27 01:47:57.058299
Model ind 665 epoch 1156 batch: 500 avg loss -2.840844 avg loss no lamb -2.840844 time 2020-06-27 01:48:07.925207
Model ind 665 epoch 1156 batch: 600 avg loss -2.904209 avg loss no lamb -2.904209 time 2020-06-27 01:48:18.848149
Model ind 665 epoch 1156 batch: 700 avg loss -2.805213 avg loss no lamb -2.805213 time 2020-06-27 01:48:29.812391
Model ind 665 epoch 1156 batch: 800 avg loss -2.895320 avg loss no lamb -2.895320 time 2020-06-27 01:48:40.877119
last batch sz 10
Pre: time 2020-06-27 01:48:55.030087: 
 	std: 0.0027096956
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9793, 0.9737, 0.9802, 0.9759]
	train_accs: [0.9813167, 0.9805167, 0.9755333, 0.9810333, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97796
	best: 0.9807

Starting e_i: 1157
Model ind 665 epoch 1157 batch: 0 avg loss -2.933295 avg loss no lamb -2.933295 time 2020-06-27 01:48:56.121854
Model ind 665 epoch 1157 batch: 100 avg loss -2.877514 avg loss no lamb -2.877514 time 2020-06-27 01:49:07.211479
Model ind 665 epoch 1157 batch: 200 avg loss -2.836262 avg loss no lamb -2.836262 time 2020-06-27 01:49:18.252758
Model ind 665 epoch 1157 batch: 300 avg loss -2.893041 avg loss no lamb -2.893041 time 2020-06-27 01:49:29.110670
Model ind 665 epoch 1157 batch: 400 avg loss -2.791988 avg loss no lamb -2.791988 time 2020-06-27 01:49:40.109688
Model ind 665 epoch 1157 batch: 500 avg loss -2.779133 avg loss no lamb -2.779133 time 2020-06-27 01:49:50.915273
Model ind 665 epoch 1157 batch: 600 avg loss -2.877446 avg loss no lamb -2.877446 time 2020-06-27 01:50:01.760816
Model ind 665 epoch 1157 batch: 700 avg loss -2.741431 avg loss no lamb -2.741431 time 2020-06-27 01:50:12.851016
Model ind 665 epoch 1157 batch: 800 avg loss -2.915571 avg loss no lamb -2.915571 time 2020-06-27 01:50:23.764051
last batch sz 10
Pre: time 2020-06-27 01:50:37.993874: 
 	std: 0.003493653
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9795, 0.9721, 0.98, 0.9736]
	train_accs: [0.9809667, 0.9799167, 0.9741667, 0.98066664, 0.97581667]
	best_train_sub_head: 0
	worst: 0.9721
	avg: 0.97708
	best: 0.9802

Starting e_i: 1158
Model ind 665 epoch 1158 batch: 0 avg loss -2.959425 avg loss no lamb -2.959425 time 2020-06-27 01:50:39.129269
Model ind 665 epoch 1158 batch: 100 avg loss -2.909452 avg loss no lamb -2.909452 time 2020-06-27 01:50:49.860752
Model ind 665 epoch 1158 batch: 200 avg loss -2.869530 avg loss no lamb -2.869530 time 2020-06-27 01:51:00.881234
Model ind 665 epoch 1158 batch: 300 avg loss -2.807278 avg loss no lamb -2.807278 time 2020-06-27 01:51:12.100484
Model ind 665 epoch 1158 batch: 400 avg loss -2.809786 avg loss no lamb -2.809786 time 2020-06-27 01:51:23.077420
Model ind 665 epoch 1158 batch: 500 avg loss -2.826792 avg loss no lamb -2.826792 time 2020-06-27 01:51:33.956946
Model ind 665 epoch 1158 batch: 600 avg loss -2.826906 avg loss no lamb -2.826906 time 2020-06-27 01:51:45.215741
Model ind 665 epoch 1158 batch: 700 avg loss -2.798757 avg loss no lamb -2.798757 time 2020-06-27 01:51:56.156903
Model ind 665 epoch 1158 batch: 800 avg loss -2.850833 avg loss no lamb -2.850833 time 2020-06-27 01:52:07.159040
last batch sz 10
Pre: time 2020-06-27 01:52:21.454141: 
 	std: 0.0031719978
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9806, 0.9739, 0.9804, 0.9743]
	train_accs: [0.98123336, 0.98045, 0.97473335, 0.98118335, 0.97535]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97798
	best: 0.9807

Starting e_i: 1159
Model ind 665 epoch 1159 batch: 0 avg loss -2.926185 avg loss no lamb -2.926185 time 2020-06-27 01:52:22.669370
Model ind 665 epoch 1159 batch: 100 avg loss -2.909582 avg loss no lamb -2.909582 time 2020-06-27 01:52:33.682269
Model ind 665 epoch 1159 batch: 200 avg loss -2.905357 avg loss no lamb -2.905357 time 2020-06-27 01:52:44.793947
Model ind 665 epoch 1159 batch: 300 avg loss -2.854143 avg loss no lamb -2.854143 time 2020-06-27 01:52:55.852038
Model ind 665 epoch 1159 batch: 400 avg loss -2.739324 avg loss no lamb -2.739324 time 2020-06-27 01:53:06.991157
Model ind 665 epoch 1159 batch: 500 avg loss -2.826380 avg loss no lamb -2.826380 time 2020-06-27 01:53:17.725360
Model ind 665 epoch 1159 batch: 600 avg loss -2.890648 avg loss no lamb -2.890648 time 2020-06-27 01:53:28.490544
Model ind 665 epoch 1159 batch: 700 avg loss -2.809894 avg loss no lamb -2.809894 time 2020-06-27 01:53:39.365831
Model ind 665 epoch 1159 batch: 800 avg loss -2.851393 avg loss no lamb -2.851393 time 2020-06-27 01:53:50.234600
last batch sz 10
Pre: time 2020-06-27 01:54:04.742155: 
 	std: 0.0034238065
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9814, 0.9729, 0.9806, 0.975]
	train_accs: [0.9813833, 0.9809333, 0.97475, 0.98146665, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.9780399
	best: 0.9806

Starting e_i: 1160
Model ind 665 epoch 1160 batch: 0 avg loss -2.947963 avg loss no lamb -2.947963 time 2020-06-27 01:54:05.852393
Model ind 665 epoch 1160 batch: 100 avg loss -2.912712 avg loss no lamb -2.912712 time 2020-06-27 01:54:16.777740
Model ind 665 epoch 1160 batch: 200 avg loss -2.877485 avg loss no lamb -2.877485 time 2020-06-27 01:54:27.510238
Model ind 665 epoch 1160 batch: 300 avg loss -2.860143 avg loss no lamb -2.860143 time 2020-06-27 01:54:38.438697
Model ind 665 epoch 1160 batch: 400 avg loss -2.812639 avg loss no lamb -2.812639 time 2020-06-27 01:54:49.251377
Model ind 665 epoch 1160 batch: 500 avg loss -2.810154 avg loss no lamb -2.810154 time 2020-06-27 01:55:00.066193
Model ind 665 epoch 1160 batch: 600 avg loss -2.879934 avg loss no lamb -2.879934 time 2020-06-27 01:55:11.123667
Model ind 665 epoch 1160 batch: 700 avg loss -2.760487 avg loss no lamb -2.760487 time 2020-06-27 01:55:21.855545
Model ind 665 epoch 1160 batch: 800 avg loss -2.887970 avg loss no lamb -2.887970 time 2020-06-27 01:55:32.743092
last batch sz 10
Pre: time 2020-06-27 01:55:46.686101: 
 	std: 0.003471833
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9806, 0.9734, 0.9819, 0.9751]
	train_accs: [0.98151666, 0.9812667, 0.97576666, 0.9817167, 0.97606665]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97841996
	best: 0.9819

Starting e_i: 1161
Model ind 665 epoch 1161 batch: 0 avg loss -2.925806 avg loss no lamb -2.925806 time 2020-06-27 01:55:48.951385
Model ind 665 epoch 1161 batch: 100 avg loss -2.960380 avg loss no lamb -2.960380 time 2020-06-27 01:55:59.960838
Model ind 665 epoch 1161 batch: 200 avg loss -2.873162 avg loss no lamb -2.873162 time 2020-06-27 01:56:10.921423
Model ind 665 epoch 1161 batch: 300 avg loss -2.849494 avg loss no lamb -2.849494 time 2020-06-27 01:56:21.841485
Model ind 665 epoch 1161 batch: 400 avg loss -2.822420 avg loss no lamb -2.822420 time 2020-06-27 01:56:32.860903
Model ind 665 epoch 1161 batch: 500 avg loss -2.866748 avg loss no lamb -2.866748 time 2020-06-27 01:56:43.673044
Model ind 665 epoch 1161 batch: 600 avg loss -2.847656 avg loss no lamb -2.847656 time 2020-06-27 01:56:54.286828
Model ind 665 epoch 1161 batch: 700 avg loss -2.760442 avg loss no lamb -2.760442 time 2020-06-27 01:57:05.135140
Model ind 665 epoch 1161 batch: 800 avg loss -2.855633 avg loss no lamb -2.855633 time 2020-06-27 01:57:15.924281
last batch sz 10
Pre: time 2020-06-27 01:57:30.173642: 
 	std: 0.0020874897
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9818, 0.977, 0.9815, 0.9779]
	train_accs: [0.9815, 0.9813833, 0.97718334, 0.9816, 0.97748333]
	best_train_sub_head: 3
	worst: 0.977
	avg: 0.97998
	best: 0.9815

Starting e_i: 1162
Model ind 665 epoch 1162 batch: 0 avg loss -2.939893 avg loss no lamb -2.939893 time 2020-06-27 01:57:31.265060
Model ind 665 epoch 1162 batch: 100 avg loss -2.931671 avg loss no lamb -2.931671 time 2020-06-27 01:57:42.358299
Model ind 665 epoch 1162 batch: 200 avg loss -2.896246 avg loss no lamb -2.896246 time 2020-06-27 01:57:53.174212
Model ind 665 epoch 1162 batch: 300 avg loss -2.871444 avg loss no lamb -2.871444 time 2020-06-27 01:58:03.870223
Model ind 665 epoch 1162 batch: 400 avg loss -2.718094 avg loss no lamb -2.718094 time 2020-06-27 01:58:14.363604
Model ind 665 epoch 1162 batch: 500 avg loss -2.884557 avg loss no lamb -2.884557 time 2020-06-27 01:58:25.292810
Model ind 665 epoch 1162 batch: 600 avg loss -2.866393 avg loss no lamb -2.866393 time 2020-06-27 01:58:36.283360
Model ind 665 epoch 1162 batch: 700 avg loss -2.738477 avg loss no lamb -2.738477 time 2020-06-27 01:58:47.097405
Model ind 665 epoch 1162 batch: 800 avg loss -2.831184 avg loss no lamb -2.831184 time 2020-06-27 01:58:57.868727
last batch sz 10
Pre: time 2020-06-27 01:59:12.159943: 
 	std: 0.002414469
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9797, 0.9737, 0.9796, 0.9763]
	train_accs: [0.9812, 0.9806, 0.9757, 0.9809833, 0.97683334]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97778
	best: 0.9796

Starting e_i: 1163
Model ind 665 epoch 1163 batch: 0 avg loss -2.927260 avg loss no lamb -2.927260 time 2020-06-27 01:59:13.317557
Model ind 665 epoch 1163 batch: 100 avg loss -2.861763 avg loss no lamb -2.861763 time 2020-06-27 01:59:24.299809
Model ind 665 epoch 1163 batch: 200 avg loss -2.889474 avg loss no lamb -2.889474 time 2020-06-27 01:59:35.185483
Model ind 665 epoch 1163 batch: 300 avg loss -2.903903 avg loss no lamb -2.903903 time 2020-06-27 01:59:46.046841
Model ind 665 epoch 1163 batch: 400 avg loss -2.811872 avg loss no lamb -2.811872 time 2020-06-27 01:59:56.694882
Model ind 665 epoch 1163 batch: 500 avg loss -2.881574 avg loss no lamb -2.881574 time 2020-06-27 02:00:08.015504
Model ind 665 epoch 1163 batch: 600 avg loss -2.889807 avg loss no lamb -2.889807 time 2020-06-27 02:00:19.187284
Model ind 665 epoch 1163 batch: 700 avg loss -2.854204 avg loss no lamb -2.854204 time 2020-06-27 02:00:30.082198
Model ind 665 epoch 1163 batch: 800 avg loss -2.834625 avg loss no lamb -2.834625 time 2020-06-27 02:00:40.774650
last batch sz 10
Pre: time 2020-06-27 02:00:55.107632: 
 	std: 0.002947801
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9809, 0.9742, 0.9809, 0.9761]
	train_accs: [0.9818, 0.9812667, 0.9758833, 0.98178333, 0.9769167]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97868
	best: 0.9813

Starting e_i: 1164
Model ind 665 epoch 1164 batch: 0 avg loss -2.925530 avg loss no lamb -2.925530 time 2020-06-27 02:00:56.189136
Model ind 665 epoch 1164 batch: 100 avg loss -2.805148 avg loss no lamb -2.805148 time 2020-06-27 02:01:07.278272
Model ind 665 epoch 1164 batch: 200 avg loss -2.871838 avg loss no lamb -2.871838 time 2020-06-27 02:01:17.994756
Model ind 665 epoch 1164 batch: 300 avg loss -2.861863 avg loss no lamb -2.861863 time 2020-06-27 02:01:28.855676
Model ind 665 epoch 1164 batch: 400 avg loss -2.788097 avg loss no lamb -2.788097 time 2020-06-27 02:01:39.597970
Model ind 665 epoch 1164 batch: 500 avg loss -2.828444 avg loss no lamb -2.828444 time 2020-06-27 02:01:50.585904
Model ind 665 epoch 1164 batch: 600 avg loss -2.840504 avg loss no lamb -2.840504 time 2020-06-27 02:02:01.529477
Model ind 665 epoch 1164 batch: 700 avg loss -2.839715 avg loss no lamb -2.839715 time 2020-06-27 02:02:12.277832
Model ind 665 epoch 1164 batch: 800 avg loss -2.867746 avg loss no lamb -2.867746 time 2020-06-27 02:02:23.190325
last batch sz 10
Pre: time 2020-06-27 02:02:37.194173: 
 	std: 0.002694756
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9794, 0.9736, 0.9797, 0.9747]
	train_accs: [0.9809667, 0.98018336, 0.9752333, 0.98083335, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97742003
	best: 0.9797

Starting e_i: 1165
Model ind 665 epoch 1165 batch: 0 avg loss -2.916043 avg loss no lamb -2.916043 time 2020-06-27 02:02:38.354361
Model ind 665 epoch 1165 batch: 100 avg loss -2.891163 avg loss no lamb -2.891163 time 2020-06-27 02:02:49.342873
Model ind 665 epoch 1165 batch: 200 avg loss -2.870829 avg loss no lamb -2.870829 time 2020-06-27 02:03:00.232891
Model ind 665 epoch 1165 batch: 300 avg loss -2.888501 avg loss no lamb -2.888501 time 2020-06-27 02:03:11.152097
Model ind 665 epoch 1165 batch: 400 avg loss -2.765828 avg loss no lamb -2.765828 time 2020-06-27 02:03:22.082537
Model ind 665 epoch 1165 batch: 500 avg loss -2.865844 avg loss no lamb -2.865844 time 2020-06-27 02:03:32.979848
Model ind 665 epoch 1165 batch: 600 avg loss -2.891778 avg loss no lamb -2.891778 time 2020-06-27 02:03:44.030347
Model ind 665 epoch 1165 batch: 700 avg loss -2.799387 avg loss no lamb -2.799387 time 2020-06-27 02:03:54.940893
Model ind 665 epoch 1165 batch: 800 avg loss -2.836594 avg loss no lamb -2.836594 time 2020-06-27 02:04:05.760515
last batch sz 10
Pre: time 2020-06-27 02:04:19.601536: 
 	std: 0.002780933
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9801, 0.974, 0.9808, 0.9763]
	train_accs: [0.98185, 0.98123336, 0.976, 0.98178333, 0.97686666]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97841996
	best: 0.9809

Starting e_i: 1166
Model ind 665 epoch 1166 batch: 0 avg loss -2.952510 avg loss no lamb -2.952510 time 2020-06-27 02:04:20.692717
Model ind 665 epoch 1166 batch: 100 avg loss -2.935971 avg loss no lamb -2.935971 time 2020-06-27 02:04:31.546022
Model ind 665 epoch 1166 batch: 200 avg loss -2.871064 avg loss no lamb -2.871064 time 2020-06-27 02:04:42.448343
Model ind 665 epoch 1166 batch: 300 avg loss -2.841289 avg loss no lamb -2.841289 time 2020-06-27 02:04:53.315594
Model ind 665 epoch 1166 batch: 400 avg loss -2.761569 avg loss no lamb -2.761569 time 2020-06-27 02:05:04.042767
Model ind 665 epoch 1166 batch: 500 avg loss -2.850702 avg loss no lamb -2.850702 time 2020-06-27 02:05:14.518922
Model ind 665 epoch 1166 batch: 600 avg loss -2.861392 avg loss no lamb -2.861392 time 2020-06-27 02:05:25.410349
Model ind 665 epoch 1166 batch: 700 avg loss -2.744399 avg loss no lamb -2.744399 time 2020-06-27 02:05:36.348203
Model ind 665 epoch 1166 batch: 800 avg loss -2.886033 avg loss no lamb -2.886033 time 2020-06-27 02:05:47.088785
last batch sz 10
Pre: time 2020-06-27 02:06:00.886944: 
 	std: 0.0029110867
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9801, 0.9744, 0.9811, 0.9757]
	train_accs: [0.9816, 0.98075, 0.9752333, 0.9813333, 0.9767333]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97854006
	best: 0.9814

Starting e_i: 1167
Model ind 665 epoch 1167 batch: 0 avg loss -2.880848 avg loss no lamb -2.880848 time 2020-06-27 02:06:02.016194
Model ind 665 epoch 1167 batch: 100 avg loss -2.830569 avg loss no lamb -2.830569 time 2020-06-27 02:06:12.600913
Model ind 665 epoch 1167 batch: 200 avg loss -2.907120 avg loss no lamb -2.907120 time 2020-06-27 02:06:23.653946
Model ind 665 epoch 1167 batch: 300 avg loss -2.824182 avg loss no lamb -2.824182 time 2020-06-27 02:06:34.759422
Model ind 665 epoch 1167 batch: 400 avg loss -2.819214 avg loss no lamb -2.819214 time 2020-06-27 02:06:45.567503
Model ind 665 epoch 1167 batch: 500 avg loss -2.852064 avg loss no lamb -2.852064 time 2020-06-27 02:06:56.261391
Model ind 665 epoch 1167 batch: 600 avg loss -2.888350 avg loss no lamb -2.888350 time 2020-06-27 02:07:07.287255
Model ind 665 epoch 1167 batch: 700 avg loss -2.769494 avg loss no lamb -2.769494 time 2020-06-27 02:07:18.394789
Model ind 665 epoch 1167 batch: 800 avg loss -2.806889 avg loss no lamb -2.806889 time 2020-06-27 02:07:29.231513
last batch sz 10
Pre: time 2020-06-27 02:07:43.381043: 
 	std: 0.0032859675
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9805, 0.9732, 0.9811, 0.9759]
	train_accs: [0.9817, 0.98095, 0.9755833, 0.98156667, 0.97705]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.9784201
	best: 0.9814

Starting e_i: 1168
Model ind 665 epoch 1168 batch: 0 avg loss -2.950531 avg loss no lamb -2.950531 time 2020-06-27 02:07:44.504055
Model ind 665 epoch 1168 batch: 100 avg loss -2.839182 avg loss no lamb -2.839182 time 2020-06-27 02:07:55.328354
Model ind 665 epoch 1168 batch: 200 avg loss -2.794009 avg loss no lamb -2.794009 time 2020-06-27 02:08:06.301830
Model ind 665 epoch 1168 batch: 300 avg loss -2.845125 avg loss no lamb -2.845125 time 2020-06-27 02:08:17.539695
Model ind 665 epoch 1168 batch: 400 avg loss -2.797676 avg loss no lamb -2.797676 time 2020-06-27 02:08:28.458624
Model ind 665 epoch 1168 batch: 500 avg loss -2.900248 avg loss no lamb -2.900248 time 2020-06-27 02:08:39.402986
Model ind 665 epoch 1168 batch: 600 avg loss -2.898682 avg loss no lamb -2.898682 time 2020-06-27 02:08:50.379034
Model ind 665 epoch 1168 batch: 700 avg loss -2.818470 avg loss no lamb -2.818470 time 2020-06-27 02:09:01.320630
Model ind 665 epoch 1168 batch: 800 avg loss -2.828411 avg loss no lamb -2.828411 time 2020-06-27 02:09:12.258262
last batch sz 10
Pre: time 2020-06-27 02:09:26.145252: 
 	std: 0.003187105
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9785, 0.9712, 0.9784, 0.974]
	train_accs: [0.9810167, 0.98006666, 0.9742, 0.98055, 0.9759667]
	best_train_sub_head: 0
	worst: 0.9712
	avg: 0.97631997
	best: 0.9795

Starting e_i: 1169
Model ind 665 epoch 1169 batch: 0 avg loss -2.944502 avg loss no lamb -2.944502 time 2020-06-27 02:09:27.257837
Model ind 665 epoch 1169 batch: 100 avg loss -2.883561 avg loss no lamb -2.883561 time 2020-06-27 02:09:37.927870
Model ind 665 epoch 1169 batch: 200 avg loss -2.852476 avg loss no lamb -2.852476 time 2020-06-27 02:09:49.058462
Model ind 665 epoch 1169 batch: 300 avg loss -2.906535 avg loss no lamb -2.906535 time 2020-06-27 02:10:00.206272
Model ind 665 epoch 1169 batch: 400 avg loss -2.827644 avg loss no lamb -2.827644 time 2020-06-27 02:10:11.174590
Model ind 665 epoch 1169 batch: 500 avg loss -2.876832 avg loss no lamb -2.876832 time 2020-06-27 02:10:22.277962
Model ind 665 epoch 1169 batch: 600 avg loss -2.907176 avg loss no lamb -2.907176 time 2020-06-27 02:10:33.329765
Model ind 665 epoch 1169 batch: 700 avg loss -2.787587 avg loss no lamb -2.787587 time 2020-06-27 02:10:44.362603
Model ind 665 epoch 1169 batch: 800 avg loss -2.933811 avg loss no lamb -2.933811 time 2020-06-27 02:10:55.499230
last batch sz 10
Pre: time 2020-06-27 02:11:09.872085: 
 	std: 0.0022458916
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.98, 0.9754, 0.9809, 0.9764]
	train_accs: [0.98105, 0.98065, 0.9762833, 0.9812, 0.9769]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.9786
	best: 0.9809

Starting e_i: 1170
Model ind 665 epoch 1170 batch: 0 avg loss -2.944056 avg loss no lamb -2.944056 time 2020-06-27 02:11:11.015773
Model ind 665 epoch 1170 batch: 100 avg loss -2.905275 avg loss no lamb -2.905275 time 2020-06-27 02:11:21.830158
Model ind 665 epoch 1170 batch: 200 avg loss -2.881469 avg loss no lamb -2.881469 time 2020-06-27 02:11:32.766844
Model ind 665 epoch 1170 batch: 300 avg loss -2.854279 avg loss no lamb -2.854279 time 2020-06-27 02:11:43.766203
Model ind 665 epoch 1170 batch: 400 avg loss -2.772995 avg loss no lamb -2.772995 time 2020-06-27 02:11:54.580687
Model ind 665 epoch 1170 batch: 500 avg loss -2.808434 avg loss no lamb -2.808434 time 2020-06-27 02:12:05.608245
Model ind 665 epoch 1170 batch: 600 avg loss -2.898804 avg loss no lamb -2.898804 time 2020-06-27 02:12:16.386701
Model ind 665 epoch 1170 batch: 700 avg loss -2.803581 avg loss no lamb -2.803581 time 2020-06-27 02:12:27.212974
Model ind 665 epoch 1170 batch: 800 avg loss -2.874308 avg loss no lamb -2.874308 time 2020-06-27 02:12:38.157812
last batch sz 10
Pre: time 2020-06-27 02:12:52.388744: 
 	std: 0.0028694335
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9805, 0.9747, 0.9813, 0.9758]
	train_accs: [0.98148334, 0.98065, 0.97595, 0.98158336, 0.9766833]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97872
	best: 0.9813

Starting e_i: 1171
Model ind 665 epoch 1171 batch: 0 avg loss -2.967666 avg loss no lamb -2.967666 time 2020-06-27 02:12:54.647082
Model ind 665 epoch 1171 batch: 100 avg loss -2.969854 avg loss no lamb -2.969854 time 2020-06-27 02:13:05.503328
Model ind 665 epoch 1171 batch: 200 avg loss -2.891497 avg loss no lamb -2.891497 time 2020-06-27 02:13:16.512169
Model ind 665 epoch 1171 batch: 300 avg loss -2.843618 avg loss no lamb -2.843618 time 2020-06-27 02:13:27.498291
Model ind 665 epoch 1171 batch: 400 avg loss -2.766776 avg loss no lamb -2.766776 time 2020-06-27 02:13:38.394270
Model ind 665 epoch 1171 batch: 500 avg loss -2.889318 avg loss no lamb -2.889318 time 2020-06-27 02:13:49.476980
Model ind 665 epoch 1171 batch: 600 avg loss -2.887784 avg loss no lamb -2.887784 time 2020-06-27 02:14:00.447890
Model ind 665 epoch 1171 batch: 700 avg loss -2.817089 avg loss no lamb -2.817089 time 2020-06-27 02:14:11.508461
Model ind 665 epoch 1171 batch: 800 avg loss -2.904912 avg loss no lamb -2.904912 time 2020-06-27 02:14:22.278769
last batch sz 10
Pre: time 2020-06-27 02:14:36.721605: 
 	std: 0.0028858304
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9802, 0.974, 0.9805, 0.9752]
	train_accs: [0.98125, 0.9806, 0.9752833, 0.9809667, 0.97625]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.9781
	best: 0.9806

Starting e_i: 1172
Model ind 665 epoch 1172 batch: 0 avg loss -2.936705 avg loss no lamb -2.936705 time 2020-06-27 02:14:37.804607
Model ind 665 epoch 1172 batch: 100 avg loss -2.867107 avg loss no lamb -2.867107 time 2020-06-27 02:14:48.975012
Model ind 665 epoch 1172 batch: 200 avg loss -2.891285 avg loss no lamb -2.891285 time 2020-06-27 02:15:00.102504
Model ind 665 epoch 1172 batch: 300 avg loss -2.916877 avg loss no lamb -2.916877 time 2020-06-27 02:15:11.073225
Model ind 665 epoch 1172 batch: 400 avg loss -2.711610 avg loss no lamb -2.711610 time 2020-06-27 02:15:21.880734
Model ind 665 epoch 1172 batch: 500 avg loss -2.877030 avg loss no lamb -2.877030 time 2020-06-27 02:15:32.968770
Model ind 665 epoch 1172 batch: 600 avg loss -2.858041 avg loss no lamb -2.858041 time 2020-06-27 02:15:44.060717
Model ind 665 epoch 1172 batch: 700 avg loss -2.846683 avg loss no lamb -2.846683 time 2020-06-27 02:15:55.025793
Model ind 665 epoch 1172 batch: 800 avg loss -2.903036 avg loss no lamb -2.903036 time 2020-06-27 02:16:05.930210
last batch sz 10
Pre: time 2020-06-27 02:16:19.894631: 
 	std: 0.0032283897
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9799, 0.9736, 0.9811, 0.9745]
	train_accs: [0.98146665, 0.98073334, 0.9743, 0.98185, 0.9755667]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97796
	best: 0.9811

Starting e_i: 1173
Model ind 665 epoch 1173 batch: 0 avg loss -2.946034 avg loss no lamb -2.946034 time 2020-06-27 02:16:21.051859
Model ind 665 epoch 1173 batch: 100 avg loss -2.886015 avg loss no lamb -2.886015 time 2020-06-27 02:16:32.052393
Model ind 665 epoch 1173 batch: 200 avg loss -2.960680 avg loss no lamb -2.960680 time 2020-06-27 02:16:43.056344
Model ind 665 epoch 1173 batch: 300 avg loss -2.842787 avg loss no lamb -2.842787 time 2020-06-27 02:16:54.068070
Model ind 665 epoch 1173 batch: 400 avg loss -2.816757 avg loss no lamb -2.816757 time 2020-06-27 02:17:04.907888
Model ind 665 epoch 1173 batch: 500 avg loss -2.889983 avg loss no lamb -2.889983 time 2020-06-27 02:17:15.668538
Model ind 665 epoch 1173 batch: 600 avg loss -2.863944 avg loss no lamb -2.863944 time 2020-06-27 02:17:26.367435
Model ind 665 epoch 1173 batch: 700 avg loss -2.785996 avg loss no lamb -2.785996 time 2020-06-27 02:17:37.188333
Model ind 665 epoch 1173 batch: 800 avg loss -2.880067 avg loss no lamb -2.880067 time 2020-06-27 02:17:48.110292
last batch sz 10
Pre: time 2020-06-27 02:18:02.406844: 
 	std: 0.0030295874
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9792, 0.9742, 0.9811, 0.9749]
	train_accs: [0.9816167, 0.98, 0.97543335, 0.98146665, 0.97578335]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97814006
	best: 0.9813

Starting e_i: 1174
Model ind 665 epoch 1174 batch: 0 avg loss -2.895600 avg loss no lamb -2.895600 time 2020-06-27 02:18:03.543350
Model ind 665 epoch 1174 batch: 100 avg loss -2.916089 avg loss no lamb -2.916089 time 2020-06-27 02:18:14.151128
Model ind 665 epoch 1174 batch: 200 avg loss -2.865089 avg loss no lamb -2.865089 time 2020-06-27 02:18:25.050945
Model ind 665 epoch 1174 batch: 300 avg loss -2.811086 avg loss no lamb -2.811086 time 2020-06-27 02:18:35.828720
Model ind 665 epoch 1174 batch: 400 avg loss -2.842061 avg loss no lamb -2.842061 time 2020-06-27 02:18:46.778895
Model ind 665 epoch 1174 batch: 500 avg loss -2.881242 avg loss no lamb -2.881242 time 2020-06-27 02:18:57.786649
Model ind 665 epoch 1174 batch: 600 avg loss -2.853050 avg loss no lamb -2.853050 time 2020-06-27 02:19:08.902232
Model ind 665 epoch 1174 batch: 700 avg loss -2.779676 avg loss no lamb -2.779676 time 2020-06-27 02:19:19.628524
Model ind 665 epoch 1174 batch: 800 avg loss -2.897349 avg loss no lamb -2.897349 time 2020-06-27 02:19:30.295757
last batch sz 10
Pre: time 2020-06-27 02:19:44.337140: 
 	std: 0.0032627592
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9812, 0.9751, 0.9821, 0.9749]
	train_accs: [0.98176664, 0.98106664, 0.9759667, 0.9815, 0.97568333]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97898006
	best: 0.9816

Starting e_i: 1175
Model ind 665 epoch 1175 batch: 0 avg loss -2.942494 avg loss no lamb -2.942494 time 2020-06-27 02:19:45.437758
Model ind 665 epoch 1175 batch: 100 avg loss -2.916527 avg loss no lamb -2.916527 time 2020-06-27 02:19:56.454688
Model ind 665 epoch 1175 batch: 200 avg loss -2.896451 avg loss no lamb -2.896451 time 2020-06-27 02:20:07.587752
Model ind 665 epoch 1175 batch: 300 avg loss -2.900947 avg loss no lamb -2.900947 time 2020-06-27 02:20:18.407154
Model ind 665 epoch 1175 batch: 400 avg loss -2.789157 avg loss no lamb -2.789157 time 2020-06-27 02:20:29.279217
Model ind 665 epoch 1175 batch: 500 avg loss -2.839724 avg loss no lamb -2.839724 time 2020-06-27 02:20:40.213022
Model ind 665 epoch 1175 batch: 600 avg loss -2.914006 avg loss no lamb -2.914006 time 2020-06-27 02:20:51.272379
Model ind 665 epoch 1175 batch: 700 avg loss -2.802966 avg loss no lamb -2.802966 time 2020-06-27 02:21:02.104478
Model ind 665 epoch 1175 batch: 800 avg loss -2.846210 avg loss no lamb -2.846210 time 2020-06-27 02:21:12.903701
last batch sz 10
Pre: time 2020-06-27 02:21:27.044117: 
 	std: 0.0026012154
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9787, 0.978, 0.9725, 0.9791, 0.9745]
	train_accs: [0.9806, 0.9799333, 0.9741167, 0.9805833, 0.97546667]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97656
	best: 0.9787

Starting e_i: 1176
Model ind 665 epoch 1176 batch: 0 avg loss -2.979135 avg loss no lamb -2.979135 time 2020-06-27 02:21:28.170631
Model ind 665 epoch 1176 batch: 100 avg loss -2.966427 avg loss no lamb -2.966427 time 2020-06-27 02:21:38.980422
Model ind 665 epoch 1176 batch: 200 avg loss -2.878691 avg loss no lamb -2.878691 time 2020-06-27 02:21:49.684708
Model ind 665 epoch 1176 batch: 300 avg loss -2.873612 avg loss no lamb -2.873612 time 2020-06-27 02:22:00.597019
Model ind 665 epoch 1176 batch: 400 avg loss -2.831598 avg loss no lamb -2.831598 time 2020-06-27 02:22:11.516685
Model ind 665 epoch 1176 batch: 500 avg loss -2.890039 avg loss no lamb -2.890039 time 2020-06-27 02:22:22.289158
Model ind 665 epoch 1176 batch: 600 avg loss -2.850915 avg loss no lamb -2.850915 time 2020-06-27 02:22:33.280071
Model ind 665 epoch 1176 batch: 700 avg loss -2.806544 avg loss no lamb -2.806544 time 2020-06-27 02:22:43.965029
Model ind 665 epoch 1176 batch: 800 avg loss -2.828635 avg loss no lamb -2.828635 time 2020-06-27 02:22:54.915243
last batch sz 10
Pre: time 2020-06-27 02:23:09.266503: 
 	std: 0.0025190418
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.979, 0.9741, 0.9796, 0.975]
	train_accs: [0.98116666, 0.9806167, 0.9756333, 0.98143333, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97757995
	best: 0.9796

Starting e_i: 1177
Model ind 665 epoch 1177 batch: 0 avg loss -2.980962 avg loss no lamb -2.980962 time 2020-06-27 02:23:10.408309
Model ind 665 epoch 1177 batch: 100 avg loss -2.911041 avg loss no lamb -2.911041 time 2020-06-27 02:23:21.277069
Model ind 665 epoch 1177 batch: 200 avg loss -2.827108 avg loss no lamb -2.827108 time 2020-06-27 02:23:32.184750
Model ind 665 epoch 1177 batch: 300 avg loss -2.852417 avg loss no lamb -2.852417 time 2020-06-27 02:23:43.246387
Model ind 665 epoch 1177 batch: 400 avg loss -2.769806 avg loss no lamb -2.769806 time 2020-06-27 02:23:54.260714
Model ind 665 epoch 1177 batch: 500 avg loss -2.864849 avg loss no lamb -2.864849 time 2020-06-27 02:24:05.354638
Model ind 665 epoch 1177 batch: 600 avg loss -2.870817 avg loss no lamb -2.870817 time 2020-06-27 02:24:16.107454
Model ind 665 epoch 1177 batch: 700 avg loss -2.790867 avg loss no lamb -2.790867 time 2020-06-27 02:24:27.041165
Model ind 665 epoch 1177 batch: 800 avg loss -2.878388 avg loss no lamb -2.878388 time 2020-06-27 02:24:37.841324
last batch sz 10
Pre: time 2020-06-27 02:24:52.308990: 
 	std: 0.003030917
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9791, 0.9726, 0.98, 0.9752]
	train_accs: [0.98148334, 0.98036665, 0.97495, 0.98146665, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97744
	best: 0.9803

Starting e_i: 1178
Model ind 665 epoch 1178 batch: 0 avg loss -2.932109 avg loss no lamb -2.932109 time 2020-06-27 02:24:53.444478
Model ind 665 epoch 1178 batch: 100 avg loss -2.875204 avg loss no lamb -2.875204 time 2020-06-27 02:25:04.479987
Model ind 665 epoch 1178 batch: 200 avg loss -2.814513 avg loss no lamb -2.814513 time 2020-06-27 02:25:15.507615
Model ind 665 epoch 1178 batch: 300 avg loss -2.878992 avg loss no lamb -2.878992 time 2020-06-27 02:25:26.533785
Model ind 665 epoch 1178 batch: 400 avg loss -2.816889 avg loss no lamb -2.816889 time 2020-06-27 02:25:37.354223
Model ind 665 epoch 1178 batch: 500 avg loss -2.826093 avg loss no lamb -2.826093 time 2020-06-27 02:25:48.052045
Model ind 665 epoch 1178 batch: 600 avg loss -2.883518 avg loss no lamb -2.883518 time 2020-06-27 02:25:58.898931
Model ind 665 epoch 1178 batch: 700 avg loss -2.851493 avg loss no lamb -2.851493 time 2020-06-27 02:26:09.738229
Model ind 665 epoch 1178 batch: 800 avg loss -2.835900 avg loss no lamb -2.835900 time 2020-06-27 02:26:20.729853
last batch sz 10
Pre: time 2020-06-27 02:26:34.865229: 
 	std: 0.0033890426
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9807, 0.9735, 0.981, 0.9747]
	train_accs: [0.98148334, 0.98076665, 0.9752833, 0.9812667, 0.9756167]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97822
	best: 0.9812

Starting e_i: 1179
Model ind 665 epoch 1179 batch: 0 avg loss -2.941888 avg loss no lamb -2.941888 time 2020-06-27 02:26:36.032415
Model ind 665 epoch 1179 batch: 100 avg loss -2.883511 avg loss no lamb -2.883511 time 2020-06-27 02:26:46.906623
Model ind 665 epoch 1179 batch: 200 avg loss -2.855315 avg loss no lamb -2.855315 time 2020-06-27 02:26:57.629497
Model ind 665 epoch 1179 batch: 300 avg loss -2.892078 avg loss no lamb -2.892078 time 2020-06-27 02:27:08.674859
Model ind 665 epoch 1179 batch: 400 avg loss -2.843442 avg loss no lamb -2.843442 time 2020-06-27 02:27:19.647958
Model ind 665 epoch 1179 batch: 500 avg loss -2.861704 avg loss no lamb -2.861704 time 2020-06-27 02:27:30.457514
Model ind 665 epoch 1179 batch: 600 avg loss -2.884187 avg loss no lamb -2.884187 time 2020-06-27 02:27:41.280995
Model ind 665 epoch 1179 batch: 700 avg loss -2.756803 avg loss no lamb -2.756803 time 2020-06-27 02:27:52.191443
Model ind 665 epoch 1179 batch: 800 avg loss -2.817006 avg loss no lamb -2.817006 time 2020-06-27 02:28:03.290953
last batch sz 10
Pre: time 2020-06-27 02:28:17.452791: 
 	std: 0.0029942628
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9796, 0.9743, 0.981, 0.9751]
	train_accs: [0.98181665, 0.9802833, 0.975, 0.9813333, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97827995
	best: 0.9814

Starting e_i: 1180
Model ind 665 epoch 1180 batch: 0 avg loss -2.948048 avg loss no lamb -2.948048 time 2020-06-27 02:28:18.653572
Model ind 665 epoch 1180 batch: 100 avg loss -2.939192 avg loss no lamb -2.939192 time 2020-06-27 02:28:29.312437
Model ind 665 epoch 1180 batch: 200 avg loss -2.850019 avg loss no lamb -2.850019 time 2020-06-27 02:28:40.192379
Model ind 665 epoch 1180 batch: 300 avg loss -2.887382 avg loss no lamb -2.887382 time 2020-06-27 02:28:51.011240
Model ind 665 epoch 1180 batch: 400 avg loss -2.815243 avg loss no lamb -2.815243 time 2020-06-27 02:29:01.840501
Model ind 665 epoch 1180 batch: 500 avg loss -2.754170 avg loss no lamb -2.754170 time 2020-06-27 02:29:12.833744
Model ind 665 epoch 1180 batch: 600 avg loss -2.843021 avg loss no lamb -2.843021 time 2020-06-27 02:29:23.742438
Model ind 665 epoch 1180 batch: 700 avg loss -2.810238 avg loss no lamb -2.810238 time 2020-06-27 02:29:34.865323
Model ind 665 epoch 1180 batch: 800 avg loss -2.894151 avg loss no lamb -2.894151 time 2020-06-27 02:29:48.280064
last batch sz 10
Pre: time 2020-06-27 02:30:04.791836: 
 	std: 0.0024289775
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9793, 0.9743, 0.9803, 0.9768]
	train_accs: [0.98175, 0.98118335, 0.9762667, 0.9816833, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.9783
	best: 0.9808

Starting e_i: 1181
Model ind 665 epoch 1181 batch: 0 avg loss -2.908506 avg loss no lamb -2.908506 time 2020-06-27 02:30:07.107880
Model ind 665 epoch 1181 batch: 100 avg loss -2.837057 avg loss no lamb -2.837057 time 2020-06-27 02:30:18.034629
Model ind 665 epoch 1181 batch: 200 avg loss -2.842777 avg loss no lamb -2.842777 time 2020-06-27 02:30:29.035401
Model ind 665 epoch 1181 batch: 300 avg loss -2.885216 avg loss no lamb -2.885216 time 2020-06-27 02:30:39.905404
Model ind 665 epoch 1181 batch: 400 avg loss -2.833251 avg loss no lamb -2.833251 time 2020-06-27 02:30:50.650502
Model ind 665 epoch 1181 batch: 500 avg loss -2.850394 avg loss no lamb -2.850394 time 2020-06-27 02:31:01.630356
Model ind 665 epoch 1181 batch: 600 avg loss -2.906650 avg loss no lamb -2.906650 time 2020-06-27 02:31:12.449984
Model ind 665 epoch 1181 batch: 700 avg loss -2.799974 avg loss no lamb -2.799974 time 2020-06-27 02:31:23.557319
Model ind 665 epoch 1181 batch: 800 avg loss -2.869287 avg loss no lamb -2.869287 time 2020-06-27 02:31:34.671039
last batch sz 10
Pre: time 2020-06-27 02:31:49.104646: 
 	std: 0.0025887538
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9786, 0.9733, 0.9795, 0.9749]
	train_accs: [0.98111665, 0.98011667, 0.97583336, 0.98123336, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97718
	best: 0.9795

Starting e_i: 1182
Model ind 665 epoch 1182 batch: 0 avg loss -2.897847 avg loss no lamb -2.897847 time 2020-06-27 02:31:50.243358
Model ind 665 epoch 1182 batch: 100 avg loss -2.928281 avg loss no lamb -2.928281 time 2020-06-27 02:32:00.849149
Model ind 665 epoch 1182 batch: 200 avg loss -2.870261 avg loss no lamb -2.870261 time 2020-06-27 02:32:11.713282
Model ind 665 epoch 1182 batch: 300 avg loss -2.879284 avg loss no lamb -2.879284 time 2020-06-27 02:32:22.470728
Model ind 665 epoch 1182 batch: 400 avg loss -2.830232 avg loss no lamb -2.830232 time 2020-06-27 02:32:33.600345
Model ind 665 epoch 1182 batch: 500 avg loss -2.865059 avg loss no lamb -2.865059 time 2020-06-27 02:32:44.727422
Model ind 665 epoch 1182 batch: 600 avg loss -2.890209 avg loss no lamb -2.890209 time 2020-06-27 02:32:55.780704
Model ind 665 epoch 1182 batch: 700 avg loss -2.849695 avg loss no lamb -2.849695 time 2020-06-27 02:33:06.774151
Model ind 665 epoch 1182 batch: 800 avg loss -2.778444 avg loss no lamb -2.778444 time 2020-06-27 02:33:17.392977
last batch sz 10
Pre: time 2020-06-27 02:33:31.686648: 
 	std: 0.002878607
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.979, 0.9728, 0.9794, 0.9744]
	train_accs: [0.98116666, 0.98048335, 0.97533333, 0.9808667, 0.97585]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97705996
	best: 0.9797

Starting e_i: 1183
Model ind 665 epoch 1183 batch: 0 avg loss -2.882952 avg loss no lamb -2.882952 time 2020-06-27 02:33:32.865734
Model ind 665 epoch 1183 batch: 100 avg loss -2.923116 avg loss no lamb -2.923116 time 2020-06-27 02:33:43.911178
Model ind 665 epoch 1183 batch: 200 avg loss -2.904989 avg loss no lamb -2.904989 time 2020-06-27 02:33:54.969926
Model ind 665 epoch 1183 batch: 300 avg loss -2.871481 avg loss no lamb -2.871481 time 2020-06-27 02:34:05.817241
Model ind 665 epoch 1183 batch: 400 avg loss -2.779091 avg loss no lamb -2.779091 time 2020-06-27 02:34:16.590221
Model ind 665 epoch 1183 batch: 500 avg loss -2.867384 avg loss no lamb -2.867384 time 2020-06-27 02:34:27.655819
Model ind 665 epoch 1183 batch: 600 avg loss -2.798530 avg loss no lamb -2.798530 time 2020-06-27 02:34:38.573511
Model ind 665 epoch 1183 batch: 700 avg loss -2.822204 avg loss no lamb -2.822204 time 2020-06-27 02:34:49.597668
Model ind 665 epoch 1183 batch: 800 avg loss -2.859150 avg loss no lamb -2.859150 time 2020-06-27 02:35:00.529089
last batch sz 10
Pre: time 2020-06-27 02:35:14.859156: 
 	std: 0.0028610458
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.979, 0.9734, 0.9802, 0.9748]
	train_accs: [0.98121667, 0.9803333, 0.97485, 0.98121667, 0.97535]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97752
	best: 0.9802

Starting e_i: 1184
Model ind 665 epoch 1184 batch: 0 avg loss -2.929459 avg loss no lamb -2.929459 time 2020-06-27 02:35:15.993730
Model ind 665 epoch 1184 batch: 100 avg loss -2.928490 avg loss no lamb -2.928490 time 2020-06-27 02:35:26.680370
Model ind 665 epoch 1184 batch: 200 avg loss -2.814259 avg loss no lamb -2.814259 time 2020-06-27 02:35:37.541459
Model ind 665 epoch 1184 batch: 300 avg loss -2.874797 avg loss no lamb -2.874797 time 2020-06-27 02:35:48.583139
Model ind 665 epoch 1184 batch: 400 avg loss -2.864016 avg loss no lamb -2.864016 time 2020-06-27 02:35:59.634184
Model ind 665 epoch 1184 batch: 500 avg loss -2.897182 avg loss no lamb -2.897182 time 2020-06-27 02:36:10.472029
Model ind 665 epoch 1184 batch: 600 avg loss -2.890131 avg loss no lamb -2.890131 time 2020-06-27 02:36:21.275934
Model ind 665 epoch 1184 batch: 700 avg loss -2.740297 avg loss no lamb -2.740297 time 2020-06-27 02:36:31.987632
Model ind 665 epoch 1184 batch: 800 avg loss -2.860199 avg loss no lamb -2.860199 time 2020-06-27 02:36:43.002704
last batch sz 10
Pre: time 2020-06-27 02:36:57.237429: 
 	std: 0.002838592
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9802, 0.9745, 0.9818, 0.977]
	train_accs: [0.9821, 0.9813, 0.9762, 0.9819667, 0.97745]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97902
	best: 0.9816

Starting e_i: 1185
Model ind 665 epoch 1185 batch: 0 avg loss -2.925820 avg loss no lamb -2.925820 time 2020-06-27 02:36:58.452552
Model ind 665 epoch 1185 batch: 100 avg loss -2.888448 avg loss no lamb -2.888448 time 2020-06-27 02:37:09.151722
Model ind 665 epoch 1185 batch: 200 avg loss -2.832384 avg loss no lamb -2.832384 time 2020-06-27 02:37:19.892355
Model ind 665 epoch 1185 batch: 300 avg loss -2.891117 avg loss no lamb -2.891117 time 2020-06-27 02:37:30.832145
Model ind 665 epoch 1185 batch: 400 avg loss -2.822727 avg loss no lamb -2.822727 time 2020-06-27 02:37:41.648004
Model ind 665 epoch 1185 batch: 500 avg loss -2.817890 avg loss no lamb -2.817890 time 2020-06-27 02:37:52.634759
Model ind 665 epoch 1185 batch: 600 avg loss -2.899181 avg loss no lamb -2.899181 time 2020-06-27 02:38:03.877146
Model ind 665 epoch 1185 batch: 700 avg loss -2.741173 avg loss no lamb -2.741173 time 2020-06-27 02:38:14.957098
Model ind 665 epoch 1185 batch: 800 avg loss -2.893913 avg loss no lamb -2.893913 time 2020-06-27 02:38:25.966118
last batch sz 10
Pre: time 2020-06-27 02:38:40.234824: 
 	std: 0.0029651264
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.98, 0.9739, 0.9805, 0.9749]
	train_accs: [0.98148334, 0.9809333, 0.9755833, 0.98143333, 0.9763833]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97800004
	best: 0.9807

Starting e_i: 1186
Model ind 665 epoch 1186 batch: 0 avg loss -2.915498 avg loss no lamb -2.915498 time 2020-06-27 02:38:41.335727
Model ind 665 epoch 1186 batch: 100 avg loss -2.844459 avg loss no lamb -2.844459 time 2020-06-27 02:38:52.458388
Model ind 665 epoch 1186 batch: 200 avg loss -2.842267 avg loss no lamb -2.842267 time 2020-06-27 02:39:03.372480
Model ind 665 epoch 1186 batch: 300 avg loss -2.862164 avg loss no lamb -2.862164 time 2020-06-27 02:39:14.181696
Model ind 665 epoch 1186 batch: 400 avg loss -2.838026 avg loss no lamb -2.838026 time 2020-06-27 02:39:24.956599
Model ind 665 epoch 1186 batch: 500 avg loss -2.837997 avg loss no lamb -2.837997 time 2020-06-27 02:39:35.775099
Model ind 665 epoch 1186 batch: 600 avg loss -2.824416 avg loss no lamb -2.824416 time 2020-06-27 02:39:46.864474
Model ind 665 epoch 1186 batch: 700 avg loss -2.744526 avg loss no lamb -2.744526 time 2020-06-27 02:39:57.787603
Model ind 665 epoch 1186 batch: 800 avg loss -2.844028 avg loss no lamb -2.844028 time 2020-06-27 02:40:08.714905
last batch sz 10
Pre: time 2020-06-27 02:40:23.286177: 
 	std: 0.0030629293
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9777, 0.9725, 0.9795, 0.9734]
	train_accs: [0.98111665, 0.9801667, 0.97503334, 0.98076665, 0.97575]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97658
	best: 0.9798

Starting e_i: 1187
Model ind 665 epoch 1187 batch: 0 avg loss -2.976965 avg loss no lamb -2.976965 time 2020-06-27 02:40:24.464032
Model ind 665 epoch 1187 batch: 100 avg loss -2.828281 avg loss no lamb -2.828281 time 2020-06-27 02:40:35.340929
Model ind 665 epoch 1187 batch: 200 avg loss -2.870409 avg loss no lamb -2.870409 time 2020-06-27 02:40:46.365011
Model ind 665 epoch 1187 batch: 300 avg loss -2.856508 avg loss no lamb -2.856508 time 2020-06-27 02:40:57.133855
Model ind 665 epoch 1187 batch: 400 avg loss -2.820194 avg loss no lamb -2.820194 time 2020-06-27 02:41:08.220884
Model ind 665 epoch 1187 batch: 500 avg loss -2.860526 avg loss no lamb -2.860526 time 2020-06-27 02:41:19.165804
Model ind 665 epoch 1187 batch: 600 avg loss -2.814551 avg loss no lamb -2.814551 time 2020-06-27 02:41:30.303580
Model ind 665 epoch 1187 batch: 700 avg loss -2.787896 avg loss no lamb -2.787896 time 2020-06-27 02:41:41.238583
Model ind 665 epoch 1187 batch: 800 avg loss -2.821487 avg loss no lamb -2.821487 time 2020-06-27 02:41:52.355862
last batch sz 10
Pre: time 2020-06-27 02:42:06.891733: 
 	std: 0.0033778134
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9807, 0.9743, 0.9825, 0.9758]
	train_accs: [0.9816167, 0.9808667, 0.9755667, 0.9816333, 0.9766]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97908
	best: 0.9825

Starting e_i: 1188
Model ind 665 epoch 1188 batch: 0 avg loss -2.968466 avg loss no lamb -2.968466 time 2020-06-27 02:42:08.056558
Model ind 665 epoch 1188 batch: 100 avg loss -2.887357 avg loss no lamb -2.887357 time 2020-06-27 02:42:18.963078
Model ind 665 epoch 1188 batch: 200 avg loss -2.871780 avg loss no lamb -2.871780 time 2020-06-27 02:42:29.846091
Model ind 665 epoch 1188 batch: 300 avg loss -2.824269 avg loss no lamb -2.824269 time 2020-06-27 02:42:40.789343
Model ind 665 epoch 1188 batch: 400 avg loss -2.777307 avg loss no lamb -2.777307 time 2020-06-27 02:42:51.787081
Model ind 665 epoch 1188 batch: 500 avg loss -2.823882 avg loss no lamb -2.823882 time 2020-06-27 02:43:02.718213
Model ind 665 epoch 1188 batch: 600 avg loss -2.943728 avg loss no lamb -2.943728 time 2020-06-27 02:43:13.586566
Model ind 665 epoch 1188 batch: 700 avg loss -2.748063 avg loss no lamb -2.748063 time 2020-06-27 02:43:24.653284
Model ind 665 epoch 1188 batch: 800 avg loss -2.831730 avg loss no lamb -2.831730 time 2020-06-27 02:43:35.608674
last batch sz 10
Pre: time 2020-06-27 02:43:49.975747: 
 	std: 0.0032121004
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9799, 0.9733, 0.9808, 0.9748]
	train_accs: [0.98165, 0.98083335, 0.9748333, 0.98178333, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97791994
	best: 0.9808

Starting e_i: 1189
Model ind 665 epoch 1189 batch: 0 avg loss -2.935042 avg loss no lamb -2.935042 time 2020-06-27 02:43:51.119479
Model ind 665 epoch 1189 batch: 100 avg loss -2.874410 avg loss no lamb -2.874410 time 2020-06-27 02:44:02.155137
Model ind 665 epoch 1189 batch: 200 avg loss -2.855058 avg loss no lamb -2.855058 time 2020-06-27 02:44:13.049420
Model ind 665 epoch 1189 batch: 300 avg loss -2.855347 avg loss no lamb -2.855347 time 2020-06-27 02:44:23.952893
Model ind 665 epoch 1189 batch: 400 avg loss -2.858523 avg loss no lamb -2.858523 time 2020-06-27 02:44:34.695724
Model ind 665 epoch 1189 batch: 500 avg loss -2.860857 avg loss no lamb -2.860857 time 2020-06-27 02:44:45.558885
Model ind 665 epoch 1189 batch: 600 avg loss -2.871294 avg loss no lamb -2.871294 time 2020-06-27 02:44:56.430717
Model ind 665 epoch 1189 batch: 700 avg loss -2.790997 avg loss no lamb -2.790997 time 2020-06-27 02:45:07.411079
Model ind 665 epoch 1189 batch: 800 avg loss -2.880362 avg loss no lamb -2.880362 time 2020-06-27 02:45:18.390344
last batch sz 10
Pre: time 2020-06-27 02:45:32.899574: 
 	std: 0.003064242
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9791, 0.9734, 0.9805, 0.9744]
	train_accs: [0.98121667, 0.9805167, 0.97466666, 0.98115, 0.97503334]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97757995
	best: 0.9805

Starting e_i: 1190
Model ind 665 epoch 1190 batch: 0 avg loss -2.938676 avg loss no lamb -2.938676 time 2020-06-27 02:45:34.041602
Model ind 665 epoch 1190 batch: 100 avg loss -2.920268 avg loss no lamb -2.920268 time 2020-06-27 02:45:44.991964
Model ind 665 epoch 1190 batch: 200 avg loss -2.874631 avg loss no lamb -2.874631 time 2020-06-27 02:45:56.003103
Model ind 665 epoch 1190 batch: 300 avg loss -2.905795 avg loss no lamb -2.905795 time 2020-06-27 02:46:06.896711
Model ind 665 epoch 1190 batch: 400 avg loss -2.815782 avg loss no lamb -2.815782 time 2020-06-27 02:46:17.816910
Model ind 665 epoch 1190 batch: 500 avg loss -2.864603 avg loss no lamb -2.864603 time 2020-06-27 02:46:28.745109
Model ind 665 epoch 1190 batch: 600 avg loss -2.859853 avg loss no lamb -2.859853 time 2020-06-27 02:46:39.640970
Model ind 665 epoch 1190 batch: 700 avg loss -2.805764 avg loss no lamb -2.805764 time 2020-06-27 02:46:50.461648
Model ind 665 epoch 1190 batch: 800 avg loss -2.876523 avg loss no lamb -2.876523 time 2020-06-27 02:47:01.462721
last batch sz 10
Pre: time 2020-06-27 02:47:15.690667: 
 	std: 0.002657363
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9801, 0.9749, 0.9802, 0.9752]
	train_accs: [0.98145, 0.98108333, 0.9758667, 0.9812, 0.97645]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97827995
	best: 0.981

Starting e_i: 1191
Model ind 665 epoch 1191 batch: 0 avg loss -2.967355 avg loss no lamb -2.967355 time 2020-06-27 02:47:18.050528
Model ind 665 epoch 1191 batch: 100 avg loss -2.888135 avg loss no lamb -2.888135 time 2020-06-27 02:47:28.908830
Model ind 665 epoch 1191 batch: 200 avg loss -2.902759 avg loss no lamb -2.902759 time 2020-06-27 02:47:39.679227
Model ind 665 epoch 1191 batch: 300 avg loss -2.825583 avg loss no lamb -2.825583 time 2020-06-27 02:47:50.479640
Model ind 665 epoch 1191 batch: 400 avg loss -2.769757 avg loss no lamb -2.769757 time 2020-06-27 02:48:01.442464
Model ind 665 epoch 1191 batch: 500 avg loss -2.842200 avg loss no lamb -2.842200 time 2020-06-27 02:48:12.275001
Model ind 665 epoch 1191 batch: 600 avg loss -2.890116 avg loss no lamb -2.890116 time 2020-06-27 02:48:23.249850
Model ind 665 epoch 1191 batch: 700 avg loss -2.777990 avg loss no lamb -2.777990 time 2020-06-27 02:48:33.979567
Model ind 665 epoch 1191 batch: 800 avg loss -2.848663 avg loss no lamb -2.848663 time 2020-06-27 02:48:44.839933
last batch sz 10
Pre: time 2020-06-27 02:48:59.176157: 
 	std: 0.0030837646
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9811, 0.9744, 0.9809, 0.9749]
	train_accs: [0.98176664, 0.9815, 0.97565, 0.98165, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.9784201
	best: 0.9808

Starting e_i: 1192
Model ind 665 epoch 1192 batch: 0 avg loss -2.924183 avg loss no lamb -2.924183 time 2020-06-27 02:49:00.355691
Model ind 665 epoch 1192 batch: 100 avg loss -2.954096 avg loss no lamb -2.954096 time 2020-06-27 02:49:11.360932
Model ind 665 epoch 1192 batch: 200 avg loss -2.822309 avg loss no lamb -2.822309 time 2020-06-27 02:49:22.356283
Model ind 665 epoch 1192 batch: 300 avg loss -2.865307 avg loss no lamb -2.865307 time 2020-06-27 02:49:33.191257
Model ind 665 epoch 1192 batch: 400 avg loss -2.799401 avg loss no lamb -2.799401 time 2020-06-27 02:49:44.041606
Model ind 665 epoch 1192 batch: 500 avg loss -2.863090 avg loss no lamb -2.863090 time 2020-06-27 02:49:55.003858
Model ind 665 epoch 1192 batch: 600 avg loss -2.885939 avg loss no lamb -2.885939 time 2020-06-27 02:50:06.208402
Model ind 665 epoch 1192 batch: 700 avg loss -2.793362 avg loss no lamb -2.793362 time 2020-06-27 02:50:17.256161
Model ind 665 epoch 1192 batch: 800 avg loss -2.892399 avg loss no lamb -2.892399 time 2020-06-27 02:50:28.006803
last batch sz 10
Pre: time 2020-06-27 02:50:42.091134: 
 	std: 0.0027045202
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.981, 0.9747, 0.9811, 0.9768]
	train_accs: [0.98216665, 0.98151666, 0.97581667, 0.9820667, 0.9772]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97896004
	best: 0.9812

Starting e_i: 1193
Model ind 665 epoch 1193 batch: 0 avg loss -2.968539 avg loss no lamb -2.968539 time 2020-06-27 02:50:43.232692
Model ind 665 epoch 1193 batch: 100 avg loss -2.858509 avg loss no lamb -2.858509 time 2020-06-27 02:50:54.115348
Model ind 665 epoch 1193 batch: 200 avg loss -2.887780 avg loss no lamb -2.887780 time 2020-06-27 02:51:05.063272
Model ind 665 epoch 1193 batch: 300 avg loss -2.886554 avg loss no lamb -2.886554 time 2020-06-27 02:51:16.284558
Model ind 665 epoch 1193 batch: 400 avg loss -2.789284 avg loss no lamb -2.789284 time 2020-06-27 02:51:27.314157
Model ind 665 epoch 1193 batch: 500 avg loss -2.840252 avg loss no lamb -2.840252 time 2020-06-27 02:51:38.256521
Model ind 665 epoch 1193 batch: 600 avg loss -2.904930 avg loss no lamb -2.904930 time 2020-06-27 02:51:49.181673
Model ind 665 epoch 1193 batch: 700 avg loss -2.773088 avg loss no lamb -2.773088 time 2020-06-27 02:52:00.638556
Model ind 665 epoch 1193 batch: 800 avg loss -2.837037 avg loss no lamb -2.837037 time 2020-06-27 02:52:11.629717
last batch sz 10
Pre: time 2020-06-27 02:52:26.234367: 
 	std: 0.0030564037
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9807, 0.9745, 0.982, 0.9764]
	train_accs: [0.9820333, 0.9811, 0.97611666, 0.98181665, 0.9767]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97908
	best: 0.9818

Starting e_i: 1194
Model ind 665 epoch 1194 batch: 0 avg loss -2.921221 avg loss no lamb -2.921221 time 2020-06-27 02:52:27.475037
Model ind 665 epoch 1194 batch: 100 avg loss -2.939390 avg loss no lamb -2.939390 time 2020-06-27 02:52:38.407716
Model ind 665 epoch 1194 batch: 200 avg loss -2.906562 avg loss no lamb -2.906562 time 2020-06-27 02:52:49.432226
Model ind 665 epoch 1194 batch: 300 avg loss -2.928630 avg loss no lamb -2.928630 time 2020-06-27 02:53:00.397810
Model ind 665 epoch 1194 batch: 400 avg loss -2.808368 avg loss no lamb -2.808368 time 2020-06-27 02:53:11.584895
Model ind 665 epoch 1194 batch: 500 avg loss -2.867559 avg loss no lamb -2.867559 time 2020-06-27 02:53:22.552971
Model ind 665 epoch 1194 batch: 600 avg loss -2.834937 avg loss no lamb -2.834937 time 2020-06-27 02:53:33.363427
Model ind 665 epoch 1194 batch: 700 avg loss -2.799771 avg loss no lamb -2.799771 time 2020-06-27 02:53:44.249740
Model ind 665 epoch 1194 batch: 800 avg loss -2.916047 avg loss no lamb -2.916047 time 2020-06-27 02:53:55.072928
last batch sz 10
Pre: time 2020-06-27 02:54:09.103738: 
 	std: 0.0032879212
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9791, 0.9727, 0.9795, 0.973]
	train_accs: [0.98135, 0.9809333, 0.97538334, 0.98118335, 0.97545]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.9768599
	best: 0.98

Starting e_i: 1195
Model ind 665 epoch 1195 batch: 0 avg loss -2.911473 avg loss no lamb -2.911473 time 2020-06-27 02:54:10.234242
Model ind 665 epoch 1195 batch: 100 avg loss -2.875027 avg loss no lamb -2.875027 time 2020-06-27 02:54:21.317010
Model ind 665 epoch 1195 batch: 200 avg loss -2.907159 avg loss no lamb -2.907159 time 2020-06-27 02:54:32.370094
Model ind 665 epoch 1195 batch: 300 avg loss -2.904098 avg loss no lamb -2.904098 time 2020-06-27 02:54:43.268611
Model ind 665 epoch 1195 batch: 400 avg loss -2.769726 avg loss no lamb -2.769726 time 2020-06-27 02:54:54.149702
Model ind 665 epoch 1195 batch: 500 avg loss -2.852863 avg loss no lamb -2.852863 time 2020-06-27 02:55:05.193194
Model ind 665 epoch 1195 batch: 600 avg loss -2.885912 avg loss no lamb -2.885912 time 2020-06-27 02:55:16.170916
Model ind 665 epoch 1195 batch: 700 avg loss -2.793045 avg loss no lamb -2.793045 time 2020-06-27 02:55:27.642695
Model ind 665 epoch 1195 batch: 800 avg loss -2.866068 avg loss no lamb -2.866068 time 2020-06-27 02:55:38.477233
last batch sz 10
Pre: time 2020-06-27 02:55:52.666120: 
 	std: 0.002658271
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.9786, 0.9731, 0.9799, 0.9749]
	train_accs: [0.9813667, 0.9809333, 0.97585, 0.98115, 0.9766833]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97714007
	best: 0.9792

Starting e_i: 1196
Model ind 665 epoch 1196 batch: 0 avg loss -2.978031 avg loss no lamb -2.978031 time 2020-06-27 02:55:53.881949
Model ind 665 epoch 1196 batch: 100 avg loss -2.827490 avg loss no lamb -2.827490 time 2020-06-27 02:56:05.028534
Model ind 665 epoch 1196 batch: 200 avg loss -2.861375 avg loss no lamb -2.861375 time 2020-06-27 02:56:16.161317
Model ind 665 epoch 1196 batch: 300 avg loss -2.877357 avg loss no lamb -2.877357 time 2020-06-27 02:56:26.958823
Model ind 665 epoch 1196 batch: 400 avg loss -2.789040 avg loss no lamb -2.789040 time 2020-06-27 02:56:37.659228
Model ind 665 epoch 1196 batch: 500 avg loss -2.883910 avg loss no lamb -2.883910 time 2020-06-27 02:56:48.600471
Model ind 665 epoch 1196 batch: 600 avg loss -2.858857 avg loss no lamb -2.858857 time 2020-06-27 02:56:59.805929
Model ind 665 epoch 1196 batch: 700 avg loss -2.722412 avg loss no lamb -2.722412 time 2020-06-27 02:57:10.735582
Model ind 665 epoch 1196 batch: 800 avg loss -2.805948 avg loss no lamb -2.805948 time 2020-06-27 02:57:21.568590
last batch sz 10
Pre: time 2020-06-27 02:57:35.549154: 
 	std: 0.003305374
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9796, 0.9725, 0.9801, 0.9735]
	train_accs: [0.9811, 0.9802833, 0.97466666, 0.98085, 0.97548336]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97701997
	best: 0.9794

Starting e_i: 1197
Model ind 665 epoch 1197 batch: 0 avg loss -2.965842 avg loss no lamb -2.965842 time 2020-06-27 02:57:36.651251
Model ind 665 epoch 1197 batch: 100 avg loss -2.918758 avg loss no lamb -2.918758 time 2020-06-27 02:57:47.746558
Model ind 665 epoch 1197 batch: 200 avg loss -2.928369 avg loss no lamb -2.928369 time 2020-06-27 02:57:59.055291
Model ind 665 epoch 1197 batch: 300 avg loss -2.909680 avg loss no lamb -2.909680 time 2020-06-27 02:58:09.846040
Model ind 665 epoch 1197 batch: 400 avg loss -2.732113 avg loss no lamb -2.732113 time 2020-06-27 02:58:20.606666
Model ind 665 epoch 1197 batch: 500 avg loss -2.829590 avg loss no lamb -2.829590 time 2020-06-27 02:58:31.750438
Model ind 665 epoch 1197 batch: 600 avg loss -2.854242 avg loss no lamb -2.854242 time 2020-06-27 02:58:42.915830
Model ind 665 epoch 1197 batch: 700 avg loss -2.747512 avg loss no lamb -2.747512 time 2020-06-27 02:58:53.887121
Model ind 665 epoch 1197 batch: 800 avg loss -2.866091 avg loss no lamb -2.866091 time 2020-06-27 02:59:04.981157
last batch sz 10
Pre: time 2020-06-27 02:59:19.451483: 
 	std: 0.0028485737
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9802, 0.9742, 0.9813, 0.9757]
	train_accs: [0.98153335, 0.9808, 0.97583336, 0.98175, 0.97711664]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97836
	best: 0.9813

Starting e_i: 1198
Model ind 665 epoch 1198 batch: 0 avg loss -2.962870 avg loss no lamb -2.962870 time 2020-06-27 02:59:20.571572
Model ind 665 epoch 1198 batch: 100 avg loss -2.899123 avg loss no lamb -2.899123 time 2020-06-27 02:59:31.596939
Model ind 665 epoch 1198 batch: 200 avg loss -2.854121 avg loss no lamb -2.854121 time 2020-06-27 02:59:42.598164
Model ind 665 epoch 1198 batch: 300 avg loss -2.845951 avg loss no lamb -2.845951 time 2020-06-27 02:59:53.851337
Model ind 665 epoch 1198 batch: 400 avg loss -2.796014 avg loss no lamb -2.796014 time 2020-06-27 03:00:04.949627
Model ind 665 epoch 1198 batch: 500 avg loss -2.821476 avg loss no lamb -2.821476 time 2020-06-27 03:00:15.856239
Model ind 665 epoch 1198 batch: 600 avg loss -2.871223 avg loss no lamb -2.871223 time 2020-06-27 03:00:26.827099
Model ind 665 epoch 1198 batch: 700 avg loss -2.766742 avg loss no lamb -2.766742 time 2020-06-27 03:00:37.845089
Model ind 665 epoch 1198 batch: 800 avg loss -2.860415 avg loss no lamb -2.860415 time 2020-06-27 03:00:48.609649
last batch sz 10
Pre: time 2020-06-27 03:01:02.693442: 
 	std: 0.002699182
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9811, 0.9749, 0.9808, 0.9757]
	train_accs: [0.98141664, 0.9812, 0.97585, 0.98156667, 0.97693336]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97858
	best: 0.9808

Starting e_i: 1199
Model ind 665 epoch 1199 batch: 0 avg loss -2.985601 avg loss no lamb -2.985601 time 2020-06-27 03:01:03.845162
Model ind 665 epoch 1199 batch: 100 avg loss -2.908793 avg loss no lamb -2.908793 time 2020-06-27 03:01:14.971503
Model ind 665 epoch 1199 batch: 200 avg loss -2.875329 avg loss no lamb -2.875329 time 2020-06-27 03:01:26.038146
Model ind 665 epoch 1199 batch: 300 avg loss -2.916822 avg loss no lamb -2.916822 time 2020-06-27 03:01:37.062806
Model ind 665 epoch 1199 batch: 400 avg loss -2.847088 avg loss no lamb -2.847088 time 2020-06-27 03:01:48.030860
Model ind 665 epoch 1199 batch: 500 avg loss -2.852973 avg loss no lamb -2.852973 time 2020-06-27 03:01:59.104827
Model ind 665 epoch 1199 batch: 600 avg loss -2.887112 avg loss no lamb -2.887112 time 2020-06-27 03:02:10.449405
Model ind 665 epoch 1199 batch: 700 avg loss -2.736951 avg loss no lamb -2.736951 time 2020-06-27 03:02:21.335679
Model ind 665 epoch 1199 batch: 800 avg loss -2.880439 avg loss no lamb -2.880439 time 2020-06-27 03:02:32.069483
last batch sz 10
Pre: time 2020-06-27 03:02:46.369773: 
 	std: 0.003224906
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9801, 0.9733, 0.9809, 0.9747]
	train_accs: [0.9816333, 0.98106664, 0.97505, 0.9817333, 0.97613335]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9778999
	best: 0.9809

Starting e_i: 1200
Model ind 665 epoch 1200 batch: 0 avg loss -2.918652 avg loss no lamb -2.918652 time 2020-06-27 03:02:47.508460
Model ind 665 epoch 1200 batch: 100 avg loss -2.869625 avg loss no lamb -2.869625 time 2020-06-27 03:02:58.481875
Model ind 665 epoch 1200 batch: 200 avg loss -2.938280 avg loss no lamb -2.938280 time 2020-06-27 03:03:09.492596
Model ind 665 epoch 1200 batch: 300 avg loss -2.852228 avg loss no lamb -2.852228 time 2020-06-27 03:03:20.597358
Model ind 665 epoch 1200 batch: 400 avg loss -2.755454 avg loss no lamb -2.755454 time 2020-06-27 03:03:31.492440
Model ind 665 epoch 1200 batch: 500 avg loss -2.829707 avg loss no lamb -2.829707 time 2020-06-27 03:03:42.333614
Model ind 665 epoch 1200 batch: 600 avg loss -2.873681 avg loss no lamb -2.873681 time 2020-06-27 03:03:53.494248
Model ind 665 epoch 1200 batch: 700 avg loss -2.796692 avg loss no lamb -2.796692 time 2020-06-27 03:04:04.531308
Model ind 665 epoch 1200 batch: 800 avg loss -2.882436 avg loss no lamb -2.882436 time 2020-06-27 03:04:15.439407
last batch sz 10
Pre: time 2020-06-27 03:04:29.255499: 
 	std: 0.0029982622
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.98, 0.9742, 0.9816, 0.9757]
	train_accs: [0.9812, 0.98013335, 0.97511667, 0.98155, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97852004
	best: 0.9816

Starting e_i: 1201
Model ind 665 epoch 1201 batch: 0 avg loss -2.896739 avg loss no lamb -2.896739 time 2020-06-27 03:04:31.582615
Model ind 665 epoch 1201 batch: 100 avg loss -2.936158 avg loss no lamb -2.936158 time 2020-06-27 03:04:42.707306
Model ind 665 epoch 1201 batch: 200 avg loss -2.918818 avg loss no lamb -2.918818 time 2020-06-27 03:04:53.991873
Model ind 665 epoch 1201 batch: 300 avg loss -2.876894 avg loss no lamb -2.876894 time 2020-06-27 03:05:05.017814
Model ind 665 epoch 1201 batch: 400 avg loss -2.755296 avg loss no lamb -2.755296 time 2020-06-27 03:05:16.107984
Model ind 665 epoch 1201 batch: 500 avg loss -2.889876 avg loss no lamb -2.889876 time 2020-06-27 03:05:27.206542
Model ind 665 epoch 1201 batch: 600 avg loss -2.817258 avg loss no lamb -2.817258 time 2020-06-27 03:05:38.217810
Model ind 665 epoch 1201 batch: 700 avg loss -2.782975 avg loss no lamb -2.782975 time 2020-06-27 03:05:49.301555
Model ind 665 epoch 1201 batch: 800 avg loss -2.812982 avg loss no lamb -2.812982 time 2020-06-27 03:06:00.040883
last batch sz 10
Pre: time 2020-06-27 03:06:14.292858: 
 	std: 0.003223429
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9815, 0.9745, 0.9822, 0.9765]
	train_accs: [0.98175, 0.98135, 0.9757, 0.9821333, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97936
	best: 0.9822

Starting e_i: 1202
Model ind 665 epoch 1202 batch: 0 avg loss -2.949832 avg loss no lamb -2.949832 time 2020-06-27 03:06:15.446718
Model ind 665 epoch 1202 batch: 100 avg loss -2.887570 avg loss no lamb -2.887570 time 2020-06-27 03:06:26.485207
Model ind 665 epoch 1202 batch: 200 avg loss -2.870930 avg loss no lamb -2.870930 time 2020-06-27 03:06:37.417529
Model ind 665 epoch 1202 batch: 300 avg loss -2.875043 avg loss no lamb -2.875043 time 2020-06-27 03:06:48.163823
Model ind 665 epoch 1202 batch: 400 avg loss -2.798996 avg loss no lamb -2.798996 time 2020-06-27 03:06:58.934526
Model ind 665 epoch 1202 batch: 500 avg loss -2.782811 avg loss no lamb -2.782811 time 2020-06-27 03:07:09.968135
Model ind 665 epoch 1202 batch: 600 avg loss -2.934119 avg loss no lamb -2.934119 time 2020-06-27 03:07:21.157225
Model ind 665 epoch 1202 batch: 700 avg loss -2.819624 avg loss no lamb -2.819624 time 2020-06-27 03:07:32.087662
Model ind 665 epoch 1202 batch: 800 avg loss -2.836900 avg loss no lamb -2.836900 time 2020-06-27 03:07:43.029387
last batch sz 10
Pre: time 2020-06-27 03:07:57.153571: 
 	std: 0.0025350167
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9795, 0.975, 0.9813, 0.9761]
	train_accs: [0.98141664, 0.9806833, 0.9762167, 0.9816, 0.97693336]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97853994
	best: 0.9813

Starting e_i: 1203
Model ind 665 epoch 1203 batch: 0 avg loss -2.957617 avg loss no lamb -2.957617 time 2020-06-27 03:07:58.320074
Model ind 665 epoch 1203 batch: 100 avg loss -2.865642 avg loss no lamb -2.865642 time 2020-06-27 03:08:09.364610
Model ind 665 epoch 1203 batch: 200 avg loss -2.866196 avg loss no lamb -2.866196 time 2020-06-27 03:08:20.163463
Model ind 665 epoch 1203 batch: 300 avg loss -2.911447 avg loss no lamb -2.911447 time 2020-06-27 03:08:30.987850
Model ind 665 epoch 1203 batch: 400 avg loss -2.781747 avg loss no lamb -2.781747 time 2020-06-27 03:08:41.973018
Model ind 665 epoch 1203 batch: 500 avg loss -2.863530 avg loss no lamb -2.863530 time 2020-06-27 03:08:52.848609
Model ind 665 epoch 1203 batch: 600 avg loss -2.882874 avg loss no lamb -2.882874 time 2020-06-27 03:09:03.879852
Model ind 665 epoch 1203 batch: 700 avg loss -2.843828 avg loss no lamb -2.843828 time 2020-06-27 03:09:14.704664
Model ind 665 epoch 1203 batch: 800 avg loss -2.852475 avg loss no lamb -2.852475 time 2020-06-27 03:09:25.593804
last batch sz 10
Pre: time 2020-06-27 03:09:39.573935: 
 	std: 0.002641982
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.978, 0.9736, 0.9804, 0.9757]
	train_accs: [0.98141664, 0.98005, 0.97513336, 0.98153335, 0.97693336]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.9776
	best: 0.9804

Starting e_i: 1204
Model ind 665 epoch 1204 batch: 0 avg loss -2.909796 avg loss no lamb -2.909796 time 2020-06-27 03:09:40.735021
Model ind 665 epoch 1204 batch: 100 avg loss -2.897132 avg loss no lamb -2.897132 time 2020-06-27 03:09:51.844841
Model ind 665 epoch 1204 batch: 200 avg loss -2.887722 avg loss no lamb -2.887722 time 2020-06-27 03:10:02.740754
Model ind 665 epoch 1204 batch: 300 avg loss -2.870581 avg loss no lamb -2.870581 time 2020-06-27 03:10:13.496471
Model ind 665 epoch 1204 batch: 400 avg loss -2.773019 avg loss no lamb -2.773019 time 2020-06-27 03:10:24.472439
Model ind 665 epoch 1204 batch: 500 avg loss -2.799590 avg loss no lamb -2.799590 time 2020-06-27 03:10:35.264196
Model ind 665 epoch 1204 batch: 600 avg loss -2.884500 avg loss no lamb -2.884500 time 2020-06-27 03:10:46.322870
Model ind 665 epoch 1204 batch: 700 avg loss -2.793218 avg loss no lamb -2.793218 time 2020-06-27 03:10:57.282200
Model ind 665 epoch 1204 batch: 800 avg loss -2.875085 avg loss no lamb -2.875085 time 2020-06-27 03:11:08.144219
last batch sz 10
Pre: time 2020-06-27 03:11:22.342245: 
 	std: 0.0037698867
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.98, 0.9729, 0.9822, 0.975]
	train_accs: [0.98216665, 0.98106664, 0.9751667, 0.98221666, 0.97655]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97839993
	best: 0.9822

Starting e_i: 1205
Model ind 665 epoch 1205 batch: 0 avg loss -2.990206 avg loss no lamb -2.990206 time 2020-06-27 03:11:23.508586
Model ind 665 epoch 1205 batch: 100 avg loss -2.979455 avg loss no lamb -2.979455 time 2020-06-27 03:11:34.296683
Model ind 665 epoch 1205 batch: 200 avg loss -2.885061 avg loss no lamb -2.885061 time 2020-06-27 03:11:45.376672
Model ind 665 epoch 1205 batch: 300 avg loss -2.856997 avg loss no lamb -2.856997 time 2020-06-27 03:11:56.130001
Model ind 665 epoch 1205 batch: 400 avg loss -2.762289 avg loss no lamb -2.762289 time 2020-06-27 03:12:06.957516
Model ind 665 epoch 1205 batch: 500 avg loss -2.849838 avg loss no lamb -2.849838 time 2020-06-27 03:12:17.740944
Model ind 665 epoch 1205 batch: 600 avg loss -2.859973 avg loss no lamb -2.859973 time 2020-06-27 03:12:28.632951
Model ind 665 epoch 1205 batch: 700 avg loss -2.784831 avg loss no lamb -2.784831 time 2020-06-27 03:12:39.541449
Model ind 665 epoch 1205 batch: 800 avg loss -2.860811 avg loss no lamb -2.860811 time 2020-06-27 03:12:50.377180
last batch sz 10
Pre: time 2020-06-27 03:13:04.620005: 
 	std: 0.0029594554
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9785, 0.9731, 0.9799, 0.9741]
	train_accs: [0.981, 0.97966665, 0.97495, 0.9809, 0.9758667]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97714007
	best: 0.9801

Starting e_i: 1206
Model ind 665 epoch 1206 batch: 0 avg loss -2.973704 avg loss no lamb -2.973704 time 2020-06-27 03:13:05.955617
Model ind 665 epoch 1206 batch: 100 avg loss -2.934366 avg loss no lamb -2.934366 time 2020-06-27 03:13:16.875151
Model ind 665 epoch 1206 batch: 200 avg loss -2.879221 avg loss no lamb -2.879221 time 2020-06-27 03:13:27.747931
Model ind 665 epoch 1206 batch: 300 avg loss -2.817368 avg loss no lamb -2.817368 time 2020-06-27 03:13:38.873620
Model ind 665 epoch 1206 batch: 400 avg loss -2.793021 avg loss no lamb -2.793021 time 2020-06-27 03:13:49.762194
Model ind 665 epoch 1206 batch: 500 avg loss -2.889456 avg loss no lamb -2.889456 time 2020-06-27 03:14:00.822537
Model ind 665 epoch 1206 batch: 600 avg loss -2.923949 avg loss no lamb -2.923949 time 2020-06-27 03:14:11.769947
Model ind 665 epoch 1206 batch: 700 avg loss -2.777797 avg loss no lamb -2.777797 time 2020-06-27 03:14:22.758094
Model ind 665 epoch 1206 batch: 800 avg loss -2.823881 avg loss no lamb -2.823881 time 2020-06-27 03:14:33.603502
last batch sz 10
Pre: time 2020-06-27 03:14:47.827598: 
 	std: 0.0026810425
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.981, 0.9753, 0.982, 0.9769]
	train_accs: [0.98188335, 0.9809, 0.97616667, 0.9820833, 0.97761667]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.9793
	best: 0.982

Starting e_i: 1207
Model ind 665 epoch 1207 batch: 0 avg loss -2.936546 avg loss no lamb -2.936546 time 2020-06-27 03:14:49.006400
Model ind 665 epoch 1207 batch: 100 avg loss -2.893644 avg loss no lamb -2.893644 time 2020-06-27 03:14:59.866738
Model ind 665 epoch 1207 batch: 200 avg loss -2.904188 avg loss no lamb -2.904188 time 2020-06-27 03:15:10.852872
Model ind 665 epoch 1207 batch: 300 avg loss -2.866722 avg loss no lamb -2.866722 time 2020-06-27 03:15:21.683933
Model ind 665 epoch 1207 batch: 400 avg loss -2.759756 avg loss no lamb -2.759756 time 2020-06-27 03:15:32.679175
Model ind 665 epoch 1207 batch: 500 avg loss -2.872627 avg loss no lamb -2.872627 time 2020-06-27 03:15:43.674791
Model ind 665 epoch 1207 batch: 600 avg loss -2.883859 avg loss no lamb -2.883859 time 2020-06-27 03:15:54.737311
Model ind 665 epoch 1207 batch: 700 avg loss -2.775309 avg loss no lamb -2.775309 time 2020-06-27 03:16:06.051467
Model ind 665 epoch 1207 batch: 800 avg loss -2.851901 avg loss no lamb -2.851901 time 2020-06-27 03:16:16.969600
last batch sz 10
Pre: time 2020-06-27 03:16:31.363297: 
 	std: 0.0033242132
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9806, 0.9738, 0.9819, 0.9758]
	train_accs: [0.98176664, 0.98071665, 0.97546667, 0.98176664, 0.9767]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97875994
	best: 0.9817

Starting e_i: 1208
Model ind 665 epoch 1208 batch: 0 avg loss -2.839650 avg loss no lamb -2.839650 time 2020-06-27 03:16:32.656762
Model ind 665 epoch 1208 batch: 100 avg loss -2.882336 avg loss no lamb -2.882336 time 2020-06-27 03:16:43.270708
Model ind 665 epoch 1208 batch: 200 avg loss -2.903722 avg loss no lamb -2.903722 time 2020-06-27 03:16:54.036886
Model ind 665 epoch 1208 batch: 300 avg loss -2.890488 avg loss no lamb -2.890488 time 2020-06-27 03:17:04.945980
Model ind 665 epoch 1208 batch: 400 avg loss -2.809974 avg loss no lamb -2.809974 time 2020-06-27 03:17:15.935262
Model ind 665 epoch 1208 batch: 500 avg loss -2.848140 avg loss no lamb -2.848140 time 2020-06-27 03:17:26.793669
Model ind 665 epoch 1208 batch: 600 avg loss -2.869524 avg loss no lamb -2.869524 time 2020-06-27 03:17:37.539158
Model ind 665 epoch 1208 batch: 700 avg loss -2.783341 avg loss no lamb -2.783341 time 2020-06-27 03:17:48.424394
Model ind 665 epoch 1208 batch: 800 avg loss -2.835534 avg loss no lamb -2.835534 time 2020-06-27 03:17:59.338569
last batch sz 10
Pre: time 2020-06-27 03:18:13.465592: 
 	std: 0.0024237134
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9798, 0.9743, 0.9799, 0.9758]
	train_accs: [0.98211664, 0.98106664, 0.9759667, 0.9817, 0.977]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97796
	best: 0.98

Starting e_i: 1209
Model ind 665 epoch 1209 batch: 0 avg loss -2.928783 avg loss no lamb -2.928783 time 2020-06-27 03:18:14.602787
Model ind 665 epoch 1209 batch: 100 avg loss -2.851649 avg loss no lamb -2.851649 time 2020-06-27 03:18:25.529617
Model ind 665 epoch 1209 batch: 200 avg loss -2.849108 avg loss no lamb -2.849108 time 2020-06-27 03:18:36.451089
Model ind 665 epoch 1209 batch: 300 avg loss -2.845258 avg loss no lamb -2.845258 time 2020-06-27 03:18:47.194677
Model ind 665 epoch 1209 batch: 400 avg loss -2.778105 avg loss no lamb -2.778105 time 2020-06-27 03:18:58.094199
Model ind 665 epoch 1209 batch: 500 avg loss -2.853226 avg loss no lamb -2.853226 time 2020-06-27 03:19:09.140543
Model ind 665 epoch 1209 batch: 600 avg loss -2.858215 avg loss no lamb -2.858215 time 2020-06-27 03:19:19.942326
Model ind 665 epoch 1209 batch: 700 avg loss -2.775789 avg loss no lamb -2.775789 time 2020-06-27 03:19:30.741719
Model ind 665 epoch 1209 batch: 800 avg loss -2.863794 avg loss no lamb -2.863794 time 2020-06-27 03:19:41.466191
last batch sz 10
Pre: time 2020-06-27 03:19:55.670714: 
 	std: 0.0031921132
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9808, 0.9743, 0.9817, 0.9754]
	train_accs: [0.9817167, 0.98105, 0.97575, 0.98181665, 0.9763833]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97872
	best: 0.9817

Starting e_i: 1210
Model ind 665 epoch 1210 batch: 0 avg loss -2.988798 avg loss no lamb -2.988798 time 2020-06-27 03:19:56.983432
Model ind 665 epoch 1210 batch: 100 avg loss -2.881466 avg loss no lamb -2.881466 time 2020-06-27 03:20:07.838200
Model ind 665 epoch 1210 batch: 200 avg loss -2.870477 avg loss no lamb -2.870477 time 2020-06-27 03:20:18.738002
Model ind 665 epoch 1210 batch: 300 avg loss -2.876229 avg loss no lamb -2.876229 time 2020-06-27 03:20:29.382431
Model ind 665 epoch 1210 batch: 400 avg loss -2.800130 avg loss no lamb -2.800130 time 2020-06-27 03:20:40.052738
Model ind 665 epoch 1210 batch: 500 avg loss -2.855455 avg loss no lamb -2.855455 time 2020-06-27 03:20:50.720836
Model ind 665 epoch 1210 batch: 600 avg loss -2.808913 avg loss no lamb -2.808913 time 2020-06-27 03:21:01.666774
Model ind 665 epoch 1210 batch: 700 avg loss -2.725394 avg loss no lamb -2.725394 time 2020-06-27 03:21:12.542376
Model ind 665 epoch 1210 batch: 800 avg loss -2.887139 avg loss no lamb -2.887139 time 2020-06-27 03:21:23.341793
last batch sz 10
Pre: time 2020-06-27 03:21:37.324568: 
 	std: 0.0024927058
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.9792, 0.9743, 0.981, 0.9767]
	train_accs: [0.9820833, 0.9810167, 0.97608334, 0.9820333, 0.9777833]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97831994
	best: 0.9804

Starting e_i: 1211
Model ind 665 epoch 1211 batch: 0 avg loss -2.959387 avg loss no lamb -2.959387 time 2020-06-27 03:21:39.524828
Model ind 665 epoch 1211 batch: 100 avg loss -2.823349 avg loss no lamb -2.823349 time 2020-06-27 03:21:50.333743
Model ind 665 epoch 1211 batch: 200 avg loss -2.833756 avg loss no lamb -2.833756 time 2020-06-27 03:22:01.162560
Model ind 665 epoch 1211 batch: 300 avg loss -2.858427 avg loss no lamb -2.858427 time 2020-06-27 03:22:11.930333
Model ind 665 epoch 1211 batch: 400 avg loss -2.812267 avg loss no lamb -2.812267 time 2020-06-27 03:22:22.884719
Model ind 665 epoch 1211 batch: 500 avg loss -2.826849 avg loss no lamb -2.826849 time 2020-06-27 03:22:33.749891
Model ind 665 epoch 1211 batch: 600 avg loss -2.885032 avg loss no lamb -2.885032 time 2020-06-27 03:22:44.419607
Model ind 665 epoch 1211 batch: 700 avg loss -2.794051 avg loss no lamb -2.794051 time 2020-06-27 03:22:55.246266
Model ind 665 epoch 1211 batch: 800 avg loss -2.866185 avg loss no lamb -2.866185 time 2020-06-27 03:23:06.049725
last batch sz 10
Pre: time 2020-06-27 03:23:20.237094: 
 	std: 0.0032424752
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9794, 0.9734, 0.9809, 0.9754]
	train_accs: [0.98195, 0.9809167, 0.97565, 0.98178333, 0.9769]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97817993
	best: 0.9818

Starting e_i: 1212
Model ind 665 epoch 1212 batch: 0 avg loss -2.941761 avg loss no lamb -2.941761 time 2020-06-27 03:23:21.537310
Model ind 665 epoch 1212 batch: 100 avg loss -2.901083 avg loss no lamb -2.901083 time 2020-06-27 03:23:32.368772
Model ind 665 epoch 1212 batch: 200 avg loss -2.891352 avg loss no lamb -2.891352 time 2020-06-27 03:23:43.182948
Model ind 665 epoch 1212 batch: 300 avg loss -2.893199 avg loss no lamb -2.893199 time 2020-06-27 03:23:54.046707
Model ind 665 epoch 1212 batch: 400 avg loss -2.808792 avg loss no lamb -2.808792 time 2020-06-27 03:24:05.270794
Model ind 665 epoch 1212 batch: 500 avg loss -2.825194 avg loss no lamb -2.825194 time 2020-06-27 03:24:16.177997
Model ind 665 epoch 1212 batch: 600 avg loss -2.884996 avg loss no lamb -2.884996 time 2020-06-27 03:24:27.103933
Model ind 665 epoch 1212 batch: 700 avg loss -2.769036 avg loss no lamb -2.769036 time 2020-06-27 03:24:37.840986
Model ind 665 epoch 1212 batch: 800 avg loss -2.798736 avg loss no lamb -2.798736 time 2020-06-27 03:24:48.656695
last batch sz 10
Pre: time 2020-06-27 03:25:03.072144: 
 	std: 0.0037965272
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9788, 0.9709, 0.9799, 0.9733]
	train_accs: [0.9809333, 0.9799167, 0.97353333, 0.9809333, 0.97536665]
	best_train_sub_head: 0
	worst: 0.9709
	avg: 0.97662
	best: 0.9802

Starting e_i: 1213
Model ind 665 epoch 1213 batch: 0 avg loss -2.929440 avg loss no lamb -2.929440 time 2020-06-27 03:25:04.229962
Model ind 665 epoch 1213 batch: 100 avg loss -2.894281 avg loss no lamb -2.894281 time 2020-06-27 03:25:15.073104
Model ind 665 epoch 1213 batch: 200 avg loss -2.862297 avg loss no lamb -2.862297 time 2020-06-27 03:25:25.797653
Model ind 665 epoch 1213 batch: 300 avg loss -2.895516 avg loss no lamb -2.895516 time 2020-06-27 03:25:36.777519
Model ind 665 epoch 1213 batch: 400 avg loss -2.751385 avg loss no lamb -2.751385 time 2020-06-27 03:25:47.740956
Model ind 665 epoch 1213 batch: 500 avg loss -2.833581 avg loss no lamb -2.833581 time 2020-06-27 03:25:58.446998
Model ind 665 epoch 1213 batch: 600 avg loss -2.836008 avg loss no lamb -2.836008 time 2020-06-27 03:26:09.423065
Model ind 665 epoch 1213 batch: 700 avg loss -2.768511 avg loss no lamb -2.768511 time 2020-06-27 03:26:20.172726
Model ind 665 epoch 1213 batch: 800 avg loss -2.889059 avg loss no lamb -2.889059 time 2020-06-27 03:26:30.981511
last batch sz 10
Pre: time 2020-06-27 03:26:44.922835: 
 	std: 0.0032765844
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9805, 0.9737, 0.9817, 0.9762]
	train_accs: [0.98183334, 0.98088336, 0.97565, 0.9818, 0.9766167]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.9788
	best: 0.9819

Starting e_i: 1214
Model ind 665 epoch 1214 batch: 0 avg loss -2.926768 avg loss no lamb -2.926768 time 2020-06-27 03:26:46.189645
Model ind 665 epoch 1214 batch: 100 avg loss -2.941627 avg loss no lamb -2.941627 time 2020-06-27 03:26:57.370979
Model ind 665 epoch 1214 batch: 200 avg loss -2.866868 avg loss no lamb -2.866868 time 2020-06-27 03:27:08.273517
Model ind 665 epoch 1214 batch: 300 avg loss -2.835309 avg loss no lamb -2.835309 time 2020-06-27 03:27:19.239236
Model ind 665 epoch 1214 batch: 400 avg loss -2.783789 avg loss no lamb -2.783789 time 2020-06-27 03:27:30.304687
Model ind 665 epoch 1214 batch: 500 avg loss -2.876276 avg loss no lamb -2.876276 time 2020-06-27 03:27:41.319503
Model ind 665 epoch 1214 batch: 600 avg loss -2.889699 avg loss no lamb -2.889699 time 2020-06-27 03:27:52.303896
Model ind 665 epoch 1214 batch: 700 avg loss -2.774148 avg loss no lamb -2.774148 time 2020-06-27 03:28:03.460450
Model ind 665 epoch 1214 batch: 800 avg loss -2.837460 avg loss no lamb -2.837460 time 2020-06-27 03:28:14.478934
last batch sz 10
Pre: time 2020-06-27 03:28:28.845815: 
 	std: 0.0029165852
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9798, 0.9745, 0.981, 0.9754]
	train_accs: [0.9816, 0.9809, 0.97603333, 0.9816167, 0.9762]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97844
	best: 0.981

Starting e_i: 1215
Model ind 665 epoch 1215 batch: 0 avg loss -2.960209 avg loss no lamb -2.960209 time 2020-06-27 03:28:30.037208
Model ind 665 epoch 1215 batch: 100 avg loss -2.885251 avg loss no lamb -2.885251 time 2020-06-27 03:28:40.998150
Model ind 665 epoch 1215 batch: 200 avg loss -2.875352 avg loss no lamb -2.875352 time 2020-06-27 03:28:51.827899
Model ind 665 epoch 1215 batch: 300 avg loss -2.874784 avg loss no lamb -2.874784 time 2020-06-27 03:29:02.748930
Model ind 665 epoch 1215 batch: 400 avg loss -2.761521 avg loss no lamb -2.761521 time 2020-06-27 03:29:13.539786
Model ind 665 epoch 1215 batch: 500 avg loss -2.832120 avg loss no lamb -2.832120 time 2020-06-27 03:29:24.401809
Model ind 665 epoch 1215 batch: 600 avg loss -2.895716 avg loss no lamb -2.895716 time 2020-06-27 03:29:35.431997
Model ind 665 epoch 1215 batch: 700 avg loss -2.815470 avg loss no lamb -2.815470 time 2020-06-27 03:29:46.232962
Model ind 665 epoch 1215 batch: 800 avg loss -2.805307 avg loss no lamb -2.805307 time 2020-06-27 03:29:57.138690
last batch sz 10
Pre: time 2020-06-27 03:30:11.620325: 
 	std: 0.0033532032
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9793, 0.9734, 0.9808, 0.9737]
	train_accs: [0.9810167, 0.9802333, 0.975, 0.9809, 0.9752]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.9775999
	best: 0.9808

Starting e_i: 1216
Model ind 665 epoch 1216 batch: 0 avg loss -2.932323 avg loss no lamb -2.932323 time 2020-06-27 03:30:12.948857
Model ind 665 epoch 1216 batch: 100 avg loss -2.934073 avg loss no lamb -2.934073 time 2020-06-27 03:30:24.165500
Model ind 665 epoch 1216 batch: 200 avg loss -2.923921 avg loss no lamb -2.923921 time 2020-06-27 03:30:34.929624
Model ind 665 epoch 1216 batch: 300 avg loss -2.864207 avg loss no lamb -2.864207 time 2020-06-27 03:30:45.870924
Model ind 665 epoch 1216 batch: 400 avg loss -2.756155 avg loss no lamb -2.756155 time 2020-06-27 03:30:56.876953
Model ind 665 epoch 1216 batch: 500 avg loss -2.903434 avg loss no lamb -2.903434 time 2020-06-27 03:31:08.243360
Model ind 665 epoch 1216 batch: 600 avg loss -2.864478 avg loss no lamb -2.864478 time 2020-06-27 03:31:19.140746
Model ind 665 epoch 1216 batch: 700 avg loss -2.699306 avg loss no lamb -2.699306 time 2020-06-27 03:31:30.121895
Model ind 665 epoch 1216 batch: 800 avg loss -2.854609 avg loss no lamb -2.854609 time 2020-06-27 03:31:41.054681
last batch sz 10
Pre: time 2020-06-27 03:31:55.347032: 
 	std: 0.0026902782
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9793, 0.9741, 0.9802, 0.9746]
	train_accs: [0.98113334, 0.98073334, 0.97536665, 0.9809833, 0.97601664]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97762
	best: 0.9799

Starting e_i: 1217
Model ind 665 epoch 1217 batch: 0 avg loss -2.969816 avg loss no lamb -2.969816 time 2020-06-27 03:31:56.490712
Model ind 665 epoch 1217 batch: 100 avg loss -2.896619 avg loss no lamb -2.896619 time 2020-06-27 03:32:07.605767
Model ind 665 epoch 1217 batch: 200 avg loss -2.861211 avg loss no lamb -2.861211 time 2020-06-27 03:32:18.343472
Model ind 665 epoch 1217 batch: 300 avg loss -2.842760 avg loss no lamb -2.842760 time 2020-06-27 03:32:29.321647
Model ind 665 epoch 1217 batch: 400 avg loss -2.755703 avg loss no lamb -2.755703 time 2020-06-27 03:32:40.343416
Model ind 665 epoch 1217 batch: 500 avg loss -2.846732 avg loss no lamb -2.846732 time 2020-06-27 03:32:51.041410
Model ind 665 epoch 1217 batch: 600 avg loss -2.887018 avg loss no lamb -2.887018 time 2020-06-27 03:33:01.995879
Model ind 665 epoch 1217 batch: 700 avg loss -2.738376 avg loss no lamb -2.738376 time 2020-06-27 03:33:12.965231
Model ind 665 epoch 1217 batch: 800 avg loss -2.847535 avg loss no lamb -2.847535 time 2020-06-27 03:33:23.896278
last batch sz 10
Pre: time 2020-06-27 03:33:38.371370: 
 	std: 0.00275434
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9795, 0.9745, 0.9802, 0.975]
	train_accs: [0.9819, 0.98055, 0.97566664, 0.9815, 0.9766]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97806007
	best: 0.9811

Starting e_i: 1218
Model ind 665 epoch 1218 batch: 0 avg loss -2.942463 avg loss no lamb -2.942463 time 2020-06-27 03:33:39.718846
Model ind 665 epoch 1218 batch: 100 avg loss -2.885049 avg loss no lamb -2.885049 time 2020-06-27 03:33:50.887938
Model ind 665 epoch 1218 batch: 200 avg loss -2.895272 avg loss no lamb -2.895272 time 2020-06-27 03:34:01.920006
Model ind 665 epoch 1218 batch: 300 avg loss -2.802576 avg loss no lamb -2.802576 time 2020-06-27 03:34:13.053038
Model ind 665 epoch 1218 batch: 400 avg loss -2.745879 avg loss no lamb -2.745879 time 2020-06-27 03:34:23.908073
Model ind 665 epoch 1218 batch: 500 avg loss -2.854076 avg loss no lamb -2.854076 time 2020-06-27 03:34:34.780637
Model ind 665 epoch 1218 batch: 600 avg loss -2.866730 avg loss no lamb -2.866730 time 2020-06-27 03:34:45.682106
Model ind 665 epoch 1218 batch: 700 avg loss -2.808085 avg loss no lamb -2.808085 time 2020-06-27 03:34:56.347206
Model ind 665 epoch 1218 batch: 800 avg loss -2.831120 avg loss no lamb -2.831120 time 2020-06-27 03:35:07.329462
last batch sz 10
Pre: time 2020-06-27 03:35:21.577378: 
 	std: 0.0027681026
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9791, 0.9739, 0.9806, 0.9755]
	train_accs: [0.9817167, 0.9805667, 0.9756167, 0.9816167, 0.97705]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97796
	best: 0.9807

Starting e_i: 1219
Model ind 665 epoch 1219 batch: 0 avg loss -2.924075 avg loss no lamb -2.924075 time 2020-06-27 03:35:22.761630
Model ind 665 epoch 1219 batch: 100 avg loss -2.951600 avg loss no lamb -2.951600 time 2020-06-27 03:35:33.690603
Model ind 665 epoch 1219 batch: 200 avg loss -2.882956 avg loss no lamb -2.882956 time 2020-06-27 03:35:44.435402
Model ind 665 epoch 1219 batch: 300 avg loss -2.847380 avg loss no lamb -2.847380 time 2020-06-27 03:35:55.417575
Model ind 665 epoch 1219 batch: 400 avg loss -2.776149 avg loss no lamb -2.776149 time 2020-06-27 03:36:06.394508
Model ind 665 epoch 1219 batch: 500 avg loss -2.862137 avg loss no lamb -2.862137 time 2020-06-27 03:36:17.098898
Model ind 665 epoch 1219 batch: 600 avg loss -2.879390 avg loss no lamb -2.879390 time 2020-06-27 03:36:28.094244
Model ind 665 epoch 1219 batch: 700 avg loss -2.703395 avg loss no lamb -2.703395 time 2020-06-27 03:36:39.124419
Model ind 665 epoch 1219 batch: 800 avg loss -2.859401 avg loss no lamb -2.859401 time 2020-06-27 03:36:50.037846
last batch sz 10
Pre: time 2020-06-27 03:37:04.370623: 
 	std: 0.0034417443
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9797, 0.9724, 0.9805, 0.9748]
	train_accs: [0.98185, 0.98095, 0.9748333, 0.98176664, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97768
	best: 0.981

Starting e_i: 1220
Model ind 665 epoch 1220 batch: 0 avg loss -2.904311 avg loss no lamb -2.904311 time 2020-06-27 03:37:05.747089
Model ind 665 epoch 1220 batch: 100 avg loss -2.888417 avg loss no lamb -2.888417 time 2020-06-27 03:37:16.686084
Model ind 665 epoch 1220 batch: 200 avg loss -2.882662 avg loss no lamb -2.882662 time 2020-06-27 03:37:27.503392
Model ind 665 epoch 1220 batch: 300 avg loss -2.895828 avg loss no lamb -2.895828 time 2020-06-27 03:37:38.475670
Model ind 665 epoch 1220 batch: 400 avg loss -2.775052 avg loss no lamb -2.775052 time 2020-06-27 03:37:49.171612
Model ind 665 epoch 1220 batch: 500 avg loss -2.816830 avg loss no lamb -2.816830 time 2020-06-27 03:37:59.944670
Model ind 665 epoch 1220 batch: 600 avg loss -2.911827 avg loss no lamb -2.911827 time 2020-06-27 03:38:10.743480
Model ind 665 epoch 1220 batch: 700 avg loss -2.795886 avg loss no lamb -2.795886 time 2020-06-27 03:38:21.619724
Model ind 665 epoch 1220 batch: 800 avg loss -2.832262 avg loss no lamb -2.832262 time 2020-06-27 03:38:32.483408
last batch sz 10
Pre: time 2020-06-27 03:38:46.535556: 
 	std: 0.0029400727
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9785, 0.9719, 0.9784, 0.9734]
	train_accs: [0.9808, 0.98053336, 0.97491664, 0.9805833, 0.97571665]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.9762
	best: 0.9788

Starting e_i: 1221
Model ind 665 epoch 1221 batch: 0 avg loss -2.920617 avg loss no lamb -2.920617 time 2020-06-27 03:38:48.921199
Model ind 665 epoch 1221 batch: 100 avg loss -2.918662 avg loss no lamb -2.918662 time 2020-06-27 03:38:59.737377
Model ind 665 epoch 1221 batch: 200 avg loss -2.909409 avg loss no lamb -2.909409 time 2020-06-27 03:39:10.529702
Model ind 665 epoch 1221 batch: 300 avg loss -2.799366 avg loss no lamb -2.799366 time 2020-06-27 03:39:21.456691
Model ind 665 epoch 1221 batch: 400 avg loss -2.717541 avg loss no lamb -2.717541 time 2020-06-27 03:39:32.567612
Model ind 665 epoch 1221 batch: 500 avg loss -2.866040 avg loss no lamb -2.866040 time 2020-06-27 03:39:43.404327
Model ind 665 epoch 1221 batch: 600 avg loss -2.932391 avg loss no lamb -2.932391 time 2020-06-27 03:39:54.192653
Model ind 665 epoch 1221 batch: 700 avg loss -2.780146 avg loss no lamb -2.780146 time 2020-06-27 03:40:05.285908
Model ind 665 epoch 1221 batch: 800 avg loss -2.854881 avg loss no lamb -2.854881 time 2020-06-27 03:40:16.388981
last batch sz 10
Pre: time 2020-06-27 03:40:30.442411: 
 	std: 0.0035268134
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9789, 0.9716, 0.9795, 0.9727]
	train_accs: [0.9810167, 0.98041666, 0.9744667, 0.9809167, 0.9759167]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.9764401
	best: 0.9795

Starting e_i: 1222
Model ind 665 epoch 1222 batch: 0 avg loss -2.924050 avg loss no lamb -2.924050 time 2020-06-27 03:40:31.802962
Model ind 665 epoch 1222 batch: 100 avg loss -2.914874 avg loss no lamb -2.914874 time 2020-06-27 03:40:42.846992
Model ind 665 epoch 1222 batch: 200 avg loss -2.857172 avg loss no lamb -2.857172 time 2020-06-27 03:40:53.696100
Model ind 665 epoch 1222 batch: 300 avg loss -2.877007 avg loss no lamb -2.877007 time 2020-06-27 03:41:04.543790
Model ind 665 epoch 1222 batch: 400 avg loss -2.811789 avg loss no lamb -2.811789 time 2020-06-27 03:41:15.429153
Model ind 665 epoch 1222 batch: 500 avg loss -2.825444 avg loss no lamb -2.825444 time 2020-06-27 03:41:26.264345
Model ind 665 epoch 1222 batch: 600 avg loss -2.863795 avg loss no lamb -2.863795 time 2020-06-27 03:41:37.269186
Model ind 665 epoch 1222 batch: 700 avg loss -2.795035 avg loss no lamb -2.795035 time 2020-06-27 03:41:48.034067
Model ind 665 epoch 1222 batch: 800 avg loss -2.905149 avg loss no lamb -2.905149 time 2020-06-27 03:41:58.903916
last batch sz 10
Pre: time 2020-06-27 03:42:13.522846: 
 	std: 0.002835773
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9823, 0.9813, 0.9755, 0.9818, 0.9767]
	train_accs: [0.9816833, 0.9813167, 0.9764, 0.98155, 0.97716665]
	best_train_sub_head: 0
	worst: 0.9755
	avg: 0.97951996
	best: 0.9823

Starting e_i: 1223
Model ind 665 epoch 1223 batch: 0 avg loss -2.962275 avg loss no lamb -2.962275 time 2020-06-27 03:42:14.698585
Model ind 665 epoch 1223 batch: 100 avg loss -2.836370 avg loss no lamb -2.836370 time 2020-06-27 03:42:25.325453
Model ind 665 epoch 1223 batch: 200 avg loss -2.910049 avg loss no lamb -2.910049 time 2020-06-27 03:42:36.250516
Model ind 665 epoch 1223 batch: 300 avg loss -2.872138 avg loss no lamb -2.872138 time 2020-06-27 03:42:47.024602
Model ind 665 epoch 1223 batch: 400 avg loss -2.719152 avg loss no lamb -2.719152 time 2020-06-27 03:42:58.059830
Model ind 665 epoch 1223 batch: 500 avg loss -2.806653 avg loss no lamb -2.806653 time 2020-06-27 03:43:08.871571
Model ind 665 epoch 1223 batch: 600 avg loss -2.859652 avg loss no lamb -2.859652 time 2020-06-27 03:43:19.717098
Model ind 665 epoch 1223 batch: 700 avg loss -2.708725 avg loss no lamb -2.708725 time 2020-06-27 03:43:30.450468
Model ind 665 epoch 1223 batch: 800 avg loss -2.917333 avg loss no lamb -2.917333 time 2020-06-27 03:43:41.430021
last batch sz 10
Pre: time 2020-06-27 03:43:55.642879: 
 	std: 0.0035335026
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9779, 0.9704, 0.979, 0.9726]
	train_accs: [0.9802, 0.9792333, 0.9734167, 0.98046666, 0.97505]
	best_train_sub_head: 3
	worst: 0.9704
	avg: 0.97572005
	best: 0.979

Starting e_i: 1224
Model ind 665 epoch 1224 batch: 0 avg loss -2.958913 avg loss no lamb -2.958913 time 2020-06-27 03:43:57.019717
Model ind 665 epoch 1224 batch: 100 avg loss -2.812570 avg loss no lamb -2.812570 time 2020-06-27 03:44:07.879122
Model ind 665 epoch 1224 batch: 200 avg loss -2.903257 avg loss no lamb -2.903257 time 2020-06-27 03:44:18.473317
Model ind 665 epoch 1224 batch: 300 avg loss -2.828410 avg loss no lamb -2.828410 time 2020-06-27 03:44:29.539159
Model ind 665 epoch 1224 batch: 400 avg loss -2.768656 avg loss no lamb -2.768656 time 2020-06-27 03:44:40.498474
Model ind 665 epoch 1224 batch: 500 avg loss -2.832179 avg loss no lamb -2.832179 time 2020-06-27 03:44:51.169550
Model ind 665 epoch 1224 batch: 600 avg loss -2.882137 avg loss no lamb -2.882137 time 2020-06-27 03:45:02.037536
Model ind 665 epoch 1224 batch: 700 avg loss -2.817187 avg loss no lamb -2.817187 time 2020-06-27 03:45:13.238603
Model ind 665 epoch 1224 batch: 800 avg loss -2.879941 avg loss no lamb -2.879941 time 2020-06-27 03:45:24.232591
last batch sz 10
Pre: time 2020-06-27 03:45:38.312638: 
 	std: 0.002350651
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9815, 0.9764, 0.982, 0.9778]
	train_accs: [0.9819, 0.98125, 0.97646666, 0.9817167, 0.9774333]
	best_train_sub_head: 0
	worst: 0.9764
	avg: 0.97992
	best: 0.9819

Starting e_i: 1225
Model ind 665 epoch 1225 batch: 0 avg loss -2.960371 avg loss no lamb -2.960371 time 2020-06-27 03:45:39.523537
Model ind 665 epoch 1225 batch: 100 avg loss -2.856315 avg loss no lamb -2.856315 time 2020-06-27 03:45:50.077216
Model ind 665 epoch 1225 batch: 200 avg loss -2.885297 avg loss no lamb -2.885297 time 2020-06-27 03:46:01.105474
Model ind 665 epoch 1225 batch: 300 avg loss -2.877856 avg loss no lamb -2.877856 time 2020-06-27 03:46:11.917011
Model ind 665 epoch 1225 batch: 400 avg loss -2.846360 avg loss no lamb -2.846360 time 2020-06-27 03:46:22.753033
Model ind 665 epoch 1225 batch: 500 avg loss -2.788357 avg loss no lamb -2.788357 time 2020-06-27 03:46:33.679854
Model ind 665 epoch 1225 batch: 600 avg loss -2.852239 avg loss no lamb -2.852239 time 2020-06-27 03:46:44.557098
Model ind 665 epoch 1225 batch: 700 avg loss -2.817594 avg loss no lamb -2.817594 time 2020-06-27 03:46:55.338595
Model ind 665 epoch 1225 batch: 800 avg loss -2.907500 avg loss no lamb -2.907500 time 2020-06-27 03:47:06.203586
last batch sz 10
Pre: time 2020-06-27 03:47:20.527004: 
 	std: 0.002705832
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9779, 0.9725, 0.9787, 0.9743]
	train_accs: [0.9805833, 0.9795667, 0.97501665, 0.98046666, 0.97605]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97658
	best: 0.9795

Starting e_i: 1226
Model ind 665 epoch 1226 batch: 0 avg loss -2.942144 avg loss no lamb -2.942144 time 2020-06-27 03:47:21.869563
Model ind 665 epoch 1226 batch: 100 avg loss -2.883594 avg loss no lamb -2.883594 time 2020-06-27 03:47:32.723742
Model ind 665 epoch 1226 batch: 200 avg loss -2.913488 avg loss no lamb -2.913488 time 2020-06-27 03:47:43.559322
Model ind 665 epoch 1226 batch: 300 avg loss -2.873421 avg loss no lamb -2.873421 time 2020-06-27 03:47:54.318056
Model ind 665 epoch 1226 batch: 400 avg loss -2.796275 avg loss no lamb -2.796275 time 2020-06-27 03:48:05.296348
Model ind 665 epoch 1226 batch: 500 avg loss -2.829039 avg loss no lamb -2.829039 time 2020-06-27 03:48:16.117225
Model ind 665 epoch 1226 batch: 600 avg loss -2.900300 avg loss no lamb -2.900300 time 2020-06-27 03:48:26.945449
Model ind 665 epoch 1226 batch: 700 avg loss -2.818007 avg loss no lamb -2.818007 time 2020-06-27 03:48:37.685924
Model ind 665 epoch 1226 batch: 800 avg loss -2.894045 avg loss no lamb -2.894045 time 2020-06-27 03:48:48.471699
last batch sz 10
Pre: time 2020-06-27 03:49:02.514258: 
 	std: 0.0026822283
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9819, 0.9761, 0.9829, 0.9778]
	train_accs: [0.9819667, 0.9813167, 0.9765, 0.9821333, 0.9774167]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.98014003
	best: 0.9829

Starting e_i: 1227
Model ind 665 epoch 1227 batch: 0 avg loss -2.938725 avg loss no lamb -2.938725 time 2020-06-27 03:49:03.666634
Model ind 665 epoch 1227 batch: 100 avg loss -2.917702 avg loss no lamb -2.917702 time 2020-06-27 03:49:14.381267
Model ind 665 epoch 1227 batch: 200 avg loss -2.843758 avg loss no lamb -2.843758 time 2020-06-27 03:49:25.268502
Model ind 665 epoch 1227 batch: 300 avg loss -2.899765 avg loss no lamb -2.899765 time 2020-06-27 03:49:35.942962
Model ind 665 epoch 1227 batch: 400 avg loss -2.793074 avg loss no lamb -2.793074 time 2020-06-27 03:49:46.778208
Model ind 665 epoch 1227 batch: 500 avg loss -2.851697 avg loss no lamb -2.851697 time 2020-06-27 03:49:57.766897
Model ind 665 epoch 1227 batch: 600 avg loss -2.886776 avg loss no lamb -2.886776 time 2020-06-27 03:50:08.818715
Model ind 665 epoch 1227 batch: 700 avg loss -2.791430 avg loss no lamb -2.791430 time 2020-06-27 03:50:19.705204
Model ind 665 epoch 1227 batch: 800 avg loss -2.902626 avg loss no lamb -2.902626 time 2020-06-27 03:50:30.631970
last batch sz 10
Pre: time 2020-06-27 03:50:44.764690: 
 	std: 0.003589766
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9826, 0.9806, 0.974, 0.9822, 0.9753]
	train_accs: [0.98221666, 0.98118335, 0.97545, 0.9820667, 0.9762167]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97894
	best: 0.9826

Starting e_i: 1228
Model ind 665 epoch 1228 batch: 0 avg loss -2.952787 avg loss no lamb -2.952787 time 2020-06-27 03:50:46.150295
Model ind 665 epoch 1228 batch: 100 avg loss -2.911771 avg loss no lamb -2.911771 time 2020-06-27 03:50:57.135176
Model ind 665 epoch 1228 batch: 200 avg loss -2.815312 avg loss no lamb -2.815312 time 2020-06-27 03:51:07.971290
Model ind 665 epoch 1228 batch: 300 avg loss -2.877331 avg loss no lamb -2.877331 time 2020-06-27 03:51:18.944042
Model ind 665 epoch 1228 batch: 400 avg loss -2.778358 avg loss no lamb -2.778358 time 2020-06-27 03:51:29.716824
Model ind 665 epoch 1228 batch: 500 avg loss -2.783022 avg loss no lamb -2.783022 time 2020-06-27 03:51:40.596709
Model ind 665 epoch 1228 batch: 600 avg loss -2.867281 avg loss no lamb -2.867281 time 2020-06-27 03:51:51.542935
Model ind 665 epoch 1228 batch: 700 avg loss -2.773805 avg loss no lamb -2.773805 time 2020-06-27 03:52:02.433859
Model ind 665 epoch 1228 batch: 800 avg loss -2.822622 avg loss no lamb -2.822622 time 2020-06-27 03:52:13.435274
last batch sz 10
Pre: time 2020-06-27 03:52:27.550250: 
 	std: 0.0033079232
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9809, 0.9742, 0.9815, 0.9753]
	train_accs: [0.98148334, 0.98108333, 0.9757, 0.98145, 0.97625]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97875994
	best: 0.9819

Starting e_i: 1229
Model ind 665 epoch 1229 batch: 0 avg loss -2.950916 avg loss no lamb -2.950916 time 2020-06-27 03:52:28.688560
Model ind 665 epoch 1229 batch: 100 avg loss -2.933460 avg loss no lamb -2.933460 time 2020-06-27 03:52:39.433689
Model ind 665 epoch 1229 batch: 200 avg loss -2.842626 avg loss no lamb -2.842626 time 2020-06-27 03:52:50.330079
Model ind 665 epoch 1229 batch: 300 avg loss -2.839695 avg loss no lamb -2.839695 time 2020-06-27 03:53:01.280137
Model ind 665 epoch 1229 batch: 400 avg loss -2.819627 avg loss no lamb -2.819627 time 2020-06-27 03:53:12.336140
Model ind 665 epoch 1229 batch: 500 avg loss -2.876110 avg loss no lamb -2.876110 time 2020-06-27 03:53:23.158758
Model ind 665 epoch 1229 batch: 600 avg loss -2.894024 avg loss no lamb -2.894024 time 2020-06-27 03:53:34.074775
Model ind 665 epoch 1229 batch: 700 avg loss -2.773633 avg loss no lamb -2.773633 time 2020-06-27 03:53:44.956869
Model ind 665 epoch 1229 batch: 800 avg loss -2.878733 avg loss no lamb -2.878733 time 2020-06-27 03:53:55.884012
last batch sz 10
Pre: time 2020-06-27 03:54:10.138213: 
 	std: 0.0038364576
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9804, 0.972, 0.981, 0.9738]
	train_accs: [0.98135, 0.98081666, 0.9748667, 0.98148334, 0.9762]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97754
	best: 0.981

Starting e_i: 1230
Model ind 665 epoch 1230 batch: 0 avg loss -2.929129 avg loss no lamb -2.929129 time 2020-06-27 03:54:11.493391
Model ind 665 epoch 1230 batch: 100 avg loss -2.840019 avg loss no lamb -2.840019 time 2020-06-27 03:54:22.034694
Model ind 665 epoch 1230 batch: 200 avg loss -2.885649 avg loss no lamb -2.885649 time 2020-06-27 03:54:32.768454
Model ind 665 epoch 1230 batch: 300 avg loss -2.826414 avg loss no lamb -2.826414 time 2020-06-27 03:54:43.682691
Model ind 665 epoch 1230 batch: 400 avg loss -2.788784 avg loss no lamb -2.788784 time 2020-06-27 03:54:54.555243
Model ind 665 epoch 1230 batch: 500 avg loss -2.834347 avg loss no lamb -2.834347 time 2020-06-27 03:55:05.178641
Model ind 665 epoch 1230 batch: 600 avg loss -2.895406 avg loss no lamb -2.895406 time 2020-06-27 03:55:16.035555
Model ind 665 epoch 1230 batch: 700 avg loss -2.746148 avg loss no lamb -2.746148 time 2020-06-27 03:55:26.783800
Model ind 665 epoch 1230 batch: 800 avg loss -2.864637 avg loss no lamb -2.864637 time 2020-06-27 03:55:37.551762
last batch sz 10
Pre: time 2020-06-27 03:55:51.653951: 
 	std: 0.002856146
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9782, 0.9724, 0.9794, 0.9737]
	train_accs: [0.98065, 0.9801, 0.97475, 0.98078334, 0.9756]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.97648
	best: 0.9794

Starting e_i: 1231
Model ind 665 epoch 1231 batch: 0 avg loss -2.986690 avg loss no lamb -2.986690 time 2020-06-27 03:55:54.038585
Model ind 665 epoch 1231 batch: 100 avg loss -2.916315 avg loss no lamb -2.916315 time 2020-06-27 03:56:04.886410
Model ind 665 epoch 1231 batch: 200 avg loss -2.856600 avg loss no lamb -2.856600 time 2020-06-27 03:56:15.898889
Model ind 665 epoch 1231 batch: 300 avg loss -2.875849 avg loss no lamb -2.875849 time 2020-06-27 03:56:26.740476
Model ind 665 epoch 1231 batch: 400 avg loss -2.787420 avg loss no lamb -2.787420 time 2020-06-27 03:56:37.578865
Model ind 665 epoch 1231 batch: 500 avg loss -2.892561 avg loss no lamb -2.892561 time 2020-06-27 03:56:48.495176
Model ind 665 epoch 1231 batch: 600 avg loss -2.924254 avg loss no lamb -2.924254 time 2020-06-27 03:56:59.376400
Model ind 665 epoch 1231 batch: 700 avg loss -2.782359 avg loss no lamb -2.782359 time 2020-06-27 03:57:10.268577
Model ind 665 epoch 1231 batch: 800 avg loss -2.857224 avg loss no lamb -2.857224 time 2020-06-27 03:57:21.099739
last batch sz 10
Pre: time 2020-06-27 03:57:35.281657: 
 	std: 0.0028111152
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9801, 0.9739, 0.9803, 0.9754]
	train_accs: [0.98175, 0.9812667, 0.9759333, 0.98181665, 0.97686666]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.9780399
	best: 0.9803

Starting e_i: 1232
Model ind 665 epoch 1232 batch: 0 avg loss -2.927137 avg loss no lamb -2.927137 time 2020-06-27 03:57:36.628754
Model ind 665 epoch 1232 batch: 100 avg loss -2.917039 avg loss no lamb -2.917039 time 2020-06-27 03:57:47.475431
Model ind 665 epoch 1232 batch: 200 avg loss -2.878601 avg loss no lamb -2.878601 time 2020-06-27 03:57:58.542156
Model ind 665 epoch 1232 batch: 300 avg loss -2.829631 avg loss no lamb -2.829631 time 2020-06-27 03:58:09.708167
Model ind 665 epoch 1232 batch: 400 avg loss -2.783451 avg loss no lamb -2.783451 time 2020-06-27 03:58:20.857224
Model ind 665 epoch 1232 batch: 500 avg loss -2.897147 avg loss no lamb -2.897147 time 2020-06-27 03:58:31.979818
Model ind 665 epoch 1232 batch: 600 avg loss -2.902699 avg loss no lamb -2.902699 time 2020-06-27 03:58:43.280741
Model ind 665 epoch 1232 batch: 700 avg loss -2.753097 avg loss no lamb -2.753097 time 2020-06-27 03:58:54.422331
Model ind 665 epoch 1232 batch: 800 avg loss -2.805515 avg loss no lamb -2.805515 time 2020-06-27 03:59:05.481650
last batch sz 10
Pre: time 2020-06-27 03:59:19.568483: 
 	std: 0.00287652
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9796, 0.9743, 0.981, 0.9751]
	train_accs: [0.98193336, 0.98078334, 0.9759833, 0.98188335, 0.9767]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97816
	best: 0.9808

Starting e_i: 1233
Model ind 665 epoch 1233 batch: 0 avg loss -2.961391 avg loss no lamb -2.961391 time 2020-06-27 03:59:20.700234
Model ind 665 epoch 1233 batch: 100 avg loss -2.898360 avg loss no lamb -2.898360 time 2020-06-27 03:59:31.477751
Model ind 665 epoch 1233 batch: 200 avg loss -2.886219 avg loss no lamb -2.886219 time 2020-06-27 03:59:42.517589
Model ind 665 epoch 1233 batch: 300 avg loss -2.841012 avg loss no lamb -2.841012 time 2020-06-27 03:59:53.300142
Model ind 665 epoch 1233 batch: 400 avg loss -2.778768 avg loss no lamb -2.778768 time 2020-06-27 04:00:04.065357
Model ind 665 epoch 1233 batch: 500 avg loss -2.884881 avg loss no lamb -2.884881 time 2020-06-27 04:00:14.954216
Model ind 665 epoch 1233 batch: 600 avg loss -2.876768 avg loss no lamb -2.876768 time 2020-06-27 04:00:25.669383
Model ind 665 epoch 1233 batch: 700 avg loss -2.779512 avg loss no lamb -2.779512 time 2020-06-27 04:00:36.587632
Model ind 665 epoch 1233 batch: 800 avg loss -2.852907 avg loss no lamb -2.852907 time 2020-06-27 04:00:47.300293
last batch sz 10
Pre: time 2020-06-27 04:01:01.419763: 
 	std: 0.0029318873
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9791, 0.9728, 0.9794, 0.9741]
	train_accs: [0.9810333, 0.9807, 0.9754, 0.98095, 0.97625]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97699994
	best: 0.9796

Starting e_i: 1234
Model ind 665 epoch 1234 batch: 0 avg loss -2.933255 avg loss no lamb -2.933255 time 2020-06-27 04:01:02.705582
Model ind 665 epoch 1234 batch: 100 avg loss -2.933231 avg loss no lamb -2.933231 time 2020-06-27 04:01:13.459488
Model ind 665 epoch 1234 batch: 200 avg loss -2.838660 avg loss no lamb -2.838660 time 2020-06-27 04:01:24.423024
Model ind 665 epoch 1234 batch: 300 avg loss -2.905959 avg loss no lamb -2.905959 time 2020-06-27 04:01:35.071969
Model ind 665 epoch 1234 batch: 400 avg loss -2.813148 avg loss no lamb -2.813148 time 2020-06-27 04:01:46.039448
Model ind 665 epoch 1234 batch: 500 avg loss -2.900909 avg loss no lamb -2.900909 time 2020-06-27 04:01:56.894009
Model ind 665 epoch 1234 batch: 600 avg loss -2.859013 avg loss no lamb -2.859013 time 2020-06-27 04:02:07.741134
Model ind 665 epoch 1234 batch: 700 avg loss -2.730368 avg loss no lamb -2.730368 time 2020-06-27 04:02:18.686777
Model ind 665 epoch 1234 batch: 800 avg loss -2.901162 avg loss no lamb -2.901162 time 2020-06-27 04:02:29.399676
last batch sz 10
Pre: time 2020-06-27 04:02:43.517987: 
 	std: 0.0027102767
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9793, 0.9739, 0.9803, 0.9754]
	train_accs: [0.9817333, 0.9809, 0.97575, 0.98165, 0.97695]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97788
	best: 0.9805

Starting e_i: 1235
Model ind 665 epoch 1235 batch: 0 avg loss -2.907561 avg loss no lamb -2.907561 time 2020-06-27 04:02:44.642699
Model ind 665 epoch 1235 batch: 100 avg loss -2.846448 avg loss no lamb -2.846448 time 2020-06-27 04:02:55.198325
Model ind 665 epoch 1235 batch: 200 avg loss -2.856612 avg loss no lamb -2.856612 time 2020-06-27 04:03:06.055432
Model ind 665 epoch 1235 batch: 300 avg loss -2.862668 avg loss no lamb -2.862668 time 2020-06-27 04:03:16.676828
Model ind 665 epoch 1235 batch: 400 avg loss -2.742639 avg loss no lamb -2.742639 time 2020-06-27 04:03:27.295997
Model ind 665 epoch 1235 batch: 500 avg loss -2.854467 avg loss no lamb -2.854467 time 2020-06-27 04:03:37.917113
Model ind 665 epoch 1235 batch: 600 avg loss -2.853822 avg loss no lamb -2.853822 time 2020-06-27 04:03:48.737070
Model ind 665 epoch 1235 batch: 700 avg loss -2.793079 avg loss no lamb -2.793079 time 2020-06-27 04:03:59.777033
Model ind 665 epoch 1235 batch: 800 avg loss -2.843168 avg loss no lamb -2.843168 time 2020-06-27 04:04:10.449065
last batch sz 10
Pre: time 2020-06-27 04:04:24.390087: 
 	std: 0.0027276315
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9802, 0.9744, 0.9805, 0.9756]
	train_accs: [0.9820667, 0.9812, 0.9761, 0.98195, 0.97685]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.9783
	best: 0.9808

Starting e_i: 1236
Model ind 665 epoch 1236 batch: 0 avg loss -2.936030 avg loss no lamb -2.936030 time 2020-06-27 04:04:25.708794
Model ind 665 epoch 1236 batch: 100 avg loss -2.882045 avg loss no lamb -2.882045 time 2020-06-27 04:04:36.545847
Model ind 665 epoch 1236 batch: 200 avg loss -2.877064 avg loss no lamb -2.877064 time 2020-06-27 04:04:47.335013
Model ind 665 epoch 1236 batch: 300 avg loss -2.872352 avg loss no lamb -2.872352 time 2020-06-27 04:04:58.367444
Model ind 665 epoch 1236 batch: 400 avg loss -2.821264 avg loss no lamb -2.821264 time 2020-06-27 04:05:09.353513
Model ind 665 epoch 1236 batch: 500 avg loss -2.809270 avg loss no lamb -2.809270 time 2020-06-27 04:05:20.271210
Model ind 665 epoch 1236 batch: 600 avg loss -2.857351 avg loss no lamb -2.857351 time 2020-06-27 04:05:31.052687
Model ind 665 epoch 1236 batch: 700 avg loss -2.815200 avg loss no lamb -2.815200 time 2020-06-27 04:05:41.805607
Model ind 665 epoch 1236 batch: 800 avg loss -2.839079 avg loss no lamb -2.839079 time 2020-06-27 04:05:52.673484
last batch sz 10
Pre: time 2020-06-27 04:06:06.828471: 
 	std: 0.0027214715
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9795, 0.9742, 0.9804, 0.9754]
	train_accs: [0.98145, 0.9807, 0.9756167, 0.98145, 0.9765667]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97805995
	best: 0.9808

Starting e_i: 1237
Model ind 665 epoch 1237 batch: 0 avg loss -2.961350 avg loss no lamb -2.961350 time 2020-06-27 04:06:08.028203
Model ind 665 epoch 1237 batch: 100 avg loss -2.888518 avg loss no lamb -2.888518 time 2020-06-27 04:06:18.819191
Model ind 665 epoch 1237 batch: 200 avg loss -2.797234 avg loss no lamb -2.797234 time 2020-06-27 04:06:29.751691
Model ind 665 epoch 1237 batch: 300 avg loss -2.898766 avg loss no lamb -2.898766 time 2020-06-27 04:06:40.637630
Model ind 665 epoch 1237 batch: 400 avg loss -2.778884 avg loss no lamb -2.778884 time 2020-06-27 04:06:51.582205
Model ind 665 epoch 1237 batch: 500 avg loss -2.849654 avg loss no lamb -2.849654 time 2020-06-27 04:07:02.361485
Model ind 665 epoch 1237 batch: 600 avg loss -2.876569 avg loss no lamb -2.876569 time 2020-06-27 04:07:13.202125
Model ind 665 epoch 1237 batch: 700 avg loss -2.797471 avg loss no lamb -2.797471 time 2020-06-27 04:07:23.990561
Model ind 665 epoch 1237 batch: 800 avg loss -2.815599 avg loss no lamb -2.815599 time 2020-06-27 04:07:34.949082
last batch sz 10
Pre: time 2020-06-27 04:07:49.093671: 
 	std: 0.0028117
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9796, 0.9744, 0.9814, 0.9762]
	train_accs: [0.98188335, 0.9809167, 0.9755833, 0.98176664, 0.97716665]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97858
	best: 0.9813

Starting e_i: 1238
Model ind 665 epoch 1238 batch: 0 avg loss -2.982716 avg loss no lamb -2.982716 time 2020-06-27 04:07:50.473242
Model ind 665 epoch 1238 batch: 100 avg loss -2.907324 avg loss no lamb -2.907324 time 2020-06-27 04:08:01.419090
Model ind 665 epoch 1238 batch: 200 avg loss -2.841861 avg loss no lamb -2.841861 time 2020-06-27 04:08:12.271461
Model ind 665 epoch 1238 batch: 300 avg loss -2.935858 avg loss no lamb -2.935858 time 2020-06-27 04:08:23.089935
Model ind 665 epoch 1238 batch: 400 avg loss -2.816427 avg loss no lamb -2.816427 time 2020-06-27 04:08:33.889450
Model ind 665 epoch 1238 batch: 500 avg loss -2.899141 avg loss no lamb -2.899141 time 2020-06-27 04:08:44.732909
Model ind 665 epoch 1238 batch: 600 avg loss -2.818890 avg loss no lamb -2.818890 time 2020-06-27 04:08:55.587169
Model ind 665 epoch 1238 batch: 700 avg loss -2.864507 avg loss no lamb -2.864507 time 2020-06-27 04:09:06.457150
Model ind 665 epoch 1238 batch: 800 avg loss -2.843733 avg loss no lamb -2.843733 time 2020-06-27 04:09:17.333657
last batch sz 10
Pre: time 2020-06-27 04:09:31.509938: 
 	std: 0.002876104
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9819, 0.9741, 0.9807, 0.9767]
	train_accs: [0.9817333, 0.9817333, 0.9764, 0.98175, 0.97728336]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.9787
	best: 0.9807

Starting e_i: 1239
Model ind 665 epoch 1239 batch: 0 avg loss -2.969119 avg loss no lamb -2.969119 time 2020-06-27 04:09:32.642119
Model ind 665 epoch 1239 batch: 100 avg loss -2.847075 avg loss no lamb -2.847075 time 2020-06-27 04:09:43.441908
Model ind 665 epoch 1239 batch: 200 avg loss -2.889453 avg loss no lamb -2.889453 time 2020-06-27 04:09:54.050208
Model ind 665 epoch 1239 batch: 300 avg loss -2.856745 avg loss no lamb -2.856745 time 2020-06-27 04:10:04.922797
Model ind 665 epoch 1239 batch: 400 avg loss -2.789110 avg loss no lamb -2.789110 time 2020-06-27 04:10:15.694585
Model ind 665 epoch 1239 batch: 500 avg loss -2.790645 avg loss no lamb -2.790645 time 2020-06-27 04:10:26.552040
Model ind 665 epoch 1239 batch: 600 avg loss -2.921850 avg loss no lamb -2.921850 time 2020-06-27 04:10:37.364917
Model ind 665 epoch 1239 batch: 700 avg loss -2.798448 avg loss no lamb -2.798448 time 2020-06-27 04:10:48.351621
Model ind 665 epoch 1239 batch: 800 avg loss -2.822994 avg loss no lamb -2.822994 time 2020-06-27 04:10:59.540489
last batch sz 10
Pre: time 2020-06-27 04:11:13.700721: 
 	std: 0.0029072342
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9804, 0.9745, 0.982, 0.9767]
	train_accs: [0.9816333, 0.98108333, 0.9757, 0.9817, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.979
	best: 0.982

Starting e_i: 1240
Model ind 665 epoch 1240 batch: 0 avg loss -2.941041 avg loss no lamb -2.941041 time 2020-06-27 04:11:14.845238
Model ind 665 epoch 1240 batch: 100 avg loss -2.901033 avg loss no lamb -2.901033 time 2020-06-27 04:11:25.565572
Model ind 665 epoch 1240 batch: 200 avg loss -2.922694 avg loss no lamb -2.922694 time 2020-06-27 04:11:36.424464
Model ind 665 epoch 1240 batch: 300 avg loss -2.899834 avg loss no lamb -2.899834 time 2020-06-27 04:11:47.270396
Model ind 665 epoch 1240 batch: 400 avg loss -2.812163 avg loss no lamb -2.812163 time 2020-06-27 04:11:58.354701
Model ind 665 epoch 1240 batch: 500 avg loss -2.920614 avg loss no lamb -2.920614 time 2020-06-27 04:12:09.290964
Model ind 665 epoch 1240 batch: 600 avg loss -2.852985 avg loss no lamb -2.852985 time 2020-06-27 04:12:20.093757
Model ind 665 epoch 1240 batch: 700 avg loss -2.781806 avg loss no lamb -2.781806 time 2020-06-27 04:12:31.002361
Model ind 665 epoch 1240 batch: 800 avg loss -2.803723 avg loss no lamb -2.803723 time 2020-06-27 04:12:42.002507
last batch sz 10
Pre: time 2020-06-27 04:12:56.158668: 
 	std: 0.0032319594
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9807, 0.9746, 0.9819, 0.9752]
	train_accs: [0.9818, 0.9812, 0.9762, 0.98193336, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97882
	best: 0.9819

Starting e_i: 1241
Model ind 665 epoch 1241 batch: 0 avg loss -2.940611 avg loss no lamb -2.940611 time 2020-06-27 04:12:58.522046
Model ind 665 epoch 1241 batch: 100 avg loss -2.862564 avg loss no lamb -2.862564 time 2020-06-27 04:13:09.685209
Model ind 665 epoch 1241 batch: 200 avg loss -2.894276 avg loss no lamb -2.894276 time 2020-06-27 04:13:20.579187
Model ind 665 epoch 1241 batch: 300 avg loss -2.810745 avg loss no lamb -2.810745 time 2020-06-27 04:13:31.397188
Model ind 665 epoch 1241 batch: 400 avg loss -2.744184 avg loss no lamb -2.744184 time 2020-06-27 04:13:42.363557
Model ind 665 epoch 1241 batch: 500 avg loss -2.822591 avg loss no lamb -2.822591 time 2020-06-27 04:13:53.101280
Model ind 665 epoch 1241 batch: 600 avg loss -2.875534 avg loss no lamb -2.875534 time 2020-06-27 04:14:03.973519
Model ind 665 epoch 1241 batch: 700 avg loss -2.758785 avg loss no lamb -2.758785 time 2020-06-27 04:14:14.832881
Model ind 665 epoch 1241 batch: 800 avg loss -2.826743 avg loss no lamb -2.826743 time 2020-06-27 04:14:25.600188
last batch sz 10
Pre: time 2020-06-27 04:14:39.716797: 
 	std: 0.0026242002
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.9789, 0.973, 0.9785, 0.9739]
	train_accs: [0.9814, 0.98046666, 0.97536665, 0.9813833, 0.9763167]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.9766399
	best: 0.9789

Starting e_i: 1242
Model ind 665 epoch 1242 batch: 0 avg loss -2.938994 avg loss no lamb -2.938994 time 2020-06-27 04:14:40.944038
Model ind 665 epoch 1242 batch: 100 avg loss -2.950439 avg loss no lamb -2.950439 time 2020-06-27 04:14:51.599519
Model ind 665 epoch 1242 batch: 200 avg loss -2.885747 avg loss no lamb -2.885747 time 2020-06-27 04:15:02.469586
Model ind 665 epoch 1242 batch: 300 avg loss -2.874624 avg loss no lamb -2.874624 time 2020-06-27 04:15:13.643158
Model ind 665 epoch 1242 batch: 400 avg loss -2.753683 avg loss no lamb -2.753683 time 2020-06-27 04:15:24.635251
Model ind 665 epoch 1242 batch: 500 avg loss -2.836899 avg loss no lamb -2.836899 time 2020-06-27 04:15:35.450443
Model ind 665 epoch 1242 batch: 600 avg loss -2.908177 avg loss no lamb -2.908177 time 2020-06-27 04:15:46.323318
Model ind 665 epoch 1242 batch: 700 avg loss -2.800894 avg loss no lamb -2.800894 time 2020-06-27 04:15:57.163679
Model ind 665 epoch 1242 batch: 800 avg loss -2.873244 avg loss no lamb -2.873244 time 2020-06-27 04:16:08.059181
last batch sz 10
Pre: time 2020-06-27 04:16:22.169736: 
 	std: 0.0034002247
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9784, 0.9718, 0.9795, 0.9732]
	train_accs: [0.98105, 0.9796, 0.97396666, 0.98088336, 0.97533333]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.97658
	best: 0.98

Starting e_i: 1243
Model ind 665 epoch 1243 batch: 0 avg loss -2.905732 avg loss no lamb -2.905732 time 2020-06-27 04:16:23.345792
Model ind 665 epoch 1243 batch: 100 avg loss -2.840554 avg loss no lamb -2.840554 time 2020-06-27 04:16:34.231141
Model ind 665 epoch 1243 batch: 200 avg loss -2.846749 avg loss no lamb -2.846749 time 2020-06-27 04:16:44.862416
Model ind 665 epoch 1243 batch: 300 avg loss -2.912686 avg loss no lamb -2.912686 time 2020-06-27 04:16:55.742212
Model ind 665 epoch 1243 batch: 400 avg loss -2.740459 avg loss no lamb -2.740459 time 2020-06-27 04:17:06.677889
Model ind 665 epoch 1243 batch: 500 avg loss -2.824755 avg loss no lamb -2.824755 time 2020-06-27 04:17:17.454454
Model ind 665 epoch 1243 batch: 600 avg loss -2.893589 avg loss no lamb -2.893589 time 2020-06-27 04:17:28.196878
Model ind 665 epoch 1243 batch: 700 avg loss -2.891177 avg loss no lamb -2.891177 time 2020-06-27 04:17:39.027899
Model ind 665 epoch 1243 batch: 800 avg loss -2.903827 avg loss no lamb -2.903827 time 2020-06-27 04:17:49.782160
last batch sz 10
Pre: time 2020-06-27 04:18:04.008998: 
 	std: 0.003511132
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9802, 0.9733, 0.9812, 0.9744]
	train_accs: [0.9812833, 0.98046666, 0.97491664, 0.98125, 0.97611666]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.9781
	best: 0.9814

Starting e_i: 1244
Model ind 665 epoch 1244 batch: 0 avg loss -2.962540 avg loss no lamb -2.962540 time 2020-06-27 04:18:05.241814
Model ind 665 epoch 1244 batch: 100 avg loss -2.896163 avg loss no lamb -2.896163 time 2020-06-27 04:18:15.963087
Model ind 665 epoch 1244 batch: 200 avg loss -2.867686 avg loss no lamb -2.867686 time 2020-06-27 04:18:26.884335
Model ind 665 epoch 1244 batch: 300 avg loss -2.877836 avg loss no lamb -2.877836 time 2020-06-27 04:18:37.599215
Model ind 665 epoch 1244 batch: 400 avg loss -2.787085 avg loss no lamb -2.787085 time 2020-06-27 04:18:48.405393
Model ind 665 epoch 1244 batch: 500 avg loss -2.849740 avg loss no lamb -2.849740 time 2020-06-27 04:18:59.125104
Model ind 665 epoch 1244 batch: 600 avg loss -2.891561 avg loss no lamb -2.891561 time 2020-06-27 04:19:10.065379
Model ind 665 epoch 1244 batch: 700 avg loss -2.779893 avg loss no lamb -2.779893 time 2020-06-27 04:19:21.129604
Model ind 665 epoch 1244 batch: 800 avg loss -2.820754 avg loss no lamb -2.820754 time 2020-06-27 04:19:31.899677
last batch sz 10
Pre: time 2020-06-27 04:19:46.208529: 
 	std: 0.0029722701
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9791, 0.9728, 0.9802, 0.9754]
	train_accs: [0.98113334, 0.9798833, 0.9748, 0.98113334, 0.9762667]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97756004
	best: 0.9803

Starting e_i: 1245
Model ind 665 epoch 1245 batch: 0 avg loss -2.959038 avg loss no lamb -2.959038 time 2020-06-27 04:19:47.349854
Model ind 665 epoch 1245 batch: 100 avg loss -2.821964 avg loss no lamb -2.821964 time 2020-06-27 04:19:58.240902
Model ind 665 epoch 1245 batch: 200 avg loss -2.880949 avg loss no lamb -2.880949 time 2020-06-27 04:20:09.188261
Model ind 665 epoch 1245 batch: 300 avg loss -2.793846 avg loss no lamb -2.793846 time 2020-06-27 04:20:20.111985
Model ind 665 epoch 1245 batch: 400 avg loss -2.812429 avg loss no lamb -2.812429 time 2020-06-27 04:20:30.685914
Model ind 665 epoch 1245 batch: 500 avg loss -2.840577 avg loss no lamb -2.840577 time 2020-06-27 04:20:41.469742
Model ind 665 epoch 1245 batch: 600 avg loss -2.842530 avg loss no lamb -2.842530 time 2020-06-27 04:20:52.288863
Model ind 665 epoch 1245 batch: 700 avg loss -2.803282 avg loss no lamb -2.803282 time 2020-06-27 04:21:03.266994
Model ind 665 epoch 1245 batch: 800 avg loss -2.887636 avg loss no lamb -2.887636 time 2020-06-27 04:21:14.041940
last batch sz 10
Pre: time 2020-06-27 04:21:28.178821: 
 	std: 0.003204374
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.981, 0.9737, 0.981, 0.9761]
	train_accs: [0.98186666, 0.9810333, 0.97535, 0.98156667, 0.9769667]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.9787
	best: 0.9817

Starting e_i: 1246
Model ind 665 epoch 1246 batch: 0 avg loss -2.916912 avg loss no lamb -2.916912 time 2020-06-27 04:21:29.304503
Model ind 665 epoch 1246 batch: 100 avg loss -2.877508 avg loss no lamb -2.877508 time 2020-06-27 04:21:40.324409
Model ind 665 epoch 1246 batch: 200 avg loss -2.913227 avg loss no lamb -2.913227 time 2020-06-27 04:21:51.367372
Model ind 665 epoch 1246 batch: 300 avg loss -2.827836 avg loss no lamb -2.827836 time 2020-06-27 04:22:02.401249
Model ind 665 epoch 1246 batch: 400 avg loss -2.808972 avg loss no lamb -2.808972 time 2020-06-27 04:22:13.340540
Model ind 665 epoch 1246 batch: 500 avg loss -2.880818 avg loss no lamb -2.880818 time 2020-06-27 04:22:24.122012
Model ind 665 epoch 1246 batch: 600 avg loss -2.842626 avg loss no lamb -2.842626 time 2020-06-27 04:22:34.976479
Model ind 665 epoch 1246 batch: 700 avg loss -2.842221 avg loss no lamb -2.842221 time 2020-06-27 04:22:45.983237
Model ind 665 epoch 1246 batch: 800 avg loss -2.890977 avg loss no lamb -2.890977 time 2020-06-27 04:22:56.730158
last batch sz 10
Pre: time 2020-06-27 04:23:11.097208: 
 	std: 0.0031170568
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9804, 0.9736, 0.9812, 0.9769]
	train_accs: [0.9816333, 0.98095, 0.97536665, 0.98145, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.9788
	best: 0.9819

Starting e_i: 1247
Model ind 665 epoch 1247 batch: 0 avg loss -2.952194 avg loss no lamb -2.952194 time 2020-06-27 04:23:12.257290
Model ind 665 epoch 1247 batch: 100 avg loss -2.916139 avg loss no lamb -2.916139 time 2020-06-27 04:23:23.085418
Model ind 665 epoch 1247 batch: 200 avg loss -2.893976 avg loss no lamb -2.893976 time 2020-06-27 04:23:33.654745
Model ind 665 epoch 1247 batch: 300 avg loss -2.907730 avg loss no lamb -2.907730 time 2020-06-27 04:23:44.483244
Model ind 665 epoch 1247 batch: 400 avg loss -2.818578 avg loss no lamb -2.818578 time 2020-06-27 04:23:55.464799
Model ind 665 epoch 1247 batch: 500 avg loss -2.855704 avg loss no lamb -2.855704 time 2020-06-27 04:24:06.364872
Model ind 665 epoch 1247 batch: 600 avg loss -2.858676 avg loss no lamb -2.858676 time 2020-06-27 04:24:17.222975
Model ind 665 epoch 1247 batch: 700 avg loss -2.786101 avg loss no lamb -2.786101 time 2020-06-27 04:24:27.878861
Model ind 665 epoch 1247 batch: 800 avg loss -2.761698 avg loss no lamb -2.761698 time 2020-06-27 04:24:38.804566
last batch sz 10
Pre: time 2020-06-27 04:24:52.960792: 
 	std: 0.0034614408
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9796, 0.9719, 0.9798, 0.9743]
	train_accs: [0.98113334, 0.98065, 0.97468334, 0.98081666, 0.97578335]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97721994
	best: 0.9805

Starting e_i: 1248
Model ind 665 epoch 1248 batch: 0 avg loss -2.974054 avg loss no lamb -2.974054 time 2020-06-27 04:24:54.108096
Model ind 665 epoch 1248 batch: 100 avg loss -2.923048 avg loss no lamb -2.923048 time 2020-06-27 04:25:05.304188
Model ind 665 epoch 1248 batch: 200 avg loss -2.895252 avg loss no lamb -2.895252 time 2020-06-27 04:25:16.031525
Model ind 665 epoch 1248 batch: 300 avg loss -2.886269 avg loss no lamb -2.886269 time 2020-06-27 04:25:26.995633
Model ind 665 epoch 1248 batch: 400 avg loss -2.831498 avg loss no lamb -2.831498 time 2020-06-27 04:25:37.990033
Model ind 665 epoch 1248 batch: 500 avg loss -2.841689 avg loss no lamb -2.841689 time 2020-06-27 04:25:48.971097
Model ind 665 epoch 1248 batch: 600 avg loss -2.916358 avg loss no lamb -2.916358 time 2020-06-27 04:26:00.049789
Model ind 665 epoch 1248 batch: 700 avg loss -2.770545 avg loss no lamb -2.770545 time 2020-06-27 04:26:10.888407
Model ind 665 epoch 1248 batch: 800 avg loss -2.844955 avg loss no lamb -2.844955 time 2020-06-27 04:26:21.767688
last batch sz 10
Pre: time 2020-06-27 04:26:36.175289: 
 	std: 0.0026298228
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9786, 0.9737, 0.9794, 0.9739]
	train_accs: [0.98108333, 0.98038334, 0.97571665, 0.98118335, 0.9763833]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97699994
	best: 0.9794

Starting e_i: 1249
Model ind 665 epoch 1249 batch: 0 avg loss -2.960506 avg loss no lamb -2.960506 time 2020-06-27 04:26:37.329665
Model ind 665 epoch 1249 batch: 100 avg loss -2.900158 avg loss no lamb -2.900158 time 2020-06-27 04:26:48.146339
Model ind 665 epoch 1249 batch: 200 avg loss -2.880860 avg loss no lamb -2.880860 time 2020-06-27 04:26:58.846877
Model ind 665 epoch 1249 batch: 300 avg loss -2.867291 avg loss no lamb -2.867291 time 2020-06-27 04:27:09.888013
Model ind 665 epoch 1249 batch: 400 avg loss -2.832696 avg loss no lamb -2.832696 time 2020-06-27 04:27:20.801088
Model ind 665 epoch 1249 batch: 500 avg loss -2.836051 avg loss no lamb -2.836051 time 2020-06-27 04:27:31.644057
Model ind 665 epoch 1249 batch: 600 avg loss -2.895900 avg loss no lamb -2.895900 time 2020-06-27 04:27:42.498016
Model ind 665 epoch 1249 batch: 700 avg loss -2.808586 avg loss no lamb -2.808586 time 2020-06-27 04:27:53.460182
Model ind 665 epoch 1249 batch: 800 avg loss -2.854576 avg loss no lamb -2.854576 time 2020-06-27 04:28:04.484261
last batch sz 10
Pre: time 2020-06-27 04:28:18.647675: 
 	std: 0.0030268098
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9805, 0.9743, 0.9807, 0.9751]
	train_accs: [0.98156667, 0.9811, 0.9755833, 0.98165, 0.9766]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97838
	best: 0.9807

Starting e_i: 1250
Model ind 665 epoch 1250 batch: 0 avg loss -2.908959 avg loss no lamb -2.908959 time 2020-06-27 04:28:19.826958
Model ind 665 epoch 1250 batch: 100 avg loss -2.882487 avg loss no lamb -2.882487 time 2020-06-27 04:28:31.054027
Model ind 665 epoch 1250 batch: 200 avg loss -2.860841 avg loss no lamb -2.860841 time 2020-06-27 04:28:42.066468
Model ind 665 epoch 1250 batch: 300 avg loss -2.914649 avg loss no lamb -2.914649 time 2020-06-27 04:28:53.018962
Model ind 665 epoch 1250 batch: 400 avg loss -2.757766 avg loss no lamb -2.757766 time 2020-06-27 04:29:04.159327
Model ind 665 epoch 1250 batch: 500 avg loss -2.873843 avg loss no lamb -2.873843 time 2020-06-27 04:29:15.156487
Model ind 665 epoch 1250 batch: 600 avg loss -2.894729 avg loss no lamb -2.894729 time 2020-06-27 04:29:26.121311
Model ind 665 epoch 1250 batch: 700 avg loss -2.779509 avg loss no lamb -2.779509 time 2020-06-27 04:29:36.938353
Model ind 665 epoch 1250 batch: 800 avg loss -2.853972 avg loss no lamb -2.853972 time 2020-06-27 04:29:48.227628
last batch sz 10
Pre: time 2020-06-27 04:30:02.669411: 
 	std: 0.00302655
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9782, 0.9724, 0.9791, 0.9738]
	train_accs: [0.9809667, 0.98013335, 0.9744667, 0.9809, 0.97555]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.9767
	best: 0.98

Starting e_i: 1251
Model ind 665 epoch 1251 batch: 0 avg loss -2.910190 avg loss no lamb -2.910190 time 2020-06-27 04:30:05.075504
Model ind 665 epoch 1251 batch: 100 avg loss -2.918765 avg loss no lamb -2.918765 time 2020-06-27 04:30:15.700439
Model ind 665 epoch 1251 batch: 200 avg loss -2.877979 avg loss no lamb -2.877979 time 2020-06-27 04:30:26.623969
Model ind 665 epoch 1251 batch: 300 avg loss -2.880889 avg loss no lamb -2.880889 time 2020-06-27 04:30:37.283911
Model ind 665 epoch 1251 batch: 400 avg loss -2.781267 avg loss no lamb -2.781267 time 2020-06-27 04:30:48.086352
Model ind 665 epoch 1251 batch: 500 avg loss -2.786869 avg loss no lamb -2.786869 time 2020-06-27 04:30:59.025114
Model ind 665 epoch 1251 batch: 600 avg loss -2.866632 avg loss no lamb -2.866632 time 2020-06-27 04:31:09.999369
Model ind 665 epoch 1251 batch: 700 avg loss -2.791802 avg loss no lamb -2.791802 time 2020-06-27 04:31:20.844573
Model ind 665 epoch 1251 batch: 800 avg loss -2.844366 avg loss no lamb -2.844366 time 2020-06-27 04:31:31.436426
last batch sz 10
Pre: time 2020-06-27 04:31:45.525708: 
 	std: 0.002818801
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9787, 0.9786, 0.9726, 0.9776, 0.9726]
	train_accs: [0.98071665, 0.98005, 0.9743, 0.98046666, 0.97511667]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97602
	best: 0.9787

Starting e_i: 1252
Model ind 665 epoch 1252 batch: 0 avg loss -2.957547 avg loss no lamb -2.957547 time 2020-06-27 04:31:46.677224
Model ind 665 epoch 1252 batch: 100 avg loss -2.936346 avg loss no lamb -2.936346 time 2020-06-27 04:31:57.476915
Model ind 665 epoch 1252 batch: 200 avg loss -2.874473 avg loss no lamb -2.874473 time 2020-06-27 04:32:08.390193
Model ind 665 epoch 1252 batch: 300 avg loss -2.868181 avg loss no lamb -2.868181 time 2020-06-27 04:32:19.362140
Model ind 665 epoch 1252 batch: 400 avg loss -2.774355 avg loss no lamb -2.774355 time 2020-06-27 04:32:29.974628
Model ind 665 epoch 1252 batch: 500 avg loss -2.883497 avg loss no lamb -2.883497 time 2020-06-27 04:32:40.853601
Model ind 665 epoch 1252 batch: 600 avg loss -2.909523 avg loss no lamb -2.909523 time 2020-06-27 04:32:51.872457
Model ind 665 epoch 1252 batch: 700 avg loss -2.814854 avg loss no lamb -2.814854 time 2020-06-27 04:33:02.574136
Model ind 665 epoch 1252 batch: 800 avg loss -2.924395 avg loss no lamb -2.924395 time 2020-06-27 04:33:13.482775
last batch sz 10
Pre: time 2020-06-27 04:33:27.885093: 
 	std: 0.0030594117
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9797, 0.9739, 0.98, 0.9743]
	train_accs: [0.98111665, 0.9804, 0.97568333, 0.98071665, 0.9758]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.9778
	best: 0.9811

Starting e_i: 1253
Model ind 665 epoch 1253 batch: 0 avg loss -2.929297 avg loss no lamb -2.929297 time 2020-06-27 04:33:29.044854
Model ind 665 epoch 1253 batch: 100 avg loss -2.918437 avg loss no lamb -2.918437 time 2020-06-27 04:33:39.898018
Model ind 665 epoch 1253 batch: 200 avg loss -2.867627 avg loss no lamb -2.867627 time 2020-06-27 04:33:50.362045
Model ind 665 epoch 1253 batch: 300 avg loss -2.903456 avg loss no lamb -2.903456 time 2020-06-27 04:34:01.132429
Model ind 665 epoch 1253 batch: 400 avg loss -2.732500 avg loss no lamb -2.732500 time 2020-06-27 04:34:11.879017
Model ind 665 epoch 1253 batch: 500 avg loss -2.825690 avg loss no lamb -2.825690 time 2020-06-27 04:34:22.752113
Model ind 665 epoch 1253 batch: 600 avg loss -2.885693 avg loss no lamb -2.885693 time 2020-06-27 04:34:33.668353
Model ind 665 epoch 1253 batch: 700 avg loss -2.755923 avg loss no lamb -2.755923 time 2020-06-27 04:34:44.634277
Model ind 665 epoch 1253 batch: 800 avg loss -2.891722 avg loss no lamb -2.891722 time 2020-06-27 04:34:55.567411
last batch sz 10
Pre: time 2020-06-27 04:35:09.637795: 
 	std: 0.003562032
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9811, 0.974, 0.9817, 0.9745]
	train_accs: [0.98181665, 0.98118335, 0.97575, 0.98186666, 0.97681665]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.9786
	best: 0.9817

Starting e_i: 1254
Model ind 665 epoch 1254 batch: 0 avg loss -2.975519 avg loss no lamb -2.975519 time 2020-06-27 04:35:10.782351
Model ind 665 epoch 1254 batch: 100 avg loss -2.935704 avg loss no lamb -2.935704 time 2020-06-27 04:35:21.430192
Model ind 665 epoch 1254 batch: 200 avg loss -2.866335 avg loss no lamb -2.866335 time 2020-06-27 04:35:32.376827
Model ind 665 epoch 1254 batch: 300 avg loss -2.872340 avg loss no lamb -2.872340 time 2020-06-27 04:35:43.189632
Model ind 665 epoch 1254 batch: 400 avg loss -2.786887 avg loss no lamb -2.786887 time 2020-06-27 04:35:53.946760
Model ind 665 epoch 1254 batch: 500 avg loss -2.879397 avg loss no lamb -2.879397 time 2020-06-27 04:36:04.890129
Model ind 665 epoch 1254 batch: 600 avg loss -2.864579 avg loss no lamb -2.864579 time 2020-06-27 04:36:15.632983
Model ind 665 epoch 1254 batch: 700 avg loss -2.847220 avg loss no lamb -2.847220 time 2020-06-27 04:36:26.501717
Model ind 665 epoch 1254 batch: 800 avg loss -2.859356 avg loss no lamb -2.859356 time 2020-06-27 04:36:37.346027
last batch sz 10
Pre: time 2020-06-27 04:36:52.052330: 
 	std: 0.0034424542
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9797, 0.9726, 0.9812, 0.9758]
	train_accs: [0.9817, 0.98036665, 0.97501665, 0.98156667, 0.9769833]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97816
	best: 0.9815

Starting e_i: 1255
Model ind 665 epoch 1255 batch: 0 avg loss -2.983511 avg loss no lamb -2.983511 time 2020-06-27 04:36:53.203314
Model ind 665 epoch 1255 batch: 100 avg loss -2.865402 avg loss no lamb -2.865402 time 2020-06-27 04:37:04.133085
Model ind 665 epoch 1255 batch: 200 avg loss -2.792111 avg loss no lamb -2.792111 time 2020-06-27 04:37:15.039563
Model ind 665 epoch 1255 batch: 300 avg loss -2.835855 avg loss no lamb -2.835855 time 2020-06-27 04:37:26.179135
Model ind 665 epoch 1255 batch: 400 avg loss -2.830635 avg loss no lamb -2.830635 time 2020-06-27 04:37:36.889208
Model ind 665 epoch 1255 batch: 500 avg loss -2.826782 avg loss no lamb -2.826782 time 2020-06-27 04:37:48.065525
Model ind 665 epoch 1255 batch: 600 avg loss -2.883562 avg loss no lamb -2.883562 time 2020-06-27 04:37:58.949100
Model ind 665 epoch 1255 batch: 700 avg loss -2.787855 avg loss no lamb -2.787855 time 2020-06-27 04:38:09.850941
Model ind 665 epoch 1255 batch: 800 avg loss -2.845566 avg loss no lamb -2.845566 time 2020-06-27 04:38:20.841565
last batch sz 10
Pre: time 2020-06-27 04:38:35.367266: 
 	std: 0.003513737
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9784, 0.971, 0.9798, 0.9731]
	train_accs: [0.98073334, 0.97978336, 0.97321665, 0.9809167, 0.97513336]
	best_train_sub_head: 3
	worst: 0.971
	avg: 0.97624
	best: 0.9798

Starting e_i: 1256
Model ind 665 epoch 1256 batch: 0 avg loss -2.983163 avg loss no lamb -2.983163 time 2020-06-27 04:38:36.588261
Model ind 665 epoch 1256 batch: 100 avg loss -2.870755 avg loss no lamb -2.870755 time 2020-06-27 04:38:47.639514
Model ind 665 epoch 1256 batch: 200 avg loss -2.915998 avg loss no lamb -2.915998 time 2020-06-27 04:38:58.440524
Model ind 665 epoch 1256 batch: 300 avg loss -2.809791 avg loss no lamb -2.809791 time 2020-06-27 04:39:09.459698
Model ind 665 epoch 1256 batch: 400 avg loss -2.801869 avg loss no lamb -2.801869 time 2020-06-27 04:39:20.549651
Model ind 665 epoch 1256 batch: 500 avg loss -2.872775 avg loss no lamb -2.872775 time 2020-06-27 04:39:31.405349
Model ind 665 epoch 1256 batch: 600 avg loss -2.872616 avg loss no lamb -2.872616 time 2020-06-27 04:39:42.224533
Model ind 665 epoch 1256 batch: 700 avg loss -2.788132 avg loss no lamb -2.788132 time 2020-06-27 04:39:52.888643
Model ind 665 epoch 1256 batch: 800 avg loss -2.882388 avg loss no lamb -2.882388 time 2020-06-27 04:40:03.584757
last batch sz 10
Pre: time 2020-06-27 04:40:17.824678: 
 	std: 0.0031428584
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9805, 0.974, 0.9809, 0.975]
	train_accs: [0.98146665, 0.9808667, 0.9756167, 0.98181665, 0.97655]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97832
	best: 0.9809

Starting e_i: 1257
Model ind 665 epoch 1257 batch: 0 avg loss -2.996027 avg loss no lamb -2.996027 time 2020-06-27 04:40:18.985233
Model ind 665 epoch 1257 batch: 100 avg loss -2.904739 avg loss no lamb -2.904739 time 2020-06-27 04:40:29.973303
Model ind 665 epoch 1257 batch: 200 avg loss -2.851575 avg loss no lamb -2.851575 time 2020-06-27 04:40:40.683638
Model ind 665 epoch 1257 batch: 300 avg loss -2.886793 avg loss no lamb -2.886793 time 2020-06-27 04:40:51.638834
Model ind 665 epoch 1257 batch: 400 avg loss -2.855136 avg loss no lamb -2.855136 time 2020-06-27 04:41:02.351138
Model ind 665 epoch 1257 batch: 500 avg loss -2.839888 avg loss no lamb -2.839888 time 2020-06-27 04:41:13.129858
Model ind 665 epoch 1257 batch: 600 avg loss -2.904143 avg loss no lamb -2.904143 time 2020-06-27 04:41:24.352683
Model ind 665 epoch 1257 batch: 700 avg loss -2.755317 avg loss no lamb -2.755317 time 2020-06-27 04:41:35.305893
Model ind 665 epoch 1257 batch: 800 avg loss -2.897012 avg loss no lamb -2.897012 time 2020-06-27 04:41:46.075499
last batch sz 10
Pre: time 2020-06-27 04:42:00.092242: 
 	std: 0.0027895623
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9825, 0.9804, 0.9755, 0.9823, 0.9772]
	train_accs: [0.9824167, 0.98135, 0.97625, 0.9827, 0.97761667]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97958004
	best: 0.9823

Starting e_i: 1258
Model ind 665 epoch 1258 batch: 0 avg loss -2.898967 avg loss no lamb -2.898967 time 2020-06-27 04:42:01.272886
Model ind 665 epoch 1258 batch: 100 avg loss -2.904015 avg loss no lamb -2.904015 time 2020-06-27 04:42:12.167374
Model ind 665 epoch 1258 batch: 200 avg loss -2.831137 avg loss no lamb -2.831137 time 2020-06-27 04:42:23.071184
Model ind 665 epoch 1258 batch: 300 avg loss -2.882903 avg loss no lamb -2.882903 time 2020-06-27 04:42:33.864117
Model ind 665 epoch 1258 batch: 400 avg loss -2.833245 avg loss no lamb -2.833245 time 2020-06-27 04:42:44.570564
Model ind 665 epoch 1258 batch: 500 avg loss -2.864063 avg loss no lamb -2.864063 time 2020-06-27 04:42:55.080359
Model ind 665 epoch 1258 batch: 600 avg loss -2.898887 avg loss no lamb -2.898887 time 2020-06-27 04:43:06.240012
Model ind 665 epoch 1258 batch: 700 avg loss -2.802310 avg loss no lamb -2.802310 time 2020-06-27 04:43:17.168390
Model ind 665 epoch 1258 batch: 800 avg loss -2.875604 avg loss no lamb -2.875604 time 2020-06-27 04:43:28.085579
last batch sz 10
Pre: time 2020-06-27 04:43:42.443041: 
 	std: 0.0021094056
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9788, 0.9747, 0.9792, 0.975]
	train_accs: [0.9812833, 0.98043334, 0.97565, 0.9813167, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97742
	best: 0.9792

Starting e_i: 1259
Model ind 665 epoch 1259 batch: 0 avg loss -2.979553 avg loss no lamb -2.979553 time 2020-06-27 04:43:43.629246
Model ind 665 epoch 1259 batch: 100 avg loss -2.932130 avg loss no lamb -2.932130 time 2020-06-27 04:43:54.483208
Model ind 665 epoch 1259 batch: 200 avg loss -2.846177 avg loss no lamb -2.846177 time 2020-06-27 04:44:05.439648
Model ind 665 epoch 1259 batch: 300 avg loss -2.837826 avg loss no lamb -2.837826 time 2020-06-27 04:44:16.431823
Model ind 665 epoch 1259 batch: 400 avg loss -2.834051 avg loss no lamb -2.834051 time 2020-06-27 04:44:27.222592
Model ind 665 epoch 1259 batch: 500 avg loss -2.851264 avg loss no lamb -2.851264 time 2020-06-27 04:44:38.105871
Model ind 665 epoch 1259 batch: 600 avg loss -2.897496 avg loss no lamb -2.897496 time 2020-06-27 04:44:48.902936
Model ind 665 epoch 1259 batch: 700 avg loss -2.781852 avg loss no lamb -2.781852 time 2020-06-27 04:44:59.468515
Model ind 665 epoch 1259 batch: 800 avg loss -2.803432 avg loss no lamb -2.803432 time 2020-06-27 04:45:10.252379
last batch sz 10
Pre: time 2020-06-27 04:45:24.226522: 
 	std: 0.0024344136
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9783, 0.9737, 0.9792, 0.9739]
	train_accs: [0.98078334, 0.98013335, 0.97573334, 0.98106664, 0.97665]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97676
	best: 0.9792

Starting e_i: 1260
Model ind 665 epoch 1260 batch: 0 avg loss -2.928656 avg loss no lamb -2.928656 time 2020-06-27 04:45:25.371064
Model ind 665 epoch 1260 batch: 100 avg loss -2.872817 avg loss no lamb -2.872817 time 2020-06-27 04:45:36.239273
Model ind 665 epoch 1260 batch: 200 avg loss -2.900326 avg loss no lamb -2.900326 time 2020-06-27 04:45:47.048051
Model ind 665 epoch 1260 batch: 300 avg loss -2.843985 avg loss no lamb -2.843985 time 2020-06-27 04:45:57.865923
Model ind 665 epoch 1260 batch: 400 avg loss -2.735613 avg loss no lamb -2.735613 time 2020-06-27 04:46:08.989727
Model ind 665 epoch 1260 batch: 500 avg loss -2.846114 avg loss no lamb -2.846114 time 2020-06-27 04:46:19.669633
Model ind 665 epoch 1260 batch: 600 avg loss -2.828990 avg loss no lamb -2.828990 time 2020-06-27 04:46:30.740033
Model ind 665 epoch 1260 batch: 700 avg loss -2.819807 avg loss no lamb -2.819807 time 2020-06-27 04:46:41.549989
Model ind 665 epoch 1260 batch: 800 avg loss -2.918061 avg loss no lamb -2.918061 time 2020-06-27 04:46:52.367971
last batch sz 10
Pre: time 2020-06-27 04:47:06.513749: 
 	std: 0.0024511116
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9806, 0.9753, 0.9812, 0.9766]
	train_accs: [0.98185, 0.9810333, 0.9764, 0.9820167, 0.97758335]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.9789001
	best: 0.9812

Starting e_i: 1261
Model ind 665 epoch 1261 batch: 0 avg loss -2.994886 avg loss no lamb -2.994886 time 2020-06-27 04:47:08.875124
Model ind 665 epoch 1261 batch: 100 avg loss -2.850309 avg loss no lamb -2.850309 time 2020-06-27 04:47:19.735371
Model ind 665 epoch 1261 batch: 200 avg loss -2.801856 avg loss no lamb -2.801856 time 2020-06-27 04:47:30.605128
Model ind 665 epoch 1261 batch: 300 avg loss -2.866970 avg loss no lamb -2.866970 time 2020-06-27 04:47:41.354845
Model ind 665 epoch 1261 batch: 400 avg loss -2.800571 avg loss no lamb -2.800571 time 2020-06-27 04:47:52.135747
Model ind 665 epoch 1261 batch: 500 avg loss -2.875845 avg loss no lamb -2.875845 time 2020-06-27 04:48:02.916575
Model ind 665 epoch 1261 batch: 600 avg loss -2.860801 avg loss no lamb -2.860801 time 2020-06-27 04:48:13.952973
Model ind 665 epoch 1261 batch: 700 avg loss -2.833509 avg loss no lamb -2.833509 time 2020-06-27 04:48:24.641696
Model ind 665 epoch 1261 batch: 800 avg loss -2.902714 avg loss no lamb -2.902714 time 2020-06-27 04:48:35.253692
last batch sz 10
Pre: time 2020-06-27 04:48:49.188341: 
 	std: 0.0022526425
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.9801, 0.9752, 0.9799, 0.9752]
	train_accs: [0.9813667, 0.98066664, 0.97601664, 0.98115, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97793996
	best: 0.9793

Starting e_i: 1262
Model ind 665 epoch 1262 batch: 0 avg loss -2.937908 avg loss no lamb -2.937908 time 2020-06-27 04:48:50.379745
Model ind 665 epoch 1262 batch: 100 avg loss -2.863190 avg loss no lamb -2.863190 time 2020-06-27 04:49:01.395276
Model ind 665 epoch 1262 batch: 200 avg loss -2.915623 avg loss no lamb -2.915623 time 2020-06-27 04:49:12.187573
Model ind 665 epoch 1262 batch: 300 avg loss -2.880697 avg loss no lamb -2.880697 time 2020-06-27 04:49:22.882928
Model ind 665 epoch 1262 batch: 400 avg loss -2.826180 avg loss no lamb -2.826180 time 2020-06-27 04:49:33.557999
Model ind 665 epoch 1262 batch: 500 avg loss -2.853896 avg loss no lamb -2.853896 time 2020-06-27 04:49:44.156181
Model ind 665 epoch 1262 batch: 600 avg loss -2.917017 avg loss no lamb -2.917017 time 2020-06-27 04:49:54.931856
Model ind 665 epoch 1262 batch: 700 avg loss -2.661052 avg loss no lamb -2.661052 time 2020-06-27 04:50:05.864905
Model ind 665 epoch 1262 batch: 800 avg loss -2.902276 avg loss no lamb -2.902276 time 2020-06-27 04:50:16.482949
last batch sz 10
Pre: time 2020-06-27 04:50:30.502429: 
 	std: 0.0028249037
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9792, 0.9733, 0.9795, 0.9736]
	train_accs: [0.98043334, 0.98018336, 0.97478336, 0.9808667, 0.9759167]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9769
	best: 0.9795

Starting e_i: 1263
Model ind 665 epoch 1263 batch: 0 avg loss -2.919430 avg loss no lamb -2.919430 time 2020-06-27 04:50:31.655978
Model ind 665 epoch 1263 batch: 100 avg loss -2.872700 avg loss no lamb -2.872700 time 2020-06-27 04:50:42.345910
Model ind 665 epoch 1263 batch: 200 avg loss -2.855523 avg loss no lamb -2.855523 time 2020-06-27 04:50:53.137146
Model ind 665 epoch 1263 batch: 300 avg loss -2.901288 avg loss no lamb -2.901288 time 2020-06-27 04:51:03.956929
Model ind 665 epoch 1263 batch: 400 avg loss -2.815704 avg loss no lamb -2.815704 time 2020-06-27 04:51:14.845632
Model ind 665 epoch 1263 batch: 500 avg loss -2.838956 avg loss no lamb -2.838956 time 2020-06-27 04:51:25.791003
Model ind 665 epoch 1263 batch: 600 avg loss -2.932711 avg loss no lamb -2.932711 time 2020-06-27 04:51:36.744396
Model ind 665 epoch 1263 batch: 700 avg loss -2.771189 avg loss no lamb -2.771189 time 2020-06-27 04:51:47.897293
Model ind 665 epoch 1263 batch: 800 avg loss -2.923706 avg loss no lamb -2.923706 time 2020-06-27 04:51:58.737037
last batch sz 10
Pre: time 2020-06-27 04:52:12.760613: 
 	std: 0.0034324334
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9799, 0.9734, 0.982, 0.9751]
	train_accs: [0.98183334, 0.98046666, 0.97505, 0.9820333, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97832
	best: 0.982

Starting e_i: 1264
Model ind 665 epoch 1264 batch: 0 avg loss -2.938105 avg loss no lamb -2.938105 time 2020-06-27 04:52:13.917929
Model ind 665 epoch 1264 batch: 100 avg loss -2.858439 avg loss no lamb -2.858439 time 2020-06-27 04:52:24.950265
Model ind 665 epoch 1264 batch: 200 avg loss -2.861997 avg loss no lamb -2.861997 time 2020-06-27 04:52:35.922034
Model ind 665 epoch 1264 batch: 300 avg loss -2.864828 avg loss no lamb -2.864828 time 2020-06-27 04:52:46.916964
Model ind 665 epoch 1264 batch: 400 avg loss -2.747314 avg loss no lamb -2.747314 time 2020-06-27 04:52:57.553322
Model ind 665 epoch 1264 batch: 500 avg loss -2.891824 avg loss no lamb -2.891824 time 2020-06-27 04:53:08.450261
Model ind 665 epoch 1264 batch: 600 avg loss -2.876446 avg loss no lamb -2.876446 time 2020-06-27 04:53:19.441227
Model ind 665 epoch 1264 batch: 700 avg loss -2.731075 avg loss no lamb -2.731075 time 2020-06-27 04:53:30.553697
Model ind 665 epoch 1264 batch: 800 avg loss -2.854729 avg loss no lamb -2.854729 time 2020-06-27 04:53:41.642409
last batch sz 10
Pre: time 2020-06-27 04:53:55.954638: 
 	std: 0.0035543903
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9802, 0.9736, 0.9802, 0.9729]
	train_accs: [0.98176664, 0.98071665, 0.97548336, 0.98156667, 0.9762167]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97757995
	best: 0.981

Starting e_i: 1265
Model ind 665 epoch 1265 batch: 0 avg loss -2.930164 avg loss no lamb -2.930164 time 2020-06-27 04:53:57.148894
Model ind 665 epoch 1265 batch: 100 avg loss -2.883758 avg loss no lamb -2.883758 time 2020-06-27 04:54:08.127682
Model ind 665 epoch 1265 batch: 200 avg loss -2.925155 avg loss no lamb -2.925155 time 2020-06-27 04:54:19.159145
Model ind 665 epoch 1265 batch: 300 avg loss -2.897040 avg loss no lamb -2.897040 time 2020-06-27 04:54:30.036669
Model ind 665 epoch 1265 batch: 400 avg loss -2.768387 avg loss no lamb -2.768387 time 2020-06-27 04:54:40.948427
Model ind 665 epoch 1265 batch: 500 avg loss -2.882381 avg loss no lamb -2.882381 time 2020-06-27 04:54:51.993632
Model ind 665 epoch 1265 batch: 600 avg loss -2.889552 avg loss no lamb -2.889552 time 2020-06-27 04:55:03.027451
Model ind 665 epoch 1265 batch: 700 avg loss -2.727546 avg loss no lamb -2.727546 time 2020-06-27 04:55:14.039549
Model ind 665 epoch 1265 batch: 800 avg loss -2.828125 avg loss no lamb -2.828125 time 2020-06-27 04:55:24.967960
last batch sz 10
Pre: time 2020-06-27 04:55:39.345540: 
 	std: 0.0030327397
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9827, 0.9818, 0.9757, 0.9826, 0.9768]
	train_accs: [0.9823, 0.98156667, 0.9762333, 0.98246664, 0.97728336]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97992
	best: 0.9826

Starting e_i: 1266
Model ind 665 epoch 1266 batch: 0 avg loss -2.956843 avg loss no lamb -2.956843 time 2020-06-27 04:55:40.552312
Model ind 665 epoch 1266 batch: 100 avg loss -2.892124 avg loss no lamb -2.892124 time 2020-06-27 04:55:51.317718
Model ind 665 epoch 1266 batch: 200 avg loss -2.877412 avg loss no lamb -2.877412 time 2020-06-27 04:56:02.111810
Model ind 665 epoch 1266 batch: 300 avg loss -2.873372 avg loss no lamb -2.873372 time 2020-06-27 04:56:13.040052
Model ind 665 epoch 1266 batch: 400 avg loss -2.744970 avg loss no lamb -2.744970 time 2020-06-27 04:56:23.894643
Model ind 665 epoch 1266 batch: 500 avg loss -2.828888 avg loss no lamb -2.828888 time 2020-06-27 04:56:34.747062
Model ind 665 epoch 1266 batch: 600 avg loss -2.864017 avg loss no lamb -2.864017 time 2020-06-27 04:56:45.579483
Model ind 665 epoch 1266 batch: 700 avg loss -2.789139 avg loss no lamb -2.789139 time 2020-06-27 04:56:56.498159
Model ind 665 epoch 1266 batch: 800 avg loss -2.873114 avg loss no lamb -2.873114 time 2020-06-27 04:57:07.577578
last batch sz 10
Pre: time 2020-06-27 04:57:21.947177: 
 	std: 0.002361352
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9807, 0.976, 0.9817, 0.9767]
	train_accs: [0.98185, 0.98123336, 0.97611666, 0.9819833, 0.9770667]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.9792
	best: 0.9817

Starting e_i: 1267
Model ind 665 epoch 1267 batch: 0 avg loss -2.950590 avg loss no lamb -2.950590 time 2020-06-27 04:57:23.088251
Model ind 665 epoch 1267 batch: 100 avg loss -2.855624 avg loss no lamb -2.855624 time 2020-06-27 04:57:33.844622
Model ind 665 epoch 1267 batch: 200 avg loss -2.871322 avg loss no lamb -2.871322 time 2020-06-27 04:57:44.841242
Model ind 665 epoch 1267 batch: 300 avg loss -2.822012 avg loss no lamb -2.822012 time 2020-06-27 04:57:55.638356
Model ind 665 epoch 1267 batch: 400 avg loss -2.764158 avg loss no lamb -2.764158 time 2020-06-27 04:58:06.534388
Model ind 665 epoch 1267 batch: 500 avg loss -2.886194 avg loss no lamb -2.886194 time 2020-06-27 04:58:17.380719
Model ind 665 epoch 1267 batch: 600 avg loss -2.923199 avg loss no lamb -2.923199 time 2020-06-27 04:58:28.311842
Model ind 665 epoch 1267 batch: 700 avg loss -2.795526 avg loss no lamb -2.795526 time 2020-06-27 04:58:39.275679
Model ind 665 epoch 1267 batch: 800 avg loss -2.807403 avg loss no lamb -2.807403 time 2020-06-27 04:58:50.377539
last batch sz 10
Pre: time 2020-06-27 04:59:04.579582: 
 	std: 0.003191246
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9813, 0.9748, 0.9818, 0.9752]
	train_accs: [0.9815, 0.98123336, 0.9756, 0.9817, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.9789001
	best: 0.9818

Starting e_i: 1268
Model ind 665 epoch 1268 batch: 0 avg loss -2.958241 avg loss no lamb -2.958241 time 2020-06-27 04:59:05.744735
Model ind 665 epoch 1268 batch: 100 avg loss -2.944105 avg loss no lamb -2.944105 time 2020-06-27 04:59:16.692326
Model ind 665 epoch 1268 batch: 200 avg loss -2.889259 avg loss no lamb -2.889259 time 2020-06-27 04:59:27.577730
Model ind 665 epoch 1268 batch: 300 avg loss -2.852793 avg loss no lamb -2.852793 time 2020-06-27 04:59:38.280905
Model ind 665 epoch 1268 batch: 400 avg loss -2.801782 avg loss no lamb -2.801782 time 2020-06-27 04:59:49.284971
Model ind 665 epoch 1268 batch: 500 avg loss -2.825569 avg loss no lamb -2.825569 time 2020-06-27 05:00:00.168827
Model ind 665 epoch 1268 batch: 600 avg loss -2.937738 avg loss no lamb -2.937738 time 2020-06-27 05:00:10.951722
Model ind 665 epoch 1268 batch: 700 avg loss -2.750274 avg loss no lamb -2.750274 time 2020-06-27 05:00:21.787433
Model ind 665 epoch 1268 batch: 800 avg loss -2.919639 avg loss no lamb -2.919639 time 2020-06-27 05:00:32.714284
last batch sz 10
Pre: time 2020-06-27 05:00:47.036048: 
 	std: 0.003315174
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9828, 0.9805, 0.9745, 0.9827, 0.9767]
	train_accs: [0.9820667, 0.98095, 0.97538334, 0.9821, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.9794399
	best: 0.9827

Starting e_i: 1269
Model ind 665 epoch 1269 batch: 0 avg loss -2.945412 avg loss no lamb -2.945412 time 2020-06-27 05:00:48.191443
Model ind 665 epoch 1269 batch: 100 avg loss -2.828316 avg loss no lamb -2.828316 time 2020-06-27 05:00:59.023032
Model ind 665 epoch 1269 batch: 200 avg loss -2.882159 avg loss no lamb -2.882159 time 2020-06-27 05:01:09.849529
Model ind 665 epoch 1269 batch: 300 avg loss -2.911319 avg loss no lamb -2.911319 time 2020-06-27 05:01:20.422476
Model ind 665 epoch 1269 batch: 400 avg loss -2.825486 avg loss no lamb -2.825486 time 2020-06-27 05:01:31.280645
Model ind 665 epoch 1269 batch: 500 avg loss -2.872617 avg loss no lamb -2.872617 time 2020-06-27 05:01:42.099776
Model ind 665 epoch 1269 batch: 600 avg loss -2.877803 avg loss no lamb -2.877803 time 2020-06-27 05:01:53.041116
Model ind 665 epoch 1269 batch: 700 avg loss -2.782897 avg loss no lamb -2.782897 time 2020-06-27 05:02:04.037403
Model ind 665 epoch 1269 batch: 800 avg loss -2.843717 avg loss no lamb -2.843717 time 2020-06-27 05:02:15.005166
last batch sz 10
Pre: time 2020-06-27 05:02:28.960722: 
 	std: 0.0035997943
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.98, 0.9738, 0.9815, 0.9736]
	train_accs: [0.9824167, 0.98118335, 0.97608334, 0.9824833, 0.97671664]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97805995
	best: 0.9815

Starting e_i: 1270
Model ind 665 epoch 1270 batch: 0 avg loss -2.937487 avg loss no lamb -2.937487 time 2020-06-27 05:02:30.078794
Model ind 665 epoch 1270 batch: 100 avg loss -2.879953 avg loss no lamb -2.879953 time 2020-06-27 05:02:41.305656
Model ind 665 epoch 1270 batch: 200 avg loss -2.870656 avg loss no lamb -2.870656 time 2020-06-27 05:02:52.217468
Model ind 665 epoch 1270 batch: 300 avg loss -2.841524 avg loss no lamb -2.841524 time 2020-06-27 05:03:02.999477
Model ind 665 epoch 1270 batch: 400 avg loss -2.788096 avg loss no lamb -2.788096 time 2020-06-27 05:03:13.877680
Model ind 665 epoch 1270 batch: 500 avg loss -2.853287 avg loss no lamb -2.853287 time 2020-06-27 05:03:24.717676
Model ind 665 epoch 1270 batch: 600 avg loss -2.915593 avg loss no lamb -2.915593 time 2020-06-27 05:03:35.520279
Model ind 665 epoch 1270 batch: 700 avg loss -2.869411 avg loss no lamb -2.869411 time 2020-06-27 05:03:46.368193
Model ind 665 epoch 1270 batch: 800 avg loss -2.898273 avg loss no lamb -2.898273 time 2020-06-27 05:03:57.463430
last batch sz 10
Pre: time 2020-06-27 05:04:11.876641: 
 	std: 0.0034073985
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9801, 0.9738, 0.9808, 0.9734]
	train_accs: [0.9819833, 0.98111665, 0.97545, 0.9816167, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97775996
	best: 0.9807

Starting e_i: 1271
Model ind 665 epoch 1271 batch: 0 avg loss -2.960813 avg loss no lamb -2.960813 time 2020-06-27 05:04:14.243352
Model ind 665 epoch 1271 batch: 100 avg loss -2.917917 avg loss no lamb -2.917917 time 2020-06-27 05:04:25.021980
Model ind 665 epoch 1271 batch: 200 avg loss -2.858330 avg loss no lamb -2.858330 time 2020-06-27 05:04:35.933552
Model ind 665 epoch 1271 batch: 300 avg loss -2.907618 avg loss no lamb -2.907618 time 2020-06-27 05:04:46.714101
Model ind 665 epoch 1271 batch: 400 avg loss -2.821838 avg loss no lamb -2.821838 time 2020-06-27 05:04:57.619020
Model ind 665 epoch 1271 batch: 500 avg loss -2.818275 avg loss no lamb -2.818275 time 2020-06-27 05:05:08.372537
Model ind 665 epoch 1271 batch: 600 avg loss -2.843041 avg loss no lamb -2.843041 time 2020-06-27 05:05:19.153677
Model ind 665 epoch 1271 batch: 700 avg loss -2.792476 avg loss no lamb -2.792476 time 2020-06-27 05:05:29.829659
Model ind 665 epoch 1271 batch: 800 avg loss -2.856078 avg loss no lamb -2.856078 time 2020-06-27 05:05:40.777387
last batch sz 10
Pre: time 2020-06-27 05:05:54.679243: 
 	std: 0.0032952093
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9798, 0.9733, 0.9808, 0.9747]
	train_accs: [0.9816, 0.98053336, 0.9748833, 0.98165, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97796
	best: 0.9808

Starting e_i: 1272
Model ind 665 epoch 1272 batch: 0 avg loss -2.972730 avg loss no lamb -2.972730 time 2020-06-27 05:05:55.854953
Model ind 665 epoch 1272 batch: 100 avg loss -2.916802 avg loss no lamb -2.916802 time 2020-06-27 05:06:06.844647
Model ind 665 epoch 1272 batch: 200 avg loss -2.873770 avg loss no lamb -2.873770 time 2020-06-27 05:06:17.801505
Model ind 665 epoch 1272 batch: 300 avg loss -2.845938 avg loss no lamb -2.845938 time 2020-06-27 05:06:28.550099
Model ind 665 epoch 1272 batch: 400 avg loss -2.800348 avg loss no lamb -2.800348 time 2020-06-27 05:06:39.502051
Model ind 665 epoch 1272 batch: 500 avg loss -2.871154 avg loss no lamb -2.871154 time 2020-06-27 05:06:50.268170
Model ind 665 epoch 1272 batch: 600 avg loss -2.880996 avg loss no lamb -2.880996 time 2020-06-27 05:07:00.990907
Model ind 665 epoch 1272 batch: 700 avg loss -2.817850 avg loss no lamb -2.817850 time 2020-06-27 05:07:11.784909
Model ind 665 epoch 1272 batch: 800 avg loss -2.915881 avg loss no lamb -2.915881 time 2020-06-27 05:07:22.484377
last batch sz 10
Pre: time 2020-06-27 05:07:36.647910: 
 	std: 0.0030531413
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9782, 0.9726, 0.9798, 0.974]
	train_accs: [0.9813833, 0.97985, 0.97525, 0.98123336, 0.9759167]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97691995
	best: 0.98

Starting e_i: 1273
Model ind 665 epoch 1273 batch: 0 avg loss -2.912575 avg loss no lamb -2.912575 time 2020-06-27 05:07:37.947151
Model ind 665 epoch 1273 batch: 100 avg loss -2.911502 avg loss no lamb -2.911502 time 2020-06-27 05:07:48.889484
Model ind 665 epoch 1273 batch: 200 avg loss -2.849352 avg loss no lamb -2.849352 time 2020-06-27 05:07:59.728306
Model ind 665 epoch 1273 batch: 300 avg loss -2.913889 avg loss no lamb -2.913889 time 2020-06-27 05:08:10.627071
Model ind 665 epoch 1273 batch: 400 avg loss -2.782052 avg loss no lamb -2.782052 time 2020-06-27 05:08:21.491777
Model ind 665 epoch 1273 batch: 500 avg loss -2.810687 avg loss no lamb -2.810687 time 2020-06-27 05:08:32.277911
Model ind 665 epoch 1273 batch: 600 avg loss -2.910854 avg loss no lamb -2.910854 time 2020-06-27 05:08:43.194526
Model ind 665 epoch 1273 batch: 700 avg loss -2.745164 avg loss no lamb -2.745164 time 2020-06-27 05:08:53.960812
Model ind 665 epoch 1273 batch: 800 avg loss -2.791739 avg loss no lamb -2.791739 time 2020-06-27 05:09:04.819276
last batch sz 10
Pre: time 2020-06-27 05:09:18.729004: 
 	std: 0.002731299
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9798, 0.9735, 0.9797, 0.9749]
	train_accs: [0.98113334, 0.9803, 0.9748333, 0.98121667, 0.97655]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97749996
	best: 0.9797

Starting e_i: 1274
Model ind 665 epoch 1274 batch: 0 avg loss -2.942829 avg loss no lamb -2.942829 time 2020-06-27 05:09:19.861365
Model ind 665 epoch 1274 batch: 100 avg loss -2.938448 avg loss no lamb -2.938448 time 2020-06-27 05:09:30.649847
Model ind 665 epoch 1274 batch: 200 avg loss -2.883868 avg loss no lamb -2.883868 time 2020-06-27 05:09:41.654917
Model ind 665 epoch 1274 batch: 300 avg loss -2.880628 avg loss no lamb -2.880628 time 2020-06-27 05:09:52.436991
Model ind 665 epoch 1274 batch: 400 avg loss -2.826130 avg loss no lamb -2.826130 time 2020-06-27 05:10:03.345611
Model ind 665 epoch 1274 batch: 500 avg loss -2.846107 avg loss no lamb -2.846107 time 2020-06-27 05:10:14.249380
Model ind 665 epoch 1274 batch: 600 avg loss -2.896368 avg loss no lamb -2.896368 time 2020-06-27 05:10:25.046310
Model ind 665 epoch 1274 batch: 700 avg loss -2.722688 avg loss no lamb -2.722688 time 2020-06-27 05:10:35.985083
Model ind 665 epoch 1274 batch: 800 avg loss -2.869256 avg loss no lamb -2.869256 time 2020-06-27 05:10:46.581128
last batch sz 10
Pre: time 2020-06-27 05:11:00.670572: 
 	std: 0.0029608156
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9806, 0.9755, 0.9818, 0.9752]
	train_accs: [0.9817167, 0.98116666, 0.9762167, 0.9817167, 0.9765667]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.97894
	best: 0.9816

Starting e_i: 1275
Model ind 665 epoch 1275 batch: 0 avg loss -2.913735 avg loss no lamb -2.913735 time 2020-06-27 05:11:02.011240
Model ind 665 epoch 1275 batch: 100 avg loss -2.902018 avg loss no lamb -2.902018 time 2020-06-27 05:11:12.945412
Model ind 665 epoch 1275 batch: 200 avg loss -2.925995 avg loss no lamb -2.925995 time 2020-06-27 05:11:23.919278
Model ind 665 epoch 1275 batch: 300 avg loss -2.854074 avg loss no lamb -2.854074 time 2020-06-27 05:11:34.729853
Model ind 665 epoch 1275 batch: 400 avg loss -2.791075 avg loss no lamb -2.791075 time 2020-06-27 05:11:45.648992
Model ind 665 epoch 1275 batch: 500 avg loss -2.832542 avg loss no lamb -2.832542 time 2020-06-27 05:11:56.461613
Model ind 665 epoch 1275 batch: 600 avg loss -2.907438 avg loss no lamb -2.907438 time 2020-06-27 05:12:07.492938
Model ind 665 epoch 1275 batch: 700 avg loss -2.794837 avg loss no lamb -2.794837 time 2020-06-27 05:12:18.237725
Model ind 665 epoch 1275 batch: 800 avg loss -2.886448 avg loss no lamb -2.886448 time 2020-06-27 05:12:28.983816
last batch sz 10
Pre: time 2020-06-27 05:12:42.792231: 
 	std: 0.0028781926
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9798, 0.9743, 0.9807, 0.9754]
	train_accs: [0.98165, 0.98108333, 0.97615, 0.98143333, 0.9768]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.9783
	best: 0.9813

Starting e_i: 1276
Model ind 665 epoch 1276 batch: 0 avg loss -2.954732 avg loss no lamb -2.954732 time 2020-06-27 05:12:44.032406
Model ind 665 epoch 1276 batch: 100 avg loss -2.905886 avg loss no lamb -2.905886 time 2020-06-27 05:12:54.840463
Model ind 665 epoch 1276 batch: 200 avg loss -2.940931 avg loss no lamb -2.940931 time 2020-06-27 05:13:05.817246
Model ind 665 epoch 1276 batch: 300 avg loss -2.899282 avg loss no lamb -2.899282 time 2020-06-27 05:13:16.764749
Model ind 665 epoch 1276 batch: 400 avg loss -2.803638 avg loss no lamb -2.803638 time 2020-06-27 05:13:27.516217
Model ind 665 epoch 1276 batch: 500 avg loss -2.825077 avg loss no lamb -2.825077 time 2020-06-27 05:13:38.346330
Model ind 665 epoch 1276 batch: 600 avg loss -2.812616 avg loss no lamb -2.812616 time 2020-06-27 05:13:49.239858
Model ind 665 epoch 1276 batch: 700 avg loss -2.743280 avg loss no lamb -2.743280 time 2020-06-27 05:14:00.322481
Model ind 665 epoch 1276 batch: 800 avg loss -2.841914 avg loss no lamb -2.841914 time 2020-06-27 05:14:11.170205
last batch sz 10
Pre: time 2020-06-27 05:14:25.446191: 
 	std: 0.0032835396
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9803, 0.9741, 0.9816, 0.9754]
	train_accs: [0.98145, 0.98076665, 0.9756333, 0.98155, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97868
	best: 0.9816

Starting e_i: 1277
Model ind 665 epoch 1277 batch: 0 avg loss -2.908172 avg loss no lamb -2.908172 time 2020-06-27 05:14:26.826357
Model ind 665 epoch 1277 batch: 100 avg loss -2.857257 avg loss no lamb -2.857257 time 2020-06-27 05:14:37.729367
Model ind 665 epoch 1277 batch: 200 avg loss -2.850246 avg loss no lamb -2.850246 time 2020-06-27 05:14:48.658645
Model ind 665 epoch 1277 batch: 300 avg loss -2.864619 avg loss no lamb -2.864619 time 2020-06-27 05:14:59.557304
Model ind 665 epoch 1277 batch: 400 avg loss -2.774013 avg loss no lamb -2.774013 time 2020-06-27 05:15:10.694381
Model ind 665 epoch 1277 batch: 500 avg loss -2.796029 avg loss no lamb -2.796029 time 2020-06-27 05:15:21.482630
Model ind 665 epoch 1277 batch: 600 avg loss -2.853857 avg loss no lamb -2.853857 time 2020-06-27 05:15:32.506175
Model ind 665 epoch 1277 batch: 700 avg loss -2.714457 avg loss no lamb -2.714457 time 2020-06-27 05:15:43.433423
Model ind 665 epoch 1277 batch: 800 avg loss -2.867990 avg loss no lamb -2.867990 time 2020-06-27 05:15:54.393252
last batch sz 10
Pre: time 2020-06-27 05:16:08.654592: 
 	std: 0.0026416692
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9813, 0.9756, 0.9818, 0.9769]
	train_accs: [0.98153335, 0.98118335, 0.97636664, 0.98185, 0.9776667]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97944003
	best: 0.9818

Starting e_i: 1278
Model ind 665 epoch 1278 batch: 0 avg loss -2.970210 avg loss no lamb -2.970210 time 2020-06-27 05:16:09.838609
Model ind 665 epoch 1278 batch: 100 avg loss -2.945393 avg loss no lamb -2.945393 time 2020-06-27 05:16:20.692943
Model ind 665 epoch 1278 batch: 200 avg loss -2.828079 avg loss no lamb -2.828079 time 2020-06-27 05:16:31.668386
Model ind 665 epoch 1278 batch: 300 avg loss -2.845243 avg loss no lamb -2.845243 time 2020-06-27 05:16:42.905771
Model ind 665 epoch 1278 batch: 400 avg loss -2.809401 avg loss no lamb -2.809401 time 2020-06-27 05:16:53.743965
Model ind 665 epoch 1278 batch: 500 avg loss -2.890660 avg loss no lamb -2.890660 time 2020-06-27 05:17:04.747563
Model ind 665 epoch 1278 batch: 600 avg loss -2.871917 avg loss no lamb -2.871917 time 2020-06-27 05:17:15.632107
Model ind 665 epoch 1278 batch: 700 avg loss -2.750002 avg loss no lamb -2.750002 time 2020-06-27 05:17:26.376990
Model ind 665 epoch 1278 batch: 800 avg loss -2.922670 avg loss no lamb -2.922670 time 2020-06-27 05:17:37.465911
last batch sz 10
Pre: time 2020-06-27 05:17:51.525389: 
 	std: 0.0038447822
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9808, 0.9731, 0.9814, 0.9738]
	train_accs: [0.98143333, 0.981, 0.97475, 0.9813667, 0.97595]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97814
	best: 0.9816

Starting e_i: 1279
Model ind 665 epoch 1279 batch: 0 avg loss -2.941936 avg loss no lamb -2.941936 time 2020-06-27 05:17:52.859200
Model ind 665 epoch 1279 batch: 100 avg loss -2.888790 avg loss no lamb -2.888790 time 2020-06-27 05:18:03.785171
Model ind 665 epoch 1279 batch: 200 avg loss -2.879611 avg loss no lamb -2.879611 time 2020-06-27 05:18:14.838277
Model ind 665 epoch 1279 batch: 300 avg loss -2.854716 avg loss no lamb -2.854716 time 2020-06-27 05:18:25.648100
Model ind 665 epoch 1279 batch: 400 avg loss -2.805737 avg loss no lamb -2.805737 time 2020-06-27 05:18:36.513592
Model ind 665 epoch 1279 batch: 500 avg loss -2.849057 avg loss no lamb -2.849057 time 2020-06-27 05:18:47.246617
Model ind 665 epoch 1279 batch: 600 avg loss -2.902200 avg loss no lamb -2.902200 time 2020-06-27 05:18:58.329344
Model ind 665 epoch 1279 batch: 700 avg loss -2.776582 avg loss no lamb -2.776582 time 2020-06-27 05:19:09.176155
Model ind 665 epoch 1279 batch: 800 avg loss -2.853309 avg loss no lamb -2.853309 time 2020-06-27 05:19:19.982355
last batch sz 10
Pre: time 2020-06-27 05:19:34.169645: 
 	std: 0.003118073
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9802, 0.9735, 0.9807, 0.9751]
	train_accs: [0.9816167, 0.9806333, 0.97546667, 0.98165, 0.97648335]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97805995
	best: 0.9807

Starting e_i: 1280
Model ind 665 epoch 1280 batch: 0 avg loss -2.941669 avg loss no lamb -2.941669 time 2020-06-27 05:19:35.394320
Model ind 665 epoch 1280 batch: 100 avg loss -2.851923 avg loss no lamb -2.851923 time 2020-06-27 05:19:46.231168
Model ind 665 epoch 1280 batch: 200 avg loss -2.882229 avg loss no lamb -2.882229 time 2020-06-27 05:19:57.145930
Model ind 665 epoch 1280 batch: 300 avg loss -2.890341 avg loss no lamb -2.890341 time 2020-06-27 05:20:07.838063
Model ind 665 epoch 1280 batch: 400 avg loss -2.764469 avg loss no lamb -2.764469 time 2020-06-27 05:20:18.632918
Model ind 665 epoch 1280 batch: 500 avg loss -2.851477 avg loss no lamb -2.851477 time 2020-06-27 05:20:29.558984
Model ind 665 epoch 1280 batch: 600 avg loss -2.868107 avg loss no lamb -2.868107 time 2020-06-27 05:20:40.189565
Model ind 665 epoch 1280 batch: 700 avg loss -2.837009 avg loss no lamb -2.837009 time 2020-06-27 05:20:51.193172
Model ind 665 epoch 1280 batch: 800 avg loss -2.914877 avg loss no lamb -2.914877 time 2020-06-27 05:21:02.118696
last batch sz 10
Pre: time 2020-06-27 05:21:16.309392: 
 	std: 0.0035829674
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9795, 0.9726, 0.9807, 0.9742]
	train_accs: [0.98123336, 0.9801667, 0.97415, 0.9813667, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97768
	best: 0.9807

Starting e_i: 1281
Model ind 665 epoch 1281 batch: 0 avg loss -2.985036 avg loss no lamb -2.985036 time 2020-06-27 05:21:18.845804
Model ind 665 epoch 1281 batch: 100 avg loss -2.896596 avg loss no lamb -2.896596 time 2020-06-27 05:21:29.402753
Model ind 665 epoch 1281 batch: 200 avg loss -2.791335 avg loss no lamb -2.791335 time 2020-06-27 05:21:40.165519
Model ind 665 epoch 1281 batch: 300 avg loss -2.776047 avg loss no lamb -2.776047 time 2020-06-27 05:21:51.016848
Model ind 665 epoch 1281 batch: 400 avg loss -2.768762 avg loss no lamb -2.768762 time 2020-06-27 05:22:01.885185
Model ind 665 epoch 1281 batch: 500 avg loss -2.831977 avg loss no lamb -2.831977 time 2020-06-27 05:22:12.783020
Model ind 665 epoch 1281 batch: 600 avg loss -2.874504 avg loss no lamb -2.874504 time 2020-06-27 05:22:23.710978
Model ind 665 epoch 1281 batch: 700 avg loss -2.777293 avg loss no lamb -2.777293 time 2020-06-27 05:22:34.735213
Model ind 665 epoch 1281 batch: 800 avg loss -2.832060 avg loss no lamb -2.832060 time 2020-06-27 05:22:45.623861
last batch sz 10
Pre: time 2020-06-27 05:22:59.638992: 
 	std: 0.0026462052
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9806, 0.9753, 0.9814, 0.9767]
	train_accs: [0.98156667, 0.9809333, 0.97643334, 0.9815, 0.9774]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97916
	best: 0.9818

Starting e_i: 1282
Model ind 665 epoch 1282 batch: 0 avg loss -2.908552 avg loss no lamb -2.908552 time 2020-06-27 05:23:00.844262
Model ind 665 epoch 1282 batch: 100 avg loss -2.923534 avg loss no lamb -2.923534 time 2020-06-27 05:23:11.639916
Model ind 665 epoch 1282 batch: 200 avg loss -2.843199 avg loss no lamb -2.843199 time 2020-06-27 05:23:22.530022
Model ind 665 epoch 1282 batch: 300 avg loss -2.858307 avg loss no lamb -2.858307 time 2020-06-27 05:23:33.346436
Model ind 665 epoch 1282 batch: 400 avg loss -2.773370 avg loss no lamb -2.773370 time 2020-06-27 05:23:44.432630
Model ind 665 epoch 1282 batch: 500 avg loss -2.827208 avg loss no lamb -2.827208 time 2020-06-27 05:23:55.332219
Model ind 665 epoch 1282 batch: 600 avg loss -2.918304 avg loss no lamb -2.918304 time 2020-06-27 05:24:06.428400
Model ind 665 epoch 1282 batch: 700 avg loss -2.812106 avg loss no lamb -2.812106 time 2020-06-27 05:24:17.242416
Model ind 665 epoch 1282 batch: 800 avg loss -2.814560 avg loss no lamb -2.814560 time 2020-06-27 05:24:28.118223
last batch sz 10
Pre: time 2020-06-27 05:24:42.464467: 
 	std: 0.0028442412
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9807, 0.9747, 0.9807, 0.9751]
	train_accs: [0.98183334, 0.98105, 0.97611666, 0.98175, 0.97718334]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97838
	best: 0.9807

Starting e_i: 1283
Model ind 665 epoch 1283 batch: 0 avg loss -2.974897 avg loss no lamb -2.974897 time 2020-06-27 05:24:43.821945
Model ind 665 epoch 1283 batch: 100 avg loss -2.885313 avg loss no lamb -2.885313 time 2020-06-27 05:24:54.924121
Model ind 665 epoch 1283 batch: 200 avg loss -2.914539 avg loss no lamb -2.914539 time 2020-06-27 05:25:06.019274
Model ind 665 epoch 1283 batch: 300 avg loss -2.896528 avg loss no lamb -2.896528 time 2020-06-27 05:25:16.719984
Model ind 665 epoch 1283 batch: 400 avg loss -2.772408 avg loss no lamb -2.772408 time 2020-06-27 05:25:27.622735
Model ind 665 epoch 1283 batch: 500 avg loss -2.799915 avg loss no lamb -2.799915 time 2020-06-27 05:25:38.464222
Model ind 665 epoch 1283 batch: 600 avg loss -2.939422 avg loss no lamb -2.939422 time 2020-06-27 05:25:49.603360
Model ind 665 epoch 1283 batch: 700 avg loss -2.769658 avg loss no lamb -2.769658 time 2020-06-27 05:26:00.542048
Model ind 665 epoch 1283 batch: 800 avg loss -2.874427 avg loss no lamb -2.874427 time 2020-06-27 05:26:11.484097
last batch sz 10
Pre: time 2020-06-27 05:26:25.379936: 
 	std: 0.0037733878
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9797, 0.973, 0.9809, 0.9723]
	train_accs: [0.9812833, 0.9805667, 0.97498333, 0.98108333, 0.97473335]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97723997
	best: 0.9803

Starting e_i: 1284
Model ind 665 epoch 1284 batch: 0 avg loss -2.935543 avg loss no lamb -2.935543 time 2020-06-27 05:26:26.541884
Model ind 665 epoch 1284 batch: 100 avg loss -2.914462 avg loss no lamb -2.914462 time 2020-06-27 05:26:37.482587
Model ind 665 epoch 1284 batch: 200 avg loss -2.858991 avg loss no lamb -2.858991 time 2020-06-27 05:26:48.351305
Model ind 665 epoch 1284 batch: 300 avg loss -2.863433 avg loss no lamb -2.863433 time 2020-06-27 05:26:59.277994
Model ind 665 epoch 1284 batch: 400 avg loss -2.799314 avg loss no lamb -2.799314 time 2020-06-27 05:27:10.228815
Model ind 665 epoch 1284 batch: 500 avg loss -2.849374 avg loss no lamb -2.849374 time 2020-06-27 05:27:21.044681
Model ind 665 epoch 1284 batch: 600 avg loss -2.884027 avg loss no lamb -2.884027 time 2020-06-27 05:27:31.712596
Model ind 665 epoch 1284 batch: 700 avg loss -2.798429 avg loss no lamb -2.798429 time 2020-06-27 05:27:42.800619
Model ind 665 epoch 1284 batch: 800 avg loss -2.896617 avg loss no lamb -2.896617 time 2020-06-27 05:27:53.747913
last batch sz 10
Pre: time 2020-06-27 05:28:07.861936: 
 	std: 0.003095417
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9812, 0.9749, 0.9817, 0.9751]
	train_accs: [0.98181665, 0.98113334, 0.97605, 0.98186666, 0.97678334]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97878
	best: 0.9817

Starting e_i: 1285
Model ind 665 epoch 1285 batch: 0 avg loss -2.928700 avg loss no lamb -2.928700 time 2020-06-27 05:28:09.242821
Model ind 665 epoch 1285 batch: 100 avg loss -2.902142 avg loss no lamb -2.902142 time 2020-06-27 05:28:20.165650
Model ind 665 epoch 1285 batch: 200 avg loss -2.892755 avg loss no lamb -2.892755 time 2020-06-27 05:28:31.232248
Model ind 665 epoch 1285 batch: 300 avg loss -2.837789 avg loss no lamb -2.837789 time 2020-06-27 05:28:42.092172
Model ind 665 epoch 1285 batch: 400 avg loss -2.812257 avg loss no lamb -2.812257 time 2020-06-27 05:28:53.147940
Model ind 665 epoch 1285 batch: 500 avg loss -2.858773 avg loss no lamb -2.858773 time 2020-06-27 05:29:04.216840
Model ind 665 epoch 1285 batch: 600 avg loss -2.937269 avg loss no lamb -2.937269 time 2020-06-27 05:29:15.044952
Model ind 665 epoch 1285 batch: 700 avg loss -2.769209 avg loss no lamb -2.769209 time 2020-06-27 05:29:25.959711
Model ind 665 epoch 1285 batch: 800 avg loss -2.868607 avg loss no lamb -2.868607 time 2020-06-27 05:29:36.828801
last batch sz 10
Pre: time 2020-06-27 05:29:51.199349: 
 	std: 0.0029282044
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9794, 0.9737, 0.9808, 0.9752]
	train_accs: [0.9813833, 0.98013335, 0.97506666, 0.98145, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97793996
	best: 0.9808

Starting e_i: 1286
Model ind 665 epoch 1286 batch: 0 avg loss -2.942865 avg loss no lamb -2.942865 time 2020-06-27 05:29:52.395052
Model ind 665 epoch 1286 batch: 100 avg loss -2.922988 avg loss no lamb -2.922988 time 2020-06-27 05:30:03.314031
Model ind 665 epoch 1286 batch: 200 avg loss -2.920270 avg loss no lamb -2.920270 time 2020-06-27 05:30:14.177581
Model ind 665 epoch 1286 batch: 300 avg loss -2.901034 avg loss no lamb -2.901034 time 2020-06-27 05:30:25.141166
Model ind 665 epoch 1286 batch: 400 avg loss -2.836263 avg loss no lamb -2.836263 time 2020-06-27 05:30:36.039509
Model ind 665 epoch 1286 batch: 500 avg loss -2.872689 avg loss no lamb -2.872689 time 2020-06-27 05:30:47.036669
Model ind 665 epoch 1286 batch: 600 avg loss -2.869974 avg loss no lamb -2.869974 time 2020-06-27 05:30:57.668133
Model ind 665 epoch 1286 batch: 700 avg loss -2.729011 avg loss no lamb -2.729011 time 2020-06-27 05:31:08.333298
Model ind 665 epoch 1286 batch: 800 avg loss -2.852089 avg loss no lamb -2.852089 time 2020-06-27 05:31:19.147994
last batch sz 10
Pre: time 2020-06-27 05:31:33.625096: 
 	std: 0.003549884
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.981, 0.9734, 0.9818, 0.9752]
	train_accs: [0.98181665, 0.981, 0.97503334, 0.98176664, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.9785801
	best: 0.9815

Starting e_i: 1287
Model ind 665 epoch 1287 batch: 0 avg loss -2.974684 avg loss no lamb -2.974684 time 2020-06-27 05:31:34.858669
Model ind 665 epoch 1287 batch: 100 avg loss -2.873597 avg loss no lamb -2.873597 time 2020-06-27 05:31:45.835091
Model ind 665 epoch 1287 batch: 200 avg loss -2.825866 avg loss no lamb -2.825866 time 2020-06-27 05:31:56.916386
Model ind 665 epoch 1287 batch: 300 avg loss -2.928556 avg loss no lamb -2.928556 time 2020-06-27 05:32:08.156691
Model ind 665 epoch 1287 batch: 400 avg loss -2.866859 avg loss no lamb -2.866859 time 2020-06-27 05:32:19.054891
Model ind 665 epoch 1287 batch: 500 avg loss -2.845710 avg loss no lamb -2.845710 time 2020-06-27 05:32:29.888172
Model ind 665 epoch 1287 batch: 600 avg loss -2.914356 avg loss no lamb -2.914356 time 2020-06-27 05:32:40.942813
Model ind 665 epoch 1287 batch: 700 avg loss -2.771257 avg loss no lamb -2.771257 time 2020-06-27 05:32:51.765176
Model ind 665 epoch 1287 batch: 800 avg loss -2.883226 avg loss no lamb -2.883226 time 2020-06-27 05:33:02.653639
last batch sz 10
Pre: time 2020-06-27 05:33:16.967629: 
 	std: 0.003449405
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9793, 0.9719, 0.9795, 0.9732]
	train_accs: [0.98115, 0.98013335, 0.97421664, 0.98081666, 0.97478336]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97674
	best: 0.9798

Starting e_i: 1288
Model ind 665 epoch 1288 batch: 0 avg loss -2.981231 avg loss no lamb -2.981231 time 2020-06-27 05:33:18.146257
Model ind 665 epoch 1288 batch: 100 avg loss -2.883480 avg loss no lamb -2.883480 time 2020-06-27 05:33:29.171555
Model ind 665 epoch 1288 batch: 200 avg loss -2.882972 avg loss no lamb -2.882972 time 2020-06-27 05:33:40.270962
Model ind 665 epoch 1288 batch: 300 avg loss -2.854660 avg loss no lamb -2.854660 time 2020-06-27 05:33:51.159492
Model ind 665 epoch 1288 batch: 400 avg loss -2.811735 avg loss no lamb -2.811735 time 2020-06-27 05:34:02.420429
Model ind 665 epoch 1288 batch: 500 avg loss -2.864875 avg loss no lamb -2.864875 time 2020-06-27 05:34:13.136376
Model ind 665 epoch 1288 batch: 600 avg loss -2.857333 avg loss no lamb -2.857333 time 2020-06-27 05:34:23.835309
Model ind 665 epoch 1288 batch: 700 avg loss -2.795156 avg loss no lamb -2.795156 time 2020-06-27 05:34:34.635577
Model ind 665 epoch 1288 batch: 800 avg loss -2.878015 avg loss no lamb -2.878015 time 2020-06-27 05:34:45.471406
last batch sz 10
Pre: time 2020-06-27 05:34:59.447266: 
 	std: 0.0031512617
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9797, 0.9731, 0.9806, 0.9744]
	train_accs: [0.9808667, 0.9803333, 0.97496665, 0.9813667, 0.97575]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97756004
	best: 0.9806

Starting e_i: 1289
Model ind 665 epoch 1289 batch: 0 avg loss -2.942281 avg loss no lamb -2.942281 time 2020-06-27 05:35:00.653874
Model ind 665 epoch 1289 batch: 100 avg loss -2.897252 avg loss no lamb -2.897252 time 2020-06-27 05:35:11.588917
Model ind 665 epoch 1289 batch: 200 avg loss -2.876032 avg loss no lamb -2.876032 time 2020-06-27 05:35:22.607188
Model ind 665 epoch 1289 batch: 300 avg loss -2.879647 avg loss no lamb -2.879647 time 2020-06-27 05:35:33.747122
Model ind 665 epoch 1289 batch: 400 avg loss -2.795795 avg loss no lamb -2.795795 time 2020-06-27 05:35:44.630490
Model ind 665 epoch 1289 batch: 500 avg loss -2.818284 avg loss no lamb -2.818284 time 2020-06-27 05:35:55.392430
Model ind 665 epoch 1289 batch: 600 avg loss -2.861320 avg loss no lamb -2.861320 time 2020-06-27 05:36:06.227961
Model ind 665 epoch 1289 batch: 700 avg loss -2.808359 avg loss no lamb -2.808359 time 2020-06-27 05:36:17.039583
Model ind 665 epoch 1289 batch: 800 avg loss -2.912750 avg loss no lamb -2.912750 time 2020-06-27 05:36:28.065780
last batch sz 10
Pre: time 2020-06-27 05:36:42.602943: 
 	std: 0.0029006249
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9792, 0.9729, 0.9793, 0.9746]
	train_accs: [0.98106664, 0.9802667, 0.97496665, 0.98073334, 0.9763333]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97721994
	best: 0.9801

Starting e_i: 1290
Model ind 665 epoch 1290 batch: 0 avg loss -3.000616 avg loss no lamb -3.000616 time 2020-06-27 05:36:43.762337
Model ind 665 epoch 1290 batch: 100 avg loss -2.866485 avg loss no lamb -2.866485 time 2020-06-27 05:36:54.546174
Model ind 665 epoch 1290 batch: 200 avg loss -2.826335 avg loss no lamb -2.826335 time 2020-06-27 05:37:05.319670
Model ind 665 epoch 1290 batch: 300 avg loss -2.859529 avg loss no lamb -2.859529 time 2020-06-27 05:37:16.313697
Model ind 665 epoch 1290 batch: 400 avg loss -2.791982 avg loss no lamb -2.791982 time 2020-06-27 05:37:27.395555
Model ind 665 epoch 1290 batch: 500 avg loss -2.865959 avg loss no lamb -2.865959 time 2020-06-27 05:37:38.655706
Model ind 665 epoch 1290 batch: 600 avg loss -2.883816 avg loss no lamb -2.883816 time 2020-06-27 05:37:49.770210
Model ind 665 epoch 1290 batch: 700 avg loss -2.818821 avg loss no lamb -2.818821 time 2020-06-27 05:38:00.866551
Model ind 665 epoch 1290 batch: 800 avg loss -2.887852 avg loss no lamb -2.887852 time 2020-06-27 05:38:12.040601
last batch sz 10
Pre: time 2020-06-27 05:38:26.296970: 
 	std: 0.002930804
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9803, 0.974, 0.9814, 0.9761]
	train_accs: [0.98165, 0.9809333, 0.9748833, 0.98183334, 0.9762333]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97852004
	best: 0.9814

Starting e_i: 1291
Model ind 665 epoch 1291 batch: 0 avg loss -2.896184 avg loss no lamb -2.896184 time 2020-06-27 05:38:28.666968
Model ind 665 epoch 1291 batch: 100 avg loss -2.899775 avg loss no lamb -2.899775 time 2020-06-27 05:38:39.521951
Model ind 665 epoch 1291 batch: 200 avg loss -2.842829 avg loss no lamb -2.842829 time 2020-06-27 05:38:50.443055
Model ind 665 epoch 1291 batch: 300 avg loss -2.875555 avg loss no lamb -2.875555 time 2020-06-27 05:39:01.502658
Model ind 665 epoch 1291 batch: 400 avg loss -2.770445 avg loss no lamb -2.770445 time 2020-06-27 05:39:12.225085
Model ind 665 epoch 1291 batch: 500 avg loss -2.788926 avg loss no lamb -2.788926 time 2020-06-27 05:39:22.870512
Model ind 665 epoch 1291 batch: 600 avg loss -2.875513 avg loss no lamb -2.875513 time 2020-06-27 05:39:33.595410
Model ind 665 epoch 1291 batch: 700 avg loss -2.803670 avg loss no lamb -2.803670 time 2020-06-27 05:39:44.476170
Model ind 665 epoch 1291 batch: 800 avg loss -2.854667 avg loss no lamb -2.854667 time 2020-06-27 05:39:55.360745
last batch sz 10
Pre: time 2020-06-27 05:40:09.871858: 
 	std: 0.0028820862
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9812, 0.9751, 0.9818, 0.9767]
	train_accs: [0.9816833, 0.98108333, 0.97613335, 0.9821333, 0.97721666]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97936
	best: 0.9818

Starting e_i: 1292
Model ind 665 epoch 1292 batch: 0 avg loss -2.956486 avg loss no lamb -2.956486 time 2020-06-27 05:40:11.143052
Model ind 665 epoch 1292 batch: 100 avg loss -2.891387 avg loss no lamb -2.891387 time 2020-06-27 05:40:22.250755
Model ind 665 epoch 1292 batch: 200 avg loss -2.862400 avg loss no lamb -2.862400 time 2020-06-27 05:40:33.149801
Model ind 665 epoch 1292 batch: 300 avg loss -2.899927 avg loss no lamb -2.899927 time 2020-06-27 05:40:43.944802
Model ind 665 epoch 1292 batch: 400 avg loss -2.824501 avg loss no lamb -2.824501 time 2020-06-27 05:40:54.801280
Model ind 665 epoch 1292 batch: 500 avg loss -2.866837 avg loss no lamb -2.866837 time 2020-06-27 05:41:05.727993
Model ind 665 epoch 1292 batch: 600 avg loss -2.910342 avg loss no lamb -2.910342 time 2020-06-27 05:41:16.617160
Model ind 665 epoch 1292 batch: 700 avg loss -2.719690 avg loss no lamb -2.719690 time 2020-06-27 05:41:27.575918
Model ind 665 epoch 1292 batch: 800 avg loss -2.890628 avg loss no lamb -2.890628 time 2020-06-27 05:41:38.685967
last batch sz 10
Pre: time 2020-06-27 05:41:52.926826: 
 	std: 0.002782377
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9804, 0.9742, 0.9807, 0.9753]
	train_accs: [0.9808667, 0.9805667, 0.97568333, 0.9813333, 0.97583336]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97812
	best: 0.9807

Starting e_i: 1293
Model ind 665 epoch 1293 batch: 0 avg loss -2.952667 avg loss no lamb -2.952667 time 2020-06-27 05:41:54.061126
Model ind 665 epoch 1293 batch: 100 avg loss -2.888840 avg loss no lamb -2.888840 time 2020-06-27 05:42:05.182119
Model ind 665 epoch 1293 batch: 200 avg loss -2.873770 avg loss no lamb -2.873770 time 2020-06-27 05:42:16.088071
Model ind 665 epoch 1293 batch: 300 avg loss -2.892736 avg loss no lamb -2.892736 time 2020-06-27 05:42:27.217928
Model ind 665 epoch 1293 batch: 400 avg loss -2.732430 avg loss no lamb -2.732430 time 2020-06-27 05:42:38.127995
Model ind 665 epoch 1293 batch: 500 avg loss -2.779350 avg loss no lamb -2.779350 time 2020-06-27 05:42:48.882541
Model ind 665 epoch 1293 batch: 600 avg loss -2.864724 avg loss no lamb -2.864724 time 2020-06-27 05:42:59.687060
Model ind 665 epoch 1293 batch: 700 avg loss -2.732758 avg loss no lamb -2.732758 time 2020-06-27 05:43:10.442020
Model ind 665 epoch 1293 batch: 800 avg loss -2.877537 avg loss no lamb -2.877537 time 2020-06-27 05:43:21.439228
last batch sz 10
Pre: time 2020-06-27 05:43:35.810515: 
 	std: 0.0024947918
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9812, 0.9761, 0.9822, 0.9778]
	train_accs: [0.98153335, 0.9813167, 0.97636664, 0.98178333, 0.9773333]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.9799
	best: 0.9822

Starting e_i: 1294
Model ind 665 epoch 1294 batch: 0 avg loss -2.940513 avg loss no lamb -2.940513 time 2020-06-27 05:43:37.032932
Model ind 665 epoch 1294 batch: 100 avg loss -2.929186 avg loss no lamb -2.929186 time 2020-06-27 05:43:47.859390
Model ind 665 epoch 1294 batch: 200 avg loss -2.866230 avg loss no lamb -2.866230 time 2020-06-27 05:43:58.821549
Model ind 665 epoch 1294 batch: 300 avg loss -2.830260 avg loss no lamb -2.830260 time 2020-06-27 05:44:09.660528
Model ind 665 epoch 1294 batch: 400 avg loss -2.871779 avg loss no lamb -2.871779 time 2020-06-27 05:44:20.452428
Model ind 665 epoch 1294 batch: 500 avg loss -2.805250 avg loss no lamb -2.805250 time 2020-06-27 05:44:31.158586
Model ind 665 epoch 1294 batch: 600 avg loss -2.911922 avg loss no lamb -2.911922 time 2020-06-27 05:44:41.877439
Model ind 665 epoch 1294 batch: 700 avg loss -2.772659 avg loss no lamb -2.772659 time 2020-06-27 05:44:52.782101
Model ind 665 epoch 1294 batch: 800 avg loss -2.912954 avg loss no lamb -2.912954 time 2020-06-27 05:45:03.518648
last batch sz 10
Pre: time 2020-06-27 05:45:17.353719: 
 	std: 0.0031211695
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.98, 0.9734, 0.9799, 0.9736]
	train_accs: [0.9809, 0.98055, 0.97485, 0.9813667, 0.97583336]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97732
	best: 0.9799

Starting e_i: 1295
Model ind 665 epoch 1295 batch: 0 avg loss -2.908300 avg loss no lamb -2.908300 time 2020-06-27 05:45:18.553618
Model ind 665 epoch 1295 batch: 100 avg loss -2.942755 avg loss no lamb -2.942755 time 2020-06-27 05:45:29.525331
Model ind 665 epoch 1295 batch: 200 avg loss -2.866854 avg loss no lamb -2.866854 time 2020-06-27 05:45:40.372265
Model ind 665 epoch 1295 batch: 300 avg loss -2.898527 avg loss no lamb -2.898527 time 2020-06-27 05:45:51.160121
Model ind 665 epoch 1295 batch: 400 avg loss -2.728336 avg loss no lamb -2.728336 time 2020-06-27 05:46:02.096728
Model ind 665 epoch 1295 batch: 500 avg loss -2.786613 avg loss no lamb -2.786613 time 2020-06-27 05:46:12.994577
Model ind 665 epoch 1295 batch: 600 avg loss -2.879507 avg loss no lamb -2.879507 time 2020-06-27 05:46:23.898304
Model ind 665 epoch 1295 batch: 700 avg loss -2.786923 avg loss no lamb -2.786923 time 2020-06-27 05:46:34.750205
Model ind 665 epoch 1295 batch: 800 avg loss -2.851542 avg loss no lamb -2.851542 time 2020-06-27 05:46:45.706628
last batch sz 10
Pre: time 2020-06-27 05:46:59.891278: 
 	std: 0.0023794079
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9794, 0.9745, 0.9803, 0.976]
	train_accs: [0.9808, 0.9802667, 0.9752, 0.9811, 0.97643334]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97808
	best: 0.9803

Starting e_i: 1296
Model ind 665 epoch 1296 batch: 0 avg loss -2.904390 avg loss no lamb -2.904390 time 2020-06-27 05:47:01.095764
Model ind 665 epoch 1296 batch: 100 avg loss -2.942193 avg loss no lamb -2.942193 time 2020-06-27 05:47:11.791767
Model ind 665 epoch 1296 batch: 200 avg loss -2.896124 avg loss no lamb -2.896124 time 2020-06-27 05:47:22.795779
Model ind 665 epoch 1296 batch: 300 avg loss -2.856887 avg loss no lamb -2.856887 time 2020-06-27 05:47:33.917652
Model ind 665 epoch 1296 batch: 400 avg loss -2.729147 avg loss no lamb -2.729147 time 2020-06-27 05:47:44.839556
Model ind 665 epoch 1296 batch: 500 avg loss -2.863756 avg loss no lamb -2.863756 time 2020-06-27 05:47:55.618352
Model ind 665 epoch 1296 batch: 600 avg loss -2.890757 avg loss no lamb -2.890757 time 2020-06-27 05:48:06.638177
Model ind 665 epoch 1296 batch: 700 avg loss -2.735904 avg loss no lamb -2.735904 time 2020-06-27 05:48:17.553398
Model ind 665 epoch 1296 batch: 800 avg loss -2.942189 avg loss no lamb -2.942189 time 2020-06-27 05:48:28.353934
last batch sz 10
Pre: time 2020-06-27 05:48:42.624876: 
 	std: 0.002742271
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9788, 0.973, 0.9792, 0.9744]
	train_accs: [0.9809833, 0.9799, 0.97485, 0.98078334, 0.976]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97700006
	best: 0.9796

Starting e_i: 1297
Model ind 665 epoch 1297 batch: 0 avg loss -2.914386 avg loss no lamb -2.914386 time 2020-06-27 05:48:43.835113
Model ind 665 epoch 1297 batch: 100 avg loss -2.931322 avg loss no lamb -2.931322 time 2020-06-27 05:48:54.579595
Model ind 665 epoch 1297 batch: 200 avg loss -2.812638 avg loss no lamb -2.812638 time 2020-06-27 05:49:05.573662
Model ind 665 epoch 1297 batch: 300 avg loss -2.871196 avg loss no lamb -2.871196 time 2020-06-27 05:49:16.712772
Model ind 665 epoch 1297 batch: 400 avg loss -2.840159 avg loss no lamb -2.840159 time 2020-06-27 05:49:27.699578
Model ind 665 epoch 1297 batch: 500 avg loss -2.831680 avg loss no lamb -2.831680 time 2020-06-27 05:49:38.434738
Model ind 665 epoch 1297 batch: 600 avg loss -2.870900 avg loss no lamb -2.870900 time 2020-06-27 05:49:49.302388
Model ind 665 epoch 1297 batch: 700 avg loss -2.814164 avg loss no lamb -2.814164 time 2020-06-27 05:49:59.983468
Model ind 665 epoch 1297 batch: 800 avg loss -2.839781 avg loss no lamb -2.839781 time 2020-06-27 05:50:10.682214
last batch sz 10
Pre: time 2020-06-27 05:50:25.076120: 
 	std: 0.0032335639
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9795, 0.9731, 0.9813, 0.9753]
	train_accs: [0.98106664, 0.9799833, 0.97461665, 0.9813, 0.97625]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.978
	best: 0.9813

Starting e_i: 1298
Model ind 665 epoch 1298 batch: 0 avg loss -2.921844 avg loss no lamb -2.921844 time 2020-06-27 05:50:26.246648
Model ind 665 epoch 1298 batch: 100 avg loss -2.885382 avg loss no lamb -2.885382 time 2020-06-27 05:50:36.938992
Model ind 665 epoch 1298 batch: 200 avg loss -2.921080 avg loss no lamb -2.921080 time 2020-06-27 05:50:47.655700
Model ind 665 epoch 1298 batch: 300 avg loss -2.869122 avg loss no lamb -2.869122 time 2020-06-27 05:50:58.498860
Model ind 665 epoch 1298 batch: 400 avg loss -2.846535 avg loss no lamb -2.846535 time 2020-06-27 05:51:09.665307
Model ind 665 epoch 1298 batch: 500 avg loss -2.808497 avg loss no lamb -2.808497 time 2020-06-27 05:51:20.463937
Model ind 665 epoch 1298 batch: 600 avg loss -2.897954 avg loss no lamb -2.897954 time 2020-06-27 05:51:31.310646
Model ind 665 epoch 1298 batch: 700 avg loss -2.727426 avg loss no lamb -2.727426 time 2020-06-27 05:51:42.028434
Model ind 665 epoch 1298 batch: 800 avg loss -2.855262 avg loss no lamb -2.855262 time 2020-06-27 05:51:52.885040
last batch sz 10
Pre: time 2020-06-27 05:52:07.204382: 
 	std: 0.0028937142
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9806, 0.9753, 0.9819, 0.9762]
	train_accs: [0.9820667, 0.98113334, 0.9755833, 0.98216665, 0.97648335]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97922003
	best: 0.9819

Starting e_i: 1299
Model ind 665 epoch 1299 batch: 0 avg loss -2.971478 avg loss no lamb -2.971478 time 2020-06-27 05:52:08.395740
Model ind 665 epoch 1299 batch: 100 avg loss -2.945207 avg loss no lamb -2.945207 time 2020-06-27 05:52:19.137698
Model ind 665 epoch 1299 batch: 200 avg loss -2.889059 avg loss no lamb -2.889059 time 2020-06-27 05:52:29.936472
Model ind 665 epoch 1299 batch: 300 avg loss -2.918022 avg loss no lamb -2.918022 time 2020-06-27 05:52:40.986538
Model ind 665 epoch 1299 batch: 400 avg loss -2.860742 avg loss no lamb -2.860742 time 2020-06-27 05:52:51.839639
Model ind 665 epoch 1299 batch: 500 avg loss -2.891175 avg loss no lamb -2.891175 time 2020-06-27 05:53:02.674744
Model ind 665 epoch 1299 batch: 600 avg loss -2.887616 avg loss no lamb -2.887616 time 2020-06-27 05:53:13.520788
Model ind 665 epoch 1299 batch: 700 avg loss -2.761432 avg loss no lamb -2.761432 time 2020-06-27 05:53:24.298080
Model ind 665 epoch 1299 batch: 800 avg loss -2.847001 avg loss no lamb -2.847001 time 2020-06-27 05:53:35.358550
last batch sz 10
Pre: time 2020-06-27 05:53:49.836622: 
 	std: 0.002820365
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9802, 0.9744, 0.9806, 0.9755]
	train_accs: [0.98151666, 0.98075, 0.975, 0.98156667, 0.97581667]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97836
	best: 0.9806

Starting e_i: 1300
Model ind 665 epoch 1300 batch: 0 avg loss -2.933813 avg loss no lamb -2.933813 time 2020-06-27 05:53:51.127327
Model ind 665 epoch 1300 batch: 100 avg loss -2.850087 avg loss no lamb -2.850087 time 2020-06-27 05:54:01.864286
Model ind 665 epoch 1300 batch: 200 avg loss -2.857107 avg loss no lamb -2.857107 time 2020-06-27 05:54:13.094264
Model ind 665 epoch 1300 batch: 300 avg loss -2.841597 avg loss no lamb -2.841597 time 2020-06-27 05:54:24.193937
Model ind 665 epoch 1300 batch: 400 avg loss -2.777528 avg loss no lamb -2.777528 time 2020-06-27 05:54:35.142603
Model ind 665 epoch 1300 batch: 500 avg loss -2.867730 avg loss no lamb -2.867730 time 2020-06-27 05:54:45.991222
Model ind 665 epoch 1300 batch: 600 avg loss -2.898753 avg loss no lamb -2.898753 time 2020-06-27 05:54:56.831225
Model ind 665 epoch 1300 batch: 700 avg loss -2.764933 avg loss no lamb -2.764933 time 2020-06-27 05:55:07.736888
Model ind 665 epoch 1300 batch: 800 avg loss -2.837870 avg loss no lamb -2.837870 time 2020-06-27 05:55:18.579366
last batch sz 10
Pre: time 2020-06-27 05:55:32.618399: 
 	std: 0.0026844793
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9797, 0.9745, 0.9816, 0.9769]
	train_accs: [0.98118335, 0.98055, 0.9752333, 0.9816167, 0.9768]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97876006
	best: 0.9816

Starting e_i: 1301
Model ind 665 epoch 1301 batch: 0 avg loss -2.968368 avg loss no lamb -2.968368 time 2020-06-27 05:55:34.993041
Model ind 665 epoch 1301 batch: 100 avg loss -2.877863 avg loss no lamb -2.877863 time 2020-06-27 05:55:45.876301
Model ind 665 epoch 1301 batch: 200 avg loss -2.909378 avg loss no lamb -2.909378 time 2020-06-27 05:55:56.470980
Model ind 665 epoch 1301 batch: 300 avg loss -2.842366 avg loss no lamb -2.842366 time 2020-06-27 05:56:07.386720
Model ind 665 epoch 1301 batch: 400 avg loss -2.814663 avg loss no lamb -2.814663 time 2020-06-27 05:56:18.330848
Model ind 665 epoch 1301 batch: 500 avg loss -2.825542 avg loss no lamb -2.825542 time 2020-06-27 05:56:29.168788
Model ind 665 epoch 1301 batch: 600 avg loss -2.860754 avg loss no lamb -2.860754 time 2020-06-27 05:56:39.933049
Model ind 665 epoch 1301 batch: 700 avg loss -2.712523 avg loss no lamb -2.712523 time 2020-06-27 05:56:50.565103
Model ind 665 epoch 1301 batch: 800 avg loss -2.875425 avg loss no lamb -2.875425 time 2020-06-27 05:57:01.362242
last batch sz 10
Pre: time 2020-06-27 05:57:15.766645: 
 	std: 0.0030770272
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9811, 0.9745, 0.9811, 0.9754]
	train_accs: [0.98145, 0.9809667, 0.97573334, 0.9813, 0.9763]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.97870004
	best: 0.9814

Starting e_i: 1302
Model ind 665 epoch 1302 batch: 0 avg loss -2.957648 avg loss no lamb -2.957648 time 2020-06-27 05:57:16.983703
Model ind 665 epoch 1302 batch: 100 avg loss -2.960657 avg loss no lamb -2.960657 time 2020-06-27 05:57:27.742172
Model ind 665 epoch 1302 batch: 200 avg loss -2.841200 avg loss no lamb -2.841200 time 2020-06-27 05:57:38.648482
Model ind 665 epoch 1302 batch: 300 avg loss -2.916703 avg loss no lamb -2.916703 time 2020-06-27 05:57:49.381638
Model ind 665 epoch 1302 batch: 400 avg loss -2.772566 avg loss no lamb -2.772566 time 2020-06-27 05:58:00.320891
Model ind 665 epoch 1302 batch: 500 avg loss -2.820235 avg loss no lamb -2.820235 time 2020-06-27 05:58:11.188480
Model ind 665 epoch 1302 batch: 600 avg loss -2.880416 avg loss no lamb -2.880416 time 2020-06-27 05:58:22.118127
Model ind 665 epoch 1302 batch: 700 avg loss -2.855539 avg loss no lamb -2.855539 time 2020-06-27 05:58:33.101613
Model ind 665 epoch 1302 batch: 800 avg loss -2.877905 avg loss no lamb -2.877905 time 2020-06-27 05:58:44.155370
last batch sz 10
Pre: time 2020-06-27 05:58:58.325130: 
 	std: 0.0025919925
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9804, 0.9746, 0.9804, 0.9758]
	train_accs: [0.98123336, 0.98085, 0.9755667, 0.9813833, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97834
	best: 0.9804

Starting e_i: 1303
Model ind 665 epoch 1303 batch: 0 avg loss -2.912192 avg loss no lamb -2.912192 time 2020-06-27 05:58:59.506371
Model ind 665 epoch 1303 batch: 100 avg loss -2.843618 avg loss no lamb -2.843618 time 2020-06-27 05:59:10.370875
Model ind 665 epoch 1303 batch: 200 avg loss -2.844761 avg loss no lamb -2.844761 time 2020-06-27 05:59:21.253198
Model ind 665 epoch 1303 batch: 300 avg loss -2.854556 avg loss no lamb -2.854556 time 2020-06-27 05:59:32.130169
Model ind 665 epoch 1303 batch: 400 avg loss -2.825039 avg loss no lamb -2.825039 time 2020-06-27 05:59:43.038673
Model ind 665 epoch 1303 batch: 500 avg loss -2.874199 avg loss no lamb -2.874199 time 2020-06-27 05:59:53.958082
Model ind 665 epoch 1303 batch: 600 avg loss -2.905454 avg loss no lamb -2.905454 time 2020-06-27 06:00:04.769511
Model ind 665 epoch 1303 batch: 700 avg loss -2.745230 avg loss no lamb -2.745230 time 2020-06-27 06:00:15.648388
Model ind 665 epoch 1303 batch: 800 avg loss -2.786127 avg loss no lamb -2.786127 time 2020-06-27 06:00:26.664694
last batch sz 10
Pre: time 2020-06-27 06:00:41.070843: 
 	std: 0.0031531644
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9804, 0.974, 0.9806, 0.9742]
	train_accs: [0.98113334, 0.9802, 0.97475, 0.9816167, 0.97566664]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97796
	best: 0.9806

Starting e_i: 1304
Model ind 665 epoch 1304 batch: 0 avg loss -3.003369 avg loss no lamb -3.003369 time 2020-06-27 06:00:42.266227
Model ind 665 epoch 1304 batch: 100 avg loss -2.934105 avg loss no lamb -2.934105 time 2020-06-27 06:00:53.444325
Model ind 665 epoch 1304 batch: 200 avg loss -2.887822 avg loss no lamb -2.887822 time 2020-06-27 06:01:04.278453
Model ind 665 epoch 1304 batch: 300 avg loss -2.885792 avg loss no lamb -2.885792 time 2020-06-27 06:01:14.879336
Model ind 665 epoch 1304 batch: 400 avg loss -2.827926 avg loss no lamb -2.827926 time 2020-06-27 06:01:25.778834
Model ind 665 epoch 1304 batch: 500 avg loss -2.866613 avg loss no lamb -2.866613 time 2020-06-27 06:01:36.698543
Model ind 665 epoch 1304 batch: 600 avg loss -2.866522 avg loss no lamb -2.866522 time 2020-06-27 06:01:47.441213
Model ind 665 epoch 1304 batch: 700 avg loss -2.765868 avg loss no lamb -2.765868 time 2020-06-27 06:01:58.097899
Model ind 665 epoch 1304 batch: 800 avg loss -2.848618 avg loss no lamb -2.848618 time 2020-06-27 06:02:09.086563
last batch sz 10
Pre: time 2020-06-27 06:02:23.007351: 
 	std: 0.0034041281
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9798, 0.9733, 0.9807, 0.9736]
	train_accs: [0.98113334, 0.98046666, 0.97511667, 0.98148334, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9776
	best: 0.9807

Starting e_i: 1305
Model ind 665 epoch 1305 batch: 0 avg loss -2.959592 avg loss no lamb -2.959592 time 2020-06-27 06:02:24.167802
Model ind 665 epoch 1305 batch: 100 avg loss -2.896837 avg loss no lamb -2.896837 time 2020-06-27 06:02:34.976471
Model ind 665 epoch 1305 batch: 200 avg loss -2.932162 avg loss no lamb -2.932162 time 2020-06-27 06:02:45.747196
Model ind 665 epoch 1305 batch: 300 avg loss -2.840650 avg loss no lamb -2.840650 time 2020-06-27 06:02:56.786139
Model ind 665 epoch 1305 batch: 400 avg loss -2.730355 avg loss no lamb -2.730355 time 2020-06-27 06:03:07.628226
Model ind 665 epoch 1305 batch: 500 avg loss -2.829653 avg loss no lamb -2.829653 time 2020-06-27 06:03:18.441594
Model ind 665 epoch 1305 batch: 600 avg loss -2.920512 avg loss no lamb -2.920512 time 2020-06-27 06:03:29.295899
Model ind 665 epoch 1305 batch: 700 avg loss -2.826773 avg loss no lamb -2.826773 time 2020-06-27 06:03:40.351013
Model ind 665 epoch 1305 batch: 800 avg loss -2.829271 avg loss no lamb -2.829271 time 2020-06-27 06:03:51.502515
last batch sz 10
Pre: time 2020-06-27 06:04:05.565067: 
 	std: 0.0036521654
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9789, 0.9725, 0.9806, 0.9728]
	train_accs: [0.98083335, 0.9798333, 0.97433335, 0.98115, 0.97548336]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97705996
	best: 0.9806

Starting e_i: 1306
Model ind 665 epoch 1306 batch: 0 avg loss -2.963404 avg loss no lamb -2.963404 time 2020-06-27 06:04:06.753493
Model ind 665 epoch 1306 batch: 100 avg loss -2.910885 avg loss no lamb -2.910885 time 2020-06-27 06:04:17.750844
Model ind 665 epoch 1306 batch: 200 avg loss -2.913570 avg loss no lamb -2.913570 time 2020-06-27 06:04:28.751120
Model ind 665 epoch 1306 batch: 300 avg loss -2.935394 avg loss no lamb -2.935394 time 2020-06-27 06:04:39.772131
Model ind 665 epoch 1306 batch: 400 avg loss -2.831812 avg loss no lamb -2.831812 time 2020-06-27 06:04:50.630901
Model ind 665 epoch 1306 batch: 500 avg loss -2.813210 avg loss no lamb -2.813210 time 2020-06-27 06:05:01.562809
Model ind 665 epoch 1306 batch: 600 avg loss -2.903853 avg loss no lamb -2.903853 time 2020-06-27 06:05:12.386003
Model ind 665 epoch 1306 batch: 700 avg loss -2.761292 avg loss no lamb -2.761292 time 2020-06-27 06:05:23.169448
Model ind 665 epoch 1306 batch: 800 avg loss -2.892242 avg loss no lamb -2.892242 time 2020-06-27 06:05:33.787804
last batch sz 10
Pre: time 2020-06-27 06:05:47.911755: 
 	std: 0.003296425
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9799, 0.9732, 0.9807, 0.9741]
	train_accs: [0.981, 0.98083335, 0.9751, 0.98156667, 0.97618335]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97766
	best: 0.9807

Starting e_i: 1307
Model ind 665 epoch 1307 batch: 0 avg loss -2.954126 avg loss no lamb -2.954126 time 2020-06-27 06:05:49.111103
Model ind 665 epoch 1307 batch: 100 avg loss -2.927266 avg loss no lamb -2.927266 time 2020-06-27 06:06:00.214849
Model ind 665 epoch 1307 batch: 200 avg loss -2.899944 avg loss no lamb -2.899944 time 2020-06-27 06:06:10.920798
Model ind 665 epoch 1307 batch: 300 avg loss -2.899565 avg loss no lamb -2.899565 time 2020-06-27 06:06:21.932344
Model ind 665 epoch 1307 batch: 400 avg loss -2.788637 avg loss no lamb -2.788637 time 2020-06-27 06:06:32.358555
Model ind 665 epoch 1307 batch: 500 avg loss -2.854398 avg loss no lamb -2.854398 time 2020-06-27 06:06:43.349808
Model ind 665 epoch 1307 batch: 600 avg loss -2.849502 avg loss no lamb -2.849502 time 2020-06-27 06:06:54.160771
Model ind 665 epoch 1307 batch: 700 avg loss -2.806109 avg loss no lamb -2.806109 time 2020-06-27 06:07:04.919539
Model ind 665 epoch 1307 batch: 800 avg loss -2.906620 avg loss no lamb -2.906620 time 2020-06-27 06:07:15.725292
last batch sz 10
Pre: time 2020-06-27 06:07:29.549993: 
 	std: 0.0032270239
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9795, 0.9734, 0.9804, 0.9737]
	train_accs: [0.98113334, 0.9805833, 0.97496665, 0.9812667, 0.9756]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97748005
	best: 0.9804

Starting e_i: 1308
Model ind 665 epoch 1308 batch: 0 avg loss -2.922113 avg loss no lamb -2.922113 time 2020-06-27 06:07:30.948404
Model ind 665 epoch 1308 batch: 100 avg loss -2.899528 avg loss no lamb -2.899528 time 2020-06-27 06:07:41.717900
Model ind 665 epoch 1308 batch: 200 avg loss -2.849041 avg loss no lamb -2.849041 time 2020-06-27 06:07:52.601205
Model ind 665 epoch 1308 batch: 300 avg loss -2.889375 avg loss no lamb -2.889375 time 2020-06-27 06:08:03.438047
Model ind 665 epoch 1308 batch: 400 avg loss -2.809246 avg loss no lamb -2.809246 time 2020-06-27 06:08:14.229864
Model ind 665 epoch 1308 batch: 500 avg loss -2.832659 avg loss no lamb -2.832659 time 2020-06-27 06:08:25.369965
Model ind 665 epoch 1308 batch: 600 avg loss -2.865950 avg loss no lamb -2.865950 time 2020-06-27 06:08:36.277146
Model ind 665 epoch 1308 batch: 700 avg loss -2.700417 avg loss no lamb -2.700417 time 2020-06-27 06:08:47.063550
Model ind 665 epoch 1308 batch: 800 avg loss -2.854314 avg loss no lamb -2.854314 time 2020-06-27 06:08:57.684881
last batch sz 10
Pre: time 2020-06-27 06:09:11.918743: 
 	std: 0.0027089403
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9802, 0.9741, 0.9799, 0.9757]
	train_accs: [0.9813333, 0.98071665, 0.9755833, 0.9810333, 0.9769]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97814
	best: 0.9808

Starting e_i: 1309
Model ind 665 epoch 1309 batch: 0 avg loss -2.942238 avg loss no lamb -2.942238 time 2020-06-27 06:09:13.108292
Model ind 665 epoch 1309 batch: 100 avg loss -2.838030 avg loss no lamb -2.838030 time 2020-06-27 06:09:24.072949
Model ind 665 epoch 1309 batch: 200 avg loss -2.874156 avg loss no lamb -2.874156 time 2020-06-27 06:09:34.870507
Model ind 665 epoch 1309 batch: 300 avg loss -2.869526 avg loss no lamb -2.869526 time 2020-06-27 06:09:45.782869
Model ind 665 epoch 1309 batch: 400 avg loss -2.820839 avg loss no lamb -2.820839 time 2020-06-27 06:09:56.821319
Model ind 665 epoch 1309 batch: 500 avg loss -2.830520 avg loss no lamb -2.830520 time 2020-06-27 06:10:07.840979
Model ind 665 epoch 1309 batch: 600 avg loss -2.878876 avg loss no lamb -2.878876 time 2020-06-27 06:10:18.742011
Model ind 665 epoch 1309 batch: 700 avg loss -2.808964 avg loss no lamb -2.808964 time 2020-06-27 06:10:29.835077
Model ind 665 epoch 1309 batch: 800 avg loss -2.868985 avg loss no lamb -2.868985 time 2020-06-27 06:10:40.750705
last batch sz 10
Pre: time 2020-06-27 06:10:54.784309: 
 	std: 0.002066502
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9779, 0.9769, 0.9731, 0.9777, 0.9736]
	train_accs: [0.98015, 0.97961664, 0.9755833, 0.9801667, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97584
	best: 0.9777

Starting e_i: 1310
Model ind 665 epoch 1310 batch: 0 avg loss -2.936250 avg loss no lamb -2.936250 time 2020-06-27 06:10:56.159242
Model ind 665 epoch 1310 batch: 100 avg loss -2.888386 avg loss no lamb -2.888386 time 2020-06-27 06:11:07.043794
Model ind 665 epoch 1310 batch: 200 avg loss -2.895289 avg loss no lamb -2.895289 time 2020-06-27 06:11:17.918369
Model ind 665 epoch 1310 batch: 300 avg loss -2.916601 avg loss no lamb -2.916601 time 2020-06-27 06:11:28.970928
Model ind 665 epoch 1310 batch: 400 avg loss -2.794940 avg loss no lamb -2.794940 time 2020-06-27 06:11:40.181749
Model ind 665 epoch 1310 batch: 500 avg loss -2.836920 avg loss no lamb -2.836920 time 2020-06-27 06:11:51.099645
Model ind 665 epoch 1310 batch: 600 avg loss -2.876685 avg loss no lamb -2.876685 time 2020-06-27 06:12:01.942899
Model ind 665 epoch 1310 batch: 700 avg loss -2.807675 avg loss no lamb -2.807675 time 2020-06-27 06:12:12.681292
Model ind 665 epoch 1310 batch: 800 avg loss -2.874282 avg loss no lamb -2.874282 time 2020-06-27 06:12:23.664194
last batch sz 10
Pre: time 2020-06-27 06:12:37.595066: 
 	std: 0.0028872103
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9799, 0.9741, 0.9802, 0.9749]
	train_accs: [0.98181665, 0.9811, 0.97575, 0.98175, 0.97641665]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.978
	best: 0.9809

Starting e_i: 1311
Model ind 665 epoch 1311 batch: 0 avg loss -2.976920 avg loss no lamb -2.976920 time 2020-06-27 06:12:39.999027
Model ind 665 epoch 1311 batch: 100 avg loss -2.935218 avg loss no lamb -2.935218 time 2020-06-27 06:12:50.851603
Model ind 665 epoch 1311 batch: 200 avg loss -2.863292 avg loss no lamb -2.863292 time 2020-06-27 06:13:01.754343
Model ind 665 epoch 1311 batch: 300 avg loss -2.861218 avg loss no lamb -2.861218 time 2020-06-27 06:13:12.583788
Model ind 665 epoch 1311 batch: 400 avg loss -2.864823 avg loss no lamb -2.864823 time 2020-06-27 06:13:23.724024
Model ind 665 epoch 1311 batch: 500 avg loss -2.834028 avg loss no lamb -2.834028 time 2020-06-27 06:13:34.676719
Model ind 665 epoch 1311 batch: 600 avg loss -2.898185 avg loss no lamb -2.898185 time 2020-06-27 06:13:45.686204
Model ind 665 epoch 1311 batch: 700 avg loss -2.809801 avg loss no lamb -2.809801 time 2020-06-27 06:13:56.549355
Model ind 665 epoch 1311 batch: 800 avg loss -2.896291 avg loss no lamb -2.896291 time 2020-06-27 06:14:07.543878
last batch sz 10
Pre: time 2020-06-27 06:14:21.630227: 
 	std: 0.0033702287
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9805, 0.9733, 0.9807, 0.9746]
	train_accs: [0.98121667, 0.98071665, 0.9747, 0.9814, 0.97585]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97804004
	best: 0.9807

Starting e_i: 1312
Model ind 665 epoch 1312 batch: 0 avg loss -2.956663 avg loss no lamb -2.956663 time 2020-06-27 06:14:22.973714
Model ind 665 epoch 1312 batch: 100 avg loss -2.907278 avg loss no lamb -2.907278 time 2020-06-27 06:14:33.778295
Model ind 665 epoch 1312 batch: 200 avg loss -2.902738 avg loss no lamb -2.902738 time 2020-06-27 06:14:44.578232
Model ind 665 epoch 1312 batch: 300 avg loss -2.827415 avg loss no lamb -2.827415 time 2020-06-27 06:14:55.636959
Model ind 665 epoch 1312 batch: 400 avg loss -2.814429 avg loss no lamb -2.814429 time 2020-06-27 06:15:06.559551
Model ind 665 epoch 1312 batch: 500 avg loss -2.891076 avg loss no lamb -2.891076 time 2020-06-27 06:15:17.455351
Model ind 665 epoch 1312 batch: 600 avg loss -2.909733 avg loss no lamb -2.909733 time 2020-06-27 06:15:28.386620
Model ind 665 epoch 1312 batch: 700 avg loss -2.784994 avg loss no lamb -2.784994 time 2020-06-27 06:15:39.440390
Model ind 665 epoch 1312 batch: 800 avg loss -2.821747 avg loss no lamb -2.821747 time 2020-06-27 06:15:50.251774
last batch sz 10
Pre: time 2020-06-27 06:16:04.369381: 
 	std: 0.0032526962
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9783, 0.972, 0.9797, 0.9734]
	train_accs: [0.98088336, 0.9803, 0.9744667, 0.9809667, 0.97541666]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97660005
	best: 0.9797

Starting e_i: 1313
Model ind 665 epoch 1313 batch: 0 avg loss -2.949755 avg loss no lamb -2.949755 time 2020-06-27 06:16:05.562523
Model ind 665 epoch 1313 batch: 100 avg loss -2.973092 avg loss no lamb -2.973092 time 2020-06-27 06:16:16.170607
Model ind 665 epoch 1313 batch: 200 avg loss -2.917183 avg loss no lamb -2.917183 time 2020-06-27 06:16:27.037612
Model ind 665 epoch 1313 batch: 300 avg loss -2.810650 avg loss no lamb -2.810650 time 2020-06-27 06:16:38.087783
Model ind 665 epoch 1313 batch: 400 avg loss -2.774645 avg loss no lamb -2.774645 time 2020-06-27 06:16:48.755067
Model ind 665 epoch 1313 batch: 500 avg loss -2.841738 avg loss no lamb -2.841738 time 2020-06-27 06:16:59.473728
Model ind 665 epoch 1313 batch: 600 avg loss -2.909958 avg loss no lamb -2.909958 time 2020-06-27 06:17:10.430268
Model ind 665 epoch 1313 batch: 700 avg loss -2.848682 avg loss no lamb -2.848682 time 2020-06-27 06:17:21.312819
Model ind 665 epoch 1313 batch: 800 avg loss -2.842173 avg loss no lamb -2.842173 time 2020-06-27 06:17:32.344738
last batch sz 10
Pre: time 2020-06-27 06:17:46.454743: 
 	std: 0.002676874
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9798, 0.9744, 0.9799, 0.9747]
	train_accs: [0.9813, 0.98041666, 0.97506666, 0.9814, 0.97606665]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97782004
	best: 0.9799

Starting e_i: 1314
Model ind 665 epoch 1314 batch: 0 avg loss -2.933712 avg loss no lamb -2.933712 time 2020-06-27 06:17:47.839625
Model ind 665 epoch 1314 batch: 100 avg loss -2.898432 avg loss no lamb -2.898432 time 2020-06-27 06:17:58.669087
Model ind 665 epoch 1314 batch: 200 avg loss -2.931118 avg loss no lamb -2.931118 time 2020-06-27 06:18:09.545374
Model ind 665 epoch 1314 batch: 300 avg loss -2.870112 avg loss no lamb -2.870112 time 2020-06-27 06:18:20.355326
Model ind 665 epoch 1314 batch: 400 avg loss -2.809319 avg loss no lamb -2.809319 time 2020-06-27 06:18:31.036893
Model ind 665 epoch 1314 batch: 500 avg loss -2.841565 avg loss no lamb -2.841565 time 2020-06-27 06:18:41.941199
Model ind 665 epoch 1314 batch: 600 avg loss -2.895396 avg loss no lamb -2.895396 time 2020-06-27 06:18:53.821737
Model ind 665 epoch 1314 batch: 700 avg loss -2.772659 avg loss no lamb -2.772659 time 2020-06-27 06:19:05.780658
Model ind 665 epoch 1314 batch: 800 avg loss -2.808816 avg loss no lamb -2.808816 time 2020-06-27 06:19:16.584736
last batch sz 10
Pre: time 2020-06-27 06:19:30.699693: 
 	std: 0.00263105
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9795, 0.974, 0.9796, 0.9749]
	train_accs: [0.98088336, 0.9805, 0.97515, 0.98081666, 0.9761]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97764
	best: 0.9802

Starting e_i: 1315
Model ind 665 epoch 1315 batch: 0 avg loss -2.936129 avg loss no lamb -2.936129 time 2020-06-27 06:19:31.873207
Model ind 665 epoch 1315 batch: 100 avg loss -2.877049 avg loss no lamb -2.877049 time 2020-06-27 06:19:42.712181
Model ind 665 epoch 1315 batch: 200 avg loss -2.863050 avg loss no lamb -2.863050 time 2020-06-27 06:19:53.519146
Model ind 665 epoch 1315 batch: 300 avg loss -2.863106 avg loss no lamb -2.863106 time 2020-06-27 06:20:04.311002
Model ind 665 epoch 1315 batch: 400 avg loss -2.793364 avg loss no lamb -2.793364 time 2020-06-27 06:20:15.000256
Model ind 665 epoch 1315 batch: 500 avg loss -2.885593 avg loss no lamb -2.885593 time 2020-06-27 06:20:25.973824
Model ind 665 epoch 1315 batch: 600 avg loss -2.892902 avg loss no lamb -2.892902 time 2020-06-27 06:20:36.749541
Model ind 665 epoch 1315 batch: 700 avg loss -2.821818 avg loss no lamb -2.821818 time 2020-06-27 06:20:47.559719
Model ind 665 epoch 1315 batch: 800 avg loss -2.842017 avg loss no lamb -2.842017 time 2020-06-27 06:20:58.083337
last batch sz 10
Pre: time 2020-06-27 06:21:12.305300: 
 	std: 0.002508861
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.9798, 0.9739, 0.9793, 0.9749]
	train_accs: [0.9810333, 0.9809, 0.97571665, 0.9810167, 0.97613335]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97744006
	best: 0.9793

Starting e_i: 1316
Model ind 665 epoch 1316 batch: 0 avg loss -2.928994 avg loss no lamb -2.928994 time 2020-06-27 06:21:13.657565
Model ind 665 epoch 1316 batch: 100 avg loss -2.929517 avg loss no lamb -2.929517 time 2020-06-27 06:21:24.530431
Model ind 665 epoch 1316 batch: 200 avg loss -2.835944 avg loss no lamb -2.835944 time 2020-06-27 06:21:35.460528
Model ind 665 epoch 1316 batch: 300 avg loss -2.877790 avg loss no lamb -2.877790 time 2020-06-27 06:21:46.807275
Model ind 665 epoch 1316 batch: 400 avg loss -2.766735 avg loss no lamb -2.766735 time 2020-06-27 06:21:57.545039
Model ind 665 epoch 1316 batch: 500 avg loss -2.836345 avg loss no lamb -2.836345 time 2020-06-27 06:22:08.460005
Model ind 665 epoch 1316 batch: 600 avg loss -2.909838 avg loss no lamb -2.909838 time 2020-06-27 06:22:19.321491
Model ind 665 epoch 1316 batch: 700 avg loss -2.823386 avg loss no lamb -2.823386 time 2020-06-27 06:22:30.273197
Model ind 665 epoch 1316 batch: 800 avg loss -2.843298 avg loss no lamb -2.843298 time 2020-06-27 06:22:41.245764
last batch sz 10
Pre: time 2020-06-27 06:22:55.470524: 
 	std: 0.002851386
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9817, 0.9757, 0.9814, 0.9758]
	train_accs: [0.98176664, 0.9816667, 0.97605, 0.98181665, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9757
	avg: 0.97924006
	best: 0.9814

Starting e_i: 1317
Model ind 665 epoch 1317 batch: 0 avg loss -2.915142 avg loss no lamb -2.915142 time 2020-06-27 06:22:56.671680
Model ind 665 epoch 1317 batch: 100 avg loss -2.875593 avg loss no lamb -2.875593 time 2020-06-27 06:23:07.461661
Model ind 665 epoch 1317 batch: 200 avg loss -2.863960 avg loss no lamb -2.863960 time 2020-06-27 06:23:18.303411
Model ind 665 epoch 1317 batch: 300 avg loss -2.830011 avg loss no lamb -2.830011 time 2020-06-27 06:23:29.448267
Model ind 665 epoch 1317 batch: 400 avg loss -2.816677 avg loss no lamb -2.816677 time 2020-06-27 06:23:40.506410
Model ind 665 epoch 1317 batch: 500 avg loss -2.894873 avg loss no lamb -2.894873 time 2020-06-27 06:23:51.456859
Model ind 665 epoch 1317 batch: 600 avg loss -2.878145 avg loss no lamb -2.878145 time 2020-06-27 06:24:02.259861
Model ind 665 epoch 1317 batch: 700 avg loss -2.796716 avg loss no lamb -2.796716 time 2020-06-27 06:24:13.394319
Model ind 665 epoch 1317 batch: 800 avg loss -2.871972 avg loss no lamb -2.871972 time 2020-06-27 06:24:24.344626
last batch sz 10
Pre: time 2020-06-27 06:24:38.633074: 
 	std: 0.0036268972
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9821, 0.9818, 0.9746, 0.9821, 0.9746]
	train_accs: [0.9816333, 0.98145, 0.9757, 0.9815, 0.97545]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97904
	best: 0.9821

Starting e_i: 1318
Model ind 665 epoch 1318 batch: 0 avg loss -2.921288 avg loss no lamb -2.921288 time 2020-06-27 06:24:39.877656
Model ind 665 epoch 1318 batch: 100 avg loss -2.939759 avg loss no lamb -2.939759 time 2020-06-27 06:24:50.727693
Model ind 665 epoch 1318 batch: 200 avg loss -2.904370 avg loss no lamb -2.904370 time 2020-06-27 06:25:01.682750
Model ind 665 epoch 1318 batch: 300 avg loss -2.836329 avg loss no lamb -2.836329 time 2020-06-27 06:25:12.474367
Model ind 665 epoch 1318 batch: 400 avg loss -2.798822 avg loss no lamb -2.798822 time 2020-06-27 06:25:23.510237
Model ind 665 epoch 1318 batch: 500 avg loss -2.876215 avg loss no lamb -2.876215 time 2020-06-27 06:25:34.371079
Model ind 665 epoch 1318 batch: 600 avg loss -2.876997 avg loss no lamb -2.876997 time 2020-06-27 06:25:45.156327
Model ind 665 epoch 1318 batch: 700 avg loss -2.807706 avg loss no lamb -2.807706 time 2020-06-27 06:25:56.031362
Model ind 665 epoch 1318 batch: 800 avg loss -2.850607 avg loss no lamb -2.850607 time 2020-06-27 06:26:07.044337
last batch sz 10
Pre: time 2020-06-27 06:26:21.570271: 
 	std: 0.0032535607
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9812, 0.9748, 0.9811, 0.9744]
	train_accs: [0.98146665, 0.9809833, 0.9755833, 0.9813333, 0.9755]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97858
	best: 0.9814

Starting e_i: 1319
Model ind 665 epoch 1319 batch: 0 avg loss -2.984203 avg loss no lamb -2.984203 time 2020-06-27 06:26:22.757837
Model ind 665 epoch 1319 batch: 100 avg loss -2.859051 avg loss no lamb -2.859051 time 2020-06-27 06:26:33.674614
Model ind 665 epoch 1319 batch: 200 avg loss -2.840060 avg loss no lamb -2.840060 time 2020-06-27 06:26:44.167543
Model ind 665 epoch 1319 batch: 300 avg loss -2.872474 avg loss no lamb -2.872474 time 2020-06-27 06:26:55.026553
Model ind 665 epoch 1319 batch: 400 avg loss -2.793352 avg loss no lamb -2.793352 time 2020-06-27 06:27:06.189258
Model ind 665 epoch 1319 batch: 500 avg loss -2.834826 avg loss no lamb -2.834826 time 2020-06-27 06:27:17.026058
Model ind 665 epoch 1319 batch: 600 avg loss -2.863114 avg loss no lamb -2.863114 time 2020-06-27 06:27:27.751076
Model ind 665 epoch 1319 batch: 700 avg loss -2.800482 avg loss no lamb -2.800482 time 2020-06-27 06:27:38.704499
Model ind 665 epoch 1319 batch: 800 avg loss -2.857703 avg loss no lamb -2.857703 time 2020-06-27 06:27:49.871491
last batch sz 10
Pre: time 2020-06-27 06:28:03.911716: 
 	std: 0.003197243
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9808, 0.9743, 0.9804, 0.974]
	train_accs: [0.9811, 0.9809833, 0.97601664, 0.98151666, 0.9762833]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97805995
	best: 0.9804

Starting e_i: 1320
Model ind 665 epoch 1320 batch: 0 avg loss -2.944465 avg loss no lamb -2.944465 time 2020-06-27 06:28:05.081764
Model ind 665 epoch 1320 batch: 100 avg loss -2.907645 avg loss no lamb -2.907645 time 2020-06-27 06:28:15.880142
Model ind 665 epoch 1320 batch: 200 avg loss -2.886240 avg loss no lamb -2.886240 time 2020-06-27 06:28:26.708005
Model ind 665 epoch 1320 batch: 300 avg loss -2.913080 avg loss no lamb -2.913080 time 2020-06-27 06:28:37.830589
Model ind 665 epoch 1320 batch: 400 avg loss -2.808705 avg loss no lamb -2.808705 time 2020-06-27 06:28:48.857779
Model ind 665 epoch 1320 batch: 500 avg loss -2.869158 avg loss no lamb -2.869158 time 2020-06-27 06:28:59.543950
Model ind 665 epoch 1320 batch: 600 avg loss -2.920144 avg loss no lamb -2.920144 time 2020-06-27 06:29:10.170847
Model ind 665 epoch 1320 batch: 700 avg loss -2.804576 avg loss no lamb -2.804576 time 2020-06-27 06:29:20.911653
Model ind 665 epoch 1320 batch: 800 avg loss -2.874611 avg loss no lamb -2.874611 time 2020-06-27 06:29:32.046943
last batch sz 10
Pre: time 2020-06-27 06:29:46.289530: 
 	std: 0.003328313
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9803, 0.9747, 0.9817, 0.974]
	train_accs: [0.9813333, 0.98071665, 0.9755333, 0.98193336, 0.9756333]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97838
	best: 0.9817

Starting e_i: 1321
Model ind 665 epoch 1321 batch: 0 avg loss -2.996321 avg loss no lamb -2.996321 time 2020-06-27 06:29:48.626104
Model ind 665 epoch 1321 batch: 100 avg loss -2.909244 avg loss no lamb -2.909244 time 2020-06-27 06:29:59.541077
Model ind 665 epoch 1321 batch: 200 avg loss -2.903930 avg loss no lamb -2.903930 time 2020-06-27 06:30:10.561978
Model ind 665 epoch 1321 batch: 300 avg loss -2.866705 avg loss no lamb -2.866705 time 2020-06-27 06:30:21.431224
Model ind 665 epoch 1321 batch: 400 avg loss -2.699907 avg loss no lamb -2.699907 time 2020-06-27 06:30:32.339160
Model ind 665 epoch 1321 batch: 500 avg loss -2.848872 avg loss no lamb -2.848872 time 2020-06-27 06:30:43.131511
Model ind 665 epoch 1321 batch: 600 avg loss -2.903525 avg loss no lamb -2.903525 time 2020-06-27 06:30:54.051800
Model ind 665 epoch 1321 batch: 700 avg loss -2.814477 avg loss no lamb -2.814477 time 2020-06-27 06:31:04.801658
Model ind 665 epoch 1321 batch: 800 avg loss -2.842768 avg loss no lamb -2.842768 time 2020-06-27 06:31:15.549342
last batch sz 10
Pre: time 2020-06-27 06:31:29.796815: 
 	std: 0.0027636283
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9804, 0.9749, 0.9809, 0.9754]
	train_accs: [0.9816, 0.9811, 0.97693336, 0.98185, 0.97715]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97852004
	best: 0.9809

Starting e_i: 1322
Model ind 665 epoch 1322 batch: 0 avg loss -2.904262 avg loss no lamb -2.904262 time 2020-06-27 06:31:31.007642
Model ind 665 epoch 1322 batch: 100 avg loss -2.866455 avg loss no lamb -2.866455 time 2020-06-27 06:31:41.793720
Model ind 665 epoch 1322 batch: 200 avg loss -2.897519 avg loss no lamb -2.897519 time 2020-06-27 06:31:52.610907
Model ind 665 epoch 1322 batch: 300 avg loss -2.815943 avg loss no lamb -2.815943 time 2020-06-27 06:32:03.427232
Model ind 665 epoch 1322 batch: 400 avg loss -2.801267 avg loss no lamb -2.801267 time 2020-06-27 06:32:14.279545
Model ind 665 epoch 1322 batch: 500 avg loss -2.882237 avg loss no lamb -2.882237 time 2020-06-27 06:32:25.209581
Model ind 665 epoch 1322 batch: 600 avg loss -2.925653 avg loss no lamb -2.925653 time 2020-06-27 06:32:35.956239
Model ind 665 epoch 1322 batch: 700 avg loss -2.782136 avg loss no lamb -2.782136 time 2020-06-27 06:32:46.827367
Model ind 665 epoch 1322 batch: 800 avg loss -2.871569 avg loss no lamb -2.871569 time 2020-06-27 06:32:57.696335
last batch sz 10
Pre: time 2020-06-27 06:33:12.106524: 
 	std: 0.0029853105
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9798, 0.9736, 0.98, 0.9741]
	train_accs: [0.98116666, 0.9805, 0.9757, 0.9813833, 0.97615]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97749996
	best: 0.98

Starting e_i: 1323
Model ind 665 epoch 1323 batch: 0 avg loss -2.941091 avg loss no lamb -2.941091 time 2020-06-27 06:33:13.293368
Model ind 665 epoch 1323 batch: 100 avg loss -2.916459 avg loss no lamb -2.916459 time 2020-06-27 06:33:23.911732
Model ind 665 epoch 1323 batch: 200 avg loss -2.886234 avg loss no lamb -2.886234 time 2020-06-27 06:33:34.695810
Model ind 665 epoch 1323 batch: 300 avg loss -2.893353 avg loss no lamb -2.893353 time 2020-06-27 06:33:45.493981
Model ind 665 epoch 1323 batch: 400 avg loss -2.816289 avg loss no lamb -2.816289 time 2020-06-27 06:33:56.453162
Model ind 665 epoch 1323 batch: 500 avg loss -2.839936 avg loss no lamb -2.839936 time 2020-06-27 06:34:07.267449
Model ind 665 epoch 1323 batch: 600 avg loss -2.852114 avg loss no lamb -2.852114 time 2020-06-27 06:34:18.110423
Model ind 665 epoch 1323 batch: 700 avg loss -2.792526 avg loss no lamb -2.792526 time 2020-06-27 06:34:29.010223
Model ind 665 epoch 1323 batch: 800 avg loss -2.866339 avg loss no lamb -2.866339 time 2020-06-27 06:34:39.676507
last batch sz 10
Pre: time 2020-06-27 06:34:53.824944: 
 	std: 0.0032873077
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9797, 0.9731, 0.9809, 0.9752]
	train_accs: [0.98176664, 0.981, 0.97496665, 0.9816667, 0.97653335]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97804004
	best: 0.9813

Starting e_i: 1324
Model ind 665 epoch 1324 batch: 0 avg loss -2.937354 avg loss no lamb -2.937354 time 2020-06-27 06:34:54.965435
Model ind 665 epoch 1324 batch: 100 avg loss -2.892314 avg loss no lamb -2.892314 time 2020-06-27 06:35:05.947539
Model ind 665 epoch 1324 batch: 200 avg loss -2.914533 avg loss no lamb -2.914533 time 2020-06-27 06:35:16.688984
Model ind 665 epoch 1324 batch: 300 avg loss -2.881497 avg loss no lamb -2.881497 time 2020-06-27 06:35:27.448730
Model ind 665 epoch 1324 batch: 400 avg loss -2.844618 avg loss no lamb -2.844618 time 2020-06-27 06:35:38.204351
Model ind 665 epoch 1324 batch: 500 avg loss -2.810049 avg loss no lamb -2.810049 time 2020-06-27 06:35:49.076924
Model ind 665 epoch 1324 batch: 600 avg loss -2.810700 avg loss no lamb -2.810700 time 2020-06-27 06:35:59.960405
Model ind 665 epoch 1324 batch: 700 avg loss -2.856123 avg loss no lamb -2.856123 time 2020-06-27 06:36:10.871139
Model ind 665 epoch 1324 batch: 800 avg loss -2.899877 avg loss no lamb -2.899877 time 2020-06-27 06:36:21.803273
last batch sz 10
Pre: time 2020-06-27 06:36:35.869580: 
 	std: 0.0035391464
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9805, 0.9735, 0.9814, 0.9748]
	train_accs: [0.9810167, 0.98031664, 0.97456664, 0.98111665, 0.9762]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97841996
	best: 0.9814

Starting e_i: 1325
Model ind 665 epoch 1325 batch: 0 avg loss -2.944953 avg loss no lamb -2.944953 time 2020-06-27 06:36:37.046916
Model ind 665 epoch 1325 batch: 100 avg loss -2.915790 avg loss no lamb -2.915790 time 2020-06-27 06:36:47.649979
Model ind 665 epoch 1325 batch: 200 avg loss -2.856619 avg loss no lamb -2.856619 time 2020-06-27 06:36:58.361482
Model ind 665 epoch 1325 batch: 300 avg loss -2.873692 avg loss no lamb -2.873692 time 2020-06-27 06:37:09.200415
Model ind 665 epoch 1325 batch: 400 avg loss -2.818333 avg loss no lamb -2.818333 time 2020-06-27 06:37:20.024255
Model ind 665 epoch 1325 batch: 500 avg loss -2.858952 avg loss no lamb -2.858952 time 2020-06-27 06:37:31.156000
Model ind 665 epoch 1325 batch: 600 avg loss -2.940699 avg loss no lamb -2.940699 time 2020-06-27 06:37:42.132504
Model ind 665 epoch 1325 batch: 700 avg loss -2.707501 avg loss no lamb -2.707501 time 2020-06-27 06:37:53.062070
Model ind 665 epoch 1325 batch: 800 avg loss -2.846549 avg loss no lamb -2.846549 time 2020-06-27 06:38:03.953714
last batch sz 10
Pre: time 2020-06-27 06:38:17.853314: 
 	std: 0.0024565042
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9795, 0.9748, 0.9806, 0.9756]
	train_accs: [0.9808, 0.98036665, 0.97576666, 0.9810333, 0.97645]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.9781599
	best: 0.9806

Starting e_i: 1326
Model ind 665 epoch 1326 batch: 0 avg loss -2.960048 avg loss no lamb -2.960048 time 2020-06-27 06:38:18.999441
Model ind 665 epoch 1326 batch: 100 avg loss -2.870260 avg loss no lamb -2.870260 time 2020-06-27 06:38:29.740240
Model ind 665 epoch 1326 batch: 200 avg loss -2.837996 avg loss no lamb -2.837996 time 2020-06-27 06:38:40.846077
Model ind 665 epoch 1326 batch: 300 avg loss -2.868297 avg loss no lamb -2.868297 time 2020-06-27 06:38:52.159028
Model ind 665 epoch 1326 batch: 400 avg loss -2.818739 avg loss no lamb -2.818739 time 2020-06-27 06:39:03.159382
Model ind 665 epoch 1326 batch: 500 avg loss -2.881372 avg loss no lamb -2.881372 time 2020-06-27 06:39:14.070032
Model ind 665 epoch 1326 batch: 600 avg loss -2.816717 avg loss no lamb -2.816717 time 2020-06-27 06:39:24.757570
Model ind 665 epoch 1326 batch: 700 avg loss -2.808821 avg loss no lamb -2.808821 time 2020-06-27 06:39:35.592563
Model ind 665 epoch 1326 batch: 800 avg loss -2.832416 avg loss no lamb -2.832416 time 2020-06-27 06:39:46.348931
last batch sz 10
Pre: time 2020-06-27 06:40:00.587910: 
 	std: 0.0036657387
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9794, 0.9722, 0.9812, 0.9745]
	train_accs: [0.98118335, 0.9806, 0.97445, 0.9816, 0.97585]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97768
	best: 0.9812

Starting e_i: 1327
Model ind 665 epoch 1327 batch: 0 avg loss -2.921152 avg loss no lamb -2.921152 time 2020-06-27 06:40:01.840581
Model ind 665 epoch 1327 batch: 100 avg loss -2.885869 avg loss no lamb -2.885869 time 2020-06-27 06:40:12.733091
Model ind 665 epoch 1327 batch: 200 avg loss -2.896249 avg loss no lamb -2.896249 time 2020-06-27 06:40:23.605927
Model ind 665 epoch 1327 batch: 300 avg loss -2.830481 avg loss no lamb -2.830481 time 2020-06-27 06:40:34.467335
Model ind 665 epoch 1327 batch: 400 avg loss -2.749067 avg loss no lamb -2.749067 time 2020-06-27 06:40:45.260584
Model ind 665 epoch 1327 batch: 500 avg loss -2.900728 avg loss no lamb -2.900728 time 2020-06-27 06:40:55.984896
Model ind 665 epoch 1327 batch: 600 avg loss -2.921583 avg loss no lamb -2.921583 time 2020-06-27 06:41:07.241457
Model ind 665 epoch 1327 batch: 700 avg loss -2.686695 avg loss no lamb -2.686695 time 2020-06-27 06:41:18.270914
Model ind 665 epoch 1327 batch: 800 avg loss -2.857880 avg loss no lamb -2.857880 time 2020-06-27 06:41:29.076363
last batch sz 10
Pre: time 2020-06-27 06:41:43.369322: 
 	std: 0.0034945584
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9799, 0.9731, 0.9808, 0.9738]
	train_accs: [0.98158336, 0.9809, 0.97478336, 0.9813667, 0.9765]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97769994
	best: 0.9809

Starting e_i: 1328
Model ind 665 epoch 1328 batch: 0 avg loss -2.922643 avg loss no lamb -2.922643 time 2020-06-27 06:41:44.595527
Model ind 665 epoch 1328 batch: 100 avg loss -2.931424 avg loss no lamb -2.931424 time 2020-06-27 06:41:55.650997
Model ind 665 epoch 1328 batch: 200 avg loss -2.807228 avg loss no lamb -2.807228 time 2020-06-27 06:42:06.610704
Model ind 665 epoch 1328 batch: 300 avg loss -2.855082 avg loss no lamb -2.855082 time 2020-06-27 06:42:17.388868
Model ind 665 epoch 1328 batch: 400 avg loss -2.823945 avg loss no lamb -2.823945 time 2020-06-27 06:42:28.208701
Model ind 665 epoch 1328 batch: 500 avg loss -2.838337 avg loss no lamb -2.838337 time 2020-06-27 06:42:38.894019
Model ind 665 epoch 1328 batch: 600 avg loss -2.896524 avg loss no lamb -2.896524 time 2020-06-27 06:42:49.751991
Model ind 665 epoch 1328 batch: 700 avg loss -2.748193 avg loss no lamb -2.748193 time 2020-06-27 06:43:00.772958
Model ind 665 epoch 1328 batch: 800 avg loss -2.871505 avg loss no lamb -2.871505 time 2020-06-27 06:43:11.728290
last batch sz 10
Pre: time 2020-06-27 06:43:25.685697: 
 	std: 0.0029691714
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9808, 0.975, 0.9818, 0.9758]
	train_accs: [0.98176664, 0.9813167, 0.97576666, 0.98193336, 0.97686666]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.979
	best: 0.9818

Starting e_i: 1329
Model ind 665 epoch 1329 batch: 0 avg loss -2.914753 avg loss no lamb -2.914753 time 2020-06-27 06:43:26.930563
Model ind 665 epoch 1329 batch: 100 avg loss -2.882900 avg loss no lamb -2.882900 time 2020-06-27 06:43:37.835714
Model ind 665 epoch 1329 batch: 200 avg loss -2.883180 avg loss no lamb -2.883180 time 2020-06-27 06:43:48.763142
Model ind 665 epoch 1329 batch: 300 avg loss -2.876783 avg loss no lamb -2.876783 time 2020-06-27 06:43:59.828650
Model ind 665 epoch 1329 batch: 400 avg loss -2.739144 avg loss no lamb -2.739144 time 2020-06-27 06:44:10.588977
Model ind 665 epoch 1329 batch: 500 avg loss -2.871172 avg loss no lamb -2.871172 time 2020-06-27 06:44:21.237005
Model ind 665 epoch 1329 batch: 600 avg loss -2.853225 avg loss no lamb -2.853225 time 2020-06-27 06:44:32.288111
Model ind 665 epoch 1329 batch: 700 avg loss -2.783577 avg loss no lamb -2.783577 time 2020-06-27 06:44:43.230747
Model ind 665 epoch 1329 batch: 800 avg loss -2.912925 avg loss no lamb -2.912925 time 2020-06-27 06:44:54.076030
last batch sz 10
Pre: time 2020-06-27 06:45:08.187644: 
 	std: 0.0034808163
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9806, 0.9729, 0.9803, 0.9736]
	train_accs: [0.9809333, 0.9805, 0.97465, 0.9812667, 0.9758]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97749996
	best: 0.9803

Starting e_i: 1330
Model ind 665 epoch 1330 batch: 0 avg loss -2.959902 avg loss no lamb -2.959902 time 2020-06-27 06:45:09.371711
Model ind 665 epoch 1330 batch: 100 avg loss -2.848458 avg loss no lamb -2.848458 time 2020-06-27 06:45:20.136788
Model ind 665 epoch 1330 batch: 200 avg loss -2.888067 avg loss no lamb -2.888067 time 2020-06-27 06:45:30.837714
Model ind 665 epoch 1330 batch: 300 avg loss -2.914930 avg loss no lamb -2.914930 time 2020-06-27 06:45:41.623843
Model ind 665 epoch 1330 batch: 400 avg loss -2.810906 avg loss no lamb -2.810906 time 2020-06-27 06:45:52.403966
Model ind 665 epoch 1330 batch: 500 avg loss -2.819091 avg loss no lamb -2.819091 time 2020-06-27 06:46:03.486558
Model ind 665 epoch 1330 batch: 600 avg loss -2.924598 avg loss no lamb -2.924598 time 2020-06-27 06:46:14.468329
Model ind 665 epoch 1330 batch: 700 avg loss -2.778352 avg loss no lamb -2.778352 time 2020-06-27 06:46:25.384733
Model ind 665 epoch 1330 batch: 800 avg loss -2.876643 avg loss no lamb -2.876643 time 2020-06-27 06:46:36.074885
last batch sz 10
Pre: time 2020-06-27 06:46:50.472375: 
 	std: 0.0030611085
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9789, 0.9734, 0.98, 0.9739]
	train_accs: [0.98081666, 0.9795667, 0.97495, 0.98108333, 0.9761]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.9773399
	best: 0.98

Starting e_i: 1331
Model ind 665 epoch 1331 batch: 0 avg loss -2.960070 avg loss no lamb -2.960070 time 2020-06-27 06:46:52.874015
Model ind 665 epoch 1331 batch: 100 avg loss -2.860134 avg loss no lamb -2.860134 time 2020-06-27 06:47:03.537502
Model ind 665 epoch 1331 batch: 200 avg loss -2.881575 avg loss no lamb -2.881575 time 2020-06-27 06:47:14.470205
Model ind 665 epoch 1331 batch: 300 avg loss -2.897566 avg loss no lamb -2.897566 time 2020-06-27 06:47:25.224860
Model ind 665 epoch 1331 batch: 400 avg loss -2.816402 avg loss no lamb -2.816402 time 2020-06-27 06:47:36.155562
Model ind 665 epoch 1331 batch: 500 avg loss -2.859656 avg loss no lamb -2.859656 time 2020-06-27 06:47:46.804366
Model ind 665 epoch 1331 batch: 600 avg loss -2.913989 avg loss no lamb -2.913989 time 2020-06-27 06:47:57.691398
Model ind 665 epoch 1331 batch: 700 avg loss -2.815155 avg loss no lamb -2.815155 time 2020-06-27 06:48:08.554050
Model ind 665 epoch 1331 batch: 800 avg loss -2.829167 avg loss no lamb -2.829167 time 2020-06-27 06:48:19.579768
last batch sz 10
Pre: time 2020-06-27 06:48:33.452311: 
 	std: 0.0027731576
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.9782, 0.9721, 0.9779, 0.9738]
	train_accs: [0.9808, 0.9799167, 0.97468334, 0.9806833, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9721
	avg: 0.97624004
	best: 0.9792

Starting e_i: 1332
Model ind 665 epoch 1332 batch: 0 avg loss -2.926470 avg loss no lamb -2.926470 time 2020-06-27 06:48:34.631998
Model ind 665 epoch 1332 batch: 100 avg loss -2.934826 avg loss no lamb -2.934826 time 2020-06-27 06:48:45.524374
Model ind 665 epoch 1332 batch: 200 avg loss -2.897605 avg loss no lamb -2.897605 time 2020-06-27 06:48:56.260680
Model ind 665 epoch 1332 batch: 300 avg loss -2.848449 avg loss no lamb -2.848449 time 2020-06-27 06:49:07.006458
Model ind 665 epoch 1332 batch: 400 avg loss -2.803109 avg loss no lamb -2.803109 time 2020-06-27 06:49:17.854938
Model ind 665 epoch 1332 batch: 500 avg loss -2.874394 avg loss no lamb -2.874394 time 2020-06-27 06:49:28.719200
Model ind 665 epoch 1332 batch: 600 avg loss -2.933727 avg loss no lamb -2.933727 time 2020-06-27 06:49:39.662664
Model ind 665 epoch 1332 batch: 700 avg loss -2.843218 avg loss no lamb -2.843218 time 2020-06-27 06:49:50.777589
Model ind 665 epoch 1332 batch: 800 avg loss -2.889145 avg loss no lamb -2.889145 time 2020-06-27 06:50:01.786096
last batch sz 10
Pre: time 2020-06-27 06:50:16.002287: 
 	std: 0.0029423747
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.979, 0.9734, 0.9798, 0.9738]
	train_accs: [0.98083335, 0.9799167, 0.9751, 0.98081666, 0.9755]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97718
	best: 0.9799

Starting e_i: 1333
Model ind 665 epoch 1333 batch: 0 avg loss -2.949190 avg loss no lamb -2.949190 time 2020-06-27 06:50:17.189358
Model ind 665 epoch 1333 batch: 100 avg loss -2.856389 avg loss no lamb -2.856389 time 2020-06-27 06:50:28.261063
Model ind 665 epoch 1333 batch: 200 avg loss -2.871360 avg loss no lamb -2.871360 time 2020-06-27 06:50:39.361834
Model ind 665 epoch 1333 batch: 300 avg loss -2.868241 avg loss no lamb -2.868241 time 2020-06-27 06:50:50.135136
Model ind 665 epoch 1333 batch: 400 avg loss -2.781533 avg loss no lamb -2.781533 time 2020-06-27 06:51:00.852773
Model ind 665 epoch 1333 batch: 500 avg loss -2.849942 avg loss no lamb -2.849942 time 2020-06-27 06:51:11.703638
Model ind 665 epoch 1333 batch: 600 avg loss -2.864437 avg loss no lamb -2.864437 time 2020-06-27 06:51:22.384365
Model ind 665 epoch 1333 batch: 700 avg loss -2.810833 avg loss no lamb -2.810833 time 2020-06-27 06:51:33.197436
Model ind 665 epoch 1333 batch: 800 avg loss -2.888963 avg loss no lamb -2.888963 time 2020-06-27 06:51:44.226311
last batch sz 10
Pre: time 2020-06-27 06:51:58.222303: 
 	std: 0.0029225985
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.981, 0.9742, 0.9795, 0.9745]
	train_accs: [0.98071665, 0.9802833, 0.97543335, 0.98045, 0.9763]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97788
	best: 0.9802

Starting e_i: 1334
Model ind 665 epoch 1334 batch: 0 avg loss -2.949927 avg loss no lamb -2.949927 time 2020-06-27 06:51:59.412564
Model ind 665 epoch 1334 batch: 100 avg loss -2.889863 avg loss no lamb -2.889863 time 2020-06-27 06:52:10.359113
Model ind 665 epoch 1334 batch: 200 avg loss -2.909738 avg loss no lamb -2.909738 time 2020-06-27 06:52:21.379045
Model ind 665 epoch 1334 batch: 300 avg loss -2.878695 avg loss no lamb -2.878695 time 2020-06-27 06:52:32.317623
Model ind 665 epoch 1334 batch: 400 avg loss -2.826833 avg loss no lamb -2.826833 time 2020-06-27 06:52:43.156289
Model ind 665 epoch 1334 batch: 500 avg loss -2.879694 avg loss no lamb -2.879694 time 2020-06-27 06:52:54.013137
Model ind 665 epoch 1334 batch: 600 avg loss -2.872200 avg loss no lamb -2.872200 time 2020-06-27 06:53:04.992610
Model ind 665 epoch 1334 batch: 700 avg loss -2.785708 avg loss no lamb -2.785708 time 2020-06-27 06:53:15.846250
Model ind 665 epoch 1334 batch: 800 avg loss -2.858921 avg loss no lamb -2.858921 time 2020-06-27 06:53:26.480084
last batch sz 10
Pre: time 2020-06-27 06:53:40.763609: 
 	std: 0.0031527802
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9803, 0.974, 0.9809, 0.9752]
	train_accs: [0.9814, 0.98071665, 0.97576666, 0.98118335, 0.9767]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97840005
	best: 0.9816

Starting e_i: 1335
Model ind 665 epoch 1335 batch: 0 avg loss -2.960801 avg loss no lamb -2.960801 time 2020-06-27 06:53:42.130447
Model ind 665 epoch 1335 batch: 100 avg loss -2.895628 avg loss no lamb -2.895628 time 2020-06-27 06:53:52.799728
Model ind 665 epoch 1335 batch: 200 avg loss -2.877073 avg loss no lamb -2.877073 time 2020-06-27 06:54:03.604520
Model ind 665 epoch 1335 batch: 300 avg loss -2.814228 avg loss no lamb -2.814228 time 2020-06-27 06:54:14.407705
Model ind 665 epoch 1335 batch: 400 avg loss -2.815157 avg loss no lamb -2.815157 time 2020-06-27 06:54:25.174492
Model ind 665 epoch 1335 batch: 500 avg loss -2.814882 avg loss no lamb -2.814882 time 2020-06-27 06:54:35.875305
Model ind 665 epoch 1335 batch: 600 avg loss -2.889237 avg loss no lamb -2.889237 time 2020-06-27 06:54:46.835647
Model ind 665 epoch 1335 batch: 700 avg loss -2.755606 avg loss no lamb -2.755606 time 2020-06-27 06:54:57.908995
Model ind 665 epoch 1335 batch: 800 avg loss -2.906122 avg loss no lamb -2.906122 time 2020-06-27 06:55:08.902567
last batch sz 10
Pre: time 2020-06-27 06:55:22.836561: 
 	std: 0.0032025022
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.98, 0.9735, 0.9793, 0.9727]
	train_accs: [0.9809167, 0.98046666, 0.97498333, 0.98085, 0.97513336]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97700006
	best: 0.9795

Starting e_i: 1336
Model ind 665 epoch 1336 batch: 0 avg loss -2.945776 avg loss no lamb -2.945776 time 2020-06-27 06:55:24.035663
Model ind 665 epoch 1336 batch: 100 avg loss -2.898705 avg loss no lamb -2.898705 time 2020-06-27 06:55:35.009520
Model ind 665 epoch 1336 batch: 200 avg loss -2.896159 avg loss no lamb -2.896159 time 2020-06-27 06:55:46.254487
Model ind 665 epoch 1336 batch: 300 avg loss -2.857637 avg loss no lamb -2.857637 time 2020-06-27 06:55:57.230086
Model ind 665 epoch 1336 batch: 400 avg loss -2.835138 avg loss no lamb -2.835138 time 2020-06-27 06:56:08.124200
Model ind 665 epoch 1336 batch: 500 avg loss -2.845075 avg loss no lamb -2.845075 time 2020-06-27 06:56:19.162377
Model ind 665 epoch 1336 batch: 600 avg loss -2.886022 avg loss no lamb -2.886022 time 2020-06-27 06:56:29.977999
Model ind 665 epoch 1336 batch: 700 avg loss -2.677737 avg loss no lamb -2.677737 time 2020-06-27 06:56:40.918286
Model ind 665 epoch 1336 batch: 800 avg loss -2.877881 avg loss no lamb -2.877881 time 2020-06-27 06:56:51.755824
last batch sz 10
Pre: time 2020-06-27 06:57:05.920870: 
 	std: 0.002933667
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9782, 0.9791, 0.9732, 0.9792, 0.9726]
	train_accs: [0.9807, 0.9802167, 0.9749333, 0.9808, 0.97568333]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97646
	best: 0.9792

Starting e_i: 1337
Model ind 665 epoch 1337 batch: 0 avg loss -2.958628 avg loss no lamb -2.958628 time 2020-06-27 06:57:07.258618
Model ind 665 epoch 1337 batch: 100 avg loss -2.834071 avg loss no lamb -2.834071 time 2020-06-27 06:57:18.089585
Model ind 665 epoch 1337 batch: 200 avg loss -2.905886 avg loss no lamb -2.905886 time 2020-06-27 06:57:29.013156
Model ind 665 epoch 1337 batch: 300 avg loss -2.879325 avg loss no lamb -2.879325 time 2020-06-27 06:57:39.988241
Model ind 665 epoch 1337 batch: 400 avg loss -2.772485 avg loss no lamb -2.772485 time 2020-06-27 06:57:50.761845
Model ind 665 epoch 1337 batch: 500 avg loss -2.846101 avg loss no lamb -2.846101 time 2020-06-27 06:58:01.572738
Model ind 665 epoch 1337 batch: 600 avg loss -2.945461 avg loss no lamb -2.945461 time 2020-06-27 06:58:12.403880
Model ind 665 epoch 1337 batch: 700 avg loss -2.693595 avg loss no lamb -2.693595 time 2020-06-27 06:58:23.174354
Model ind 665 epoch 1337 batch: 800 avg loss -2.899960 avg loss no lamb -2.899960 time 2020-06-27 06:58:33.910800
last batch sz 10
Pre: time 2020-06-27 06:58:47.726493: 
 	std: 0.0032915617
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9782, 0.9789, 0.9723, 0.9791, 0.9718]
	train_accs: [0.98041666, 0.9799333, 0.9741833, 0.98088336, 0.9744167]
	best_train_sub_head: 3
	worst: 0.9718
	avg: 0.97606003
	best: 0.9791

Starting e_i: 1338
Model ind 665 epoch 1338 batch: 0 avg loss -2.976453 avg loss no lamb -2.976453 time 2020-06-27 06:58:48.915375
Model ind 665 epoch 1338 batch: 100 avg loss -2.899746 avg loss no lamb -2.899746 time 2020-06-27 06:58:59.756305
Model ind 665 epoch 1338 batch: 200 avg loss -2.819869 avg loss no lamb -2.819869 time 2020-06-27 06:59:10.779209
Model ind 665 epoch 1338 batch: 300 avg loss -2.887789 avg loss no lamb -2.887789 time 2020-06-27 06:59:21.583938
Model ind 665 epoch 1338 batch: 400 avg loss -2.785218 avg loss no lamb -2.785218 time 2020-06-27 06:59:32.302182
Model ind 665 epoch 1338 batch: 500 avg loss -2.863292 avg loss no lamb -2.863292 time 2020-06-27 06:59:43.300790
Model ind 665 epoch 1338 batch: 600 avg loss -2.893773 avg loss no lamb -2.893773 time 2020-06-27 06:59:54.435717
Model ind 665 epoch 1338 batch: 700 avg loss -2.739474 avg loss no lamb -2.739474 time 2020-06-27 07:00:05.560356
Model ind 665 epoch 1338 batch: 800 avg loss -2.913135 avg loss no lamb -2.913135 time 2020-06-27 07:00:16.479165
last batch sz 10
Pre: time 2020-06-27 07:00:30.871340: 
 	std: 0.0027911342
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9801, 0.9748, 0.9805, 0.9747]
	train_accs: [0.98143333, 0.98071665, 0.97583336, 0.98125, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.9781599
	best: 0.9807

Starting e_i: 1339
Model ind 665 epoch 1339 batch: 0 avg loss -2.989348 avg loss no lamb -2.989348 time 2020-06-27 07:00:32.296142
Model ind 665 epoch 1339 batch: 100 avg loss -2.911176 avg loss no lamb -2.911176 time 2020-06-27 07:00:43.148031
Model ind 665 epoch 1339 batch: 200 avg loss -2.897542 avg loss no lamb -2.897542 time 2020-06-27 07:00:54.004334
Model ind 665 epoch 1339 batch: 300 avg loss -2.868873 avg loss no lamb -2.868873 time 2020-06-27 07:01:05.058568
Model ind 665 epoch 1339 batch: 400 avg loss -2.807439 avg loss no lamb -2.807439 time 2020-06-27 07:01:15.884975
Model ind 665 epoch 1339 batch: 500 avg loss -2.882099 avg loss no lamb -2.882099 time 2020-06-27 07:01:26.647172
Model ind 665 epoch 1339 batch: 600 avg loss -2.888168 avg loss no lamb -2.888168 time 2020-06-27 07:01:37.349057
Model ind 665 epoch 1339 batch: 700 avg loss -2.795136 avg loss no lamb -2.795136 time 2020-06-27 07:01:48.273837
Model ind 665 epoch 1339 batch: 800 avg loss -2.786280 avg loss no lamb -2.786280 time 2020-06-27 07:01:59.324783
last batch sz 10
Pre: time 2020-06-27 07:02:13.418473: 
 	std: 0.0030675086
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9787, 0.9732, 0.9797, 0.9731]
	train_accs: [0.98083335, 0.97976667, 0.97485, 0.98083335, 0.9755667]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.9768801
	best: 0.9797

Starting e_i: 1340
Model ind 665 epoch 1340 batch: 0 avg loss -2.907603 avg loss no lamb -2.907603 time 2020-06-27 07:02:14.614627
Model ind 665 epoch 1340 batch: 100 avg loss -2.967104 avg loss no lamb -2.967104 time 2020-06-27 07:02:25.600142
Model ind 665 epoch 1340 batch: 200 avg loss -2.897767 avg loss no lamb -2.897767 time 2020-06-27 07:02:36.338691
Model ind 665 epoch 1340 batch: 300 avg loss -2.894539 avg loss no lamb -2.894539 time 2020-06-27 07:02:47.039940
Model ind 665 epoch 1340 batch: 400 avg loss -2.838604 avg loss no lamb -2.838604 time 2020-06-27 07:02:57.863524
Model ind 665 epoch 1340 batch: 500 avg loss -2.783571 avg loss no lamb -2.783571 time 2020-06-27 07:03:08.787216
Model ind 665 epoch 1340 batch: 600 avg loss -2.885804 avg loss no lamb -2.885804 time 2020-06-27 07:03:19.750960
Model ind 665 epoch 1340 batch: 700 avg loss -2.813493 avg loss no lamb -2.813493 time 2020-06-27 07:03:30.726993
Model ind 665 epoch 1340 batch: 800 avg loss -2.829563 avg loss no lamb -2.829563 time 2020-06-27 07:03:41.770066
last batch sz 10
Pre: time 2020-06-27 07:03:55.693164: 
 	std: 0.0032016141
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9802, 0.9738, 0.9805, 0.9735]
	train_accs: [0.9806167, 0.9805167, 0.9755833, 0.98083335, 0.9757]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97756004
	best: 0.9805

Starting e_i: 1341
Model ind 665 epoch 1341 batch: 0 avg loss -2.941215 avg loss no lamb -2.941215 time 2020-06-27 07:03:58.276259
Model ind 665 epoch 1341 batch: 100 avg loss -2.849248 avg loss no lamb -2.849248 time 2020-06-27 07:04:09.101477
Model ind 665 epoch 1341 batch: 200 avg loss -2.881015 avg loss no lamb -2.881015 time 2020-06-27 07:04:19.952245
Model ind 665 epoch 1341 batch: 300 avg loss -2.881208 avg loss no lamb -2.881208 time 2020-06-27 07:04:30.623776
Model ind 665 epoch 1341 batch: 400 avg loss -2.816241 avg loss no lamb -2.816241 time 2020-06-27 07:04:41.397250
Model ind 665 epoch 1341 batch: 500 avg loss -2.854080 avg loss no lamb -2.854080 time 2020-06-27 07:04:52.042289
Model ind 665 epoch 1341 batch: 600 avg loss -2.931906 avg loss no lamb -2.931906 time 2020-06-27 07:05:02.905152
Model ind 665 epoch 1341 batch: 700 avg loss -2.815303 avg loss no lamb -2.815303 time 2020-06-27 07:05:13.712689
Model ind 665 epoch 1341 batch: 800 avg loss -2.878010 avg loss no lamb -2.878010 time 2020-06-27 07:05:24.628548
last batch sz 10
Pre: time 2020-06-27 07:05:38.898806: 
 	std: 0.0026340876
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9814, 0.9753, 0.9804, 0.976]
	train_accs: [0.9816167, 0.98141664, 0.97636664, 0.9815, 0.97658336]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97884005
	best: 0.9811

Starting e_i: 1342
Model ind 665 epoch 1342 batch: 0 avg loss -2.956118 avg loss no lamb -2.956118 time 2020-06-27 07:05:40.108216
Model ind 665 epoch 1342 batch: 100 avg loss -2.915247 avg loss no lamb -2.915247 time 2020-06-27 07:05:51.104569
Model ind 665 epoch 1342 batch: 200 avg loss -2.865311 avg loss no lamb -2.865311 time 2020-06-27 07:06:01.957601
Model ind 665 epoch 1342 batch: 300 avg loss -2.896666 avg loss no lamb -2.896666 time 2020-06-27 07:06:12.701668
Model ind 665 epoch 1342 batch: 400 avg loss -2.802743 avg loss no lamb -2.802743 time 2020-06-27 07:06:23.508423
Model ind 665 epoch 1342 batch: 500 avg loss -2.853331 avg loss no lamb -2.853331 time 2020-06-27 07:06:34.358818
Model ind 665 epoch 1342 batch: 600 avg loss -2.892586 avg loss no lamb -2.892586 time 2020-06-27 07:06:45.224067
Model ind 665 epoch 1342 batch: 700 avg loss -2.770684 avg loss no lamb -2.770684 time 2020-06-27 07:06:55.970072
Model ind 665 epoch 1342 batch: 800 avg loss -2.844943 avg loss no lamb -2.844943 time 2020-06-27 07:07:06.842389
last batch sz 10
Pre: time 2020-06-27 07:07:20.927092: 
 	std: 0.0027356984
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9801, 0.9737, 0.98, 0.9751]
	train_accs: [0.981, 0.98081666, 0.97536665, 0.98108333, 0.97695]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97769994
	best: 0.98

Starting e_i: 1343
Model ind 665 epoch 1343 batch: 0 avg loss -2.921564 avg loss no lamb -2.921564 time 2020-06-27 07:07:22.351036
Model ind 665 epoch 1343 batch: 100 avg loss -2.909736 avg loss no lamb -2.909736 time 2020-06-27 07:07:33.178131
Model ind 665 epoch 1343 batch: 200 avg loss -2.852195 avg loss no lamb -2.852195 time 2020-06-27 07:07:44.231015
Model ind 665 epoch 1343 batch: 300 avg loss -2.888860 avg loss no lamb -2.888860 time 2020-06-27 07:07:54.921901
Model ind 665 epoch 1343 batch: 400 avg loss -2.785484 avg loss no lamb -2.785484 time 2020-06-27 07:08:05.703254
Model ind 665 epoch 1343 batch: 500 avg loss -2.844339 avg loss no lamb -2.844339 time 2020-06-27 07:08:16.426525
Model ind 665 epoch 1343 batch: 600 avg loss -2.888149 avg loss no lamb -2.888149 time 2020-06-27 07:08:27.229300
Model ind 665 epoch 1343 batch: 700 avg loss -2.769714 avg loss no lamb -2.769714 time 2020-06-27 07:08:37.874542
Model ind 665 epoch 1343 batch: 800 avg loss -2.883741 avg loss no lamb -2.883741 time 2020-06-27 07:08:48.904701
last batch sz 10
Pre: time 2020-06-27 07:09:03.168124: 
 	std: 0.002589662
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9788, 0.9739, 0.9792, 0.9739]
	train_accs: [0.98076665, 0.97976667, 0.97475, 0.9806167, 0.9757]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97705996
	best: 0.9795

Starting e_i: 1344
Model ind 665 epoch 1344 batch: 0 avg loss -2.976164 avg loss no lamb -2.976164 time 2020-06-27 07:09:04.390986
Model ind 665 epoch 1344 batch: 100 avg loss -2.806483 avg loss no lamb -2.806483 time 2020-06-27 07:09:15.374877
Model ind 665 epoch 1344 batch: 200 avg loss -2.866906 avg loss no lamb -2.866906 time 2020-06-27 07:09:26.028210
Model ind 665 epoch 1344 batch: 300 avg loss -2.860571 avg loss no lamb -2.860571 time 2020-06-27 07:09:36.845285
Model ind 665 epoch 1344 batch: 400 avg loss -2.777645 avg loss no lamb -2.777645 time 2020-06-27 07:09:47.523558
Model ind 665 epoch 1344 batch: 500 avg loss -2.838185 avg loss no lamb -2.838185 time 2020-06-27 07:09:58.470359
Model ind 665 epoch 1344 batch: 600 avg loss -2.897593 avg loss no lamb -2.897593 time 2020-06-27 07:10:09.501454
Model ind 665 epoch 1344 batch: 700 avg loss -2.809335 avg loss no lamb -2.809335 time 2020-06-27 07:10:20.344082
Model ind 665 epoch 1344 batch: 800 avg loss -2.809120 avg loss no lamb -2.809120 time 2020-06-27 07:10:31.337622
last batch sz 10
Pre: time 2020-06-27 07:10:45.525425: 
 	std: 0.0028986759
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9806, 0.9742, 0.9812, 0.9757]
	train_accs: [0.9809667, 0.9806, 0.9748333, 0.9809333, 0.97615]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97844
	best: 0.9805

Starting e_i: 1345
Model ind 665 epoch 1345 batch: 0 avg loss -2.990444 avg loss no lamb -2.990444 time 2020-06-27 07:10:46.891652
Model ind 665 epoch 1345 batch: 100 avg loss -2.887820 avg loss no lamb -2.887820 time 2020-06-27 07:10:57.846268
Model ind 665 epoch 1345 batch: 200 avg loss -2.909136 avg loss no lamb -2.909136 time 2020-06-27 07:11:08.782655
Model ind 665 epoch 1345 batch: 300 avg loss -2.902413 avg loss no lamb -2.902413 time 2020-06-27 07:11:19.667087
Model ind 665 epoch 1345 batch: 400 avg loss -2.801545 avg loss no lamb -2.801545 time 2020-06-27 07:11:30.434938
Model ind 665 epoch 1345 batch: 500 avg loss -2.795988 avg loss no lamb -2.795988 time 2020-06-27 07:11:41.340539
Model ind 665 epoch 1345 batch: 600 avg loss -2.874645 avg loss no lamb -2.874645 time 2020-06-27 07:11:52.389795
Model ind 665 epoch 1345 batch: 700 avg loss -2.847547 avg loss no lamb -2.847547 time 2020-06-27 07:12:03.498705
Model ind 665 epoch 1345 batch: 800 avg loss -2.790548 avg loss no lamb -2.790548 time 2020-06-27 07:12:14.361968
last batch sz 10
Pre: time 2020-06-27 07:12:28.858581: 
 	std: 0.003192121
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9795, 0.9729, 0.9804, 0.9746]
	train_accs: [0.98065, 0.98013335, 0.97495, 0.98076665, 0.9758]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97757995
	best: 0.9804

Starting e_i: 1346
Model ind 665 epoch 1346 batch: 0 avg loss -2.929265 avg loss no lamb -2.929265 time 2020-06-27 07:12:30.100233
Model ind 665 epoch 1346 batch: 100 avg loss -2.872444 avg loss no lamb -2.872444 time 2020-06-27 07:12:40.975550
Model ind 665 epoch 1346 batch: 200 avg loss -2.918460 avg loss no lamb -2.918460 time 2020-06-27 07:12:51.908357
Model ind 665 epoch 1346 batch: 300 avg loss -2.895164 avg loss no lamb -2.895164 time 2020-06-27 07:13:02.898625
Model ind 665 epoch 1346 batch: 400 avg loss -2.797977 avg loss no lamb -2.797977 time 2020-06-27 07:13:14.043986
Model ind 665 epoch 1346 batch: 500 avg loss -2.837091 avg loss no lamb -2.837091 time 2020-06-27 07:13:24.953740
Model ind 665 epoch 1346 batch: 600 avg loss -2.903033 avg loss no lamb -2.903033 time 2020-06-27 07:13:35.877046
Model ind 665 epoch 1346 batch: 700 avg loss -2.792630 avg loss no lamb -2.792630 time 2020-06-27 07:13:46.612042
Model ind 665 epoch 1346 batch: 800 avg loss -2.879634 avg loss no lamb -2.879634 time 2020-06-27 07:13:57.647506
last batch sz 10
Pre: time 2020-06-27 07:14:11.847813: 
 	std: 0.0030019984
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9785, 0.9723, 0.9788, 0.9732]
	train_accs: [0.98031664, 0.9798, 0.97433335, 0.98036665, 0.9752167]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.9764
	best: 0.9788

Starting e_i: 1347
Model ind 665 epoch 1347 batch: 0 avg loss -2.929434 avg loss no lamb -2.929434 time 2020-06-27 07:14:13.191641
Model ind 665 epoch 1347 batch: 100 avg loss -2.935871 avg loss no lamb -2.935871 time 2020-06-27 07:14:24.210549
Model ind 665 epoch 1347 batch: 200 avg loss -2.843302 avg loss no lamb -2.843302 time 2020-06-27 07:14:35.106363
Model ind 665 epoch 1347 batch: 300 avg loss -2.903390 avg loss no lamb -2.903390 time 2020-06-27 07:14:45.942413
Model ind 665 epoch 1347 batch: 400 avg loss -2.784763 avg loss no lamb -2.784763 time 2020-06-27 07:14:56.892301
Model ind 665 epoch 1347 batch: 500 avg loss -2.855990 avg loss no lamb -2.855990 time 2020-06-27 07:15:07.598440
Model ind 665 epoch 1347 batch: 600 avg loss -2.872060 avg loss no lamb -2.872060 time 2020-06-27 07:15:18.512002
Model ind 665 epoch 1347 batch: 700 avg loss -2.713552 avg loss no lamb -2.713552 time 2020-06-27 07:15:29.441558
Model ind 665 epoch 1347 batch: 800 avg loss -2.878389 avg loss no lamb -2.878389 time 2020-06-27 07:15:40.399136
last batch sz 10
Pre: time 2020-06-27 07:15:54.447768: 
 	std: 0.0037445459
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9789, 0.9723, 0.9806, 0.9727]
	train_accs: [0.9812833, 0.9799833, 0.9741333, 0.9812833, 0.97495]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.9770201
	best: 0.9806

Starting e_i: 1348
Model ind 665 epoch 1348 batch: 0 avg loss -2.929020 avg loss no lamb -2.929020 time 2020-06-27 07:15:55.719843
Model ind 665 epoch 1348 batch: 100 avg loss -2.954530 avg loss no lamb -2.954530 time 2020-06-27 07:16:06.677871
Model ind 665 epoch 1348 batch: 200 avg loss -2.896742 avg loss no lamb -2.896742 time 2020-06-27 07:16:17.751681
Model ind 665 epoch 1348 batch: 300 avg loss -2.833054 avg loss no lamb -2.833054 time 2020-06-27 07:16:28.535126
Model ind 665 epoch 1348 batch: 400 avg loss -2.827578 avg loss no lamb -2.827578 time 2020-06-27 07:16:39.466876
Model ind 665 epoch 1348 batch: 500 avg loss -2.913562 avg loss no lamb -2.913562 time 2020-06-27 07:16:50.358757
Model ind 665 epoch 1348 batch: 600 avg loss -2.860062 avg loss no lamb -2.860062 time 2020-06-27 07:17:01.161768
Model ind 665 epoch 1348 batch: 700 avg loss -2.764637 avg loss no lamb -2.764637 time 2020-06-27 07:17:12.275970
Model ind 665 epoch 1348 batch: 800 avg loss -2.890812 avg loss no lamb -2.890812 time 2020-06-27 07:17:23.187226
last batch sz 10
Pre: time 2020-06-27 07:17:37.232242: 
 	std: 0.0025492073
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9789, 0.9736, 0.9799, 0.9748]
	train_accs: [0.9804, 0.9799333, 0.9751667, 0.98083335, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97726
	best: 0.9799

Starting e_i: 1349
Model ind 665 epoch 1349 batch: 0 avg loss -2.968634 avg loss no lamb -2.968634 time 2020-06-27 07:17:38.448951
Model ind 665 epoch 1349 batch: 100 avg loss -2.900325 avg loss no lamb -2.900325 time 2020-06-27 07:17:49.385965
Model ind 665 epoch 1349 batch: 200 avg loss -2.857428 avg loss no lamb -2.857428 time 2020-06-27 07:18:00.164293
Model ind 665 epoch 1349 batch: 300 avg loss -2.878963 avg loss no lamb -2.878963 time 2020-06-27 07:18:11.057831
Model ind 665 epoch 1349 batch: 400 avg loss -2.788346 avg loss no lamb -2.788346 time 2020-06-27 07:18:22.100690
Model ind 665 epoch 1349 batch: 500 avg loss -2.861686 avg loss no lamb -2.861686 time 2020-06-27 07:18:32.999252
Model ind 665 epoch 1349 batch: 600 avg loss -2.908026 avg loss no lamb -2.908026 time 2020-06-27 07:18:43.924443
Model ind 665 epoch 1349 batch: 700 avg loss -2.787339 avg loss no lamb -2.787339 time 2020-06-27 07:18:54.954363
Model ind 665 epoch 1349 batch: 800 avg loss -2.896837 avg loss no lamb -2.896837 time 2020-06-27 07:19:05.984480
last batch sz 10
Pre: time 2020-06-27 07:19:20.324643: 
 	std: 0.0029294346
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9796, 0.9737, 0.9796, 0.9739]
	train_accs: [0.98118335, 0.98065, 0.97568333, 0.98108333, 0.9759333]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97738
	best: 0.9801

Starting e_i: 1350
Model ind 665 epoch 1350 batch: 0 avg loss -2.996846 avg loss no lamb -2.996846 time 2020-06-27 07:19:21.546665
Model ind 665 epoch 1350 batch: 100 avg loss -2.922765 avg loss no lamb -2.922765 time 2020-06-27 07:19:32.247009
Model ind 665 epoch 1350 batch: 200 avg loss -2.869535 avg loss no lamb -2.869535 time 2020-06-27 07:19:43.147527
Model ind 665 epoch 1350 batch: 300 avg loss -2.906417 avg loss no lamb -2.906417 time 2020-06-27 07:19:53.926990
Model ind 665 epoch 1350 batch: 400 avg loss -2.804605 avg loss no lamb -2.804605 time 2020-06-27 07:20:04.744312
Model ind 665 epoch 1350 batch: 500 avg loss -2.902031 avg loss no lamb -2.902031 time 2020-06-27 07:20:15.500073
Model ind 665 epoch 1350 batch: 600 avg loss -2.865817 avg loss no lamb -2.865817 time 2020-06-27 07:20:26.187249
Model ind 665 epoch 1350 batch: 700 avg loss -2.747133 avg loss no lamb -2.747133 time 2020-06-27 07:20:37.212619
Model ind 665 epoch 1350 batch: 800 avg loss -2.905587 avg loss no lamb -2.905587 time 2020-06-27 07:20:48.346223
last batch sz 10
Pre: time 2020-06-27 07:21:02.524670: 
 	std: 0.0029044885
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9796, 0.9733, 0.9795, 0.9738]
	train_accs: [0.9810167, 0.9803333, 0.9751667, 0.9811, 0.97538334]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9771
	best: 0.9795

Starting e_i: 1351
Model ind 665 epoch 1351 batch: 0 avg loss -2.989260 avg loss no lamb -2.989260 time 2020-06-27 07:21:04.900473
Model ind 665 epoch 1351 batch: 100 avg loss -2.869805 avg loss no lamb -2.869805 time 2020-06-27 07:21:15.795755
Model ind 665 epoch 1351 batch: 200 avg loss -2.861781 avg loss no lamb -2.861781 time 2020-06-27 07:21:26.525763
Model ind 665 epoch 1351 batch: 300 avg loss -2.887129 avg loss no lamb -2.887129 time 2020-06-27 07:21:37.503442
Model ind 665 epoch 1351 batch: 400 avg loss -2.859576 avg loss no lamb -2.859576 time 2020-06-27 07:21:48.334882
Model ind 665 epoch 1351 batch: 500 avg loss -2.791116 avg loss no lamb -2.791116 time 2020-06-27 07:21:59.036661
Model ind 665 epoch 1351 batch: 600 avg loss -2.912450 avg loss no lamb -2.912450 time 2020-06-27 07:22:09.886578
Model ind 665 epoch 1351 batch: 700 avg loss -2.820164 avg loss no lamb -2.820164 time 2020-06-27 07:22:20.708944
Model ind 665 epoch 1351 batch: 800 avg loss -2.882715 avg loss no lamb -2.882715 time 2020-06-27 07:22:31.439077
last batch sz 10
Pre: time 2020-06-27 07:22:45.619389: 
 	std: 0.0025755847
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9785, 0.9735, 0.9797, 0.9747]
	train_accs: [0.9811, 0.98013335, 0.97498333, 0.9810333, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97718
	best: 0.9795

Starting e_i: 1352
Model ind 665 epoch 1352 batch: 0 avg loss -2.941656 avg loss no lamb -2.941656 time 2020-06-27 07:22:46.844171
Model ind 665 epoch 1352 batch: 100 avg loss -2.882171 avg loss no lamb -2.882171 time 2020-06-27 07:22:57.457396
Model ind 665 epoch 1352 batch: 200 avg loss -2.860361 avg loss no lamb -2.860361 time 2020-06-27 07:23:08.373359
Model ind 665 epoch 1352 batch: 300 avg loss -2.864855 avg loss no lamb -2.864855 time 2020-06-27 07:23:19.199082
Model ind 665 epoch 1352 batch: 400 avg loss -2.821571 avg loss no lamb -2.821571 time 2020-06-27 07:23:30.160238
Model ind 665 epoch 1352 batch: 500 avg loss -2.869472 avg loss no lamb -2.869472 time 2020-06-27 07:23:41.050476
Model ind 665 epoch 1352 batch: 600 avg loss -2.879390 avg loss no lamb -2.879390 time 2020-06-27 07:23:51.887682
Model ind 665 epoch 1352 batch: 700 avg loss -2.707359 avg loss no lamb -2.707359 time 2020-06-27 07:24:02.723430
Model ind 665 epoch 1352 batch: 800 avg loss -2.813457 avg loss no lamb -2.813457 time 2020-06-27 07:24:13.361533
last batch sz 10
Pre: time 2020-06-27 07:24:27.437662: 
 	std: 0.0036169696
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9802, 0.9733, 0.9811, 0.973]
	train_accs: [0.98078334, 0.98073334, 0.97541666, 0.9810167, 0.9753]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97756004
	best: 0.9811

Starting e_i: 1353
Model ind 665 epoch 1353 batch: 0 avg loss -2.933099 avg loss no lamb -2.933099 time 2020-06-27 07:24:28.605875
Model ind 665 epoch 1353 batch: 100 avg loss -2.912500 avg loss no lamb -2.912500 time 2020-06-27 07:24:39.411338
Model ind 665 epoch 1353 batch: 200 avg loss -2.881363 avg loss no lamb -2.881363 time 2020-06-27 07:24:50.113420
Model ind 665 epoch 1353 batch: 300 avg loss -2.931264 avg loss no lamb -2.931264 time 2020-06-27 07:25:00.938182
Model ind 665 epoch 1353 batch: 400 avg loss -2.827106 avg loss no lamb -2.827106 time 2020-06-27 07:25:11.905051
Model ind 665 epoch 1353 batch: 500 avg loss -2.815486 avg loss no lamb -2.815486 time 2020-06-27 07:25:22.575539
Model ind 665 epoch 1353 batch: 600 avg loss -2.941913 avg loss no lamb -2.941913 time 2020-06-27 07:25:33.524397
Model ind 665 epoch 1353 batch: 700 avg loss -2.839181 avg loss no lamb -2.839181 time 2020-06-27 07:25:44.560922
Model ind 665 epoch 1353 batch: 800 avg loss -2.858099 avg loss no lamb -2.858099 time 2020-06-27 07:25:55.509092
last batch sz 10
Pre: time 2020-06-27 07:26:10.010442: 
 	std: 0.0031575842
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9801, 0.9741, 0.9808, 0.9745]
	train_accs: [0.9813833, 0.9802667, 0.97515, 0.98123336, 0.9762]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97814
	best: 0.9812

Starting e_i: 1354
Model ind 665 epoch 1354 batch: 0 avg loss -2.981208 avg loss no lamb -2.981208 time 2020-06-27 07:26:11.258405
Model ind 665 epoch 1354 batch: 100 avg loss -2.953443 avg loss no lamb -2.953443 time 2020-06-27 07:26:21.978240
Model ind 665 epoch 1354 batch: 200 avg loss -2.839650 avg loss no lamb -2.839650 time 2020-06-27 07:26:32.677333
Model ind 665 epoch 1354 batch: 300 avg loss -2.875476 avg loss no lamb -2.875476 time 2020-06-27 07:26:43.669078
Model ind 665 epoch 1354 batch: 400 avg loss -2.799987 avg loss no lamb -2.799987 time 2020-06-27 07:26:54.469712
Model ind 665 epoch 1354 batch: 500 avg loss -2.907998 avg loss no lamb -2.907998 time 2020-06-27 07:27:05.388241
Model ind 665 epoch 1354 batch: 600 avg loss -2.894393 avg loss no lamb -2.894393 time 2020-06-27 07:27:16.160062
Model ind 665 epoch 1354 batch: 700 avg loss -2.810308 avg loss no lamb -2.810308 time 2020-06-27 07:27:27.001188
Model ind 665 epoch 1354 batch: 800 avg loss -2.874303 avg loss no lamb -2.874303 time 2020-06-27 07:27:38.029455
last batch sz 10
Pre: time 2020-06-27 07:27:51.993737: 
 	std: 0.0028153907
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9801, 0.974, 0.98, 0.9746]
	train_accs: [0.9807, 0.9805833, 0.9755, 0.9805, 0.9758833]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97774
	best: 0.98

Starting e_i: 1355
Model ind 665 epoch 1355 batch: 0 avg loss -2.972807 avg loss no lamb -2.972807 time 2020-06-27 07:27:53.209671
Model ind 665 epoch 1355 batch: 100 avg loss -2.958790 avg loss no lamb -2.958790 time 2020-06-27 07:28:04.092499
Model ind 665 epoch 1355 batch: 200 avg loss -2.875462 avg loss no lamb -2.875462 time 2020-06-27 07:28:14.926440
Model ind 665 epoch 1355 batch: 300 avg loss -2.852525 avg loss no lamb -2.852525 time 2020-06-27 07:28:25.779392
Model ind 665 epoch 1355 batch: 400 avg loss -2.772782 avg loss no lamb -2.772782 time 2020-06-27 07:28:36.682142
Model ind 665 epoch 1355 batch: 500 avg loss -2.841352 avg loss no lamb -2.841352 time 2020-06-27 07:28:47.600188
Model ind 665 epoch 1355 batch: 600 avg loss -2.860303 avg loss no lamb -2.860303 time 2020-06-27 07:28:58.488827
Model ind 665 epoch 1355 batch: 700 avg loss -2.740566 avg loss no lamb -2.740566 time 2020-06-27 07:29:09.410309
Model ind 665 epoch 1355 batch: 800 avg loss -2.873920 avg loss no lamb -2.873920 time 2020-06-27 07:29:20.009536
last batch sz 10
Pre: time 2020-06-27 07:29:34.233900: 
 	std: 0.0025662598
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9796, 0.974, 0.9803, 0.9754]
	train_accs: [0.9805, 0.98013335, 0.97501665, 0.98078334, 0.97573334]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97778
	best: 0.9803

Starting e_i: 1356
Model ind 665 epoch 1356 batch: 0 avg loss -2.937001 avg loss no lamb -2.937001 time 2020-06-27 07:29:35.409395
Model ind 665 epoch 1356 batch: 100 avg loss -2.943669 avg loss no lamb -2.943669 time 2020-06-27 07:29:46.187086
Model ind 665 epoch 1356 batch: 200 avg loss -2.933452 avg loss no lamb -2.933452 time 2020-06-27 07:29:56.985985
Model ind 665 epoch 1356 batch: 300 avg loss -2.836497 avg loss no lamb -2.836497 time 2020-06-27 07:30:07.884963
Model ind 665 epoch 1356 batch: 400 avg loss -2.825991 avg loss no lamb -2.825991 time 2020-06-27 07:30:18.578423
Model ind 665 epoch 1356 batch: 500 avg loss -2.835849 avg loss no lamb -2.835849 time 2020-06-27 07:30:29.701511
Model ind 665 epoch 1356 batch: 600 avg loss -2.899790 avg loss no lamb -2.899790 time 2020-06-27 07:30:40.483468
Model ind 665 epoch 1356 batch: 700 avg loss -2.798585 avg loss no lamb -2.798585 time 2020-06-27 07:30:51.085666
Model ind 665 epoch 1356 batch: 800 avg loss -2.921509 avg loss no lamb -2.921509 time 2020-06-27 07:31:01.796103
last batch sz 10
Pre: time 2020-06-27 07:31:16.001283: 
 	std: 0.0024727487
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9789, 0.974, 0.9797, 0.9752]
	train_accs: [0.98088336, 0.98025, 0.97515, 0.98116666, 0.97636664]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97756004
	best: 0.9797

Starting e_i: 1357
Model ind 665 epoch 1357 batch: 0 avg loss -2.970256 avg loss no lamb -2.970256 time 2020-06-27 07:31:17.204493
Model ind 665 epoch 1357 batch: 100 avg loss -2.913641 avg loss no lamb -2.913641 time 2020-06-27 07:31:27.989708
Model ind 665 epoch 1357 batch: 200 avg loss -2.907698 avg loss no lamb -2.907698 time 2020-06-27 07:31:38.827117
Model ind 665 epoch 1357 batch: 300 avg loss -2.852846 avg loss no lamb -2.852846 time 2020-06-27 07:31:49.634540
Model ind 665 epoch 1357 batch: 400 avg loss -2.764545 avg loss no lamb -2.764545 time 2020-06-27 07:32:00.537383
Model ind 665 epoch 1357 batch: 500 avg loss -2.880568 avg loss no lamb -2.880568 time 2020-06-27 07:32:11.352641
Model ind 665 epoch 1357 batch: 600 avg loss -2.837431 avg loss no lamb -2.837431 time 2020-06-27 07:32:22.151783
Model ind 665 epoch 1357 batch: 700 avg loss -2.878004 avg loss no lamb -2.878004 time 2020-06-27 07:32:33.024937
Model ind 665 epoch 1357 batch: 800 avg loss -2.943308 avg loss no lamb -2.943308 time 2020-06-27 07:32:43.941022
last batch sz 10
Pre: time 2020-06-27 07:32:58.005197: 
 	std: 0.0031106411
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9803, 0.9736, 0.9798, 0.974]
	train_accs: [0.9809167, 0.98025, 0.9747, 0.98073334, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.9776
	best: 0.9803

Starting e_i: 1358
Model ind 665 epoch 1358 batch: 0 avg loss -2.971718 avg loss no lamb -2.971718 time 2020-06-27 07:32:59.198924
Model ind 665 epoch 1358 batch: 100 avg loss -2.911837 avg loss no lamb -2.911837 time 2020-06-27 07:33:10.056058
Model ind 665 epoch 1358 batch: 200 avg loss -2.879065 avg loss no lamb -2.879065 time 2020-06-27 07:33:21.051088
Model ind 665 epoch 1358 batch: 300 avg loss -2.870918 avg loss no lamb -2.870918 time 2020-06-27 07:33:31.674988
Model ind 665 epoch 1358 batch: 400 avg loss -2.722822 avg loss no lamb -2.722822 time 2020-06-27 07:33:42.305963
Model ind 665 epoch 1358 batch: 500 avg loss -2.873188 avg loss no lamb -2.873188 time 2020-06-27 07:33:52.981867
Model ind 665 epoch 1358 batch: 600 avg loss -2.905464 avg loss no lamb -2.905464 time 2020-06-27 07:34:03.853735
Model ind 665 epoch 1358 batch: 700 avg loss -2.828107 avg loss no lamb -2.828107 time 2020-06-27 07:34:14.650021
Model ind 665 epoch 1358 batch: 800 avg loss -2.899207 avg loss no lamb -2.899207 time 2020-06-27 07:34:25.292255
last batch sz 10
Pre: time 2020-06-27 07:34:39.120848: 
 	std: 0.002755644
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9804, 0.975, 0.9811, 0.976]
	train_accs: [0.9812833, 0.98073334, 0.97576666, 0.9811, 0.97643334]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97882
	best: 0.9816

Starting e_i: 1359
Model ind 665 epoch 1359 batch: 0 avg loss -2.951512 avg loss no lamb -2.951512 time 2020-06-27 07:34:40.288478
Model ind 665 epoch 1359 batch: 100 avg loss -2.880733 avg loss no lamb -2.880733 time 2020-06-27 07:34:51.008472
Model ind 665 epoch 1359 batch: 200 avg loss -2.889477 avg loss no lamb -2.889477 time 2020-06-27 07:35:01.968251
Model ind 665 epoch 1359 batch: 300 avg loss -2.917860 avg loss no lamb -2.917860 time 2020-06-27 07:35:12.916887
Model ind 665 epoch 1359 batch: 400 avg loss -2.827531 avg loss no lamb -2.827531 time 2020-06-27 07:35:23.664729
Model ind 665 epoch 1359 batch: 500 avg loss -2.799124 avg loss no lamb -2.799124 time 2020-06-27 07:35:34.566195
Model ind 665 epoch 1359 batch: 600 avg loss -2.927312 avg loss no lamb -2.927312 time 2020-06-27 07:35:45.458253
Model ind 665 epoch 1359 batch: 700 avg loss -2.802653 avg loss no lamb -2.802653 time 2020-06-27 07:35:56.368090
Model ind 665 epoch 1359 batch: 800 avg loss -2.830745 avg loss no lamb -2.830745 time 2020-06-27 07:36:07.142011
last batch sz 10
Pre: time 2020-06-27 07:36:21.515800: 
 	std: 0.0033041858
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9789, 0.9723, 0.9797, 0.9728]
	train_accs: [0.98038334, 0.98013335, 0.97345, 0.98053336, 0.9740833]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97658
	best: 0.9797

Starting e_i: 1360
Model ind 665 epoch 1360 batch: 0 avg loss -2.951860 avg loss no lamb -2.951860 time 2020-06-27 07:36:22.694634
Model ind 665 epoch 1360 batch: 100 avg loss -2.874655 avg loss no lamb -2.874655 time 2020-06-27 07:36:33.564346
Model ind 665 epoch 1360 batch: 200 avg loss -2.852595 avg loss no lamb -2.852595 time 2020-06-27 07:36:44.352058
Model ind 665 epoch 1360 batch: 300 avg loss -2.890143 avg loss no lamb -2.890143 time 2020-06-27 07:36:55.168569
Model ind 665 epoch 1360 batch: 400 avg loss -2.812612 avg loss no lamb -2.812612 time 2020-06-27 07:37:06.023908
Model ind 665 epoch 1360 batch: 500 avg loss -2.887181 avg loss no lamb -2.887181 time 2020-06-27 07:37:16.978184
Model ind 665 epoch 1360 batch: 600 avg loss -2.908777 avg loss no lamb -2.908777 time 2020-06-27 07:37:27.791417
Model ind 665 epoch 1360 batch: 700 avg loss -2.794027 avg loss no lamb -2.794027 time 2020-06-27 07:37:38.362417
Model ind 665 epoch 1360 batch: 800 avg loss -2.880215 avg loss no lamb -2.880215 time 2020-06-27 07:37:49.255080
last batch sz 10
Pre: time 2020-06-27 07:38:03.230303: 
 	std: 0.0030682907
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9807, 0.9743, 0.9799, 0.9741]
	train_accs: [0.98143333, 0.98081666, 0.97501665, 0.98111665, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97793996
	best: 0.9807

Starting e_i: 1361
Model ind 665 epoch 1361 batch: 0 avg loss -2.960715 avg loss no lamb -2.960715 time 2020-06-27 07:38:05.648714
Model ind 665 epoch 1361 batch: 100 avg loss -2.899785 avg loss no lamb -2.899785 time 2020-06-27 07:38:16.638203
Model ind 665 epoch 1361 batch: 200 avg loss -2.821820 avg loss no lamb -2.821820 time 2020-06-27 07:38:27.182622
Model ind 665 epoch 1361 batch: 300 avg loss -2.861377 avg loss no lamb -2.861377 time 2020-06-27 07:38:37.843844
Model ind 665 epoch 1361 batch: 400 avg loss -2.804679 avg loss no lamb -2.804679 time 2020-06-27 07:38:48.791101
Model ind 665 epoch 1361 batch: 500 avg loss -2.853351 avg loss no lamb -2.853351 time 2020-06-27 07:38:59.804797
Model ind 665 epoch 1361 batch: 600 avg loss -2.905433 avg loss no lamb -2.905433 time 2020-06-27 07:39:10.788652
Model ind 665 epoch 1361 batch: 700 avg loss -2.793892 avg loss no lamb -2.793892 time 2020-06-27 07:39:21.590533
Model ind 665 epoch 1361 batch: 800 avg loss -2.799974 avg loss no lamb -2.799974 time 2020-06-27 07:39:32.404817
last batch sz 10
Pre: time 2020-06-27 07:39:46.549784: 
 	std: 0.0027852466
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9794, 0.9741, 0.9803, 0.9748]
	train_accs: [0.9809333, 0.9803333, 0.9752167, 0.9809333, 0.97618335]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97782004
	best: 0.9805

Starting e_i: 1362
Model ind 665 epoch 1362 batch: 0 avg loss -2.940879 avg loss no lamb -2.940879 time 2020-06-27 07:39:47.849532
Model ind 665 epoch 1362 batch: 100 avg loss -2.868554 avg loss no lamb -2.868554 time 2020-06-27 07:39:58.567841
Model ind 665 epoch 1362 batch: 200 avg loss -2.890387 avg loss no lamb -2.890387 time 2020-06-27 07:40:09.389740
Model ind 665 epoch 1362 batch: 300 avg loss -2.804929 avg loss no lamb -2.804929 time 2020-06-27 07:40:20.174825
Model ind 665 epoch 1362 batch: 400 avg loss -2.810471 avg loss no lamb -2.810471 time 2020-06-27 07:40:31.087316
Model ind 665 epoch 1362 batch: 500 avg loss -2.861018 avg loss no lamb -2.861018 time 2020-06-27 07:40:41.906396
Model ind 665 epoch 1362 batch: 600 avg loss -2.829913 avg loss no lamb -2.829913 time 2020-06-27 07:40:52.589369
Model ind 665 epoch 1362 batch: 700 avg loss -2.812118 avg loss no lamb -2.812118 time 2020-06-27 07:41:03.424665
Model ind 665 epoch 1362 batch: 800 avg loss -2.890926 avg loss no lamb -2.890926 time 2020-06-27 07:41:14.108826
last batch sz 10
Pre: time 2020-06-27 07:41:28.399394: 
 	std: 0.0031845907
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9795, 0.9731, 0.9803, 0.974]
	train_accs: [0.98121667, 0.9803, 0.9745333, 0.98118335, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97742003
	best: 0.9802

Starting e_i: 1363
Model ind 665 epoch 1363 batch: 0 avg loss -2.927060 avg loss no lamb -2.927060 time 2020-06-27 07:41:29.598531
Model ind 665 epoch 1363 batch: 100 avg loss -2.926860 avg loss no lamb -2.926860 time 2020-06-27 07:41:40.718520
Model ind 665 epoch 1363 batch: 200 avg loss -2.877891 avg loss no lamb -2.877891 time 2020-06-27 07:41:51.552405
Model ind 665 epoch 1363 batch: 300 avg loss -2.822218 avg loss no lamb -2.822218 time 2020-06-27 07:42:02.177648
Model ind 665 epoch 1363 batch: 400 avg loss -2.778749 avg loss no lamb -2.778749 time 2020-06-27 07:42:12.864685
Model ind 665 epoch 1363 batch: 500 avg loss -2.887311 avg loss no lamb -2.887311 time 2020-06-27 07:42:23.796340
Model ind 665 epoch 1363 batch: 600 avg loss -2.919229 avg loss no lamb -2.919229 time 2020-06-27 07:42:34.466060
Model ind 665 epoch 1363 batch: 700 avg loss -2.803293 avg loss no lamb -2.803293 time 2020-06-27 07:42:45.369762
Model ind 665 epoch 1363 batch: 800 avg loss -2.857593 avg loss no lamb -2.857593 time 2020-06-27 07:42:55.885530
last batch sz 10
Pre: time 2020-06-27 07:43:09.949701: 
 	std: 0.0031682304
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.98, 0.9741, 0.9805, 0.9736]
	train_accs: [0.9813333, 0.98076665, 0.97505, 0.9816, 0.9758]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.9777201
	best: 0.9805

Starting e_i: 1364
Model ind 665 epoch 1364 batch: 0 avg loss -2.995368 avg loss no lamb -2.995368 time 2020-06-27 07:43:11.300806
Model ind 665 epoch 1364 batch: 100 avg loss -2.890177 avg loss no lamb -2.890177 time 2020-06-27 07:43:22.249688
Model ind 665 epoch 1364 batch: 200 avg loss -2.850237 avg loss no lamb -2.850237 time 2020-06-27 07:43:33.068998
Model ind 665 epoch 1364 batch: 300 avg loss -2.914105 avg loss no lamb -2.914105 time 2020-06-27 07:43:43.885685
Model ind 665 epoch 1364 batch: 400 avg loss -2.779299 avg loss no lamb -2.779299 time 2020-06-27 07:43:54.882168
Model ind 665 epoch 1364 batch: 500 avg loss -2.829934 avg loss no lamb -2.829934 time 2020-06-27 07:44:05.727420
Model ind 665 epoch 1364 batch: 600 avg loss -2.905321 avg loss no lamb -2.905321 time 2020-06-27 07:44:16.650737
Model ind 665 epoch 1364 batch: 700 avg loss -2.794400 avg loss no lamb -2.794400 time 2020-06-27 07:44:27.435371
Model ind 665 epoch 1364 batch: 800 avg loss -2.874978 avg loss no lamb -2.874978 time 2020-06-27 07:44:38.230513
last batch sz 10
Pre: time 2020-06-27 07:44:52.465551: 
 	std: 0.0023574533
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9801, 0.9757, 0.9804, 0.9753]
	train_accs: [0.98055, 0.9803, 0.9759667, 0.98073334, 0.97603333]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97838
	best: 0.9804

Starting e_i: 1365
Model ind 665 epoch 1365 batch: 0 avg loss -2.938931 avg loss no lamb -2.938931 time 2020-06-27 07:44:53.666777
Model ind 665 epoch 1365 batch: 100 avg loss -2.868212 avg loss no lamb -2.868212 time 2020-06-27 07:45:04.503805
Model ind 665 epoch 1365 batch: 200 avg loss -2.913371 avg loss no lamb -2.913371 time 2020-06-27 07:45:15.195194
Model ind 665 epoch 1365 batch: 300 avg loss -2.838819 avg loss no lamb -2.838819 time 2020-06-27 07:45:25.903229
Model ind 665 epoch 1365 batch: 400 avg loss -2.798002 avg loss no lamb -2.798002 time 2020-06-27 07:45:36.654563
Model ind 665 epoch 1365 batch: 500 avg loss -2.864228 avg loss no lamb -2.864228 time 2020-06-27 07:45:47.540967
Model ind 665 epoch 1365 batch: 600 avg loss -2.870617 avg loss no lamb -2.870617 time 2020-06-27 07:45:58.485422
Model ind 665 epoch 1365 batch: 700 avg loss -2.727279 avg loss no lamb -2.727279 time 2020-06-27 07:46:09.317597
Model ind 665 epoch 1365 batch: 800 avg loss -2.847373 avg loss no lamb -2.847373 time 2020-06-27 07:46:20.101719
last batch sz 10
Pre: time 2020-06-27 07:46:34.266705: 
 	std: 0.0038974786
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9792, 0.9716, 0.9801, 0.9718]
	train_accs: [0.98053336, 0.97928333, 0.9732, 0.98041666, 0.97355]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.97646
	best: 0.9796

Starting e_i: 1366
Model ind 665 epoch 1366 batch: 0 avg loss -2.952502 avg loss no lamb -2.952502 time 2020-06-27 07:46:35.728090
Model ind 665 epoch 1366 batch: 100 avg loss -2.930633 avg loss no lamb -2.930633 time 2020-06-27 07:46:46.555211
Model ind 665 epoch 1366 batch: 200 avg loss -2.854203 avg loss no lamb -2.854203 time 2020-06-27 07:46:57.398299
Model ind 665 epoch 1366 batch: 300 avg loss -2.943979 avg loss no lamb -2.943979 time 2020-06-27 07:47:08.273313
Model ind 665 epoch 1366 batch: 400 avg loss -2.745958 avg loss no lamb -2.745958 time 2020-06-27 07:47:19.044084
Model ind 665 epoch 1366 batch: 500 avg loss -2.873283 avg loss no lamb -2.873283 time 2020-06-27 07:47:29.977205
Model ind 665 epoch 1366 batch: 600 avg loss -2.861335 avg loss no lamb -2.861335 time 2020-06-27 07:47:40.847563
Model ind 665 epoch 1366 batch: 700 avg loss -2.702018 avg loss no lamb -2.702018 time 2020-06-27 07:47:51.749293
Model ind 665 epoch 1366 batch: 800 avg loss -2.825724 avg loss no lamb -2.825724 time 2020-06-27 07:48:02.578306
last batch sz 10
Pre: time 2020-06-27 07:48:16.980612: 
 	std: 0.003074027
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9823, 0.9819, 0.9751, 0.9818, 0.9765]
	train_accs: [0.98185, 0.98145, 0.97576666, 0.9819667, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97952
	best: 0.9818

Starting e_i: 1367
Model ind 665 epoch 1367 batch: 0 avg loss -2.920671 avg loss no lamb -2.920671 time 2020-06-27 07:48:18.183392
Model ind 665 epoch 1367 batch: 100 avg loss -2.945813 avg loss no lamb -2.945813 time 2020-06-27 07:48:29.055110
Model ind 665 epoch 1367 batch: 200 avg loss -2.852846 avg loss no lamb -2.852846 time 2020-06-27 07:48:39.942744
Model ind 665 epoch 1367 batch: 300 avg loss -2.829061 avg loss no lamb -2.829061 time 2020-06-27 07:48:50.793554
Model ind 665 epoch 1367 batch: 400 avg loss -2.808389 avg loss no lamb -2.808389 time 2020-06-27 07:49:01.738880
Model ind 665 epoch 1367 batch: 500 avg loss -2.794929 avg loss no lamb -2.794929 time 2020-06-27 07:49:12.725374
Model ind 665 epoch 1367 batch: 600 avg loss -2.932663 avg loss no lamb -2.932663 time 2020-06-27 07:49:23.514055
Model ind 665 epoch 1367 batch: 700 avg loss -2.833085 avg loss no lamb -2.833085 time 2020-06-27 07:49:34.199653
Model ind 665 epoch 1367 batch: 800 avg loss -2.829166 avg loss no lamb -2.829166 time 2020-06-27 07:49:45.238217
last batch sz 10
Pre: time 2020-06-27 07:49:59.088537: 
 	std: 0.0025223661
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9803, 0.9753, 0.9812, 0.9768]
	train_accs: [0.98148334, 0.98046666, 0.97565, 0.9813833, 0.97715]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97904
	best: 0.9816

Starting e_i: 1368
Model ind 665 epoch 1368 batch: 0 avg loss -2.974454 avg loss no lamb -2.974454 time 2020-06-27 07:50:00.546553
Model ind 665 epoch 1368 batch: 100 avg loss -2.902649 avg loss no lamb -2.902649 time 2020-06-27 07:50:11.469608
Model ind 665 epoch 1368 batch: 200 avg loss -2.863693 avg loss no lamb -2.863693 time 2020-06-27 07:50:22.528880
Model ind 665 epoch 1368 batch: 300 avg loss -2.835945 avg loss no lamb -2.835945 time 2020-06-27 07:50:33.403992
Model ind 665 epoch 1368 batch: 400 avg loss -2.795680 avg loss no lamb -2.795680 time 2020-06-27 07:50:44.098381
Model ind 665 epoch 1368 batch: 500 avg loss -2.823508 avg loss no lamb -2.823508 time 2020-06-27 07:50:54.755403
Model ind 665 epoch 1368 batch: 600 avg loss -2.917371 avg loss no lamb -2.917371 time 2020-06-27 07:51:05.791571
Model ind 665 epoch 1368 batch: 700 avg loss -2.738564 avg loss no lamb -2.738564 time 2020-06-27 07:51:16.779981
Model ind 665 epoch 1368 batch: 800 avg loss -2.839171 avg loss no lamb -2.839171 time 2020-06-27 07:51:27.547103
last batch sz 10
Pre: time 2020-06-27 07:51:41.449294: 
 	std: 0.0027095426
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9804, 0.9746, 0.9809, 0.9758]
	train_accs: [0.9816333, 0.9813333, 0.97658336, 0.9817167, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97848004
	best: 0.9809

Starting e_i: 1369
Model ind 665 epoch 1369 batch: 0 avg loss -2.949293 avg loss no lamb -2.949293 time 2020-06-27 07:51:42.641772
Model ind 665 epoch 1369 batch: 100 avg loss -2.910349 avg loss no lamb -2.910349 time 2020-06-27 07:51:53.763299
Model ind 665 epoch 1369 batch: 200 avg loss -2.875260 avg loss no lamb -2.875260 time 2020-06-27 07:52:04.692452
Model ind 665 epoch 1369 batch: 300 avg loss -2.869011 avg loss no lamb -2.869011 time 2020-06-27 07:52:15.578046
Model ind 665 epoch 1369 batch: 400 avg loss -2.765954 avg loss no lamb -2.765954 time 2020-06-27 07:52:26.308842
Model ind 665 epoch 1369 batch: 500 avg loss -2.820640 avg loss no lamb -2.820640 time 2020-06-27 07:52:36.825719
Model ind 665 epoch 1369 batch: 600 avg loss -2.823152 avg loss no lamb -2.823152 time 2020-06-27 07:52:47.595975
Model ind 665 epoch 1369 batch: 700 avg loss -2.815565 avg loss no lamb -2.815565 time 2020-06-27 07:52:58.442171
Model ind 665 epoch 1369 batch: 800 avg loss -2.857818 avg loss no lamb -2.857818 time 2020-06-27 07:53:09.334287
last batch sz 10
Pre: time 2020-06-27 07:53:23.498267: 
 	std: 0.0023837727
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9798, 0.9746, 0.9798, 0.9751]
	train_accs: [0.98123336, 0.9809167, 0.97625, 0.98153335, 0.9767]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97776
	best: 0.9798

Starting e_i: 1370
Model ind 665 epoch 1370 batch: 0 avg loss -2.930110 avg loss no lamb -2.930110 time 2020-06-27 07:53:24.868441
Model ind 665 epoch 1370 batch: 100 avg loss -2.934872 avg loss no lamb -2.934872 time 2020-06-27 07:53:35.646696
Model ind 665 epoch 1370 batch: 200 avg loss -2.963610 avg loss no lamb -2.963610 time 2020-06-27 07:53:46.540879
Model ind 665 epoch 1370 batch: 300 avg loss -2.932770 avg loss no lamb -2.932770 time 2020-06-27 07:53:57.487533
Model ind 665 epoch 1370 batch: 400 avg loss -2.797556 avg loss no lamb -2.797556 time 2020-06-27 07:54:08.472036
Model ind 665 epoch 1370 batch: 500 avg loss -2.879178 avg loss no lamb -2.879178 time 2020-06-27 07:54:19.286995
Model ind 665 epoch 1370 batch: 600 avg loss -2.834717 avg loss no lamb -2.834717 time 2020-06-27 07:54:30.186953
Model ind 665 epoch 1370 batch: 700 avg loss -2.725773 avg loss no lamb -2.725773 time 2020-06-27 07:54:40.801027
Model ind 665 epoch 1370 batch: 800 avg loss -2.851006 avg loss no lamb -2.851006 time 2020-06-27 07:54:51.708676
last batch sz 10
Pre: time 2020-06-27 07:55:05.758425: 
 	std: 0.0028701194
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9793, 0.9735, 0.9802, 0.9746]
	train_accs: [0.98115, 0.98075, 0.97548336, 0.9812833, 0.9762333]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97752
	best: 0.9802

Starting e_i: 1371
Model ind 665 epoch 1371 batch: 0 avg loss -2.938210 avg loss no lamb -2.938210 time 2020-06-27 07:55:08.199291
Model ind 665 epoch 1371 batch: 100 avg loss -2.931810 avg loss no lamb -2.931810 time 2020-06-27 07:55:18.975284
Model ind 665 epoch 1371 batch: 200 avg loss -2.891219 avg loss no lamb -2.891219 time 2020-06-27 07:55:29.763620
Model ind 665 epoch 1371 batch: 300 avg loss -2.873365 avg loss no lamb -2.873365 time 2020-06-27 07:55:40.538005
Model ind 665 epoch 1371 batch: 400 avg loss -2.806350 avg loss no lamb -2.806350 time 2020-06-27 07:55:51.452737
Model ind 665 epoch 1371 batch: 500 avg loss -2.845028 avg loss no lamb -2.845028 time 2020-06-27 07:56:02.288750
Model ind 665 epoch 1371 batch: 600 avg loss -2.891867 avg loss no lamb -2.891867 time 2020-06-27 07:56:13.047291
Model ind 665 epoch 1371 batch: 700 avg loss -2.743513 avg loss no lamb -2.743513 time 2020-06-27 07:56:23.871617
Model ind 665 epoch 1371 batch: 800 avg loss -2.909285 avg loss no lamb -2.909285 time 2020-06-27 07:56:34.536579
last batch sz 10
Pre: time 2020-06-27 07:56:48.694888: 
 	std: 0.0026069111
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9802, 0.9745, 0.9805, 0.9758]
	train_accs: [0.9811, 0.9805167, 0.9755667, 0.98106664, 0.97658336]
	best_train_sub_head: 0
	worst: 0.9745
	avg: 0.9783
	best: 0.9805

Starting e_i: 1372
Model ind 665 epoch 1372 batch: 0 avg loss -2.965920 avg loss no lamb -2.965920 time 2020-06-27 07:56:50.134183
Model ind 665 epoch 1372 batch: 100 avg loss -2.934537 avg loss no lamb -2.934537 time 2020-06-27 07:57:00.923742
Model ind 665 epoch 1372 batch: 200 avg loss -2.896716 avg loss no lamb -2.896716 time 2020-06-27 07:57:11.892173
Model ind 665 epoch 1372 batch: 300 avg loss -2.836502 avg loss no lamb -2.836502 time 2020-06-27 07:57:23.048812
Model ind 665 epoch 1372 batch: 400 avg loss -2.808892 avg loss no lamb -2.808892 time 2020-06-27 07:57:33.963800
Model ind 665 epoch 1372 batch: 500 avg loss -2.860343 avg loss no lamb -2.860343 time 2020-06-27 07:57:45.128784
Model ind 665 epoch 1372 batch: 600 avg loss -2.950686 avg loss no lamb -2.950686 time 2020-06-27 07:57:56.075010
Model ind 665 epoch 1372 batch: 700 avg loss -2.724083 avg loss no lamb -2.724083 time 2020-06-27 07:58:07.321314
Model ind 665 epoch 1372 batch: 800 avg loss -2.775354 avg loss no lamb -2.775354 time 2020-06-27 07:58:18.258285
last batch sz 10
Pre: time 2020-06-27 07:58:32.610229: 
 	std: 0.0031959966
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9813, 0.9751, 0.9817, 0.975]
	train_accs: [0.9816833, 0.9809667, 0.9759167, 0.98165, 0.9765]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97896004
	best: 0.9817

Starting e_i: 1373
Model ind 665 epoch 1373 batch: 0 avg loss -2.896375 avg loss no lamb -2.896375 time 2020-06-27 07:58:33.795872
Model ind 665 epoch 1373 batch: 100 avg loss -2.894924 avg loss no lamb -2.894924 time 2020-06-27 07:58:44.602374
Model ind 665 epoch 1373 batch: 200 avg loss -2.857491 avg loss no lamb -2.857491 time 2020-06-27 07:58:55.442906
Model ind 665 epoch 1373 batch: 300 avg loss -2.876082 avg loss no lamb -2.876082 time 2020-06-27 07:59:06.472862
Model ind 665 epoch 1373 batch: 400 avg loss -2.794681 avg loss no lamb -2.794681 time 2020-06-27 07:59:17.549141
Model ind 665 epoch 1373 batch: 500 avg loss -2.894235 avg loss no lamb -2.894235 time 2020-06-27 07:59:28.703823
Model ind 665 epoch 1373 batch: 600 avg loss -2.956540 avg loss no lamb -2.956540 time 2020-06-27 07:59:39.398048
Model ind 665 epoch 1373 batch: 700 avg loss -2.728872 avg loss no lamb -2.728872 time 2020-06-27 07:59:50.168331
Model ind 665 epoch 1373 batch: 800 avg loss -2.862470 avg loss no lamb -2.862470 time 2020-06-27 08:00:00.985260
last batch sz 10
Pre: time 2020-06-27 08:00:15.019007: 
 	std: 0.0024459006
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9797, 0.9745, 0.9803, 0.9755]
	train_accs: [0.98123336, 0.9809167, 0.97625, 0.9813333, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97796
	best: 0.9803

Starting e_i: 1374
Model ind 665 epoch 1374 batch: 0 avg loss -2.928857 avg loss no lamb -2.928857 time 2020-06-27 08:00:16.217603
Model ind 665 epoch 1374 batch: 100 avg loss -2.892267 avg loss no lamb -2.892267 time 2020-06-27 08:00:26.827777
Model ind 665 epoch 1374 batch: 200 avg loss -2.906468 avg loss no lamb -2.906468 time 2020-06-27 08:00:37.645064
Model ind 665 epoch 1374 batch: 300 avg loss -2.943815 avg loss no lamb -2.943815 time 2020-06-27 08:00:48.438190
Model ind 665 epoch 1374 batch: 400 avg loss -2.808833 avg loss no lamb -2.808833 time 2020-06-27 08:00:59.148628
Model ind 665 epoch 1374 batch: 500 avg loss -2.763608 avg loss no lamb -2.763608 time 2020-06-27 08:01:10.302185
Model ind 665 epoch 1374 batch: 600 avg loss -2.893600 avg loss no lamb -2.893600 time 2020-06-27 08:01:21.306669
Model ind 665 epoch 1374 batch: 700 avg loss -2.772798 avg loss no lamb -2.772798 time 2020-06-27 08:01:32.306612
Model ind 665 epoch 1374 batch: 800 avg loss -2.918445 avg loss no lamb -2.918445 time 2020-06-27 08:01:43.261649
last batch sz 10
Pre: time 2020-06-27 08:01:57.477491: 
 	std: 0.002194886
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9805, 0.976, 0.9809, 0.9761]
	train_accs: [0.9810333, 0.98108333, 0.9762, 0.98143333, 0.97695]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.97872
	best: 0.9809

Starting e_i: 1375
Model ind 665 epoch 1375 batch: 0 avg loss -2.963863 avg loss no lamb -2.963863 time 2020-06-27 08:01:58.727533
Model ind 665 epoch 1375 batch: 100 avg loss -2.895296 avg loss no lamb -2.895296 time 2020-06-27 08:02:09.569734
Model ind 665 epoch 1375 batch: 200 avg loss -2.930895 avg loss no lamb -2.930895 time 2020-06-27 08:02:20.406996
Model ind 665 epoch 1375 batch: 300 avg loss -2.862354 avg loss no lamb -2.862354 time 2020-06-27 08:02:31.304683
Model ind 665 epoch 1375 batch: 400 avg loss -2.801064 avg loss no lamb -2.801064 time 2020-06-27 08:02:42.347909
Model ind 665 epoch 1375 batch: 500 avg loss -2.892752 avg loss no lamb -2.892752 time 2020-06-27 08:02:52.908506
Model ind 665 epoch 1375 batch: 600 avg loss -2.909506 avg loss no lamb -2.909506 time 2020-06-27 08:03:03.672530
Model ind 665 epoch 1375 batch: 700 avg loss -2.850075 avg loss no lamb -2.850075 time 2020-06-27 08:03:14.468589
Model ind 665 epoch 1375 batch: 800 avg loss -2.825435 avg loss no lamb -2.825435 time 2020-06-27 08:03:25.184990
last batch sz 10
Pre: time 2020-06-27 08:03:39.034466: 
 	std: 0.0025179312
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9801, 0.9752, 0.981, 0.9759]
	train_accs: [0.9815, 0.98076665, 0.9759833, 0.9814, 0.977]
	best_train_sub_head: 0
	worst: 0.9752
	avg: 0.9786
	best: 0.9808

Starting e_i: 1376
Model ind 665 epoch 1376 batch: 0 avg loss -2.915814 avg loss no lamb -2.915814 time 2020-06-27 08:03:40.228104
Model ind 665 epoch 1376 batch: 100 avg loss -2.892853 avg loss no lamb -2.892853 time 2020-06-27 08:03:50.997324
Model ind 665 epoch 1376 batch: 200 avg loss -2.852188 avg loss no lamb -2.852188 time 2020-06-27 08:04:01.838994
Model ind 665 epoch 1376 batch: 300 avg loss -2.893181 avg loss no lamb -2.893181 time 2020-06-27 08:04:12.637385
Model ind 665 epoch 1376 batch: 400 avg loss -2.802629 avg loss no lamb -2.802629 time 2020-06-27 08:04:23.335635
Model ind 665 epoch 1376 batch: 500 avg loss -2.848828 avg loss no lamb -2.848828 time 2020-06-27 08:04:34.283438
Model ind 665 epoch 1376 batch: 600 avg loss -2.892183 avg loss no lamb -2.892183 time 2020-06-27 08:04:45.163091
Model ind 665 epoch 1376 batch: 700 avg loss -2.857481 avg loss no lamb -2.857481 time 2020-06-27 08:04:55.846071
Model ind 665 epoch 1376 batch: 800 avg loss -2.866050 avg loss no lamb -2.866050 time 2020-06-27 08:05:06.654675
last batch sz 10
Pre: time 2020-06-27 08:05:21.009168: 
 	std: 0.002911359
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9807, 0.9739, 0.9807, 0.9762]
	train_accs: [0.98165, 0.98065, 0.9751833, 0.9817333, 0.9770833]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.9785
	best: 0.9807

Starting e_i: 1377
Model ind 665 epoch 1377 batch: 0 avg loss -2.978240 avg loss no lamb -2.978240 time 2020-06-27 08:05:22.233186
Model ind 665 epoch 1377 batch: 100 avg loss -2.924152 avg loss no lamb -2.924152 time 2020-06-27 08:05:33.131379
Model ind 665 epoch 1377 batch: 200 avg loss -2.887515 avg loss no lamb -2.887515 time 2020-06-27 08:05:43.996094
Model ind 665 epoch 1377 batch: 300 avg loss -2.900724 avg loss no lamb -2.900724 time 2020-06-27 08:05:54.766038
Model ind 665 epoch 1377 batch: 400 avg loss -2.789881 avg loss no lamb -2.789881 time 2020-06-27 08:06:05.836804
Model ind 665 epoch 1377 batch: 500 avg loss -2.842706 avg loss no lamb -2.842706 time 2020-06-27 08:06:16.590293
Model ind 665 epoch 1377 batch: 600 avg loss -2.950465 avg loss no lamb -2.950465 time 2020-06-27 08:06:27.211390
Model ind 665 epoch 1377 batch: 700 avg loss -2.761555 avg loss no lamb -2.761555 time 2020-06-27 08:06:38.142423
Model ind 665 epoch 1377 batch: 800 avg loss -2.863190 avg loss no lamb -2.863190 time 2020-06-27 08:06:48.995410
last batch sz 10
Pre: time 2020-06-27 08:07:03.119194: 
 	std: 0.0034742476
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9809, 0.9733, 0.9805, 0.9735]
	train_accs: [0.98165, 0.9812833, 0.9763, 0.9816833, 0.9762167]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97764
	best: 0.9805

Starting e_i: 1378
Model ind 665 epoch 1378 batch: 0 avg loss -2.985364 avg loss no lamb -2.985364 time 2020-06-27 08:07:04.364715
Model ind 665 epoch 1378 batch: 100 avg loss -2.911574 avg loss no lamb -2.911574 time 2020-06-27 08:07:14.936886
Model ind 665 epoch 1378 batch: 200 avg loss -2.895199 avg loss no lamb -2.895199 time 2020-06-27 08:07:25.891820
Model ind 665 epoch 1378 batch: 300 avg loss -2.837787 avg loss no lamb -2.837787 time 2020-06-27 08:07:36.693867
Model ind 665 epoch 1378 batch: 400 avg loss -2.843398 avg loss no lamb -2.843398 time 2020-06-27 08:07:47.518743
Model ind 665 epoch 1378 batch: 500 avg loss -2.849407 avg loss no lamb -2.849407 time 2020-06-27 08:07:58.036887
Model ind 665 epoch 1378 batch: 600 avg loss -2.893491 avg loss no lamb -2.893491 time 2020-06-27 08:08:08.867307
Model ind 665 epoch 1378 batch: 700 avg loss -2.815132 avg loss no lamb -2.815132 time 2020-06-27 08:08:19.436017
Model ind 665 epoch 1378 batch: 800 avg loss -2.838327 avg loss no lamb -2.838327 time 2020-06-27 08:08:30.077675
last batch sz 10
Pre: time 2020-06-27 08:08:44.433725: 
 	std: 0.0028485798
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9806, 0.9737, 0.9803, 0.9761]
	train_accs: [0.9815, 0.98095, 0.97575, 0.98148334, 0.97748333]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97826004
	best: 0.9806

Starting e_i: 1379
Model ind 665 epoch 1379 batch: 0 avg loss -2.939411 avg loss no lamb -2.939411 time 2020-06-27 08:08:45.649714
Model ind 665 epoch 1379 batch: 100 avg loss -2.916558 avg loss no lamb -2.916558 time 2020-06-27 08:08:56.324893
Model ind 665 epoch 1379 batch: 200 avg loss -2.920451 avg loss no lamb -2.920451 time 2020-06-27 08:09:06.892230
Model ind 665 epoch 1379 batch: 300 avg loss -2.868020 avg loss no lamb -2.868020 time 2020-06-27 08:09:17.570117
Model ind 665 epoch 1379 batch: 400 avg loss -2.804502 avg loss no lamb -2.804502 time 2020-06-27 08:09:28.386177
Model ind 665 epoch 1379 batch: 500 avg loss -2.842426 avg loss no lamb -2.842426 time 2020-06-27 08:09:39.110544
Model ind 665 epoch 1379 batch: 600 avg loss -2.900895 avg loss no lamb -2.900895 time 2020-06-27 08:09:50.080886
Model ind 665 epoch 1379 batch: 700 avg loss -2.819847 avg loss no lamb -2.819847 time 2020-06-27 08:10:00.787141
Model ind 665 epoch 1379 batch: 800 avg loss -2.820687 avg loss no lamb -2.820687 time 2020-06-27 08:10:11.404421
last batch sz 10
Pre: time 2020-06-27 08:10:25.368439: 
 	std: 0.0029206972
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.98, 0.973, 0.9796, 0.9747]
	train_accs: [0.98035, 0.9804, 0.97465, 0.9805833, 0.9759167]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97735995
	best: 0.9796

Starting e_i: 1380
Model ind 665 epoch 1380 batch: 0 avg loss -2.926352 avg loss no lamb -2.926352 time 2020-06-27 08:10:26.602183
Model ind 665 epoch 1380 batch: 100 avg loss -2.886413 avg loss no lamb -2.886413 time 2020-06-27 08:10:37.472335
Model ind 665 epoch 1380 batch: 200 avg loss -2.915542 avg loss no lamb -2.915542 time 2020-06-27 08:10:48.344329
Model ind 665 epoch 1380 batch: 300 avg loss -2.882845 avg loss no lamb -2.882845 time 2020-06-27 08:10:59.263520
Model ind 665 epoch 1380 batch: 400 avg loss -2.790501 avg loss no lamb -2.790501 time 2020-06-27 08:11:10.061736
Model ind 665 epoch 1380 batch: 500 avg loss -2.848060 avg loss no lamb -2.848060 time 2020-06-27 08:11:20.949556
Model ind 665 epoch 1380 batch: 600 avg loss -2.858835 avg loss no lamb -2.858835 time 2020-06-27 08:11:31.867730
Model ind 665 epoch 1380 batch: 700 avg loss -2.815766 avg loss no lamb -2.815766 time 2020-06-27 08:11:42.778533
Model ind 665 epoch 1380 batch: 800 avg loss -2.895363 avg loss no lamb -2.895363 time 2020-06-27 08:11:53.836886
last batch sz 10
Pre: time 2020-06-27 08:12:08.031652: 
 	std: 0.0028294316
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9804, 0.9734, 0.9797, 0.9752]
	train_accs: [0.9809333, 0.9808, 0.97536665, 0.981, 0.97678334]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97768
	best: 0.9797

Starting e_i: 1381
Model ind 665 epoch 1381 batch: 0 avg loss -2.947883 avg loss no lamb -2.947883 time 2020-06-27 08:12:10.526462
Model ind 665 epoch 1381 batch: 100 avg loss -2.939230 avg loss no lamb -2.939230 time 2020-06-27 08:12:21.493201
Model ind 665 epoch 1381 batch: 200 avg loss -2.950921 avg loss no lamb -2.950921 time 2020-06-27 08:12:32.141062
Model ind 665 epoch 1381 batch: 300 avg loss -2.901742 avg loss no lamb -2.901742 time 2020-06-27 08:12:43.090594
Model ind 665 epoch 1381 batch: 400 avg loss -2.792869 avg loss no lamb -2.792869 time 2020-06-27 08:12:53.899719
Model ind 665 epoch 1381 batch: 500 avg loss -2.882720 avg loss no lamb -2.882720 time 2020-06-27 08:13:04.751096
Model ind 665 epoch 1381 batch: 600 avg loss -2.899163 avg loss no lamb -2.899163 time 2020-06-27 08:13:15.592196
Model ind 665 epoch 1381 batch: 700 avg loss -2.802588 avg loss no lamb -2.802588 time 2020-06-27 08:13:26.262500
Model ind 665 epoch 1381 batch: 800 avg loss -2.862881 avg loss no lamb -2.862881 time 2020-06-27 08:13:37.162810
last batch sz 10
Pre: time 2020-06-27 08:13:51.286520: 
 	std: 0.0025915192
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9798, 0.9741, 0.9799, 0.9746]
	train_accs: [0.98081666, 0.98073334, 0.97548336, 0.9813833, 0.9761]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97749996
	best: 0.9799

Starting e_i: 1382
Model ind 665 epoch 1382 batch: 0 avg loss -2.970347 avg loss no lamb -2.970347 time 2020-06-27 08:13:52.509151
Model ind 665 epoch 1382 batch: 100 avg loss -2.943775 avg loss no lamb -2.943775 time 2020-06-27 08:14:03.176726
Model ind 665 epoch 1382 batch: 200 avg loss -2.885605 avg loss no lamb -2.885605 time 2020-06-27 08:14:14.071698
Model ind 665 epoch 1382 batch: 300 avg loss -2.862120 avg loss no lamb -2.862120 time 2020-06-27 08:14:25.033678
Model ind 665 epoch 1382 batch: 400 avg loss -2.770747 avg loss no lamb -2.770747 time 2020-06-27 08:14:35.777347
Model ind 665 epoch 1382 batch: 500 avg loss -2.865510 avg loss no lamb -2.865510 time 2020-06-27 08:14:46.677999
Model ind 665 epoch 1382 batch: 600 avg loss -2.896404 avg loss no lamb -2.896404 time 2020-06-27 08:14:57.454572
Model ind 665 epoch 1382 batch: 700 avg loss -2.709176 avg loss no lamb -2.709176 time 2020-06-27 08:15:08.321888
Model ind 665 epoch 1382 batch: 800 avg loss -2.854234 avg loss no lamb -2.854234 time 2020-06-27 08:15:19.299689
last batch sz 10
Pre: time 2020-06-27 08:15:33.356943: 
 	std: 0.0033403032
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9807, 0.9729, 0.9801, 0.9746]
	train_accs: [0.98148334, 0.98106664, 0.9755333, 0.98146665, 0.9763833]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97778
	best: 0.9806

Starting e_i: 1383
Model ind 665 epoch 1383 batch: 0 avg loss -2.986737 avg loss no lamb -2.986737 time 2020-06-27 08:15:34.531330
Model ind 665 epoch 1383 batch: 100 avg loss -2.897891 avg loss no lamb -2.897891 time 2020-06-27 08:15:45.261850
Model ind 665 epoch 1383 batch: 200 avg loss -2.861953 avg loss no lamb -2.861953 time 2020-06-27 08:15:56.008871
Model ind 665 epoch 1383 batch: 300 avg loss -2.866204 avg loss no lamb -2.866204 time 2020-06-27 08:16:07.035456
Model ind 665 epoch 1383 batch: 400 avg loss -2.876369 avg loss no lamb -2.876369 time 2020-06-27 08:16:17.834695
Model ind 665 epoch 1383 batch: 500 avg loss -2.876240 avg loss no lamb -2.876240 time 2020-06-27 08:16:28.631938
Model ind 665 epoch 1383 batch: 600 avg loss -2.942906 avg loss no lamb -2.942906 time 2020-06-27 08:16:39.439761
Model ind 665 epoch 1383 batch: 700 avg loss -2.789848 avg loss no lamb -2.789848 time 2020-06-27 08:16:50.269633
Model ind 665 epoch 1383 batch: 800 avg loss -2.885622 avg loss no lamb -2.885622 time 2020-06-27 08:17:01.376853
last batch sz 10
Pre: time 2020-06-27 08:17:15.509321: 
 	std: 0.0028807023
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9812, 0.9747, 0.9806, 0.9754]
	train_accs: [0.9816, 0.9813833, 0.9762667, 0.98155, 0.9771]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97856
	best: 0.9809

Starting e_i: 1384
Model ind 665 epoch 1384 batch: 0 avg loss -2.961927 avg loss no lamb -2.961927 time 2020-06-27 08:17:16.718368
Model ind 665 epoch 1384 batch: 100 avg loss -2.934086 avg loss no lamb -2.934086 time 2020-06-27 08:17:27.674315
Model ind 665 epoch 1384 batch: 200 avg loss -2.876107 avg loss no lamb -2.876107 time 2020-06-27 08:17:38.457793
Model ind 665 epoch 1384 batch: 300 avg loss -2.911704 avg loss no lamb -2.911704 time 2020-06-27 08:17:49.443580
Model ind 665 epoch 1384 batch: 400 avg loss -2.767654 avg loss no lamb -2.767654 time 2020-06-27 08:18:00.277699
Model ind 665 epoch 1384 batch: 500 avg loss -2.856067 avg loss no lamb -2.856067 time 2020-06-27 08:18:11.171487
Model ind 665 epoch 1384 batch: 600 avg loss -2.917632 avg loss no lamb -2.917632 time 2020-06-27 08:18:21.965025
Model ind 665 epoch 1384 batch: 700 avg loss -2.780900 avg loss no lamb -2.780900 time 2020-06-27 08:18:32.848762
Model ind 665 epoch 1384 batch: 800 avg loss -2.900856 avg loss no lamb -2.900856 time 2020-06-27 08:18:43.464869
last batch sz 10
Pre: time 2020-06-27 08:18:57.419756: 
 	std: 0.0021940037
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9796, 0.9754, 0.9801, 0.9754]
	train_accs: [0.98108333, 0.98045, 0.9762667, 0.98118335, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97808
	best: 0.9801

Starting e_i: 1385
Model ind 665 epoch 1385 batch: 0 avg loss -2.891759 avg loss no lamb -2.891759 time 2020-06-27 08:18:58.809388
Model ind 665 epoch 1385 batch: 100 avg loss -2.872424 avg loss no lamb -2.872424 time 2020-06-27 08:19:09.739800
Model ind 665 epoch 1385 batch: 200 avg loss -2.888609 avg loss no lamb -2.888609 time 2020-06-27 08:19:20.510318
Model ind 665 epoch 1385 batch: 300 avg loss -2.903840 avg loss no lamb -2.903840 time 2020-06-27 08:19:31.541673
Model ind 665 epoch 1385 batch: 400 avg loss -2.829804 avg loss no lamb -2.829804 time 2020-06-27 08:19:42.345665
Model ind 665 epoch 1385 batch: 500 avg loss -2.889575 avg loss no lamb -2.889575 time 2020-06-27 08:19:53.251614
Model ind 665 epoch 1385 batch: 600 avg loss -2.902306 avg loss no lamb -2.902306 time 2020-06-27 08:20:04.250565
Model ind 665 epoch 1385 batch: 700 avg loss -2.804712 avg loss no lamb -2.804712 time 2020-06-27 08:20:15.383211
Model ind 665 epoch 1385 batch: 800 avg loss -2.814649 avg loss no lamb -2.814649 time 2020-06-27 08:20:26.380291
last batch sz 10
Pre: time 2020-06-27 08:20:40.505298: 
 	std: 0.0026279923
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.98, 0.9742, 0.9801, 0.9753]
	train_accs: [0.98105, 0.98083335, 0.97533333, 0.9810333, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97793996
	best: 0.9801

Starting e_i: 1386
Model ind 665 epoch 1386 batch: 0 avg loss -3.023228 avg loss no lamb -3.023228 time 2020-06-27 08:20:41.719964
Model ind 665 epoch 1386 batch: 100 avg loss -2.881947 avg loss no lamb -2.881947 time 2020-06-27 08:20:52.577357
Model ind 665 epoch 1386 batch: 200 avg loss -2.880347 avg loss no lamb -2.880347 time 2020-06-27 08:21:03.373527
Model ind 665 epoch 1386 batch: 300 avg loss -2.924164 avg loss no lamb -2.924164 time 2020-06-27 08:21:14.323516
Model ind 665 epoch 1386 batch: 400 avg loss -2.809888 avg loss no lamb -2.809888 time 2020-06-27 08:21:25.360995
Model ind 665 epoch 1386 batch: 500 avg loss -2.828877 avg loss no lamb -2.828877 time 2020-06-27 08:21:36.315803
Model ind 665 epoch 1386 batch: 600 avg loss -2.877491 avg loss no lamb -2.877491 time 2020-06-27 08:21:46.842867
Model ind 665 epoch 1386 batch: 700 avg loss -2.692037 avg loss no lamb -2.692037 time 2020-06-27 08:21:57.577604
Model ind 665 epoch 1386 batch: 800 avg loss -2.890168 avg loss no lamb -2.890168 time 2020-06-27 08:22:08.285671
last batch sz 10
Pre: time 2020-06-27 08:22:22.432033: 
 	std: 0.0032432145
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9797, 0.9728, 0.9802, 0.974]
	train_accs: [0.98176664, 0.98121667, 0.97603333, 0.9819, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97734004
	best: 0.9802

Starting e_i: 1387
Model ind 665 epoch 1387 batch: 0 avg loss -2.918405 avg loss no lamb -2.918405 time 2020-06-27 08:22:23.867378
Model ind 665 epoch 1387 batch: 100 avg loss -2.890032 avg loss no lamb -2.890032 time 2020-06-27 08:22:34.715455
Model ind 665 epoch 1387 batch: 200 avg loss -2.866431 avg loss no lamb -2.866431 time 2020-06-27 08:22:45.587082
Model ind 665 epoch 1387 batch: 300 avg loss -2.930747 avg loss no lamb -2.930747 time 2020-06-27 08:22:56.473400
Model ind 665 epoch 1387 batch: 400 avg loss -2.832041 avg loss no lamb -2.832041 time 2020-06-27 08:23:07.408772
Model ind 665 epoch 1387 batch: 500 avg loss -2.778461 avg loss no lamb -2.778461 time 2020-06-27 08:23:18.463236
Model ind 665 epoch 1387 batch: 600 avg loss -2.883301 avg loss no lamb -2.883301 time 2020-06-27 08:23:29.462448
Model ind 665 epoch 1387 batch: 700 avg loss -2.758499 avg loss no lamb -2.758499 time 2020-06-27 08:23:40.478079
Model ind 665 epoch 1387 batch: 800 avg loss -2.897163 avg loss no lamb -2.897163 time 2020-06-27 08:23:51.505633
last batch sz 10
Pre: time 2020-06-27 08:24:05.675230: 
 	std: 0.003360592
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9805, 0.9735, 0.9809, 0.9752]
	train_accs: [0.98156667, 0.9806333, 0.97585, 0.9813167, 0.9766]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97838
	best: 0.9818

Starting e_i: 1388
Model ind 665 epoch 1388 batch: 0 avg loss -2.976021 avg loss no lamb -2.976021 time 2020-06-27 08:24:06.884725
Model ind 665 epoch 1388 batch: 100 avg loss -2.880419 avg loss no lamb -2.880419 time 2020-06-27 08:24:18.068504
Model ind 665 epoch 1388 batch: 200 avg loss -2.924344 avg loss no lamb -2.924344 time 2020-06-27 08:24:29.057980
Model ind 665 epoch 1388 batch: 300 avg loss -2.833062 avg loss no lamb -2.833062 time 2020-06-27 08:24:39.984969
Model ind 665 epoch 1388 batch: 400 avg loss -2.760116 avg loss no lamb -2.760116 time 2020-06-27 08:24:50.705577
Model ind 665 epoch 1388 batch: 500 avg loss -2.884450 avg loss no lamb -2.884450 time 2020-06-27 08:25:01.445173
Model ind 665 epoch 1388 batch: 600 avg loss -2.922618 avg loss no lamb -2.922618 time 2020-06-27 08:25:12.160161
Model ind 665 epoch 1388 batch: 700 avg loss -2.781463 avg loss no lamb -2.781463 time 2020-06-27 08:25:23.115263
Model ind 665 epoch 1388 batch: 800 avg loss -2.852602 avg loss no lamb -2.852602 time 2020-06-27 08:25:33.907832
last batch sz 10
Pre: time 2020-06-27 08:25:47.596653: 
 	std: 0.0022948706
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9784, 0.9768, 0.9719, 0.9771, 0.9745]
	train_accs: [0.9802833, 0.97936666, 0.97465, 0.98011667, 0.97615]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.9757401
	best: 0.9784

Starting e_i: 1389
Model ind 665 epoch 1389 batch: 0 avg loss -2.913989 avg loss no lamb -2.913989 time 2020-06-27 08:25:49.053034
Model ind 665 epoch 1389 batch: 100 avg loss -2.905706 avg loss no lamb -2.905706 time 2020-06-27 08:25:59.940499
Model ind 665 epoch 1389 batch: 200 avg loss -2.869714 avg loss no lamb -2.869714 time 2020-06-27 08:26:10.771889
Model ind 665 epoch 1389 batch: 300 avg loss -2.860838 avg loss no lamb -2.860838 time 2020-06-27 08:26:21.666687
Model ind 665 epoch 1389 batch: 400 avg loss -2.795638 avg loss no lamb -2.795638 time 2020-06-27 08:26:32.647980
Model ind 665 epoch 1389 batch: 500 avg loss -2.886415 avg loss no lamb -2.886415 time 2020-06-27 08:26:43.709481
Model ind 665 epoch 1389 batch: 600 avg loss -2.905613 avg loss no lamb -2.905613 time 2020-06-27 08:26:54.532517
Model ind 665 epoch 1389 batch: 700 avg loss -2.791460 avg loss no lamb -2.791460 time 2020-06-27 08:27:05.384277
Model ind 665 epoch 1389 batch: 800 avg loss -2.876152 avg loss no lamb -2.876152 time 2020-06-27 08:27:16.324908
last batch sz 10
Pre: time 2020-06-27 08:27:30.150565: 
 	std: 0.0026858135
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9786, 0.9735, 0.9793, 0.9738]
	train_accs: [0.98116666, 0.98013335, 0.97568333, 0.98115, 0.9763333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97692
	best: 0.9794

Starting e_i: 1390
Model ind 665 epoch 1390 batch: 0 avg loss -2.961774 avg loss no lamb -2.961774 time 2020-06-27 08:27:31.343659
Model ind 665 epoch 1390 batch: 100 avg loss -2.862399 avg loss no lamb -2.862399 time 2020-06-27 08:27:42.497234
Model ind 665 epoch 1390 batch: 200 avg loss -2.809417 avg loss no lamb -2.809417 time 2020-06-27 08:27:53.202927
Model ind 665 epoch 1390 batch: 300 avg loss -2.860873 avg loss no lamb -2.860873 time 2020-06-27 08:28:04.078870
Model ind 665 epoch 1390 batch: 400 avg loss -2.811043 avg loss no lamb -2.811043 time 2020-06-27 08:28:14.963500
Model ind 665 epoch 1390 batch: 500 avg loss -2.837017 avg loss no lamb -2.837017 time 2020-06-27 08:28:25.949974
Model ind 665 epoch 1390 batch: 600 avg loss -2.874703 avg loss no lamb -2.874703 time 2020-06-27 08:28:36.845061
Model ind 665 epoch 1390 batch: 700 avg loss -2.825913 avg loss no lamb -2.825913 time 2020-06-27 08:28:47.655938
Model ind 665 epoch 1390 batch: 800 avg loss -2.920163 avg loss no lamb -2.920163 time 2020-06-27 08:28:58.391775
last batch sz 10
Pre: time 2020-06-27 08:29:12.462682: 
 	std: 0.0032908374
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9796, 0.9726, 0.9809, 0.9746]
	train_accs: [0.98176664, 0.9810333, 0.9755333, 0.9821333, 0.9771]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97752
	best: 0.9809

Starting e_i: 1391
Model ind 665 epoch 1391 batch: 0 avg loss -2.924827 avg loss no lamb -2.924827 time 2020-06-27 08:29:14.930114
Model ind 665 epoch 1391 batch: 100 avg loss -2.911610 avg loss no lamb -2.911610 time 2020-06-27 08:29:25.947666
Model ind 665 epoch 1391 batch: 200 avg loss -2.867611 avg loss no lamb -2.867611 time 2020-06-27 08:29:36.802638
Model ind 665 epoch 1391 batch: 300 avg loss -2.921038 avg loss no lamb -2.921038 time 2020-06-27 08:29:47.563953
Model ind 665 epoch 1391 batch: 400 avg loss -2.833962 avg loss no lamb -2.833962 time 2020-06-27 08:29:58.203831
Model ind 665 epoch 1391 batch: 500 avg loss -2.837720 avg loss no lamb -2.837720 time 2020-06-27 08:30:09.440661
Model ind 665 epoch 1391 batch: 600 avg loss -2.875136 avg loss no lamb -2.875136 time 2020-06-27 08:30:20.310081
Model ind 665 epoch 1391 batch: 700 avg loss -2.802580 avg loss no lamb -2.802580 time 2020-06-27 08:30:31.098125
Model ind 665 epoch 1391 batch: 800 avg loss -2.900088 avg loss no lamb -2.900088 time 2020-06-27 08:30:41.975559
last batch sz 10
Pre: time 2020-06-27 08:30:56.166115: 
 	std: 0.0035302152
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9794, 0.9729, 0.9813, 0.9739]
	train_accs: [0.98153335, 0.98066664, 0.97548336, 0.9816, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97764
	best: 0.9813

Starting e_i: 1392
Model ind 665 epoch 1392 batch: 0 avg loss -2.972110 avg loss no lamb -2.972110 time 2020-06-27 08:30:57.410017
Model ind 665 epoch 1392 batch: 100 avg loss -2.940477 avg loss no lamb -2.940477 time 2020-06-27 08:31:08.413134
Model ind 665 epoch 1392 batch: 200 avg loss -2.865074 avg loss no lamb -2.865074 time 2020-06-27 08:31:19.654434
Model ind 665 epoch 1392 batch: 300 avg loss -2.880450 avg loss no lamb -2.880450 time 2020-06-27 08:31:30.640156
Model ind 665 epoch 1392 batch: 400 avg loss -2.798223 avg loss no lamb -2.798223 time 2020-06-27 08:31:41.575265
Model ind 665 epoch 1392 batch: 500 avg loss -2.854025 avg loss no lamb -2.854025 time 2020-06-27 08:31:52.705400
Model ind 665 epoch 1392 batch: 600 avg loss -2.922694 avg loss no lamb -2.922694 time 2020-06-27 08:32:04.104339
Model ind 665 epoch 1392 batch: 700 avg loss -2.803838 avg loss no lamb -2.803838 time 2020-06-27 08:32:15.195571
Model ind 665 epoch 1392 batch: 800 avg loss -2.874796 avg loss no lamb -2.874796 time 2020-06-27 08:32:26.110104
last batch sz 10
Pre: time 2020-06-27 08:32:40.401940: 
 	std: 0.0027036387
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9787, 0.973, 0.9793, 0.974]
	train_accs: [0.98053336, 0.97996664, 0.97503334, 0.98065, 0.9763833]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97677994
	best: 0.9793

Starting e_i: 1393
Model ind 665 epoch 1393 batch: 0 avg loss -2.953120 avg loss no lamb -2.953120 time 2020-06-27 08:32:41.607182
Model ind 665 epoch 1393 batch: 100 avg loss -2.887464 avg loss no lamb -2.887464 time 2020-06-27 08:32:52.556633
Model ind 665 epoch 1393 batch: 200 avg loss -2.930472 avg loss no lamb -2.930472 time 2020-06-27 08:33:03.790643
Model ind 665 epoch 1393 batch: 300 avg loss -2.909524 avg loss no lamb -2.909524 time 2020-06-27 08:33:14.944878
Model ind 665 epoch 1393 batch: 400 avg loss -2.827531 avg loss no lamb -2.827531 time 2020-06-27 08:33:25.677697
Model ind 665 epoch 1393 batch: 500 avg loss -2.841728 avg loss no lamb -2.841728 time 2020-06-27 08:33:36.671433
Model ind 665 epoch 1393 batch: 600 avg loss -2.887975 avg loss no lamb -2.887975 time 2020-06-27 08:33:47.681900
Model ind 665 epoch 1393 batch: 700 avg loss -2.806616 avg loss no lamb -2.806616 time 2020-06-27 08:33:58.828601
Model ind 665 epoch 1393 batch: 800 avg loss -2.839760 avg loss no lamb -2.839760 time 2020-06-27 08:34:09.667520
last batch sz 10
Pre: time 2020-06-27 08:34:24.109415: 
 	std: 0.0033404164
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.979, 0.9724, 0.9797, 0.9732]
	train_accs: [0.98148334, 0.98031664, 0.975, 0.98113334, 0.9759]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97686005
	best: 0.98

Starting e_i: 1394
Model ind 665 epoch 1394 batch: 0 avg loss -2.940911 avg loss no lamb -2.940911 time 2020-06-27 08:34:25.385225
Model ind 665 epoch 1394 batch: 100 avg loss -2.917577 avg loss no lamb -2.917577 time 2020-06-27 08:34:36.255004
Model ind 665 epoch 1394 batch: 200 avg loss -2.848304 avg loss no lamb -2.848304 time 2020-06-27 08:34:47.301432
Model ind 665 epoch 1394 batch: 300 avg loss -2.887604 avg loss no lamb -2.887604 time 2020-06-27 08:34:58.163282
Model ind 665 epoch 1394 batch: 400 avg loss -2.821214 avg loss no lamb -2.821214 time 2020-06-27 08:35:08.976971
Model ind 665 epoch 1394 batch: 500 avg loss -2.844562 avg loss no lamb -2.844562 time 2020-06-27 08:35:20.072392
Model ind 665 epoch 1394 batch: 600 avg loss -2.906446 avg loss no lamb -2.906446 time 2020-06-27 08:35:30.998266
Model ind 665 epoch 1394 batch: 700 avg loss -2.820389 avg loss no lamb -2.820389 time 2020-06-27 08:35:41.959860
Model ind 665 epoch 1394 batch: 800 avg loss -2.934790 avg loss no lamb -2.934790 time 2020-06-27 08:35:53.087655
last batch sz 10
Pre: time 2020-06-27 08:36:07.748936: 
 	std: 0.003082212
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9787, 0.9726, 0.9799, 0.974]
	train_accs: [0.9811, 0.98041666, 0.97513336, 0.98146665, 0.97613335]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97699994
	best: 0.9799

Starting e_i: 1395
Model ind 665 epoch 1395 batch: 0 avg loss -2.879907 avg loss no lamb -2.879907 time 2020-06-27 08:36:08.999244
Model ind 665 epoch 1395 batch: 100 avg loss -2.889591 avg loss no lamb -2.889591 time 2020-06-27 08:36:19.882217
Model ind 665 epoch 1395 batch: 200 avg loss -2.895632 avg loss no lamb -2.895632 time 2020-06-27 08:36:30.903687
Model ind 665 epoch 1395 batch: 300 avg loss -2.892759 avg loss no lamb -2.892759 time 2020-06-27 08:36:42.133770
Model ind 665 epoch 1395 batch: 400 avg loss -2.821411 avg loss no lamb -2.821411 time 2020-06-27 08:36:53.263465
Model ind 665 epoch 1395 batch: 500 avg loss -2.885018 avg loss no lamb -2.885018 time 2020-06-27 08:37:04.456742
Model ind 665 epoch 1395 batch: 600 avg loss -2.889205 avg loss no lamb -2.889205 time 2020-06-27 08:37:15.660529
Model ind 665 epoch 1395 batch: 700 avg loss -2.751988 avg loss no lamb -2.751988 time 2020-06-27 08:37:26.869848
Model ind 665 epoch 1395 batch: 800 avg loss -2.846898 avg loss no lamb -2.846898 time 2020-06-27 08:37:38.202042
last batch sz 10
Pre: time 2020-06-27 08:37:52.926121: 
 	std: 0.0023387135
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9791, 0.9746, 0.9801, 0.9759]
	train_accs: [0.98155, 0.9803, 0.97636664, 0.9816667, 0.9773833]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97802
	best: 0.9801

Starting e_i: 1396
Model ind 665 epoch 1396 batch: 0 avg loss -2.966697 avg loss no lamb -2.966697 time 2020-06-27 08:37:54.190825
Model ind 665 epoch 1396 batch: 100 avg loss -2.900038 avg loss no lamb -2.900038 time 2020-06-27 08:38:05.238014
Model ind 665 epoch 1396 batch: 200 avg loss -2.865499 avg loss no lamb -2.865499 time 2020-06-27 08:38:16.152683
Model ind 665 epoch 1396 batch: 300 avg loss -2.879644 avg loss no lamb -2.879644 time 2020-06-27 08:38:27.276345
Model ind 665 epoch 1396 batch: 400 avg loss -2.820625 avg loss no lamb -2.820625 time 2020-06-27 08:38:38.259061
Model ind 665 epoch 1396 batch: 500 avg loss -2.828047 avg loss no lamb -2.828047 time 2020-06-27 08:38:49.384646
Model ind 665 epoch 1396 batch: 600 avg loss -2.882539 avg loss no lamb -2.882539 time 2020-06-27 08:39:00.214117
Model ind 665 epoch 1396 batch: 700 avg loss -2.824940 avg loss no lamb -2.824940 time 2020-06-27 08:39:11.263542
Model ind 665 epoch 1396 batch: 800 avg loss -2.877499 avg loss no lamb -2.877499 time 2020-06-27 08:39:22.344349
last batch sz 10
Pre: time 2020-06-27 08:39:36.627748: 
 	std: 0.0031833388
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9794, 0.9726, 0.9803, 0.9744]
	train_accs: [0.98143333, 0.98065, 0.97501665, 0.98175, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97732
	best: 0.9803

Starting e_i: 1397
Model ind 665 epoch 1397 batch: 0 avg loss -2.978900 avg loss no lamb -2.978900 time 2020-06-27 08:39:37.838667
Model ind 665 epoch 1397 batch: 100 avg loss -2.871879 avg loss no lamb -2.871879 time 2020-06-27 08:39:48.870487
Model ind 665 epoch 1397 batch: 200 avg loss -2.875844 avg loss no lamb -2.875844 time 2020-06-27 08:39:59.789251
Model ind 665 epoch 1397 batch: 300 avg loss -2.847712 avg loss no lamb -2.847712 time 2020-06-27 08:40:11.012070
Model ind 665 epoch 1397 batch: 400 avg loss -2.823748 avg loss no lamb -2.823748 time 2020-06-27 08:40:21.986450
Model ind 665 epoch 1397 batch: 500 avg loss -2.843852 avg loss no lamb -2.843852 time 2020-06-27 08:40:32.915228
Model ind 665 epoch 1397 batch: 600 avg loss -2.866626 avg loss no lamb -2.866626 time 2020-06-27 08:40:44.066732
Model ind 665 epoch 1397 batch: 700 avg loss -2.794055 avg loss no lamb -2.794055 time 2020-06-27 08:40:55.121364
Model ind 665 epoch 1397 batch: 800 avg loss -2.858651 avg loss no lamb -2.858651 time 2020-06-27 08:41:06.155319
last batch sz 10
Pre: time 2020-06-27 08:41:20.979491: 
 	std: 0.0028160978
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9792, 0.9729, 0.9794, 0.9742]
	train_accs: [0.98066664, 0.98041666, 0.9748667, 0.98081666, 0.9759167]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97696
	best: 0.9794

Starting e_i: 1398
Model ind 665 epoch 1398 batch: 0 avg loss -2.968590 avg loss no lamb -2.968590 time 2020-06-27 08:41:22.255648
Model ind 665 epoch 1398 batch: 100 avg loss -2.909848 avg loss no lamb -2.909848 time 2020-06-27 08:41:33.393029
Model ind 665 epoch 1398 batch: 200 avg loss -2.910740 avg loss no lamb -2.910740 time 2020-06-27 08:41:44.416820
Model ind 665 epoch 1398 batch: 300 avg loss -2.873370 avg loss no lamb -2.873370 time 2020-06-27 08:41:55.664934
Model ind 665 epoch 1398 batch: 400 avg loss -2.799761 avg loss no lamb -2.799761 time 2020-06-27 08:42:06.927062
Model ind 665 epoch 1398 batch: 500 avg loss -2.817201 avg loss no lamb -2.817201 time 2020-06-27 08:42:18.174745
Model ind 665 epoch 1398 batch: 600 avg loss -2.904529 avg loss no lamb -2.904529 time 2020-06-27 08:42:29.350852
Model ind 665 epoch 1398 batch: 700 avg loss -2.870247 avg loss no lamb -2.870247 time 2020-06-27 08:42:40.198722
Model ind 665 epoch 1398 batch: 800 avg loss -2.843744 avg loss no lamb -2.843744 time 2020-06-27 08:42:51.181397
last batch sz 10
Pre: time 2020-06-27 08:43:05.612083: 
 	std: 0.0027989931
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9798, 0.9732, 0.9799, 0.9755]
	train_accs: [0.9808667, 0.98008335, 0.97478336, 0.98116666, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97766
	best: 0.9799

Starting e_i: 1399
Model ind 665 epoch 1399 batch: 0 avg loss -2.961969 avg loss no lamb -2.961969 time 2020-06-27 08:43:06.903400
Model ind 665 epoch 1399 batch: 100 avg loss -2.915742 avg loss no lamb -2.915742 time 2020-06-27 08:43:18.034050
Model ind 665 epoch 1399 batch: 200 avg loss -2.880663 avg loss no lamb -2.880663 time 2020-06-27 08:43:29.277427
Model ind 665 epoch 1399 batch: 300 avg loss -2.912169 avg loss no lamb -2.912169 time 2020-06-27 08:43:40.247723
Model ind 665 epoch 1399 batch: 400 avg loss -2.862383 avg loss no lamb -2.862383 time 2020-06-27 08:43:51.355658
Model ind 665 epoch 1399 batch: 500 avg loss -2.870142 avg loss no lamb -2.870142 time 2020-06-27 08:44:02.254353
Model ind 665 epoch 1399 batch: 600 avg loss -2.916098 avg loss no lamb -2.916098 time 2020-06-27 08:44:13.473149
Model ind 665 epoch 1399 batch: 700 avg loss -2.837775 avg loss no lamb -2.837775 time 2020-06-27 08:44:24.473152
Model ind 665 epoch 1399 batch: 800 avg loss -2.852988 avg loss no lamb -2.852988 time 2020-06-27 08:44:35.908197
last batch sz 10
Pre: time 2020-06-27 08:44:50.487936: 
 	std: 0.003071552
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9793, 0.9727, 0.9796, 0.9737]
	train_accs: [0.98143333, 0.9809, 0.9751667, 0.9817, 0.9766]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97694
	best: 0.9796

Starting e_i: 1400
Model ind 665 epoch 1400 batch: 0 avg loss -2.950342 avg loss no lamb -2.950342 time 2020-06-27 08:44:51.733299
Model ind 665 epoch 1400 batch: 100 avg loss -2.884261 avg loss no lamb -2.884261 time 2020-06-27 08:45:02.890576
Model ind 665 epoch 1400 batch: 200 avg loss -2.888129 avg loss no lamb -2.888129 time 2020-06-27 08:45:13.989123
Model ind 665 epoch 1400 batch: 300 avg loss -2.893832 avg loss no lamb -2.893832 time 2020-06-27 08:45:25.156730
Model ind 665 epoch 1400 batch: 400 avg loss -2.792997 avg loss no lamb -2.792997 time 2020-06-27 08:45:36.246717
Model ind 665 epoch 1400 batch: 500 avg loss -2.834946 avg loss no lamb -2.834946 time 2020-06-27 08:45:47.244225
Model ind 665 epoch 1400 batch: 600 avg loss -2.892675 avg loss no lamb -2.892675 time 2020-06-27 08:45:58.306686
Model ind 665 epoch 1400 batch: 700 avg loss -2.823056 avg loss no lamb -2.823056 time 2020-06-27 08:46:09.314952
Model ind 665 epoch 1400 batch: 800 avg loss -2.857517 avg loss no lamb -2.857517 time 2020-06-27 08:46:20.330943
last batch sz 10
Pre: time 2020-06-27 08:46:34.451564: 
 	std: 0.0029048827
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.9784, 0.9718, 0.9784, 0.974]
	train_accs: [0.98078334, 0.9798667, 0.97438335, 0.98075, 0.97616667]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.97634
	best: 0.9791

Starting e_i: 1401
Model ind 665 epoch 1401 batch: 0 avg loss -2.968515 avg loss no lamb -2.968515 time 2020-06-27 08:46:36.854007
Model ind 665 epoch 1401 batch: 100 avg loss -2.938440 avg loss no lamb -2.938440 time 2020-06-27 08:46:48.048798
Model ind 665 epoch 1401 batch: 200 avg loss -2.904013 avg loss no lamb -2.904013 time 2020-06-27 08:46:58.993665
Model ind 665 epoch 1401 batch: 300 avg loss -2.854024 avg loss no lamb -2.854024 time 2020-06-27 08:47:09.997212
Model ind 665 epoch 1401 batch: 400 avg loss -2.849647 avg loss no lamb -2.849647 time 2020-06-27 08:47:21.075013
Model ind 665 epoch 1401 batch: 500 avg loss -2.870907 avg loss no lamb -2.870907 time 2020-06-27 08:47:32.087640
Model ind 665 epoch 1401 batch: 600 avg loss -2.880407 avg loss no lamb -2.880407 time 2020-06-27 08:47:43.101548
Model ind 665 epoch 1401 batch: 700 avg loss -2.676702 avg loss no lamb -2.676702 time 2020-06-27 08:47:54.049119
Model ind 665 epoch 1401 batch: 800 avg loss -2.878254 avg loss no lamb -2.878254 time 2020-06-27 08:48:05.323195
last batch sz 10
Pre: time 2020-06-27 08:48:20.148351: 
 	std: 0.0029411449
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9808, 0.9739, 0.9809, 0.9762]
	train_accs: [0.9820333, 0.9815, 0.97575, 0.98195, 0.97753334]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97854006
	best: 0.9809

Starting e_i: 1402
Model ind 665 epoch 1402 batch: 0 avg loss -2.925450 avg loss no lamb -2.925450 time 2020-06-27 08:48:21.414327
Model ind 665 epoch 1402 batch: 100 avg loss -2.903808 avg loss no lamb -2.903808 time 2020-06-27 08:48:32.637572
Model ind 665 epoch 1402 batch: 200 avg loss -2.876333 avg loss no lamb -2.876333 time 2020-06-27 08:48:43.610589
Model ind 665 epoch 1402 batch: 300 avg loss -2.849534 avg loss no lamb -2.849534 time 2020-06-27 08:48:54.445308
Model ind 665 epoch 1402 batch: 400 avg loss -2.783085 avg loss no lamb -2.783085 time 2020-06-27 08:49:05.477120
Model ind 665 epoch 1402 batch: 500 avg loss -2.879991 avg loss no lamb -2.879991 time 2020-06-27 08:49:16.452800
Model ind 665 epoch 1402 batch: 600 avg loss -2.887991 avg loss no lamb -2.887991 time 2020-06-27 08:49:27.518169
Model ind 665 epoch 1402 batch: 700 avg loss -2.759843 avg loss no lamb -2.759843 time 2020-06-27 08:49:38.682646
Model ind 665 epoch 1402 batch: 800 avg loss -2.907205 avg loss no lamb -2.907205 time 2020-06-27 08:49:49.860380
last batch sz 10
Pre: time 2020-06-27 08:50:04.190980: 
 	std: 0.0026672874
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9795, 0.974, 0.9805, 0.9755]
	train_accs: [0.9820667, 0.98078334, 0.97575, 0.9821, 0.9770167]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97793996
	best: 0.9805

Starting e_i: 1403
Model ind 665 epoch 1403 batch: 0 avg loss -2.978525 avg loss no lamb -2.978525 time 2020-06-27 08:50:05.458750
Model ind 665 epoch 1403 batch: 100 avg loss -2.914990 avg loss no lamb -2.914990 time 2020-06-27 08:50:16.460139
Model ind 665 epoch 1403 batch: 200 avg loss -2.909405 avg loss no lamb -2.909405 time 2020-06-27 08:50:27.730471
Model ind 665 epoch 1403 batch: 300 avg loss -2.865644 avg loss no lamb -2.865644 time 2020-06-27 08:50:38.871701
Model ind 665 epoch 1403 batch: 400 avg loss -2.780969 avg loss no lamb -2.780969 time 2020-06-27 08:50:49.785941
Model ind 665 epoch 1403 batch: 500 avg loss -2.829367 avg loss no lamb -2.829367 time 2020-06-27 08:51:00.694361
Model ind 665 epoch 1403 batch: 600 avg loss -2.924878 avg loss no lamb -2.924878 time 2020-06-27 08:51:11.546337
Model ind 665 epoch 1403 batch: 700 avg loss -2.822781 avg loss no lamb -2.822781 time 2020-06-27 08:51:22.262448
Model ind 665 epoch 1403 batch: 800 avg loss -2.875546 avg loss no lamb -2.875546 time 2020-06-27 08:51:33.180508
last batch sz 10
Pre: time 2020-06-27 08:51:47.587884: 
 	std: 0.003303594
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9803, 0.9729, 0.9799, 0.9736]
	train_accs: [0.9816167, 0.9815, 0.97571665, 0.9818, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97727996
	best: 0.9799

Starting e_i: 1404
Model ind 665 epoch 1404 batch: 0 avg loss -2.909207 avg loss no lamb -2.909207 time 2020-06-27 08:51:49.003835
Model ind 665 epoch 1404 batch: 100 avg loss -2.949365 avg loss no lamb -2.949365 time 2020-06-27 08:52:00.101059
Model ind 665 epoch 1404 batch: 200 avg loss -2.900653 avg loss no lamb -2.900653 time 2020-06-27 08:52:11.276603
Model ind 665 epoch 1404 batch: 300 avg loss -2.894040 avg loss no lamb -2.894040 time 2020-06-27 08:52:22.242173
Model ind 665 epoch 1404 batch: 400 avg loss -2.797176 avg loss no lamb -2.797176 time 2020-06-27 08:52:33.228301
Model ind 665 epoch 1404 batch: 500 avg loss -2.879124 avg loss no lamb -2.879124 time 2020-06-27 08:52:44.098407
Model ind 665 epoch 1404 batch: 600 avg loss -2.927600 avg loss no lamb -2.927600 time 2020-06-27 08:52:55.060124
Model ind 665 epoch 1404 batch: 700 avg loss -2.799703 avg loss no lamb -2.799703 time 2020-06-27 08:53:06.214965
Model ind 665 epoch 1404 batch: 800 avg loss -2.885465 avg loss no lamb -2.885465 time 2020-06-27 08:53:17.321520
last batch sz 10
Pre: time 2020-06-27 08:53:31.679017: 
 	std: 0.0030662166
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9797, 0.973, 0.9799, 0.9741]
	train_accs: [0.9813333, 0.98048335, 0.9752, 0.98143333, 0.97615]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97728
	best: 0.9799

Starting e_i: 1405
Model ind 665 epoch 1405 batch: 0 avg loss -2.950060 avg loss no lamb -2.950060 time 2020-06-27 08:53:32.968776
Model ind 665 epoch 1405 batch: 100 avg loss -2.919083 avg loss no lamb -2.919083 time 2020-06-27 08:53:43.929201
Model ind 665 epoch 1405 batch: 200 avg loss -2.848831 avg loss no lamb -2.848831 time 2020-06-27 08:53:55.065075
Model ind 665 epoch 1405 batch: 300 avg loss -2.851044 avg loss no lamb -2.851044 time 2020-06-27 08:54:06.151581
Model ind 665 epoch 1405 batch: 400 avg loss -2.812548 avg loss no lamb -2.812548 time 2020-06-27 08:54:17.178596
Model ind 665 epoch 1405 batch: 500 avg loss -2.861629 avg loss no lamb -2.861629 time 2020-06-27 08:54:28.078353
Model ind 665 epoch 1405 batch: 600 avg loss -2.934678 avg loss no lamb -2.934678 time 2020-06-27 08:54:39.047614
Model ind 665 epoch 1405 batch: 700 avg loss -2.791041 avg loss no lamb -2.791041 time 2020-06-27 08:54:50.032706
Model ind 665 epoch 1405 batch: 800 avg loss -2.879483 avg loss no lamb -2.879483 time 2020-06-27 08:55:01.160858
last batch sz 10
Pre: time 2020-06-27 08:55:15.382773: 
 	std: 0.0034794183
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.979, 0.9719, 0.9801, 0.9732]
	train_accs: [0.9813333, 0.98013335, 0.9746, 0.9813833, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97676
	best: 0.9801

Starting e_i: 1406
Model ind 665 epoch 1406 batch: 0 avg loss -2.917461 avg loss no lamb -2.917461 time 2020-06-27 08:55:16.812349
Model ind 665 epoch 1406 batch: 100 avg loss -2.908928 avg loss no lamb -2.908928 time 2020-06-27 08:55:27.902939
Model ind 665 epoch 1406 batch: 200 avg loss -2.881243 avg loss no lamb -2.881243 time 2020-06-27 08:55:38.959188
Model ind 665 epoch 1406 batch: 300 avg loss -2.954808 avg loss no lamb -2.954808 time 2020-06-27 08:55:50.035534
Model ind 665 epoch 1406 batch: 400 avg loss -2.814165 avg loss no lamb -2.814165 time 2020-06-27 08:56:00.998096
Model ind 665 epoch 1406 batch: 500 avg loss -2.902210 avg loss no lamb -2.902210 time 2020-06-27 08:56:11.902522
Model ind 665 epoch 1406 batch: 600 avg loss -2.820673 avg loss no lamb -2.820673 time 2020-06-27 08:56:22.860234
Model ind 665 epoch 1406 batch: 700 avg loss -2.827389 avg loss no lamb -2.827389 time 2020-06-27 08:56:33.954628
Model ind 665 epoch 1406 batch: 800 avg loss -2.828779 avg loss no lamb -2.828779 time 2020-06-27 08:56:44.995419
last batch sz 10
Pre: time 2020-06-27 08:56:59.542525: 
 	std: 0.0025539107
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9793, 0.974, 0.9798, 0.9747]
	train_accs: [0.98095, 0.9806833, 0.97566664, 0.98113334, 0.97651666]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97746
	best: 0.9798

Starting e_i: 1407
Model ind 665 epoch 1407 batch: 0 avg loss -2.957185 avg loss no lamb -2.957185 time 2020-06-27 08:57:00.881683
Model ind 665 epoch 1407 batch: 100 avg loss -2.892796 avg loss no lamb -2.892796 time 2020-06-27 08:57:12.140213
Model ind 665 epoch 1407 batch: 200 avg loss -2.868045 avg loss no lamb -2.868045 time 2020-06-27 08:57:23.051611
Model ind 665 epoch 1407 batch: 300 avg loss -2.790518 avg loss no lamb -2.790518 time 2020-06-27 08:57:34.039396
Model ind 665 epoch 1407 batch: 400 avg loss -2.816168 avg loss no lamb -2.816168 time 2020-06-27 08:57:45.049549
Model ind 665 epoch 1407 batch: 500 avg loss -2.836159 avg loss no lamb -2.836159 time 2020-06-27 08:57:56.026563
Model ind 665 epoch 1407 batch: 600 avg loss -2.892513 avg loss no lamb -2.892513 time 2020-06-27 08:58:07.055129
Model ind 665 epoch 1407 batch: 700 avg loss -2.810495 avg loss no lamb -2.810495 time 2020-06-27 08:58:18.269502
Model ind 665 epoch 1407 batch: 800 avg loss -2.861970 avg loss no lamb -2.861970 time 2020-06-27 08:58:29.252285
last batch sz 10
Pre: time 2020-06-27 08:58:43.466644: 
 	std: 0.0028899878
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9797, 0.9733, 0.9805, 0.9757]
	train_accs: [0.98143333, 0.98043334, 0.97536665, 0.98148334, 0.9769833]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9779
	best: 0.9805

Starting e_i: 1408
Model ind 665 epoch 1408 batch: 0 avg loss -2.967349 avg loss no lamb -2.967349 time 2020-06-27 08:58:44.759222
Model ind 665 epoch 1408 batch: 100 avg loss -2.889407 avg loss no lamb -2.889407 time 2020-06-27 08:58:55.644652
Model ind 665 epoch 1408 batch: 200 avg loss -2.876337 avg loss no lamb -2.876337 time 2020-06-27 08:59:06.573991
Model ind 665 epoch 1408 batch: 300 avg loss -2.921911 avg loss no lamb -2.921911 time 2020-06-27 08:59:17.522712
Model ind 665 epoch 1408 batch: 400 avg loss -2.776362 avg loss no lamb -2.776362 time 2020-06-27 08:59:28.500844
Model ind 665 epoch 1408 batch: 500 avg loss -2.899990 avg loss no lamb -2.899990 time 2020-06-27 08:59:39.579121
Model ind 665 epoch 1408 batch: 600 avg loss -2.878411 avg loss no lamb -2.878411 time 2020-06-27 08:59:50.504198
Model ind 665 epoch 1408 batch: 700 avg loss -2.813968 avg loss no lamb -2.813968 time 2020-06-27 09:00:01.560766
Model ind 665 epoch 1408 batch: 800 avg loss -2.914310 avg loss no lamb -2.914310 time 2020-06-27 09:00:12.562681
last batch sz 10
Pre: time 2020-06-27 09:00:27.092255: 
 	std: 0.0026732723
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9798, 0.9756, 0.9817, 0.9762]
	train_accs: [0.98193336, 0.9811, 0.9759, 0.98188335, 0.97723335]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97903997
	best: 0.9819

Starting e_i: 1409
Model ind 665 epoch 1409 batch: 0 avg loss -2.947395 avg loss no lamb -2.947395 time 2020-06-27 09:00:28.298405
Model ind 665 epoch 1409 batch: 100 avg loss -2.902005 avg loss no lamb -2.902005 time 2020-06-27 09:00:39.355873
Model ind 665 epoch 1409 batch: 200 avg loss -2.906915 avg loss no lamb -2.906915 time 2020-06-27 09:00:50.192535
Model ind 665 epoch 1409 batch: 300 avg loss -2.891247 avg loss no lamb -2.891247 time 2020-06-27 09:01:01.385608
Model ind 665 epoch 1409 batch: 400 avg loss -2.741847 avg loss no lamb -2.741847 time 2020-06-27 09:01:12.241994
Model ind 665 epoch 1409 batch: 500 avg loss -2.825083 avg loss no lamb -2.825083 time 2020-06-27 09:01:23.045219
Model ind 665 epoch 1409 batch: 600 avg loss -2.862921 avg loss no lamb -2.862921 time 2020-06-27 09:01:33.987062
Model ind 665 epoch 1409 batch: 700 avg loss -2.755832 avg loss no lamb -2.755832 time 2020-06-27 09:01:44.886713
Model ind 665 epoch 1409 batch: 800 avg loss -2.878799 avg loss no lamb -2.878799 time 2020-06-27 09:01:55.835757
last batch sz 10
Pre: time 2020-06-27 09:02:10.125937: 
 	std: 0.002616556
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9801, 0.9746, 0.9799, 0.9751]
	train_accs: [0.9811, 0.98071665, 0.9755667, 0.98113334, 0.9765667]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97804004
	best: 0.9799

Starting e_i: 1410
Model ind 665 epoch 1410 batch: 0 avg loss -2.932251 avg loss no lamb -2.932251 time 2020-06-27 09:02:11.402758
Model ind 665 epoch 1410 batch: 100 avg loss -2.900998 avg loss no lamb -2.900998 time 2020-06-27 09:02:22.466750
Model ind 665 epoch 1410 batch: 200 avg loss -2.905555 avg loss no lamb -2.905555 time 2020-06-27 09:02:33.465875
Model ind 665 epoch 1410 batch: 300 avg loss -2.844803 avg loss no lamb -2.844803 time 2020-06-27 09:02:44.350738
Model ind 665 epoch 1410 batch: 400 avg loss -2.811010 avg loss no lamb -2.811010 time 2020-06-27 09:02:55.062516
Model ind 665 epoch 1410 batch: 500 avg loss -2.863641 avg loss no lamb -2.863641 time 2020-06-27 09:03:06.228396
Model ind 665 epoch 1410 batch: 600 avg loss -2.892170 avg loss no lamb -2.892170 time 2020-06-27 09:03:17.558566
Model ind 665 epoch 1410 batch: 700 avg loss -2.761255 avg loss no lamb -2.761255 time 2020-06-27 09:03:28.748377
Model ind 665 epoch 1410 batch: 800 avg loss -2.879733 avg loss no lamb -2.879733 time 2020-06-27 09:03:39.663272
last batch sz 10
Pre: time 2020-06-27 09:03:54.081010: 
 	std: 0.002712644
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.9807, 0.9741, 0.9801, 0.9759]
	train_accs: [0.98121667, 0.9809167, 0.97535, 0.98118335, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97824
	best: 0.9804

Starting e_i: 1411
Model ind 665 epoch 1411 batch: 0 avg loss -2.904376 avg loss no lamb -2.904376 time 2020-06-27 09:03:56.599335
Model ind 665 epoch 1411 batch: 100 avg loss -2.938567 avg loss no lamb -2.938567 time 2020-06-27 09:04:07.690554
Model ind 665 epoch 1411 batch: 200 avg loss -2.873027 avg loss no lamb -2.873027 time 2020-06-27 09:04:18.479587
Model ind 665 epoch 1411 batch: 300 avg loss -2.898500 avg loss no lamb -2.898500 time 2020-06-27 09:04:29.340915
Model ind 665 epoch 1411 batch: 400 avg loss -2.796497 avg loss no lamb -2.796497 time 2020-06-27 09:04:40.009949
Model ind 665 epoch 1411 batch: 500 avg loss -2.835176 avg loss no lamb -2.835176 time 2020-06-27 09:04:51.181540
Model ind 665 epoch 1411 batch: 600 avg loss -2.893981 avg loss no lamb -2.893981 time 2020-06-27 09:05:02.350925
Model ind 665 epoch 1411 batch: 700 avg loss -2.730371 avg loss no lamb -2.730371 time 2020-06-27 09:05:13.459131
Model ind 665 epoch 1411 batch: 800 avg loss -2.905102 avg loss no lamb -2.905102 time 2020-06-27 09:05:24.371393
last batch sz 10
Pre: time 2020-06-27 09:05:38.977733: 
 	std: 0.0026399943
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9785, 0.9731, 0.9787, 0.9734]
	train_accs: [0.9803, 0.97971666, 0.97475, 0.9805, 0.9756]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97648
	best: 0.9787

Starting e_i: 1412
Model ind 665 epoch 1412 batch: 0 avg loss -2.982831 avg loss no lamb -2.982831 time 2020-06-27 09:05:40.280671
Model ind 665 epoch 1412 batch: 100 avg loss -2.906064 avg loss no lamb -2.906064 time 2020-06-27 09:05:51.423954
Model ind 665 epoch 1412 batch: 200 avg loss -2.898993 avg loss no lamb -2.898993 time 2020-06-27 09:06:02.540067
Model ind 665 epoch 1412 batch: 300 avg loss -2.861394 avg loss no lamb -2.861394 time 2020-06-27 09:06:13.482369
Model ind 665 epoch 1412 batch: 400 avg loss -2.863042 avg loss no lamb -2.863042 time 2020-06-27 09:06:24.479318
Model ind 665 epoch 1412 batch: 500 avg loss -2.836591 avg loss no lamb -2.836591 time 2020-06-27 09:06:35.279512
Model ind 665 epoch 1412 batch: 600 avg loss -2.876612 avg loss no lamb -2.876612 time 2020-06-27 09:06:46.273397
Model ind 665 epoch 1412 batch: 700 avg loss -2.804344 avg loss no lamb -2.804344 time 2020-06-27 09:06:57.473876
Model ind 665 epoch 1412 batch: 800 avg loss -2.827248 avg loss no lamb -2.827248 time 2020-06-27 09:07:08.645211
last batch sz 10
Pre: time 2020-06-27 09:07:23.150068: 
 	std: 0.002647728
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9787, 0.9748, 0.98, 0.974]
	train_accs: [0.98146665, 0.9806833, 0.9763167, 0.98115, 0.9765]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97756004
	best: 0.9803

Starting e_i: 1413
Model ind 665 epoch 1413 batch: 0 avg loss -2.954441 avg loss no lamb -2.954441 time 2020-06-27 09:07:24.388472
Model ind 665 epoch 1413 batch: 100 avg loss -2.909650 avg loss no lamb -2.909650 time 2020-06-27 09:07:35.352145
Model ind 665 epoch 1413 batch: 200 avg loss -2.872560 avg loss no lamb -2.872560 time 2020-06-27 09:07:46.138338
Model ind 665 epoch 1413 batch: 300 avg loss -2.858715 avg loss no lamb -2.858715 time 2020-06-27 09:07:56.994014
Model ind 665 epoch 1413 batch: 400 avg loss -2.847894 avg loss no lamb -2.847894 time 2020-06-27 09:08:07.868744
Model ind 665 epoch 1413 batch: 500 avg loss -2.857200 avg loss no lamb -2.857200 time 2020-06-27 09:08:19.038026
Model ind 665 epoch 1413 batch: 600 avg loss -2.878754 avg loss no lamb -2.878754 time 2020-06-27 09:08:29.912765
Model ind 665 epoch 1413 batch: 700 avg loss -2.739505 avg loss no lamb -2.739505 time 2020-06-27 09:08:41.087363
Model ind 665 epoch 1413 batch: 800 avg loss -2.910667 avg loss no lamb -2.910667 time 2020-06-27 09:08:52.067510
last batch sz 10
Pre: time 2020-06-27 09:09:06.215105: 
 	std: 0.0023669316
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.98, 0.9757, 0.981, 0.9761]
	train_accs: [0.982, 0.98121667, 0.97643334, 0.9819, 0.97758335]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97875994
	best: 0.981

Starting e_i: 1414
Model ind 665 epoch 1414 batch: 0 avg loss -2.939128 avg loss no lamb -2.939128 time 2020-06-27 09:09:07.431343
Model ind 665 epoch 1414 batch: 100 avg loss -2.911249 avg loss no lamb -2.911249 time 2020-06-27 09:09:18.550741
Model ind 665 epoch 1414 batch: 200 avg loss -2.868994 avg loss no lamb -2.868994 time 2020-06-27 09:09:29.458493
Model ind 665 epoch 1414 batch: 300 avg loss -2.905104 avg loss no lamb -2.905104 time 2020-06-27 09:09:40.476443
Model ind 665 epoch 1414 batch: 400 avg loss -2.831816 avg loss no lamb -2.831816 time 2020-06-27 09:09:51.541763
Model ind 665 epoch 1414 batch: 500 avg loss -2.819614 avg loss no lamb -2.819614 time 2020-06-27 09:10:02.490059
Model ind 665 epoch 1414 batch: 600 avg loss -2.890093 avg loss no lamb -2.890093 time 2020-06-27 09:10:13.551041
Model ind 665 epoch 1414 batch: 700 avg loss -2.799084 avg loss no lamb -2.799084 time 2020-06-27 09:10:24.497422
Model ind 665 epoch 1414 batch: 800 avg loss -2.885864 avg loss no lamb -2.885864 time 2020-06-27 09:10:35.530266
last batch sz 10
Pre: time 2020-06-27 09:10:50.049202: 
 	std: 0.0029342105
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9805, 0.9741, 0.9805, 0.9749]
	train_accs: [0.98146665, 0.98083335, 0.9758, 0.98156667, 0.97676665]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97808
	best: 0.9805

Starting e_i: 1415
Model ind 665 epoch 1415 batch: 0 avg loss -2.958322 avg loss no lamb -2.958322 time 2020-06-27 09:10:51.268468
Model ind 665 epoch 1415 batch: 100 avg loss -2.884112 avg loss no lamb -2.884112 time 2020-06-27 09:11:02.179040
Model ind 665 epoch 1415 batch: 200 avg loss -2.802250 avg loss no lamb -2.802250 time 2020-06-27 09:11:13.262899
Model ind 665 epoch 1415 batch: 300 avg loss -2.860686 avg loss no lamb -2.860686 time 2020-06-27 09:11:24.309988
Model ind 665 epoch 1415 batch: 400 avg loss -2.852240 avg loss no lamb -2.852240 time 2020-06-27 09:11:35.508568
Model ind 665 epoch 1415 batch: 500 avg loss -2.865903 avg loss no lamb -2.865903 time 2020-06-27 09:11:46.447049
Model ind 665 epoch 1415 batch: 600 avg loss -2.905857 avg loss no lamb -2.905857 time 2020-06-27 09:11:57.498528
Model ind 665 epoch 1415 batch: 700 avg loss -2.853976 avg loss no lamb -2.853976 time 2020-06-27 09:12:08.559131
Model ind 665 epoch 1415 batch: 800 avg loss -2.859027 avg loss no lamb -2.859027 time 2020-06-27 09:12:19.450487
last batch sz 10
Pre: time 2020-06-27 09:12:33.695881: 
 	std: 0.0023346134
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9803, 0.9762, 0.9812, 0.9765]
	train_accs: [0.9819667, 0.98113334, 0.97645, 0.9818, 0.97728336]
	best_train_sub_head: 0
	worst: 0.9762
	avg: 0.97915995
	best: 0.9816

Starting e_i: 1416
Model ind 665 epoch 1416 batch: 0 avg loss -2.988465 avg loss no lamb -2.988465 time 2020-06-27 09:12:34.948488
Model ind 665 epoch 1416 batch: 100 avg loss -2.945145 avg loss no lamb -2.945145 time 2020-06-27 09:12:45.527917
Model ind 665 epoch 1416 batch: 200 avg loss -2.933046 avg loss no lamb -2.933046 time 2020-06-27 09:12:56.426404
Model ind 665 epoch 1416 batch: 300 avg loss -2.846630 avg loss no lamb -2.846630 time 2020-06-27 09:13:07.494865
Model ind 665 epoch 1416 batch: 400 avg loss -2.829109 avg loss no lamb -2.829109 time 2020-06-27 09:13:18.532833
Model ind 665 epoch 1416 batch: 500 avg loss -2.882333 avg loss no lamb -2.882333 time 2020-06-27 09:13:29.522907
Model ind 665 epoch 1416 batch: 600 avg loss -2.891444 avg loss no lamb -2.891444 time 2020-06-27 09:13:40.349362
Model ind 665 epoch 1416 batch: 700 avg loss -2.826338 avg loss no lamb -2.826338 time 2020-06-27 09:13:51.145176
Model ind 665 epoch 1416 batch: 800 avg loss -2.946297 avg loss no lamb -2.946297 time 2020-06-27 09:14:01.937061
last batch sz 10
Pre: time 2020-06-27 09:14:16.244656: 
 	std: 0.0031403115
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9801, 0.9738, 0.9809, 0.9746]
	train_accs: [0.98125, 0.9809167, 0.97508335, 0.9812833, 0.97585]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97802
	best: 0.9809

Starting e_i: 1417
Model ind 665 epoch 1417 batch: 0 avg loss -2.894768 avg loss no lamb -2.894768 time 2020-06-27 09:14:17.572288
Model ind 665 epoch 1417 batch: 100 avg loss -2.904814 avg loss no lamb -2.904814 time 2020-06-27 09:14:28.467999
Model ind 665 epoch 1417 batch: 200 avg loss -2.864473 avg loss no lamb -2.864473 time 2020-06-27 09:14:39.197163
Model ind 665 epoch 1417 batch: 300 avg loss -2.882018 avg loss no lamb -2.882018 time 2020-06-27 09:14:49.973151
Model ind 665 epoch 1417 batch: 400 avg loss -2.803926 avg loss no lamb -2.803926 time 2020-06-27 09:15:00.869417
Model ind 665 epoch 1417 batch: 500 avg loss -2.865995 avg loss no lamb -2.865995 time 2020-06-27 09:15:11.702262
Model ind 665 epoch 1417 batch: 600 avg loss -2.888607 avg loss no lamb -2.888607 time 2020-06-27 09:15:22.473455
Model ind 665 epoch 1417 batch: 700 avg loss -2.807967 avg loss no lamb -2.807967 time 2020-06-27 09:15:33.281998
Model ind 665 epoch 1417 batch: 800 avg loss -2.904464 avg loss no lamb -2.904464 time 2020-06-27 09:15:44.031376
last batch sz 10
Pre: time 2020-06-27 09:15:57.900350: 
 	std: 0.0033909318
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9809, 0.974, 0.9812, 0.9746]
	train_accs: [0.98195, 0.9813833, 0.9758833, 0.982, 0.9766667]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97844
	best: 0.9812

Starting e_i: 1418
Model ind 665 epoch 1418 batch: 0 avg loss -2.991445 avg loss no lamb -2.991445 time 2020-06-27 09:15:59.103207
Model ind 665 epoch 1418 batch: 100 avg loss -2.928958 avg loss no lamb -2.928958 time 2020-06-27 09:16:10.156999
Model ind 665 epoch 1418 batch: 200 avg loss -2.852567 avg loss no lamb -2.852567 time 2020-06-27 09:16:20.868683
Model ind 665 epoch 1418 batch: 300 avg loss -2.904676 avg loss no lamb -2.904676 time 2020-06-27 09:16:31.643332
Model ind 665 epoch 1418 batch: 400 avg loss -2.794011 avg loss no lamb -2.794011 time 2020-06-27 09:16:42.662373
Model ind 665 epoch 1418 batch: 500 avg loss -2.851975 avg loss no lamb -2.851975 time 2020-06-27 09:16:53.593665
Model ind 665 epoch 1418 batch: 600 avg loss -2.899380 avg loss no lamb -2.899380 time 2020-06-27 09:17:04.580850
Model ind 665 epoch 1418 batch: 700 avg loss -2.797106 avg loss no lamb -2.797106 time 2020-06-27 09:17:15.571452
Model ind 665 epoch 1418 batch: 800 avg loss -2.872137 avg loss no lamb -2.872137 time 2020-06-27 09:17:26.639274
last batch sz 10
Pre: time 2020-06-27 09:17:40.498540: 
 	std: 0.0024814513
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9794, 0.9752, 0.981, 0.9755]
	train_accs: [0.98151666, 0.9808667, 0.97635, 0.98188335, 0.97716665]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97831994
	best: 0.981

Starting e_i: 1419
Model ind 665 epoch 1419 batch: 0 avg loss -2.934221 avg loss no lamb -2.934221 time 2020-06-27 09:17:41.981378
Model ind 665 epoch 1419 batch: 100 avg loss -2.905702 avg loss no lamb -2.905702 time 2020-06-27 09:17:52.798858
Model ind 665 epoch 1419 batch: 200 avg loss -2.889174 avg loss no lamb -2.889174 time 2020-06-27 09:18:03.679488
Model ind 665 epoch 1419 batch: 300 avg loss -2.892762 avg loss no lamb -2.892762 time 2020-06-27 09:18:14.594792
Model ind 665 epoch 1419 batch: 400 avg loss -2.818348 avg loss no lamb -2.818348 time 2020-06-27 09:18:25.594414
Model ind 665 epoch 1419 batch: 500 avg loss -2.866390 avg loss no lamb -2.866390 time 2020-06-27 09:18:36.466641
Model ind 665 epoch 1419 batch: 600 avg loss -2.929445 avg loss no lamb -2.929445 time 2020-06-27 09:18:47.311037
Model ind 665 epoch 1419 batch: 700 avg loss -2.819872 avg loss no lamb -2.819872 time 2020-06-27 09:18:58.301758
Model ind 665 epoch 1419 batch: 800 avg loss -2.849119 avg loss no lamb -2.849119 time 2020-06-27 09:19:09.365640
last batch sz 10
Pre: time 2020-06-27 09:19:23.733962: 
 	std: 0.0039569694
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9807, 0.9724, 0.9804, 0.9732]
	train_accs: [0.98148334, 0.98083335, 0.97513336, 0.9813, 0.97613335]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97762
	best: 0.9814

Starting e_i: 1420
Model ind 665 epoch 1420 batch: 0 avg loss -2.932374 avg loss no lamb -2.932374 time 2020-06-27 09:19:24.983344
Model ind 665 epoch 1420 batch: 100 avg loss -2.935385 avg loss no lamb -2.935385 time 2020-06-27 09:19:35.982210
Model ind 665 epoch 1420 batch: 200 avg loss -2.932990 avg loss no lamb -2.932990 time 2020-06-27 09:19:46.569336
Model ind 665 epoch 1420 batch: 300 avg loss -2.866537 avg loss no lamb -2.866537 time 2020-06-27 09:19:57.375425
Model ind 665 epoch 1420 batch: 400 avg loss -2.798477 avg loss no lamb -2.798477 time 2020-06-27 09:20:08.288525
Model ind 665 epoch 1420 batch: 500 avg loss -2.841296 avg loss no lamb -2.841296 time 2020-06-27 09:20:19.386039
Model ind 665 epoch 1420 batch: 600 avg loss -2.855052 avg loss no lamb -2.855052 time 2020-06-27 09:20:30.453722
Model ind 665 epoch 1420 batch: 700 avg loss -2.838542 avg loss no lamb -2.838542 time 2020-06-27 09:20:41.276194
Model ind 665 epoch 1420 batch: 800 avg loss -2.833996 avg loss no lamb -2.833996 time 2020-06-27 09:20:51.884742
last batch sz 10
Pre: time 2020-06-27 09:21:05.981234: 
 	std: 0.0024481828
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9813, 0.9767, 0.9821, 0.9769]
	train_accs: [0.98211664, 0.98176664, 0.97676665, 0.98225, 0.97753334]
	best_train_sub_head: 3
	worst: 0.9767
	avg: 0.97978
	best: 0.9821

Starting e_i: 1421
Model ind 665 epoch 1421 batch: 0 avg loss -2.936347 avg loss no lamb -2.936347 time 2020-06-27 09:21:08.634832
Model ind 665 epoch 1421 batch: 100 avg loss -2.898388 avg loss no lamb -2.898388 time 2020-06-27 09:21:19.756223
Model ind 665 epoch 1421 batch: 200 avg loss -2.878905 avg loss no lamb -2.878905 time 2020-06-27 09:21:30.617399
Model ind 665 epoch 1421 batch: 300 avg loss -2.871296 avg loss no lamb -2.871296 time 2020-06-27 09:21:41.524383
Model ind 665 epoch 1421 batch: 400 avg loss -2.845988 avg loss no lamb -2.845988 time 2020-06-27 09:21:52.289597
Model ind 665 epoch 1421 batch: 500 avg loss -2.860152 avg loss no lamb -2.860152 time 2020-06-27 09:22:03.375934
Model ind 665 epoch 1421 batch: 600 avg loss -2.870816 avg loss no lamb -2.870816 time 2020-06-27 09:22:14.245847
Model ind 665 epoch 1421 batch: 700 avg loss -2.788359 avg loss no lamb -2.788359 time 2020-06-27 09:22:25.341092
Model ind 665 epoch 1421 batch: 800 avg loss -2.854771 avg loss no lamb -2.854771 time 2020-06-27 09:22:36.492128
last batch sz 10
Pre: time 2020-06-27 09:22:50.588638: 
 	std: 0.0030188558
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9808, 0.9746, 0.9816, 0.9753]
	train_accs: [0.98178333, 0.98146665, 0.9752333, 0.9821333, 0.97635]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97861993
	best: 0.9816

Starting e_i: 1422
Model ind 665 epoch 1422 batch: 0 avg loss -2.946726 avg loss no lamb -2.946726 time 2020-06-27 09:22:51.842016
Model ind 665 epoch 1422 batch: 100 avg loss -2.904830 avg loss no lamb -2.904830 time 2020-06-27 09:23:02.724153
Model ind 665 epoch 1422 batch: 200 avg loss -2.843674 avg loss no lamb -2.843674 time 2020-06-27 09:23:13.679236
Model ind 665 epoch 1422 batch: 300 avg loss -2.886827 avg loss no lamb -2.886827 time 2020-06-27 09:23:24.402583
Model ind 665 epoch 1422 batch: 400 avg loss -2.850614 avg loss no lamb -2.850614 time 2020-06-27 09:23:35.018855
Model ind 665 epoch 1422 batch: 500 avg loss -2.922389 avg loss no lamb -2.922389 time 2020-06-27 09:23:45.910565
Model ind 665 epoch 1422 batch: 600 avg loss -2.871690 avg loss no lamb -2.871690 time 2020-06-27 09:23:56.898889
Model ind 665 epoch 1422 batch: 700 avg loss -2.657612 avg loss no lamb -2.657612 time 2020-06-27 09:24:07.782663
Model ind 665 epoch 1422 batch: 800 avg loss -2.892520 avg loss no lamb -2.892520 time 2020-06-27 09:24:18.388453
last batch sz 10
Pre: time 2020-06-27 09:24:32.537192: 
 	std: 0.003220795
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9794, 0.9728, 0.9798, 0.9735]
	train_accs: [0.9813167, 0.9806167, 0.97531664, 0.98135, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97708
	best: 0.9798

Starting e_i: 1423
Model ind 665 epoch 1423 batch: 0 avg loss -2.950774 avg loss no lamb -2.950774 time 2020-06-27 09:24:33.753664
Model ind 665 epoch 1423 batch: 100 avg loss -2.928774 avg loss no lamb -2.928774 time 2020-06-27 09:24:44.653319
Model ind 665 epoch 1423 batch: 200 avg loss -2.920521 avg loss no lamb -2.920521 time 2020-06-27 09:24:55.668275
Model ind 665 epoch 1423 batch: 300 avg loss -2.868161 avg loss no lamb -2.868161 time 2020-06-27 09:25:06.639649
Model ind 665 epoch 1423 batch: 400 avg loss -2.782914 avg loss no lamb -2.782914 time 2020-06-27 09:25:17.544112
Model ind 665 epoch 1423 batch: 500 avg loss -2.862473 avg loss no lamb -2.862473 time 2020-06-27 09:25:28.435229
Model ind 665 epoch 1423 batch: 600 avg loss -2.820876 avg loss no lamb -2.820876 time 2020-06-27 09:25:39.463331
Model ind 665 epoch 1423 batch: 700 avg loss -2.790615 avg loss no lamb -2.790615 time 2020-06-27 09:25:50.360567
Model ind 665 epoch 1423 batch: 800 avg loss -2.889980 avg loss no lamb -2.889980 time 2020-06-27 09:26:01.354460
last batch sz 10
Pre: time 2020-06-27 09:26:15.517870: 
 	std: 0.0028182254
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9802, 0.9747, 0.9809, 0.9749]
	train_accs: [0.98158336, 0.9806833, 0.9759, 0.98153335, 0.9766]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97824
	best: 0.9805

Starting e_i: 1424
Model ind 665 epoch 1424 batch: 0 avg loss -2.930778 avg loss no lamb -2.930778 time 2020-06-27 09:26:16.746222
Model ind 665 epoch 1424 batch: 100 avg loss -2.953995 avg loss no lamb -2.953995 time 2020-06-27 09:26:27.635762
Model ind 665 epoch 1424 batch: 200 avg loss -2.851869 avg loss no lamb -2.851869 time 2020-06-27 09:26:38.334698
Model ind 665 epoch 1424 batch: 300 avg loss -2.940204 avg loss no lamb -2.940204 time 2020-06-27 09:26:49.160781
Model ind 665 epoch 1424 batch: 400 avg loss -2.811452 avg loss no lamb -2.811452 time 2020-06-27 09:26:59.878087
Model ind 665 epoch 1424 batch: 500 avg loss -2.869176 avg loss no lamb -2.869176 time 2020-06-27 09:27:10.801891
Model ind 665 epoch 1424 batch: 600 avg loss -2.889704 avg loss no lamb -2.889704 time 2020-06-27 09:27:21.837915
Model ind 665 epoch 1424 batch: 700 avg loss -2.782169 avg loss no lamb -2.782169 time 2020-06-27 09:27:32.633645
Model ind 665 epoch 1424 batch: 800 avg loss -2.872954 avg loss no lamb -2.872954 time 2020-06-27 09:27:43.443321
last batch sz 10
Pre: time 2020-06-27 09:27:57.441653: 
 	std: 0.0027375726
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9794, 0.9728, 0.9794, 0.975]
	train_accs: [0.98088336, 0.98041666, 0.9751667, 0.9813333, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97713995
	best: 0.9794

Starting e_i: 1425
Model ind 665 epoch 1425 batch: 0 avg loss -2.966949 avg loss no lamb -2.966949 time 2020-06-27 09:27:58.697098
Model ind 665 epoch 1425 batch: 100 avg loss -2.958460 avg loss no lamb -2.958460 time 2020-06-27 09:28:09.555349
Model ind 665 epoch 1425 batch: 200 avg loss -2.881366 avg loss no lamb -2.881366 time 2020-06-27 09:28:20.314938
Model ind 665 epoch 1425 batch: 300 avg loss -2.829780 avg loss no lamb -2.829780 time 2020-06-27 09:28:31.398934
Model ind 665 epoch 1425 batch: 400 avg loss -2.787648 avg loss no lamb -2.787648 time 2020-06-27 09:28:42.133303
Model ind 665 epoch 1425 batch: 500 avg loss -2.806294 avg loss no lamb -2.806294 time 2020-06-27 09:28:53.047330
Model ind 665 epoch 1425 batch: 600 avg loss -2.883733 avg loss no lamb -2.883733 time 2020-06-27 09:29:03.976931
Model ind 665 epoch 1425 batch: 700 avg loss -2.815309 avg loss no lamb -2.815309 time 2020-06-27 09:29:14.887005
Model ind 665 epoch 1425 batch: 800 avg loss -2.825291 avg loss no lamb -2.825291 time 2020-06-27 09:29:25.784047
last batch sz 10
Pre: time 2020-06-27 09:29:39.955646: 
 	std: 0.0034646732
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9796, 0.9721, 0.9802, 0.9735]
	train_accs: [0.98118335, 0.9804, 0.9734333, 0.9813167, 0.97545]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97700006
	best: 0.9802

Starting e_i: 1426
Model ind 665 epoch 1426 batch: 0 avg loss -2.961519 avg loss no lamb -2.961519 time 2020-06-27 09:29:41.184623
Model ind 665 epoch 1426 batch: 100 avg loss -2.934628 avg loss no lamb -2.934628 time 2020-06-27 09:29:52.097456
Model ind 665 epoch 1426 batch: 200 avg loss -2.882615 avg loss no lamb -2.882615 time 2020-06-27 09:30:03.062487
Model ind 665 epoch 1426 batch: 300 avg loss -2.859600 avg loss no lamb -2.859600 time 2020-06-27 09:30:14.068326
Model ind 665 epoch 1426 batch: 400 avg loss -2.784264 avg loss no lamb -2.784264 time 2020-06-27 09:30:24.874481
Model ind 665 epoch 1426 batch: 500 avg loss -2.883502 avg loss no lamb -2.883502 time 2020-06-27 09:30:35.673834
Model ind 665 epoch 1426 batch: 600 avg loss -2.903643 avg loss no lamb -2.903643 time 2020-06-27 09:30:46.644038
Model ind 665 epoch 1426 batch: 700 avg loss -2.749544 avg loss no lamb -2.749544 time 2020-06-27 09:30:57.709017
Model ind 665 epoch 1426 batch: 800 avg loss -2.816871 avg loss no lamb -2.816871 time 2020-06-27 09:31:09.021189
last batch sz 10
Pre: time 2020-06-27 09:31:23.203657: 
 	std: 0.0023852754
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9811, 0.9761, 0.9809, 0.9761]
	train_accs: [0.98191667, 0.98123336, 0.97645, 0.98191667, 0.9769]
	best_train_sub_head: 0
	worst: 0.9761
	avg: 0.97902
	best: 0.9809

Starting e_i: 1427
Model ind 665 epoch 1427 batch: 0 avg loss -2.950283 avg loss no lamb -2.950283 time 2020-06-27 09:31:24.475453
Model ind 665 epoch 1427 batch: 100 avg loss -2.910476 avg loss no lamb -2.910476 time 2020-06-27 09:31:35.243811
Model ind 665 epoch 1427 batch: 200 avg loss -2.842859 avg loss no lamb -2.842859 time 2020-06-27 09:31:46.219423
Model ind 665 epoch 1427 batch: 300 avg loss -2.888114 avg loss no lamb -2.888114 time 2020-06-27 09:31:57.145283
Model ind 665 epoch 1427 batch: 400 avg loss -2.868104 avg loss no lamb -2.868104 time 2020-06-27 09:32:08.050507
Model ind 665 epoch 1427 batch: 500 avg loss -2.873841 avg loss no lamb -2.873841 time 2020-06-27 09:32:18.867571
Model ind 665 epoch 1427 batch: 600 avg loss -2.929893 avg loss no lamb -2.929893 time 2020-06-27 09:32:29.782708
Model ind 665 epoch 1427 batch: 700 avg loss -2.766503 avg loss no lamb -2.766503 time 2020-06-27 09:32:40.668337
Model ind 665 epoch 1427 batch: 800 avg loss -2.914320 avg loss no lamb -2.914320 time 2020-06-27 09:32:51.433512
last batch sz 10
Pre: time 2020-06-27 09:33:05.882475: 
 	std: 0.0029680862
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9812, 0.975, 0.982, 0.9764]
	train_accs: [0.98185, 0.9805667, 0.97525, 0.98181665, 0.9766167]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97928
	best: 0.9818

Starting e_i: 1428
Model ind 665 epoch 1428 batch: 0 avg loss -2.939774 avg loss no lamb -2.939774 time 2020-06-27 09:33:07.097419
Model ind 665 epoch 1428 batch: 100 avg loss -2.945319 avg loss no lamb -2.945319 time 2020-06-27 09:33:18.261496
Model ind 665 epoch 1428 batch: 200 avg loss -2.869127 avg loss no lamb -2.869127 time 2020-06-27 09:33:29.179150
Model ind 665 epoch 1428 batch: 300 avg loss -2.874421 avg loss no lamb -2.874421 time 2020-06-27 09:33:39.744805
Model ind 665 epoch 1428 batch: 400 avg loss -2.787234 avg loss no lamb -2.787234 time 2020-06-27 09:33:50.682736
Model ind 665 epoch 1428 batch: 500 avg loss -2.788460 avg loss no lamb -2.788460 time 2020-06-27 09:34:01.767577
Model ind 665 epoch 1428 batch: 600 avg loss -2.887120 avg loss no lamb -2.887120 time 2020-06-27 09:34:12.760791
Model ind 665 epoch 1428 batch: 700 avg loss -2.811046 avg loss no lamb -2.811046 time 2020-06-27 09:34:23.516588
Model ind 665 epoch 1428 batch: 800 avg loss -2.810718 avg loss no lamb -2.810718 time 2020-06-27 09:34:34.199294
last batch sz 10
Pre: time 2020-06-27 09:34:48.324299: 
 	std: 0.0031512661
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9793, 0.973, 0.9801, 0.9736]
	train_accs: [0.9812833, 0.98055, 0.9745, 0.9813667, 0.97606665]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97714007
	best: 0.9801

Starting e_i: 1429
Model ind 665 epoch 1429 batch: 0 avg loss -2.985462 avg loss no lamb -2.985462 time 2020-06-27 09:34:49.650667
Model ind 665 epoch 1429 batch: 100 avg loss -2.888016 avg loss no lamb -2.888016 time 2020-06-27 09:35:00.591801
Model ind 665 epoch 1429 batch: 200 avg loss -2.884481 avg loss no lamb -2.884481 time 2020-06-27 09:35:11.305506
Model ind 665 epoch 1429 batch: 300 avg loss -2.851516 avg loss no lamb -2.851516 time 2020-06-27 09:35:21.800540
Model ind 665 epoch 1429 batch: 400 avg loss -2.824728 avg loss no lamb -2.824728 time 2020-06-27 09:35:32.834232
Model ind 665 epoch 1429 batch: 500 avg loss -2.871873 avg loss no lamb -2.871873 time 2020-06-27 09:35:43.789363
Model ind 665 epoch 1429 batch: 600 avg loss -2.858463 avg loss no lamb -2.858463 time 2020-06-27 09:35:54.570176
Model ind 665 epoch 1429 batch: 700 avg loss -2.784039 avg loss no lamb -2.784039 time 2020-06-27 09:36:05.402872
Model ind 665 epoch 1429 batch: 800 avg loss -2.950589 avg loss no lamb -2.950589 time 2020-06-27 09:36:16.305801
last batch sz 10
Pre: time 2020-06-27 09:36:30.744604: 
 	std: 0.0033337069
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9802, 0.9742, 0.9818, 0.9748]
	train_accs: [0.9816833, 0.9812, 0.976, 0.98175, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97852004
	best: 0.9818

Starting e_i: 1430
Model ind 665 epoch 1430 batch: 0 avg loss -2.971899 avg loss no lamb -2.971899 time 2020-06-27 09:36:32.026367
Model ind 665 epoch 1430 batch: 100 avg loss -2.963875 avg loss no lamb -2.963875 time 2020-06-27 09:36:42.795981
Model ind 665 epoch 1430 batch: 200 avg loss -2.922648 avg loss no lamb -2.922648 time 2020-06-27 09:36:53.644860
Model ind 665 epoch 1430 batch: 300 avg loss -2.889153 avg loss no lamb -2.889153 time 2020-06-27 09:37:04.416077
Model ind 665 epoch 1430 batch: 400 avg loss -2.859583 avg loss no lamb -2.859583 time 2020-06-27 09:37:15.465141
Model ind 665 epoch 1430 batch: 500 avg loss -2.877026 avg loss no lamb -2.877026 time 2020-06-27 09:37:26.378773
Model ind 665 epoch 1430 batch: 600 avg loss -2.842004 avg loss no lamb -2.842004 time 2020-06-27 09:37:37.262223
Model ind 665 epoch 1430 batch: 700 avg loss -2.802092 avg loss no lamb -2.802092 time 2020-06-27 09:37:47.892223
Model ind 665 epoch 1430 batch: 800 avg loss -2.899706 avg loss no lamb -2.899706 time 2020-06-27 09:37:58.744021
last batch sz 10
Pre: time 2020-06-27 09:38:13.042616: 
 	std: 0.0030822137
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9807, 0.974, 0.9808, 0.9751]
	train_accs: [0.98158336, 0.98141664, 0.97605, 0.9816, 0.9762333]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.9783
	best: 0.9808

Starting e_i: 1431
Model ind 665 epoch 1431 batch: 0 avg loss -2.928659 avg loss no lamb -2.928659 time 2020-06-27 09:38:15.473320
Model ind 665 epoch 1431 batch: 100 avg loss -2.928210 avg loss no lamb -2.928210 time 2020-06-27 09:38:26.315226
Model ind 665 epoch 1431 batch: 200 avg loss -2.867444 avg loss no lamb -2.867444 time 2020-06-27 09:38:37.071572
Model ind 665 epoch 1431 batch: 300 avg loss -2.883116 avg loss no lamb -2.883116 time 2020-06-27 09:38:47.546052
Model ind 665 epoch 1431 batch: 400 avg loss -2.813349 avg loss no lamb -2.813349 time 2020-06-27 09:38:58.317306
Model ind 665 epoch 1431 batch: 500 avg loss -2.818682 avg loss no lamb -2.818682 time 2020-06-27 09:39:09.063720
Model ind 665 epoch 1431 batch: 600 avg loss -2.921932 avg loss no lamb -2.921932 time 2020-06-27 09:39:19.824942
Model ind 665 epoch 1431 batch: 700 avg loss -2.829649 avg loss no lamb -2.829649 time 2020-06-27 09:39:30.852072
Model ind 665 epoch 1431 batch: 800 avg loss -2.838984 avg loss no lamb -2.838984 time 2020-06-27 09:39:41.711299
last batch sz 10
Pre: time 2020-06-27 09:39:56.101129: 
 	std: 0.002286832
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9795, 0.9802, 0.9748, 0.9793, 0.9753]
	train_accs: [0.9813, 0.98135, 0.97603333, 0.9811, 0.97681665]
	best_train_sub_head: 1
	worst: 0.9748
	avg: 0.97782004
	best: 0.9802

Starting e_i: 1432
Model ind 665 epoch 1432 batch: 0 avg loss -2.992611 avg loss no lamb -2.992611 time 2020-06-27 09:39:57.364281
Model ind 665 epoch 1432 batch: 100 avg loss -2.974238 avg loss no lamb -2.974238 time 2020-06-27 09:40:08.257678
Model ind 665 epoch 1432 batch: 200 avg loss -2.863681 avg loss no lamb -2.863681 time 2020-06-27 09:40:19.031516
Model ind 665 epoch 1432 batch: 300 avg loss -2.887928 avg loss no lamb -2.887928 time 2020-06-27 09:40:30.022038
Model ind 665 epoch 1432 batch: 400 avg loss -2.843845 avg loss no lamb -2.843845 time 2020-06-27 09:40:40.835239
Model ind 665 epoch 1432 batch: 500 avg loss -2.884210 avg loss no lamb -2.884210 time 2020-06-27 09:40:51.584824
Model ind 665 epoch 1432 batch: 600 avg loss -2.910736 avg loss no lamb -2.910736 time 2020-06-27 09:41:02.404380
Model ind 665 epoch 1432 batch: 700 avg loss -2.797687 avg loss no lamb -2.797687 time 2020-06-27 09:41:13.462910
Model ind 665 epoch 1432 batch: 800 avg loss -2.895396 avg loss no lamb -2.895396 time 2020-06-27 09:41:24.394518
last batch sz 10
Pre: time 2020-06-27 09:41:38.933781: 
 	std: 0.0028611992
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9804, 0.9741, 0.9807, 0.9755]
	train_accs: [0.9817333, 0.98115, 0.97601664, 0.9816, 0.97686666]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97826004
	best: 0.9806

Starting e_i: 1433
Model ind 665 epoch 1433 batch: 0 avg loss -2.951055 avg loss no lamb -2.951055 time 2020-06-27 09:41:40.236108
Model ind 665 epoch 1433 batch: 100 avg loss -2.893541 avg loss no lamb -2.893541 time 2020-06-27 09:41:51.106078
Model ind 665 epoch 1433 batch: 200 avg loss -2.903000 avg loss no lamb -2.903000 time 2020-06-27 09:42:02.388197
Model ind 665 epoch 1433 batch: 300 avg loss -2.908728 avg loss no lamb -2.908728 time 2020-06-27 09:42:13.480508
Model ind 665 epoch 1433 batch: 400 avg loss -2.791967 avg loss no lamb -2.791967 time 2020-06-27 09:42:24.447116
Model ind 665 epoch 1433 batch: 500 avg loss -2.828506 avg loss no lamb -2.828506 time 2020-06-27 09:42:35.563275
Model ind 665 epoch 1433 batch: 600 avg loss -2.888241 avg loss no lamb -2.888241 time 2020-06-27 09:42:46.359633
Model ind 665 epoch 1433 batch: 700 avg loss -2.837783 avg loss no lamb -2.837783 time 2020-06-27 09:42:57.233248
Model ind 665 epoch 1433 batch: 800 avg loss -2.865511 avg loss no lamb -2.865511 time 2020-06-27 09:43:08.132690
last batch sz 10
Pre: time 2020-06-27 09:43:22.351489: 
 	std: 0.0028993879
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9797, 0.9739, 0.9793, 0.9733]
	train_accs: [0.9809, 0.98076665, 0.97555, 0.9809833, 0.97578335]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97714007
	best: 0.9793

Starting e_i: 1434
Model ind 665 epoch 1434 batch: 0 avg loss -2.992967 avg loss no lamb -2.992967 time 2020-06-27 09:43:23.852527
Model ind 665 epoch 1434 batch: 100 avg loss -2.898676 avg loss no lamb -2.898676 time 2020-06-27 09:43:34.823518
Model ind 665 epoch 1434 batch: 200 avg loss -2.858386 avg loss no lamb -2.858386 time 2020-06-27 09:43:45.661541
Model ind 665 epoch 1434 batch: 300 avg loss -2.939407 avg loss no lamb -2.939407 time 2020-06-27 09:43:56.706906
Model ind 665 epoch 1434 batch: 400 avg loss -2.767250 avg loss no lamb -2.767250 time 2020-06-27 09:44:07.714718
Model ind 665 epoch 1434 batch: 500 avg loss -2.906950 avg loss no lamb -2.906950 time 2020-06-27 09:44:18.775744
Model ind 665 epoch 1434 batch: 600 avg loss -2.924897 avg loss no lamb -2.924897 time 2020-06-27 09:44:29.714505
Model ind 665 epoch 1434 batch: 700 avg loss -2.807038 avg loss no lamb -2.807038 time 2020-06-27 09:44:40.760918
Model ind 665 epoch 1434 batch: 800 avg loss -2.853914 avg loss no lamb -2.853914 time 2020-06-27 09:44:51.844756
last batch sz 10
Pre: time 2020-06-27 09:45:05.899920: 
 	std: 0.002642278
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9784, 0.9733, 0.9795, 0.9744]
	train_accs: [0.98111665, 0.9805667, 0.97548336, 0.98108333, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97701997
	best: 0.9795

Starting e_i: 1435
Model ind 665 epoch 1435 batch: 0 avg loss -2.970453 avg loss no lamb -2.970453 time 2020-06-27 09:45:07.146658
Model ind 665 epoch 1435 batch: 100 avg loss -2.920910 avg loss no lamb -2.920910 time 2020-06-27 09:45:17.887873
Model ind 665 epoch 1435 batch: 200 avg loss -2.885507 avg loss no lamb -2.885507 time 2020-06-27 09:45:28.844778
Model ind 665 epoch 1435 batch: 300 avg loss -2.868571 avg loss no lamb -2.868571 time 2020-06-27 09:45:39.638595
Model ind 665 epoch 1435 batch: 400 avg loss -2.838830 avg loss no lamb -2.838830 time 2020-06-27 09:45:50.437414
Model ind 665 epoch 1435 batch: 500 avg loss -2.861131 avg loss no lamb -2.861131 time 2020-06-27 09:46:01.406587
Model ind 665 epoch 1435 batch: 600 avg loss -2.912368 avg loss no lamb -2.912368 time 2020-06-27 09:46:12.346067
Model ind 665 epoch 1435 batch: 700 avg loss -2.790285 avg loss no lamb -2.790285 time 2020-06-27 09:46:23.248428
Model ind 665 epoch 1435 batch: 800 avg loss -2.896312 avg loss no lamb -2.896312 time 2020-06-27 09:46:34.139397
last batch sz 10
Pre: time 2020-06-27 09:46:48.146981: 
 	std: 0.0031410926
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9796, 0.9729, 0.9796, 0.9735]
	train_accs: [0.9812833, 0.98075, 0.97541666, 0.9813167, 0.9755]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97704
	best: 0.9796

Starting e_i: 1436
Model ind 665 epoch 1436 batch: 0 avg loss -2.949182 avg loss no lamb -2.949182 time 2020-06-27 09:46:49.571633
Model ind 665 epoch 1436 batch: 100 avg loss -2.893172 avg loss no lamb -2.893172 time 2020-06-27 09:47:00.335944
Model ind 665 epoch 1436 batch: 200 avg loss -2.860212 avg loss no lamb -2.860212 time 2020-06-27 09:47:11.196868
Model ind 665 epoch 1436 batch: 300 avg loss -2.875628 avg loss no lamb -2.875628 time 2020-06-27 09:47:22.220507
Model ind 665 epoch 1436 batch: 400 avg loss -2.796865 avg loss no lamb -2.796865 time 2020-06-27 09:47:33.172731
Model ind 665 epoch 1436 batch: 500 avg loss -2.878715 avg loss no lamb -2.878715 time 2020-06-27 09:47:44.033545
Model ind 665 epoch 1436 batch: 600 avg loss -2.909158 avg loss no lamb -2.909158 time 2020-06-27 09:47:54.791678
Model ind 665 epoch 1436 batch: 700 avg loss -2.825316 avg loss no lamb -2.825316 time 2020-06-27 09:48:05.784414
Model ind 665 epoch 1436 batch: 800 avg loss -2.906375 avg loss no lamb -2.906375 time 2020-06-27 09:48:16.789135
last batch sz 10
Pre: time 2020-06-27 09:48:30.892875: 
 	std: 0.0031385443
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9812, 0.9747, 0.9813, 0.9751]
	train_accs: [0.98153335, 0.98123336, 0.97636664, 0.982, 0.97678334]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97874004
	best: 0.9813

Starting e_i: 1437
Model ind 665 epoch 1437 batch: 0 avg loss -2.942052 avg loss no lamb -2.942052 time 2020-06-27 09:48:32.109083
Model ind 665 epoch 1437 batch: 100 avg loss -2.907269 avg loss no lamb -2.907269 time 2020-06-27 09:48:43.006931
Model ind 665 epoch 1437 batch: 200 avg loss -2.873969 avg loss no lamb -2.873969 time 2020-06-27 09:48:54.042626
Model ind 665 epoch 1437 batch: 300 avg loss -2.873990 avg loss no lamb -2.873990 time 2020-06-27 09:49:04.967134
Model ind 665 epoch 1437 batch: 400 avg loss -2.811687 avg loss no lamb -2.811687 time 2020-06-27 09:49:15.801600
Model ind 665 epoch 1437 batch: 500 avg loss -2.881522 avg loss no lamb -2.881522 time 2020-06-27 09:49:26.678101
Model ind 665 epoch 1437 batch: 600 avg loss -2.924182 avg loss no lamb -2.924182 time 2020-06-27 09:49:37.733575
Model ind 665 epoch 1437 batch: 700 avg loss -2.796458 avg loss no lamb -2.796458 time 2020-06-27 09:49:48.439933
Model ind 665 epoch 1437 batch: 800 avg loss -2.909089 avg loss no lamb -2.909089 time 2020-06-27 09:49:59.346426
last batch sz 10
Pre: time 2020-06-27 09:50:13.643602: 
 	std: 0.0026219205
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.98, 0.9754, 0.9805, 0.9747]
	train_accs: [0.98175, 0.9802667, 0.9759167, 0.9819, 0.97655]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97824
	best: 0.9805

Starting e_i: 1438
Model ind 665 epoch 1438 batch: 0 avg loss -2.949834 avg loss no lamb -2.949834 time 2020-06-27 09:50:15.068754
Model ind 665 epoch 1438 batch: 100 avg loss -2.882960 avg loss no lamb -2.882960 time 2020-06-27 09:50:25.994745
Model ind 665 epoch 1438 batch: 200 avg loss -2.917782 avg loss no lamb -2.917782 time 2020-06-27 09:50:36.925632
Model ind 665 epoch 1438 batch: 300 avg loss -2.881300 avg loss no lamb -2.881300 time 2020-06-27 09:50:47.784126
Model ind 665 epoch 1438 batch: 400 avg loss -2.809971 avg loss no lamb -2.809971 time 2020-06-27 09:50:58.843687
Model ind 665 epoch 1438 batch: 500 avg loss -2.853868 avg loss no lamb -2.853868 time 2020-06-27 09:51:10.000238
Model ind 665 epoch 1438 batch: 600 avg loss -2.908818 avg loss no lamb -2.908818 time 2020-06-27 09:51:20.786701
Model ind 665 epoch 1438 batch: 700 avg loss -2.858298 avg loss no lamb -2.858298 time 2020-06-27 09:51:31.851423
Model ind 665 epoch 1438 batch: 800 avg loss -2.890343 avg loss no lamb -2.890343 time 2020-06-27 09:51:42.866453
last batch sz 10
Pre: time 2020-06-27 09:51:57.239521: 
 	std: 0.00205192
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9802, 0.9761, 0.9805, 0.9762]
	train_accs: [0.98108333, 0.9810167, 0.97625, 0.9812, 0.97685]
	best_train_sub_head: 3
	worst: 0.9761
	avg: 0.97866
	best: 0.9805

Starting e_i: 1439
Model ind 665 epoch 1439 batch: 0 avg loss -2.967052 avg loss no lamb -2.967052 time 2020-06-27 09:51:58.531136
Model ind 665 epoch 1439 batch: 100 avg loss -2.994525 avg loss no lamb -2.994525 time 2020-06-27 09:52:09.708016
Model ind 665 epoch 1439 batch: 200 avg loss -2.891071 avg loss no lamb -2.891071 time 2020-06-27 09:52:20.820116
Model ind 665 epoch 1439 batch: 300 avg loss -2.845210 avg loss no lamb -2.845210 time 2020-06-27 09:52:31.871244
Model ind 665 epoch 1439 batch: 400 avg loss -2.780085 avg loss no lamb -2.780085 time 2020-06-27 09:52:42.954901
Model ind 665 epoch 1439 batch: 500 avg loss -2.848668 avg loss no lamb -2.848668 time 2020-06-27 09:52:54.037405
Model ind 665 epoch 1439 batch: 600 avg loss -2.876151 avg loss no lamb -2.876151 time 2020-06-27 09:53:04.948165
Model ind 665 epoch 1439 batch: 700 avg loss -2.779336 avg loss no lamb -2.779336 time 2020-06-27 09:53:16.047323
Model ind 665 epoch 1439 batch: 800 avg loss -2.833646 avg loss no lamb -2.833646 time 2020-06-27 09:53:26.872189
last batch sz 10
Pre: time 2020-06-27 09:53:40.971243: 
 	std: 0.0027845246
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.982, 0.9756, 0.9814, 0.976]
	train_accs: [0.98165, 0.9814, 0.9765, 0.98195, 0.97718334]
	best_train_sub_head: 3
	worst: 0.9756
	avg: 0.97918
	best: 0.9814

Starting e_i: 1440
Model ind 665 epoch 1440 batch: 0 avg loss -2.922527 avg loss no lamb -2.922527 time 2020-06-27 09:53:42.244556
Model ind 665 epoch 1440 batch: 100 avg loss -2.894993 avg loss no lamb -2.894993 time 2020-06-27 09:53:53.058080
Model ind 665 epoch 1440 batch: 200 avg loss -2.890973 avg loss no lamb -2.890973 time 2020-06-27 09:54:03.835956
Model ind 665 epoch 1440 batch: 300 avg loss -2.861216 avg loss no lamb -2.861216 time 2020-06-27 09:54:14.747107
Model ind 665 epoch 1440 batch: 400 avg loss -2.749622 avg loss no lamb -2.749622 time 2020-06-27 09:54:25.770471
Model ind 665 epoch 1440 batch: 500 avg loss -2.900469 avg loss no lamb -2.900469 time 2020-06-27 09:54:36.641662
Model ind 665 epoch 1440 batch: 600 avg loss -2.890535 avg loss no lamb -2.890535 time 2020-06-27 09:54:47.564387
Model ind 665 epoch 1440 batch: 700 avg loss -2.783706 avg loss no lamb -2.783706 time 2020-06-27 09:54:58.462227
Model ind 665 epoch 1440 batch: 800 avg loss -2.891319 avg loss no lamb -2.891319 time 2020-06-27 09:55:09.272837
last batch sz 10
Pre: time 2020-06-27 09:55:23.521712: 
 	std: 0.0028687252
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9811, 0.9743, 0.98, 0.9752]
	train_accs: [0.9812667, 0.98095, 0.9755, 0.9813333, 0.9767333]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.9782201
	best: 0.98

Starting e_i: 1441
Model ind 665 epoch 1441 batch: 0 avg loss -2.944561 avg loss no lamb -2.944561 time 2020-06-27 09:55:25.908217
Model ind 665 epoch 1441 batch: 100 avg loss -2.951865 avg loss no lamb -2.951865 time 2020-06-27 09:55:36.832254
Model ind 665 epoch 1441 batch: 200 avg loss -2.869692 avg loss no lamb -2.869692 time 2020-06-27 09:55:47.579657
Model ind 665 epoch 1441 batch: 300 avg loss -2.839736 avg loss no lamb -2.839736 time 2020-06-27 09:55:58.570658
Model ind 665 epoch 1441 batch: 400 avg loss -2.791303 avg loss no lamb -2.791303 time 2020-06-27 09:56:09.539613
Model ind 665 epoch 1441 batch: 500 avg loss -2.866878 avg loss no lamb -2.866878 time 2020-06-27 09:56:20.542672
Model ind 665 epoch 1441 batch: 600 avg loss -2.865057 avg loss no lamb -2.865057 time 2020-06-27 09:56:31.687962
Model ind 665 epoch 1441 batch: 700 avg loss -2.827058 avg loss no lamb -2.827058 time 2020-06-27 09:56:42.547754
Model ind 665 epoch 1441 batch: 800 avg loss -2.898028 avg loss no lamb -2.898028 time 2020-06-27 09:56:53.401604
last batch sz 10
Pre: time 2020-06-27 09:57:07.566699: 
 	std: 0.0025950707
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9796, 0.9734, 0.9787, 0.9742]
	train_accs: [0.9809167, 0.9802667, 0.97496665, 0.98075, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97694
	best: 0.9788

Starting e_i: 1442
Model ind 665 epoch 1442 batch: 0 avg loss -2.913036 avg loss no lamb -2.913036 time 2020-06-27 09:57:08.771958
Model ind 665 epoch 1442 batch: 100 avg loss -2.886493 avg loss no lamb -2.886493 time 2020-06-27 09:57:19.437102
Model ind 665 epoch 1442 batch: 200 avg loss -2.887799 avg loss no lamb -2.887799 time 2020-06-27 09:57:30.336259
Model ind 665 epoch 1442 batch: 300 avg loss -2.833529 avg loss no lamb -2.833529 time 2020-06-27 09:57:41.053564
Model ind 665 epoch 1442 batch: 400 avg loss -2.801496 avg loss no lamb -2.801496 time 2020-06-27 09:57:51.791233
Model ind 665 epoch 1442 batch: 500 avg loss -2.856443 avg loss no lamb -2.856443 time 2020-06-27 09:58:02.718300
Model ind 665 epoch 1442 batch: 600 avg loss -2.930163 avg loss no lamb -2.930163 time 2020-06-27 09:58:13.438542
Model ind 665 epoch 1442 batch: 700 avg loss -2.767775 avg loss no lamb -2.767775 time 2020-06-27 09:58:24.296848
Model ind 665 epoch 1442 batch: 800 avg loss -2.900830 avg loss no lamb -2.900830 time 2020-06-27 09:58:35.137610
last batch sz 10
Pre: time 2020-06-27 09:58:49.369911: 
 	std: 0.0034202992
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9809, 0.9732, 0.9804, 0.9736]
	train_accs: [0.9813, 0.9813, 0.9751, 0.98151666, 0.97605]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97756004
	best: 0.9804

Starting e_i: 1443
Model ind 665 epoch 1443 batch: 0 avg loss -2.951394 avg loss no lamb -2.951394 time 2020-06-27 09:58:50.637952
Model ind 665 epoch 1443 batch: 100 avg loss -2.865469 avg loss no lamb -2.865469 time 2020-06-27 09:59:01.468815
Model ind 665 epoch 1443 batch: 200 avg loss -2.896390 avg loss no lamb -2.896390 time 2020-06-27 09:59:12.554082
Model ind 665 epoch 1443 batch: 300 avg loss -2.861801 avg loss no lamb -2.861801 time 2020-06-27 09:59:23.460645
Model ind 665 epoch 1443 batch: 400 avg loss -2.833262 avg loss no lamb -2.833262 time 2020-06-27 09:59:34.350779
Model ind 665 epoch 1443 batch: 500 avg loss -2.887892 avg loss no lamb -2.887892 time 2020-06-27 09:59:45.121021
Model ind 665 epoch 1443 batch: 600 avg loss -2.869353 avg loss no lamb -2.869353 time 2020-06-27 09:59:55.729153
Model ind 665 epoch 1443 batch: 700 avg loss -2.869307 avg loss no lamb -2.869307 time 2020-06-27 10:00:06.748730
Model ind 665 epoch 1443 batch: 800 avg loss -2.913185 avg loss no lamb -2.913185 time 2020-06-27 10:00:17.748172
last batch sz 10
Pre: time 2020-06-27 10:00:32.031064: 
 	std: 0.003107816
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9787, 0.9726, 0.9797, 0.9736]
	train_accs: [0.9809, 0.98031664, 0.97466666, 0.981, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97686005
	best: 0.9797

Starting e_i: 1444
Model ind 665 epoch 1444 batch: 0 avg loss -2.963019 avg loss no lamb -2.963019 time 2020-06-27 10:00:33.261352
Model ind 665 epoch 1444 batch: 100 avg loss -2.860769 avg loss no lamb -2.860769 time 2020-06-27 10:00:44.043208
Model ind 665 epoch 1444 batch: 200 avg loss -2.873113 avg loss no lamb -2.873113 time 2020-06-27 10:00:54.994688
Model ind 665 epoch 1444 batch: 300 avg loss -2.885123 avg loss no lamb -2.885123 time 2020-06-27 10:01:06.000204
Model ind 665 epoch 1444 batch: 400 avg loss -2.830269 avg loss no lamb -2.830269 time 2020-06-27 10:01:16.909833
Model ind 665 epoch 1444 batch: 500 avg loss -2.895392 avg loss no lamb -2.895392 time 2020-06-27 10:01:27.797221
Model ind 665 epoch 1444 batch: 600 avg loss -2.883463 avg loss no lamb -2.883463 time 2020-06-27 10:01:38.591544
Model ind 665 epoch 1444 batch: 700 avg loss -2.795418 avg loss no lamb -2.795418 time 2020-06-27 10:01:49.466436
Model ind 665 epoch 1444 batch: 800 avg loss -2.885452 avg loss no lamb -2.885452 time 2020-06-27 10:02:00.446979
last batch sz 10
Pre: time 2020-06-27 10:02:14.801313: 
 	std: 0.0033706964
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9803, 0.9733, 0.9806, 0.9739]
	train_accs: [0.98116666, 0.98071665, 0.97535, 0.9813667, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97771996
	best: 0.9806

Starting e_i: 1445
Model ind 665 epoch 1445 batch: 0 avg loss -2.906791 avg loss no lamb -2.906791 time 2020-06-27 10:02:16.030777
Model ind 665 epoch 1445 batch: 100 avg loss -2.937686 avg loss no lamb -2.937686 time 2020-06-27 10:02:26.662501
Model ind 665 epoch 1445 batch: 200 avg loss -2.895627 avg loss no lamb -2.895627 time 2020-06-27 10:02:37.461005
Model ind 665 epoch 1445 batch: 300 avg loss -2.867874 avg loss no lamb -2.867874 time 2020-06-27 10:02:48.159915
Model ind 665 epoch 1445 batch: 400 avg loss -2.796012 avg loss no lamb -2.796012 time 2020-06-27 10:02:59.057717
Model ind 665 epoch 1445 batch: 500 avg loss -2.870266 avg loss no lamb -2.870266 time 2020-06-27 10:03:09.850359
Model ind 665 epoch 1445 batch: 600 avg loss -2.853613 avg loss no lamb -2.853613 time 2020-06-27 10:03:20.788823
Model ind 665 epoch 1445 batch: 700 avg loss -2.766468 avg loss no lamb -2.766468 time 2020-06-27 10:03:31.805499
Model ind 665 epoch 1445 batch: 800 avg loss -2.874666 avg loss no lamb -2.874666 time 2020-06-27 10:03:42.778520
last batch sz 10
Pre: time 2020-06-27 10:03:56.941571: 
 	std: 0.0026962212
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9787, 0.9737, 0.9802, 0.9746]
	train_accs: [0.98178333, 0.9806, 0.9758, 0.9819, 0.9766]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97738
	best: 0.9802

Starting e_i: 1446
Model ind 665 epoch 1446 batch: 0 avg loss -2.955066 avg loss no lamb -2.955066 time 2020-06-27 10:03:58.152930
Model ind 665 epoch 1446 batch: 100 avg loss -2.933826 avg loss no lamb -2.933826 time 2020-06-27 10:04:09.396562
Model ind 665 epoch 1446 batch: 200 avg loss -2.861781 avg loss no lamb -2.861781 time 2020-06-27 10:04:20.540250
Model ind 665 epoch 1446 batch: 300 avg loss -2.898965 avg loss no lamb -2.898965 time 2020-06-27 10:04:31.435084
Model ind 665 epoch 1446 batch: 400 avg loss -2.795923 avg loss no lamb -2.795923 time 2020-06-27 10:04:42.599116
Model ind 665 epoch 1446 batch: 500 avg loss -2.854159 avg loss no lamb -2.854159 time 2020-06-27 10:04:53.595454
Model ind 665 epoch 1446 batch: 600 avg loss -2.879899 avg loss no lamb -2.879899 time 2020-06-27 10:05:04.503157
Model ind 665 epoch 1446 batch: 700 avg loss -2.863734 avg loss no lamb -2.863734 time 2020-06-27 10:05:15.472969
Model ind 665 epoch 1446 batch: 800 avg loss -2.853818 avg loss no lamb -2.853818 time 2020-06-27 10:05:26.318999
last batch sz 10
Pre: time 2020-06-27 10:05:40.479415: 
 	std: 0.0029863713
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9804, 0.9738, 0.9806, 0.9758]
	train_accs: [0.98211664, 0.9812, 0.9755667, 0.98191667, 0.97678334]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97836
	best: 0.9812

Starting e_i: 1447
Model ind 665 epoch 1447 batch: 0 avg loss -2.988347 avg loss no lamb -2.988347 time 2020-06-27 10:05:41.924528
Model ind 665 epoch 1447 batch: 100 avg loss -2.869328 avg loss no lamb -2.869328 time 2020-06-27 10:05:52.772660
Model ind 665 epoch 1447 batch: 200 avg loss -2.811670 avg loss no lamb -2.811670 time 2020-06-27 10:06:03.715411
Model ind 665 epoch 1447 batch: 300 avg loss -2.828223 avg loss no lamb -2.828223 time 2020-06-27 10:06:14.718034
Model ind 665 epoch 1447 batch: 400 avg loss -2.810378 avg loss no lamb -2.810378 time 2020-06-27 10:06:25.621322
Model ind 665 epoch 1447 batch: 500 avg loss -2.839846 avg loss no lamb -2.839846 time 2020-06-27 10:06:36.765649
Model ind 665 epoch 1447 batch: 600 avg loss -2.875083 avg loss no lamb -2.875083 time 2020-06-27 10:06:47.506906
Model ind 665 epoch 1447 batch: 700 avg loss -2.800809 avg loss no lamb -2.800809 time 2020-06-27 10:06:58.455770
Model ind 665 epoch 1447 batch: 800 avg loss -2.868045 avg loss no lamb -2.868045 time 2020-06-27 10:07:09.419493
last batch sz 10
Pre: time 2020-06-27 10:07:23.458038: 
 	std: 0.0027279325
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9789, 0.9737, 0.9795, 0.9738]
	train_accs: [0.9812833, 0.9808, 0.97546667, 0.98141664, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97708005
	best: 0.9795

Starting e_i: 1448
Model ind 665 epoch 1448 batch: 0 avg loss -2.956426 avg loss no lamb -2.956426 time 2020-06-27 10:07:24.711341
Model ind 665 epoch 1448 batch: 100 avg loss -2.893332 avg loss no lamb -2.893332 time 2020-06-27 10:07:35.592541
Model ind 665 epoch 1448 batch: 200 avg loss -2.863989 avg loss no lamb -2.863989 time 2020-06-27 10:07:46.384672
Model ind 665 epoch 1448 batch: 300 avg loss -2.821664 avg loss no lamb -2.821664 time 2020-06-27 10:07:57.348339
Model ind 665 epoch 1448 batch: 400 avg loss -2.852207 avg loss no lamb -2.852207 time 2020-06-27 10:08:08.162265
Model ind 665 epoch 1448 batch: 500 avg loss -2.876987 avg loss no lamb -2.876987 time 2020-06-27 10:08:18.912327
Model ind 665 epoch 1448 batch: 600 avg loss -2.889422 avg loss no lamb -2.889422 time 2020-06-27 10:08:29.656147
Model ind 665 epoch 1448 batch: 700 avg loss -2.810859 avg loss no lamb -2.810859 time 2020-06-27 10:08:40.556038
Model ind 665 epoch 1448 batch: 800 avg loss -2.881950 avg loss no lamb -2.881950 time 2020-06-27 10:08:51.369558
last batch sz 10
Pre: time 2020-06-27 10:09:05.482304: 
 	std: 0.0033211976
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9801, 0.9727, 0.9805, 0.9744]
	train_accs: [0.98155, 0.98108333, 0.9748667, 0.9816667, 0.976]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.9775599
	best: 0.9805

Starting e_i: 1449
Model ind 665 epoch 1449 batch: 0 avg loss -2.981858 avg loss no lamb -2.981858 time 2020-06-27 10:09:06.916160
Model ind 665 epoch 1449 batch: 100 avg loss -2.932879 avg loss no lamb -2.932879 time 2020-06-27 10:09:17.842314
Model ind 665 epoch 1449 batch: 200 avg loss -2.948286 avg loss no lamb -2.948286 time 2020-06-27 10:09:28.773937
Model ind 665 epoch 1449 batch: 300 avg loss -2.898454 avg loss no lamb -2.898454 time 2020-06-27 10:09:39.651840
Model ind 665 epoch 1449 batch: 400 avg loss -2.790951 avg loss no lamb -2.790951 time 2020-06-27 10:09:50.284100
Model ind 665 epoch 1449 batch: 500 avg loss -2.885381 avg loss no lamb -2.885381 time 2020-06-27 10:10:00.930142
Model ind 665 epoch 1449 batch: 600 avg loss -2.897717 avg loss no lamb -2.897717 time 2020-06-27 10:10:11.993468
Model ind 665 epoch 1449 batch: 700 avg loss -2.790522 avg loss no lamb -2.790522 time 2020-06-27 10:10:23.100225
Model ind 665 epoch 1449 batch: 800 avg loss -2.880168 avg loss no lamb -2.880168 time 2020-06-27 10:10:34.148672
last batch sz 10
Pre: time 2020-06-27 10:10:48.651698: 
 	std: 0.002803844
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9795, 0.9743, 0.9816, 0.976]
	train_accs: [0.9816667, 0.9805833, 0.9759667, 0.9818, 0.9769]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.9784201
	best: 0.9816

Starting e_i: 1450
Model ind 665 epoch 1450 batch: 0 avg loss -2.950358 avg loss no lamb -2.950358 time 2020-06-27 10:10:49.926224
Model ind 665 epoch 1450 batch: 100 avg loss -2.888382 avg loss no lamb -2.888382 time 2020-06-27 10:11:00.879626
Model ind 665 epoch 1450 batch: 200 avg loss -2.852526 avg loss no lamb -2.852526 time 2020-06-27 10:11:11.714740
Model ind 665 epoch 1450 batch: 300 avg loss -2.832035 avg loss no lamb -2.832035 time 2020-06-27 10:11:22.677265
Model ind 665 epoch 1450 batch: 400 avg loss -2.779696 avg loss no lamb -2.779696 time 2020-06-27 10:11:33.773142
Model ind 665 epoch 1450 batch: 500 avg loss -2.807853 avg loss no lamb -2.807853 time 2020-06-27 10:11:44.741534
Model ind 665 epoch 1450 batch: 600 avg loss -2.859437 avg loss no lamb -2.859437 time 2020-06-27 10:11:55.586388
Model ind 665 epoch 1450 batch: 700 avg loss -2.792845 avg loss no lamb -2.792845 time 2020-06-27 10:12:06.401802
Model ind 665 epoch 1450 batch: 800 avg loss -2.844212 avg loss no lamb -2.844212 time 2020-06-27 10:12:17.434102
last batch sz 10
Pre: time 2020-06-27 10:12:31.570826: 
 	std: 0.0030737517
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9787, 0.9727, 0.9799, 0.9739]
	train_accs: [0.9805, 0.98001665, 0.97445, 0.98083335, 0.9756]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97699994
	best: 0.9799

Starting e_i: 1451
Model ind 665 epoch 1451 batch: 0 avg loss -2.939305 avg loss no lamb -2.939305 time 2020-06-27 10:12:34.155832
Model ind 665 epoch 1451 batch: 100 avg loss -2.871869 avg loss no lamb -2.871869 time 2020-06-27 10:12:44.778628
Model ind 665 epoch 1451 batch: 200 avg loss -2.847485 avg loss no lamb -2.847485 time 2020-06-27 10:12:55.445561
Model ind 665 epoch 1451 batch: 300 avg loss -2.869188 avg loss no lamb -2.869188 time 2020-06-27 10:13:06.377626
Model ind 665 epoch 1451 batch: 400 avg loss -2.827457 avg loss no lamb -2.827457 time 2020-06-27 10:13:17.379887
Model ind 665 epoch 1451 batch: 500 avg loss -2.889660 avg loss no lamb -2.889660 time 2020-06-27 10:13:28.280032
Model ind 665 epoch 1451 batch: 600 avg loss -2.850444 avg loss no lamb -2.850444 time 2020-06-27 10:13:39.106412
Model ind 665 epoch 1451 batch: 700 avg loss -2.792506 avg loss no lamb -2.792506 time 2020-06-27 10:13:50.277943
Model ind 665 epoch 1451 batch: 800 avg loss -2.869679 avg loss no lamb -2.869679 time 2020-06-27 10:14:01.468623
last batch sz 10
Pre: time 2020-06-27 10:14:15.685389: 
 	std: 0.0038675112
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9787, 0.9706, 0.9804, 0.9737]
	train_accs: [0.98145, 0.9802333, 0.97391665, 0.98155, 0.9759667]
	best_train_sub_head: 3
	worst: 0.9706
	avg: 0.97668
	best: 0.9804

Starting e_i: 1452
Model ind 665 epoch 1452 batch: 0 avg loss -2.978721 avg loss no lamb -2.978721 time 2020-06-27 10:14:16.936901
Model ind 665 epoch 1452 batch: 100 avg loss -2.888053 avg loss no lamb -2.888053 time 2020-06-27 10:14:27.929739
Model ind 665 epoch 1452 batch: 200 avg loss -2.859398 avg loss no lamb -2.859398 time 2020-06-27 10:14:38.863864
Model ind 665 epoch 1452 batch: 300 avg loss -2.902948 avg loss no lamb -2.902948 time 2020-06-27 10:14:49.737321
Model ind 665 epoch 1452 batch: 400 avg loss -2.806908 avg loss no lamb -2.806908 time 2020-06-27 10:15:00.601629
Model ind 665 epoch 1452 batch: 500 avg loss -2.836575 avg loss no lamb -2.836575 time 2020-06-27 10:15:11.343319
Model ind 665 epoch 1452 batch: 600 avg loss -2.852287 avg loss no lamb -2.852287 time 2020-06-27 10:15:22.324061
Model ind 665 epoch 1452 batch: 700 avg loss -2.809410 avg loss no lamb -2.809410 time 2020-06-27 10:15:33.364784
Model ind 665 epoch 1452 batch: 800 avg loss -2.834795 avg loss no lamb -2.834795 time 2020-06-27 10:15:44.449779
last batch sz 10
Pre: time 2020-06-27 10:15:58.692892: 
 	std: 0.003022977
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9809, 0.9744, 0.9816, 0.976]
	train_accs: [0.98153335, 0.98076665, 0.97525, 0.98176664, 0.97615]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97884
	best: 0.9816

Starting e_i: 1453
Model ind 665 epoch 1453 batch: 0 avg loss -2.927797 avg loss no lamb -2.927797 time 2020-06-27 10:15:59.983857
Model ind 665 epoch 1453 batch: 100 avg loss -2.903390 avg loss no lamb -2.903390 time 2020-06-27 10:16:11.052014
Model ind 665 epoch 1453 batch: 200 avg loss -2.835637 avg loss no lamb -2.835637 time 2020-06-27 10:16:21.830436
Model ind 665 epoch 1453 batch: 300 avg loss -2.901590 avg loss no lamb -2.901590 time 2020-06-27 10:16:32.752761
Model ind 665 epoch 1453 batch: 400 avg loss -2.811496 avg loss no lamb -2.811496 time 2020-06-27 10:16:43.656355
Model ind 665 epoch 1453 batch: 500 avg loss -2.853707 avg loss no lamb -2.853707 time 2020-06-27 10:16:54.748911
Model ind 665 epoch 1453 batch: 600 avg loss -2.903342 avg loss no lamb -2.903342 time 2020-06-27 10:17:05.711561
Model ind 665 epoch 1453 batch: 700 avg loss -2.772112 avg loss no lamb -2.772112 time 2020-06-27 10:17:16.643029
Model ind 665 epoch 1453 batch: 800 avg loss -2.910229 avg loss no lamb -2.910229 time 2020-06-27 10:17:27.651361
last batch sz 10
Pre: time 2020-06-27 10:17:41.896637: 
 	std: 0.003083496
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9796, 0.9735, 0.9805, 0.9742]
	train_accs: [0.98135, 0.98041666, 0.97531664, 0.9816333, 0.9759]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.9776
	best: 0.9805

Starting e_i: 1454
Model ind 665 epoch 1454 batch: 0 avg loss -2.973727 avg loss no lamb -2.973727 time 2020-06-27 10:17:43.156752
Model ind 665 epoch 1454 batch: 100 avg loss -2.909386 avg loss no lamb -2.909386 time 2020-06-27 10:17:53.993104
Model ind 665 epoch 1454 batch: 200 avg loss -2.826719 avg loss no lamb -2.826719 time 2020-06-27 10:18:04.908477
Model ind 665 epoch 1454 batch: 300 avg loss -2.919040 avg loss no lamb -2.919040 time 2020-06-27 10:18:15.736256
Model ind 665 epoch 1454 batch: 400 avg loss -2.777553 avg loss no lamb -2.777553 time 2020-06-27 10:18:26.434247
Model ind 665 epoch 1454 batch: 500 avg loss -2.863655 avg loss no lamb -2.863655 time 2020-06-27 10:18:37.267319
Model ind 665 epoch 1454 batch: 600 avg loss -2.929184 avg loss no lamb -2.929184 time 2020-06-27 10:18:48.051964
Model ind 665 epoch 1454 batch: 700 avg loss -2.779631 avg loss no lamb -2.779631 time 2020-06-27 10:18:58.825441
Model ind 665 epoch 1454 batch: 800 avg loss -2.872087 avg loss no lamb -2.872087 time 2020-06-27 10:19:09.843284
last batch sz 10
Pre: time 2020-06-27 10:19:23.981197: 
 	std: 0.0029506732
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9796, 0.9737, 0.9799, 0.9736]
	train_accs: [0.98111665, 0.9806167, 0.97508335, 0.98145, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97726
	best: 0.9799

Starting e_i: 1455
Model ind 665 epoch 1455 batch: 0 avg loss -2.952382 avg loss no lamb -2.952382 time 2020-06-27 10:19:25.229837
Model ind 665 epoch 1455 batch: 100 avg loss -2.908042 avg loss no lamb -2.908042 time 2020-06-27 10:19:35.915331
Model ind 665 epoch 1455 batch: 200 avg loss -2.891047 avg loss no lamb -2.891047 time 2020-06-27 10:19:46.873768
Model ind 665 epoch 1455 batch: 300 avg loss -2.847111 avg loss no lamb -2.847111 time 2020-06-27 10:19:57.864523
Model ind 665 epoch 1455 batch: 400 avg loss -2.796222 avg loss no lamb -2.796222 time 2020-06-27 10:20:08.671095
Model ind 665 epoch 1455 batch: 500 avg loss -2.859153 avg loss no lamb -2.859153 time 2020-06-27 10:20:19.609741
Model ind 665 epoch 1455 batch: 600 avg loss -2.872296 avg loss no lamb -2.872296 time 2020-06-27 10:20:30.635779
Model ind 665 epoch 1455 batch: 700 avg loss -2.732860 avg loss no lamb -2.732860 time 2020-06-27 10:20:41.500942
Model ind 665 epoch 1455 batch: 800 avg loss -2.893912 avg loss no lamb -2.893912 time 2020-06-27 10:20:52.573591
last batch sz 10
Pre: time 2020-06-27 10:21:07.294519: 
 	std: 0.0030825986
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9803, 0.9747, 0.9809, 0.9741]
	train_accs: [0.9813167, 0.9809667, 0.97573334, 0.9812, 0.97565]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97816
	best: 0.9808

Starting e_i: 1456
Model ind 665 epoch 1456 batch: 0 avg loss -2.944284 avg loss no lamb -2.944284 time 2020-06-27 10:21:08.551075
Model ind 665 epoch 1456 batch: 100 avg loss -2.953452 avg loss no lamb -2.953452 time 2020-06-27 10:21:19.664231
Model ind 665 epoch 1456 batch: 200 avg loss -2.870588 avg loss no lamb -2.870588 time 2020-06-27 10:21:30.735385
Model ind 665 epoch 1456 batch: 300 avg loss -2.882794 avg loss no lamb -2.882794 time 2020-06-27 10:21:41.673515
Model ind 665 epoch 1456 batch: 400 avg loss -2.806952 avg loss no lamb -2.806952 time 2020-06-27 10:21:52.504285
Model ind 665 epoch 1456 batch: 500 avg loss -2.845921 avg loss no lamb -2.845921 time 2020-06-27 10:22:03.286929
Model ind 665 epoch 1456 batch: 600 avg loss -2.856725 avg loss no lamb -2.856725 time 2020-06-27 10:22:14.142538
Model ind 665 epoch 1456 batch: 700 avg loss -2.804148 avg loss no lamb -2.804148 time 2020-06-27 10:22:24.864676
Model ind 665 epoch 1456 batch: 800 avg loss -2.891663 avg loss no lamb -2.891663 time 2020-06-27 10:22:35.737307
last batch sz 10
Pre: time 2020-06-27 10:22:50.180947: 
 	std: 0.0026510274
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9799, 0.9746, 0.9809, 0.9756]
	train_accs: [0.98135, 0.9806833, 0.97555, 0.98158336, 0.9769167]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.9783
	best: 0.9809

Starting e_i: 1457
Model ind 665 epoch 1457 batch: 0 avg loss -2.971448 avg loss no lamb -2.971448 time 2020-06-27 10:22:51.441271
Model ind 665 epoch 1457 batch: 100 avg loss -2.894276 avg loss no lamb -2.894276 time 2020-06-27 10:23:02.288088
Model ind 665 epoch 1457 batch: 200 avg loss -2.864957 avg loss no lamb -2.864957 time 2020-06-27 10:23:13.137384
Model ind 665 epoch 1457 batch: 300 avg loss -2.918100 avg loss no lamb -2.918100 time 2020-06-27 10:23:23.866479
Model ind 665 epoch 1457 batch: 400 avg loss -2.813348 avg loss no lamb -2.813348 time 2020-06-27 10:23:34.734646
Model ind 665 epoch 1457 batch: 500 avg loss -2.898211 avg loss no lamb -2.898211 time 2020-06-27 10:23:45.585616
Model ind 665 epoch 1457 batch: 600 avg loss -2.891222 avg loss no lamb -2.891222 time 2020-06-27 10:23:56.642576
Model ind 665 epoch 1457 batch: 700 avg loss -2.767948 avg loss no lamb -2.767948 time 2020-06-27 10:24:07.837441
Model ind 665 epoch 1457 batch: 800 avg loss -2.883841 avg loss no lamb -2.883841 time 2020-06-27 10:24:18.572183
last batch sz 10
Pre: time 2020-06-27 10:24:33.247028: 
 	std: 0.002908602
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9798, 0.9742, 0.9799, 0.9741]
	train_accs: [0.9814, 0.98081666, 0.97538334, 0.98111665, 0.97635]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97770005
	best: 0.9805

Starting e_i: 1458
Model ind 665 epoch 1458 batch: 0 avg loss -2.991356 avg loss no lamb -2.991356 time 2020-06-27 10:24:34.505558
Model ind 665 epoch 1458 batch: 100 avg loss -2.897142 avg loss no lamb -2.897142 time 2020-06-27 10:24:45.442147
Model ind 665 epoch 1458 batch: 200 avg loss -2.858166 avg loss no lamb -2.858166 time 2020-06-27 10:24:56.250622
Model ind 665 epoch 1458 batch: 300 avg loss -2.883556 avg loss no lamb -2.883556 time 2020-06-27 10:25:07.249135
Model ind 665 epoch 1458 batch: 400 avg loss -2.832335 avg loss no lamb -2.832335 time 2020-06-27 10:25:18.263620
Model ind 665 epoch 1458 batch: 500 avg loss -2.878925 avg loss no lamb -2.878925 time 2020-06-27 10:25:29.333034
Model ind 665 epoch 1458 batch: 600 avg loss -2.910159 avg loss no lamb -2.910159 time 2020-06-27 10:25:40.087055
Model ind 665 epoch 1458 batch: 700 avg loss -2.870903 avg loss no lamb -2.870903 time 2020-06-27 10:25:51.035075
Model ind 665 epoch 1458 batch: 800 avg loss -2.792103 avg loss no lamb -2.792103 time 2020-06-27 10:26:02.054674
last batch sz 10
Pre: time 2020-06-27 10:26:16.297650: 
 	std: 0.003458686
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9802, 0.9736, 0.9815, 0.9753]
	train_accs: [0.98156667, 0.9805833, 0.97428334, 0.9813833, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97856
	best: 0.9822

Starting e_i: 1459
Model ind 665 epoch 1459 batch: 0 avg loss -2.909309 avg loss no lamb -2.909309 time 2020-06-27 10:26:17.559419
Model ind 665 epoch 1459 batch: 100 avg loss -2.951061 avg loss no lamb -2.951061 time 2020-06-27 10:26:28.333305
Model ind 665 epoch 1459 batch: 200 avg loss -2.874331 avg loss no lamb -2.874331 time 2020-06-27 10:26:39.317976
Model ind 665 epoch 1459 batch: 300 avg loss -2.815544 avg loss no lamb -2.815544 time 2020-06-27 10:26:50.143606
Model ind 665 epoch 1459 batch: 400 avg loss -2.816472 avg loss no lamb -2.816472 time 2020-06-27 10:27:01.065686
Model ind 665 epoch 1459 batch: 500 avg loss -2.833561 avg loss no lamb -2.833561 time 2020-06-27 10:27:11.931257
Model ind 665 epoch 1459 batch: 600 avg loss -2.896715 avg loss no lamb -2.896715 time 2020-06-27 10:27:22.912686
Model ind 665 epoch 1459 batch: 700 avg loss -2.821954 avg loss no lamb -2.821954 time 2020-06-27 10:27:33.841067
Model ind 665 epoch 1459 batch: 800 avg loss -2.877800 avg loss no lamb -2.877800 time 2020-06-27 10:27:44.646557
last batch sz 10
Pre: time 2020-06-27 10:27:58.885143: 
 	std: 0.0032838758
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9794, 0.9732, 0.9808, 0.9743]
	train_accs: [0.98113334, 0.9801667, 0.9744, 0.98115, 0.9752667]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97769994
	best: 0.9808

Starting e_i: 1460
Model ind 665 epoch 1460 batch: 0 avg loss -2.953294 avg loss no lamb -2.953294 time 2020-06-27 10:28:00.246774
Model ind 665 epoch 1460 batch: 100 avg loss -2.978969 avg loss no lamb -2.978969 time 2020-06-27 10:28:11.106725
Model ind 665 epoch 1460 batch: 200 avg loss -2.886237 avg loss no lamb -2.886237 time 2020-06-27 10:28:22.024975
Model ind 665 epoch 1460 batch: 300 avg loss -2.900410 avg loss no lamb -2.900410 time 2020-06-27 10:28:32.920519
Model ind 665 epoch 1460 batch: 400 avg loss -2.792015 avg loss no lamb -2.792015 time 2020-06-27 10:28:43.713280
Model ind 665 epoch 1460 batch: 500 avg loss -2.823672 avg loss no lamb -2.823672 time 2020-06-27 10:28:54.683927
Model ind 665 epoch 1460 batch: 600 avg loss -2.948313 avg loss no lamb -2.948313 time 2020-06-27 10:29:05.731927
Model ind 665 epoch 1460 batch: 700 avg loss -2.811772 avg loss no lamb -2.811772 time 2020-06-27 10:29:16.536341
Model ind 665 epoch 1460 batch: 800 avg loss -2.867420 avg loss no lamb -2.867420 time 2020-06-27 10:29:27.386626
last batch sz 10
Pre: time 2020-06-27 10:29:41.394992: 
 	std: 0.0027003726
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9787, 0.9731, 0.9792, 0.9744]
	train_accs: [0.9813167, 0.98043334, 0.9751667, 0.9813167, 0.9762167]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97700006
	best: 0.9796

Starting e_i: 1461
Model ind 665 epoch 1461 batch: 0 avg loss -2.958019 avg loss no lamb -2.958019 time 2020-06-27 10:29:43.889313
Model ind 665 epoch 1461 batch: 100 avg loss -2.890366 avg loss no lamb -2.890366 time 2020-06-27 10:29:54.737467
Model ind 665 epoch 1461 batch: 200 avg loss -2.853503 avg loss no lamb -2.853503 time 2020-06-27 10:30:05.635522
Model ind 665 epoch 1461 batch: 300 avg loss -2.906071 avg loss no lamb -2.906071 time 2020-06-27 10:30:16.486131
Model ind 665 epoch 1461 batch: 400 avg loss -2.771554 avg loss no lamb -2.771554 time 2020-06-27 10:30:27.353933
Model ind 665 epoch 1461 batch: 500 avg loss -2.863916 avg loss no lamb -2.863916 time 2020-06-27 10:30:38.248347
Model ind 665 epoch 1461 batch: 600 avg loss -2.915126 avg loss no lamb -2.915126 time 2020-06-27 10:30:49.165907
Model ind 665 epoch 1461 batch: 700 avg loss -2.793362 avg loss no lamb -2.793362 time 2020-06-27 10:31:00.071880
Model ind 665 epoch 1461 batch: 800 avg loss -2.933234 avg loss no lamb -2.933234 time 2020-06-27 10:31:10.949729
last batch sz 10
Pre: time 2020-06-27 10:31:25.003381: 
 	std: 0.0028820848
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9802, 0.9738, 0.9799, 0.9747]
	train_accs: [0.9816167, 0.98088336, 0.97536665, 0.98158336, 0.97651666]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97776
	best: 0.9802

Starting e_i: 1462
Model ind 665 epoch 1462 batch: 0 avg loss -2.936818 avg loss no lamb -2.936818 time 2020-06-27 10:31:26.435969
Model ind 665 epoch 1462 batch: 100 avg loss -2.904209 avg loss no lamb -2.904209 time 2020-06-27 10:31:37.235481
Model ind 665 epoch 1462 batch: 200 avg loss -2.941969 avg loss no lamb -2.941969 time 2020-06-27 10:31:48.270363
Model ind 665 epoch 1462 batch: 300 avg loss -2.861918 avg loss no lamb -2.861918 time 2020-06-27 10:31:59.303466
Model ind 665 epoch 1462 batch: 400 avg loss -2.727315 avg loss no lamb -2.727315 time 2020-06-27 10:32:10.235516
Model ind 665 epoch 1462 batch: 500 avg loss -2.830327 avg loss no lamb -2.830327 time 2020-06-27 10:32:21.170221
Model ind 665 epoch 1462 batch: 600 avg loss -2.876130 avg loss no lamb -2.876130 time 2020-06-27 10:32:32.239446
Model ind 665 epoch 1462 batch: 700 avg loss -2.738561 avg loss no lamb -2.738561 time 2020-06-27 10:32:43.150591
Model ind 665 epoch 1462 batch: 800 avg loss -2.860621 avg loss no lamb -2.860621 time 2020-06-27 10:32:54.062415
last batch sz 10
Pre: time 2020-06-27 10:33:08.393849: 
 	std: 0.0026156546
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9806, 0.9748, 0.9811, 0.9766]
	train_accs: [0.9821333, 0.98165, 0.97643334, 0.9823, 0.97761667]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97882
	best: 0.9811

Starting e_i: 1463
Model ind 665 epoch 1463 batch: 0 avg loss -2.995927 avg loss no lamb -2.995927 time 2020-06-27 10:33:09.656055
Model ind 665 epoch 1463 batch: 100 avg loss -2.869805 avg loss no lamb -2.869805 time 2020-06-27 10:33:20.726301
Model ind 665 epoch 1463 batch: 200 avg loss -2.862772 avg loss no lamb -2.862772 time 2020-06-27 10:33:31.710800
Model ind 665 epoch 1463 batch: 300 avg loss -2.856535 avg loss no lamb -2.856535 time 2020-06-27 10:33:42.682169
Model ind 665 epoch 1463 batch: 400 avg loss -2.754744 avg loss no lamb -2.754744 time 2020-06-27 10:33:53.583330
Model ind 665 epoch 1463 batch: 500 avg loss -2.821782 avg loss no lamb -2.821782 time 2020-06-27 10:34:04.878382
Model ind 665 epoch 1463 batch: 600 avg loss -2.909351 avg loss no lamb -2.909351 time 2020-06-27 10:34:15.603997
Model ind 665 epoch 1463 batch: 700 avg loss -2.725099 avg loss no lamb -2.725099 time 2020-06-27 10:34:26.520846
Model ind 665 epoch 1463 batch: 800 avg loss -2.876206 avg loss no lamb -2.876206 time 2020-06-27 10:34:37.400533
last batch sz 10
Pre: time 2020-06-27 10:34:51.582597: 
 	std: 0.002900067
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9795, 0.9732, 0.9796, 0.9745]
	train_accs: [0.9813167, 0.98018336, 0.97495, 0.98121667, 0.976]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97736007
	best: 0.98

Starting e_i: 1464
Model ind 665 epoch 1464 batch: 0 avg loss -2.980805 avg loss no lamb -2.980805 time 2020-06-27 10:34:53.048909
Model ind 665 epoch 1464 batch: 100 avg loss -2.883455 avg loss no lamb -2.883455 time 2020-06-27 10:35:04.048284
Model ind 665 epoch 1464 batch: 200 avg loss -2.921188 avg loss no lamb -2.921188 time 2020-06-27 10:35:14.829989
Model ind 665 epoch 1464 batch: 300 avg loss -2.885144 avg loss no lamb -2.885144 time 2020-06-27 10:35:25.621482
Model ind 665 epoch 1464 batch: 400 avg loss -2.770550 avg loss no lamb -2.770550 time 2020-06-27 10:35:36.512311
Model ind 665 epoch 1464 batch: 500 avg loss -2.856660 avg loss no lamb -2.856660 time 2020-06-27 10:35:47.500756
Model ind 665 epoch 1464 batch: 600 avg loss -2.882692 avg loss no lamb -2.882692 time 2020-06-27 10:35:58.456524
Model ind 665 epoch 1464 batch: 700 avg loss -2.784135 avg loss no lamb -2.784135 time 2020-06-27 10:36:09.541784
Model ind 665 epoch 1464 batch: 800 avg loss -2.898833 avg loss no lamb -2.898833 time 2020-06-27 10:36:20.551563
last batch sz 10
Pre: time 2020-06-27 10:36:34.826056: 
 	std: 0.0026705933
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9801, 0.9747, 0.9806, 0.9754]
	train_accs: [0.9816, 0.981, 0.97615, 0.9816833, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.9783
	best: 0.9806

Starting e_i: 1465
Model ind 665 epoch 1465 batch: 0 avg loss -2.978073 avg loss no lamb -2.978073 time 2020-06-27 10:36:36.067870
Model ind 665 epoch 1465 batch: 100 avg loss -2.932395 avg loss no lamb -2.932395 time 2020-06-27 10:36:46.914432
Model ind 665 epoch 1465 batch: 200 avg loss -2.921389 avg loss no lamb -2.921389 time 2020-06-27 10:36:57.768435
Model ind 665 epoch 1465 batch: 300 avg loss -2.880007 avg loss no lamb -2.880007 time 2020-06-27 10:37:08.753416
Model ind 665 epoch 1465 batch: 400 avg loss -2.826655 avg loss no lamb -2.826655 time 2020-06-27 10:37:19.874865
Model ind 665 epoch 1465 batch: 500 avg loss -2.876122 avg loss no lamb -2.876122 time 2020-06-27 10:37:30.829871
Model ind 665 epoch 1465 batch: 600 avg loss -2.905426 avg loss no lamb -2.905426 time 2020-06-27 10:37:41.903388
Model ind 665 epoch 1465 batch: 700 avg loss -2.844275 avg loss no lamb -2.844275 time 2020-06-27 10:37:52.579103
Model ind 665 epoch 1465 batch: 800 avg loss -2.801071 avg loss no lamb -2.801071 time 2020-06-27 10:38:03.494808
last batch sz 10
Pre: time 2020-06-27 10:38:17.883348: 
 	std: 0.0029357208
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9798, 0.9741, 0.9803, 0.974]
	train_accs: [0.9816333, 0.9812, 0.97576666, 0.9816833, 0.9766167]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97764
	best: 0.9803

Starting e_i: 1466
Model ind 665 epoch 1466 batch: 0 avg loss -2.936952 avg loss no lamb -2.936952 time 2020-06-27 10:38:19.121486
Model ind 665 epoch 1466 batch: 100 avg loss -2.900496 avg loss no lamb -2.900496 time 2020-06-27 10:38:30.355196
Model ind 665 epoch 1466 batch: 200 avg loss -2.839762 avg loss no lamb -2.839762 time 2020-06-27 10:38:41.275067
Model ind 665 epoch 1466 batch: 300 avg loss -2.904235 avg loss no lamb -2.904235 time 2020-06-27 10:38:52.221551
Model ind 665 epoch 1466 batch: 400 avg loss -2.785435 avg loss no lamb -2.785435 time 2020-06-27 10:39:03.069829
Model ind 665 epoch 1466 batch: 500 avg loss -2.831485 avg loss no lamb -2.831485 time 2020-06-27 10:39:13.929799
Model ind 665 epoch 1466 batch: 600 avg loss -2.862952 avg loss no lamb -2.862952 time 2020-06-27 10:39:25.036875
Model ind 665 epoch 1466 batch: 700 avg loss -2.835236 avg loss no lamb -2.835236 time 2020-06-27 10:39:35.867500
Model ind 665 epoch 1466 batch: 800 avg loss -2.912588 avg loss no lamb -2.912588 time 2020-06-27 10:39:46.822235
last batch sz 10
Pre: time 2020-06-27 10:40:01.423792: 
 	std: 0.002798578
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9793, 0.9734, 0.9796, 0.9744]
	train_accs: [0.9817333, 0.98095, 0.9758833, 0.9818, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.9773
	best: 0.9796

Starting e_i: 1467
Model ind 665 epoch 1467 batch: 0 avg loss -2.905467 avg loss no lamb -2.905467 time 2020-06-27 10:40:02.661343
Model ind 665 epoch 1467 batch: 100 avg loss -2.938499 avg loss no lamb -2.938499 time 2020-06-27 10:40:13.190113
Model ind 665 epoch 1467 batch: 200 avg loss -2.909606 avg loss no lamb -2.909606 time 2020-06-27 10:40:23.883736
Model ind 665 epoch 1467 batch: 300 avg loss -2.926366 avg loss no lamb -2.926366 time 2020-06-27 10:40:35.032286
Model ind 665 epoch 1467 batch: 400 avg loss -2.815088 avg loss no lamb -2.815088 time 2020-06-27 10:40:45.914644
Model ind 665 epoch 1467 batch: 500 avg loss -2.832526 avg loss no lamb -2.832526 time 2020-06-27 10:40:56.878428
Model ind 665 epoch 1467 batch: 600 avg loss -2.860720 avg loss no lamb -2.860720 time 2020-06-27 10:41:07.818880
Model ind 665 epoch 1467 batch: 700 avg loss -2.801772 avg loss no lamb -2.801772 time 2020-06-27 10:41:19.020096
Model ind 665 epoch 1467 batch: 800 avg loss -2.870136 avg loss no lamb -2.870136 time 2020-06-27 10:41:29.802994
last batch sz 10
Pre: time 2020-06-27 10:41:44.121642: 
 	std: 0.003531909
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9799, 0.9731, 0.9811, 0.9738]
	train_accs: [0.9821, 0.9813, 0.9762333, 0.98218334, 0.97693336]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97774
	best: 0.9811

Starting e_i: 1468
Model ind 665 epoch 1468 batch: 0 avg loss -2.918207 avg loss no lamb -2.918207 time 2020-06-27 10:41:45.384875
Model ind 665 epoch 1468 batch: 100 avg loss -2.893736 avg loss no lamb -2.893736 time 2020-06-27 10:41:56.495718
Model ind 665 epoch 1468 batch: 200 avg loss -2.892328 avg loss no lamb -2.892328 time 2020-06-27 10:42:07.717047
Model ind 665 epoch 1468 batch: 300 avg loss -2.922158 avg loss no lamb -2.922158 time 2020-06-27 10:42:18.672876
Model ind 665 epoch 1468 batch: 400 avg loss -2.829684 avg loss no lamb -2.829684 time 2020-06-27 10:42:29.764269
Model ind 665 epoch 1468 batch: 500 avg loss -2.877246 avg loss no lamb -2.877246 time 2020-06-27 10:42:40.782559
Model ind 665 epoch 1468 batch: 600 avg loss -2.893348 avg loss no lamb -2.893348 time 2020-06-27 10:42:51.702174
Model ind 665 epoch 1468 batch: 700 avg loss -2.710981 avg loss no lamb -2.710981 time 2020-06-27 10:43:02.666843
Model ind 665 epoch 1468 batch: 800 avg loss -2.906660 avg loss no lamb -2.906660 time 2020-06-27 10:43:13.563579
last batch sz 10
Pre: time 2020-06-27 10:43:27.935937: 
 	std: 0.0027441473
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9792, 0.9732, 0.9791, 0.9745]
	train_accs: [0.9813667, 0.9810333, 0.9757, 0.9813667, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97716
	best: 0.9798

Starting e_i: 1469
Model ind 665 epoch 1469 batch: 0 avg loss -2.970613 avg loss no lamb -2.970613 time 2020-06-27 10:43:29.158902
Model ind 665 epoch 1469 batch: 100 avg loss -2.838719 avg loss no lamb -2.838719 time 2020-06-27 10:43:40.122192
Model ind 665 epoch 1469 batch: 200 avg loss -2.895346 avg loss no lamb -2.895346 time 2020-06-27 10:43:51.120112
Model ind 665 epoch 1469 batch: 300 avg loss -2.816937 avg loss no lamb -2.816937 time 2020-06-27 10:44:02.337995
Model ind 665 epoch 1469 batch: 400 avg loss -2.800600 avg loss no lamb -2.800600 time 2020-06-27 10:44:13.226666
Model ind 665 epoch 1469 batch: 500 avg loss -2.826172 avg loss no lamb -2.826172 time 2020-06-27 10:44:24.253288
Model ind 665 epoch 1469 batch: 600 avg loss -2.930616 avg loss no lamb -2.930616 time 2020-06-27 10:44:35.036756
Model ind 665 epoch 1469 batch: 700 avg loss -2.741593 avg loss no lamb -2.741593 time 2020-06-27 10:44:45.940858
Model ind 665 epoch 1469 batch: 800 avg loss -2.869130 avg loss no lamb -2.869130 time 2020-06-27 10:44:56.860871
last batch sz 10
Pre: time 2020-06-27 10:45:11.296658: 
 	std: 0.0032939839
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9787, 0.9719, 0.979, 0.9728]
	train_accs: [0.98115, 0.98038334, 0.97466666, 0.98076665, 0.97573334]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97635996
	best: 0.9794

Starting e_i: 1470
Model ind 665 epoch 1470 batch: 0 avg loss -2.983434 avg loss no lamb -2.983434 time 2020-06-27 10:45:12.528691
Model ind 665 epoch 1470 batch: 100 avg loss -2.946120 avg loss no lamb -2.946120 time 2020-06-27 10:45:23.270508
Model ind 665 epoch 1470 batch: 200 avg loss -2.906373 avg loss no lamb -2.906373 time 2020-06-27 10:45:34.371635
Model ind 665 epoch 1470 batch: 300 avg loss -2.867873 avg loss no lamb -2.867873 time 2020-06-27 10:45:45.242723
Model ind 665 epoch 1470 batch: 400 avg loss -2.822522 avg loss no lamb -2.822522 time 2020-06-27 10:45:56.063961
Model ind 665 epoch 1470 batch: 500 avg loss -2.812144 avg loss no lamb -2.812144 time 2020-06-27 10:46:06.842393
Model ind 665 epoch 1470 batch: 600 avg loss -2.894655 avg loss no lamb -2.894655 time 2020-06-27 10:46:17.642361
Model ind 665 epoch 1470 batch: 700 avg loss -2.787082 avg loss no lamb -2.787082 time 2020-06-27 10:46:28.432759
Model ind 665 epoch 1470 batch: 800 avg loss -2.822262 avg loss no lamb -2.822262 time 2020-06-27 10:46:39.508766
last batch sz 10
Pre: time 2020-06-27 10:46:54.296864: 
 	std: 0.0027666418
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9787, 0.9783, 0.9727, 0.9776, 0.9725]
	train_accs: [0.9809333, 0.9806167, 0.97538334, 0.9805667, 0.9752667]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97595996
	best: 0.9787

Starting e_i: 1471
Model ind 665 epoch 1471 batch: 0 avg loss -2.991323 avg loss no lamb -2.991323 time 2020-06-27 10:46:56.786320
Model ind 665 epoch 1471 batch: 100 avg loss -2.870572 avg loss no lamb -2.870572 time 2020-06-27 10:47:07.759921
Model ind 665 epoch 1471 batch: 200 avg loss -2.923698 avg loss no lamb -2.923698 time 2020-06-27 10:47:18.656430
Model ind 665 epoch 1471 batch: 300 avg loss -2.889688 avg loss no lamb -2.889688 time 2020-06-27 10:47:29.653304
Model ind 665 epoch 1471 batch: 400 avg loss -2.857862 avg loss no lamb -2.857862 time 2020-06-27 10:47:40.288977
Model ind 665 epoch 1471 batch: 500 avg loss -2.848857 avg loss no lamb -2.848857 time 2020-06-27 10:47:51.202864
Model ind 665 epoch 1471 batch: 600 avg loss -2.863248 avg loss no lamb -2.863248 time 2020-06-27 10:48:02.162461
Model ind 665 epoch 1471 batch: 700 avg loss -2.792645 avg loss no lamb -2.792645 time 2020-06-27 10:48:12.865611
Model ind 665 epoch 1471 batch: 800 avg loss -2.852580 avg loss no lamb -2.852580 time 2020-06-27 10:48:23.756110
last batch sz 10
Pre: time 2020-06-27 10:48:37.905978: 
 	std: 0.003316872
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.979, 0.9722, 0.9796, 0.9734]
	train_accs: [0.9810333, 0.98066664, 0.9747667, 0.98105, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97682
	best: 0.9796

Starting e_i: 1472
Model ind 665 epoch 1472 batch: 0 avg loss -2.960621 avg loss no lamb -2.960621 time 2020-06-27 10:48:39.186991
Model ind 665 epoch 1472 batch: 100 avg loss -2.840579 avg loss no lamb -2.840579 time 2020-06-27 10:48:49.980081
Model ind 665 epoch 1472 batch: 200 avg loss -2.890541 avg loss no lamb -2.890541 time 2020-06-27 10:49:00.737004
Model ind 665 epoch 1472 batch: 300 avg loss -2.830689 avg loss no lamb -2.830689 time 2020-06-27 10:49:11.467988
Model ind 665 epoch 1472 batch: 400 avg loss -2.828189 avg loss no lamb -2.828189 time 2020-06-27 10:49:22.320227
Model ind 665 epoch 1472 batch: 500 avg loss -2.855102 avg loss no lamb -2.855102 time 2020-06-27 10:49:33.142310
Model ind 665 epoch 1472 batch: 600 avg loss -2.897964 avg loss no lamb -2.897964 time 2020-06-27 10:49:43.885047
Model ind 665 epoch 1472 batch: 700 avg loss -2.775372 avg loss no lamb -2.775372 time 2020-06-27 10:49:55.022624
Model ind 665 epoch 1472 batch: 800 avg loss -2.854285 avg loss no lamb -2.854285 time 2020-06-27 10:50:06.086196
last batch sz 10
Pre: time 2020-06-27 10:50:20.651058: 
 	std: 0.0034231024
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9786, 0.9716, 0.9796, 0.9731]
	train_accs: [0.9811, 0.98036665, 0.9748, 0.9809, 0.97616667]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.97648
	best: 0.9795

Starting e_i: 1473
Model ind 665 epoch 1473 batch: 0 avg loss -2.953596 avg loss no lamb -2.953596 time 2020-06-27 10:50:21.949314
Model ind 665 epoch 1473 batch: 100 avg loss -2.882460 avg loss no lamb -2.882460 time 2020-06-27 10:50:32.755238
Model ind 665 epoch 1473 batch: 200 avg loss -2.944670 avg loss no lamb -2.944670 time 2020-06-27 10:50:43.432370
Model ind 665 epoch 1473 batch: 300 avg loss -2.866072 avg loss no lamb -2.866072 time 2020-06-27 10:50:54.237746
Model ind 665 epoch 1473 batch: 400 avg loss -2.824500 avg loss no lamb -2.824500 time 2020-06-27 10:51:05.267447
Model ind 665 epoch 1473 batch: 500 avg loss -2.883498 avg loss no lamb -2.883498 time 2020-06-27 10:51:16.171088
Model ind 665 epoch 1473 batch: 600 avg loss -2.846541 avg loss no lamb -2.846541 time 2020-06-27 10:51:26.857937
Model ind 665 epoch 1473 batch: 700 avg loss -2.808353 avg loss no lamb -2.808353 time 2020-06-27 10:51:37.960630
Model ind 665 epoch 1473 batch: 800 avg loss -2.906603 avg loss no lamb -2.906603 time 2020-06-27 10:51:48.682213
last batch sz 10
Pre: time 2020-06-27 10:52:02.730054: 
 	std: 0.002715449
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9786, 0.9778, 0.9722, 0.9785, 0.9735]
	train_accs: [0.9812667, 0.98053336, 0.9752667, 0.98106664, 0.97676665]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97612
	best: 0.9786

Starting e_i: 1474
Model ind 665 epoch 1474 batch: 0 avg loss -2.949873 avg loss no lamb -2.949873 time 2020-06-27 10:52:03.960948
Model ind 665 epoch 1474 batch: 100 avg loss -2.873980 avg loss no lamb -2.873980 time 2020-06-27 10:52:14.956971
Model ind 665 epoch 1474 batch: 200 avg loss -2.908793 avg loss no lamb -2.908793 time 2020-06-27 10:52:25.894248
Model ind 665 epoch 1474 batch: 300 avg loss -2.901500 avg loss no lamb -2.901500 time 2020-06-27 10:52:36.884862
Model ind 665 epoch 1474 batch: 400 avg loss -2.840931 avg loss no lamb -2.840931 time 2020-06-27 10:52:47.768455
Model ind 665 epoch 1474 batch: 500 avg loss -2.900925 avg loss no lamb -2.900925 time 2020-06-27 10:52:58.487222
Model ind 665 epoch 1474 batch: 600 avg loss -2.857629 avg loss no lamb -2.857629 time 2020-06-27 10:53:09.372914
Model ind 665 epoch 1474 batch: 700 avg loss -2.770930 avg loss no lamb -2.770930 time 2020-06-27 10:53:20.263033
Model ind 665 epoch 1474 batch: 800 avg loss -2.899764 avg loss no lamb -2.899764 time 2020-06-27 10:53:31.039964
last batch sz 10
Pre: time 2020-06-27 10:53:45.116552: 
 	std: 0.0039362046
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9793, 0.9711, 0.9798, 0.9726]
	train_accs: [0.9812, 0.98075, 0.9744833, 0.98116666, 0.97573334]
	best_train_sub_head: 0
	worst: 0.9711
	avg: 0.9766199
	best: 0.9803

Starting e_i: 1475
Model ind 665 epoch 1475 batch: 0 avg loss -2.962021 avg loss no lamb -2.962021 time 2020-06-27 10:53:46.549014
Model ind 665 epoch 1475 batch: 100 avg loss -2.876619 avg loss no lamb -2.876619 time 2020-06-27 10:53:57.544104
Model ind 665 epoch 1475 batch: 200 avg loss -2.896673 avg loss no lamb -2.896673 time 2020-06-27 10:54:08.640259
Model ind 665 epoch 1475 batch: 300 avg loss -2.861590 avg loss no lamb -2.861590 time 2020-06-27 10:54:19.629381
Model ind 665 epoch 1475 batch: 400 avg loss -2.812669 avg loss no lamb -2.812669 time 2020-06-27 10:54:30.480536
Model ind 665 epoch 1475 batch: 500 avg loss -2.852714 avg loss no lamb -2.852714 time 2020-06-27 10:54:41.333284
Model ind 665 epoch 1475 batch: 600 avg loss -2.893913 avg loss no lamb -2.893913 time 2020-06-27 10:54:52.318934
Model ind 665 epoch 1475 batch: 700 avg loss -2.755848 avg loss no lamb -2.755848 time 2020-06-27 10:55:03.257725
Model ind 665 epoch 1475 batch: 800 avg loss -2.854133 avg loss no lamb -2.854133 time 2020-06-27 10:55:14.439297
last batch sz 10
Pre: time 2020-06-27 10:55:28.592822: 
 	std: 0.004089303
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9795, 0.9714, 0.9806, 0.9726]
	train_accs: [0.9815, 0.9810333, 0.97498333, 0.9816833, 0.9755667]
	best_train_sub_head: 3
	worst: 0.9714
	avg: 0.97696
	best: 0.9806

Starting e_i: 1476
Model ind 665 epoch 1476 batch: 0 avg loss -2.959525 avg loss no lamb -2.959525 time 2020-06-27 10:55:29.866222
Model ind 665 epoch 1476 batch: 100 avg loss -2.881726 avg loss no lamb -2.881726 time 2020-06-27 10:55:40.686248
Model ind 665 epoch 1476 batch: 200 avg loss -2.905330 avg loss no lamb -2.905330 time 2020-06-27 10:55:51.684821
Model ind 665 epoch 1476 batch: 300 avg loss -2.853450 avg loss no lamb -2.853450 time 2020-06-27 10:56:02.642540
Model ind 665 epoch 1476 batch: 400 avg loss -2.817258 avg loss no lamb -2.817258 time 2020-06-27 10:56:13.500560
Model ind 665 epoch 1476 batch: 500 avg loss -2.879156 avg loss no lamb -2.879156 time 2020-06-27 10:56:24.444638
Model ind 665 epoch 1476 batch: 600 avg loss -2.926762 avg loss no lamb -2.926762 time 2020-06-27 10:56:35.460970
Model ind 665 epoch 1476 batch: 700 avg loss -2.825881 avg loss no lamb -2.825881 time 2020-06-27 10:56:46.428828
Model ind 665 epoch 1476 batch: 800 avg loss -2.867603 avg loss no lamb -2.867603 time 2020-06-27 10:56:57.403857
last batch sz 10
Pre: time 2020-06-27 10:57:11.564913: 
 	std: 0.003087135
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9791, 0.9723, 0.9798, 0.974]
	train_accs: [0.9814, 0.98085, 0.97573334, 0.98165, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97686005
	best: 0.9798

Starting e_i: 1477
Model ind 665 epoch 1477 batch: 0 avg loss -2.964353 avg loss no lamb -2.964353 time 2020-06-27 10:57:13.033518
Model ind 665 epoch 1477 batch: 100 avg loss -2.875472 avg loss no lamb -2.875472 time 2020-06-27 10:57:24.149788
Model ind 665 epoch 1477 batch: 200 avg loss -2.898058 avg loss no lamb -2.898058 time 2020-06-27 10:57:35.186229
Model ind 665 epoch 1477 batch: 300 avg loss -2.894476 avg loss no lamb -2.894476 time 2020-06-27 10:57:46.096606
Model ind 665 epoch 1477 batch: 400 avg loss -2.816931 avg loss no lamb -2.816931 time 2020-06-27 10:57:57.105346
Model ind 665 epoch 1477 batch: 500 avg loss -2.862360 avg loss no lamb -2.862360 time 2020-06-27 10:58:08.257825
Model ind 665 epoch 1477 batch: 600 avg loss -2.848108 avg loss no lamb -2.848108 time 2020-06-27 10:58:19.358923
Model ind 665 epoch 1477 batch: 700 avg loss -2.807560 avg loss no lamb -2.807560 time 2020-06-27 10:58:30.351819
Model ind 665 epoch 1477 batch: 800 avg loss -2.828188 avg loss no lamb -2.828188 time 2020-06-27 10:58:41.241040
last batch sz 10
Pre: time 2020-06-27 10:58:55.396686: 
 	std: 0.003534623
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9786, 0.9713, 0.9794, 0.9727]
	train_accs: [0.98088336, 0.9803333, 0.9746, 0.9808, 0.9752167]
	best_train_sub_head: 0
	worst: 0.9713
	avg: 0.9762799
	best: 0.9794

Starting e_i: 1478
Model ind 665 epoch 1478 batch: 0 avg loss -2.965900 avg loss no lamb -2.965900 time 2020-06-27 10:58:56.656273
Model ind 665 epoch 1478 batch: 100 avg loss -2.880837 avg loss no lamb -2.880837 time 2020-06-27 10:59:07.512409
Model ind 665 epoch 1478 batch: 200 avg loss -2.862934 avg loss no lamb -2.862934 time 2020-06-27 10:59:18.430124
Model ind 665 epoch 1478 batch: 300 avg loss -2.862533 avg loss no lamb -2.862533 time 2020-06-27 10:59:29.244433
Model ind 665 epoch 1478 batch: 400 avg loss -2.772461 avg loss no lamb -2.772461 time 2020-06-27 10:59:40.162189
Model ind 665 epoch 1478 batch: 500 avg loss -2.839515 avg loss no lamb -2.839515 time 2020-06-27 10:59:51.007978
Model ind 665 epoch 1478 batch: 600 avg loss -2.913192 avg loss no lamb -2.913192 time 2020-06-27 11:00:01.782788
Model ind 665 epoch 1478 batch: 700 avg loss -2.807510 avg loss no lamb -2.807510 time 2020-06-27 11:00:12.568219
Model ind 665 epoch 1478 batch: 800 avg loss -2.900191 avg loss no lamb -2.900191 time 2020-06-27 11:00:23.775163
last batch sz 10
Pre: time 2020-06-27 11:00:37.819349: 
 	std: 0.003170248
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9789, 0.9722, 0.979, 0.9737]
	train_accs: [0.9812833, 0.98006666, 0.97393334, 0.9811, 0.9756333]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97676
	best: 0.98

Starting e_i: 1479
Model ind 665 epoch 1479 batch: 0 avg loss -2.936717 avg loss no lamb -2.936717 time 2020-06-27 11:00:39.075650
Model ind 665 epoch 1479 batch: 100 avg loss -2.919187 avg loss no lamb -2.919187 time 2020-06-27 11:00:50.055962
Model ind 665 epoch 1479 batch: 200 avg loss -2.878381 avg loss no lamb -2.878381 time 2020-06-27 11:01:01.104677
Model ind 665 epoch 1479 batch: 300 avg loss -2.892241 avg loss no lamb -2.892241 time 2020-06-27 11:01:12.039079
Model ind 665 epoch 1479 batch: 400 avg loss -2.768841 avg loss no lamb -2.768841 time 2020-06-27 11:01:22.954191
Model ind 665 epoch 1479 batch: 500 avg loss -2.840831 avg loss no lamb -2.840831 time 2020-06-27 11:01:33.819237
Model ind 665 epoch 1479 batch: 600 avg loss -2.878577 avg loss no lamb -2.878577 time 2020-06-27 11:01:44.864068
Model ind 665 epoch 1479 batch: 700 avg loss -2.848916 avg loss no lamb -2.848916 time 2020-06-27 11:01:55.807977
Model ind 665 epoch 1479 batch: 800 avg loss -2.900179 avg loss no lamb -2.900179 time 2020-06-27 11:02:06.872117
last batch sz 10
Pre: time 2020-06-27 11:02:21.457245: 
 	std: 0.0035838487
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9806, 0.9727, 0.9808, 0.9742]
	train_accs: [0.9812833, 0.98118335, 0.97531664, 0.9813, 0.97665]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.9778
	best: 0.9808

Starting e_i: 1480
Model ind 665 epoch 1480 batch: 0 avg loss -2.941521 avg loss no lamb -2.941521 time 2020-06-27 11:02:22.742595
Model ind 665 epoch 1480 batch: 100 avg loss -2.931604 avg loss no lamb -2.931604 time 2020-06-27 11:02:33.686602
Model ind 665 epoch 1480 batch: 200 avg loss -2.909646 avg loss no lamb -2.909646 time 2020-06-27 11:02:44.515476
Model ind 665 epoch 1480 batch: 300 avg loss -2.896224 avg loss no lamb -2.896224 time 2020-06-27 11:02:55.681470
Model ind 665 epoch 1480 batch: 400 avg loss -2.767425 avg loss no lamb -2.767425 time 2020-06-27 11:03:06.778033
Model ind 665 epoch 1480 batch: 500 avg loss -2.832654 avg loss no lamb -2.832654 time 2020-06-27 11:03:17.664508
Model ind 665 epoch 1480 batch: 600 avg loss -2.889867 avg loss no lamb -2.889867 time 2020-06-27 11:03:28.543026
Model ind 665 epoch 1480 batch: 700 avg loss -2.779574 avg loss no lamb -2.779574 time 2020-06-27 11:03:39.688781
Model ind 665 epoch 1480 batch: 800 avg loss -2.869708 avg loss no lamb -2.869708 time 2020-06-27 11:03:50.675228
last batch sz 10
Pre: time 2020-06-27 11:04:05.045869: 
 	std: 0.0035033664
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9801, 0.9735, 0.9807, 0.9733]
	train_accs: [0.98155, 0.9806833, 0.9752167, 0.9812833, 0.9757]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97768
	best: 0.9808

Starting e_i: 1481
Model ind 665 epoch 1481 batch: 0 avg loss -2.972798 avg loss no lamb -2.972798 time 2020-06-27 11:04:07.492082
Model ind 665 epoch 1481 batch: 100 avg loss -2.953823 avg loss no lamb -2.953823 time 2020-06-27 11:04:18.484747
Model ind 665 epoch 1481 batch: 200 avg loss -2.861782 avg loss no lamb -2.861782 time 2020-06-27 11:04:29.437199
Model ind 665 epoch 1481 batch: 300 avg loss -2.892833 avg loss no lamb -2.892833 time 2020-06-27 11:04:40.336117
Model ind 665 epoch 1481 batch: 400 avg loss -2.789416 avg loss no lamb -2.789416 time 2020-06-27 11:04:51.174244
Model ind 665 epoch 1481 batch: 500 avg loss -2.839813 avg loss no lamb -2.839813 time 2020-06-27 11:05:02.167395
Model ind 665 epoch 1481 batch: 600 avg loss -2.850562 avg loss no lamb -2.850562 time 2020-06-27 11:05:12.939591
Model ind 665 epoch 1481 batch: 700 avg loss -2.779234 avg loss no lamb -2.779234 time 2020-06-27 11:05:24.190092
Model ind 665 epoch 1481 batch: 800 avg loss -2.852600 avg loss no lamb -2.852600 time 2020-06-27 11:05:35.029108
last batch sz 10
Pre: time 2020-06-27 11:05:49.368148: 
 	std: 0.0024576443
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.979, 0.9742, 0.9793, 0.9744]
	train_accs: [0.9810333, 0.9803, 0.9755167, 0.98071665, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.9773
	best: 0.9796

Starting e_i: 1482
Model ind 665 epoch 1482 batch: 0 avg loss -2.952228 avg loss no lamb -2.952228 time 2020-06-27 11:05:50.707550
Model ind 665 epoch 1482 batch: 100 avg loss -2.914362 avg loss no lamb -2.914362 time 2020-06-27 11:06:01.495159
Model ind 665 epoch 1482 batch: 200 avg loss -2.907700 avg loss no lamb -2.907700 time 2020-06-27 11:06:12.438871
Model ind 665 epoch 1482 batch: 300 avg loss -2.857756 avg loss no lamb -2.857756 time 2020-06-27 11:06:23.439797
Model ind 665 epoch 1482 batch: 400 avg loss -2.877148 avg loss no lamb -2.877148 time 2020-06-27 11:06:34.372350
Model ind 665 epoch 1482 batch: 500 avg loss -2.881548 avg loss no lamb -2.881548 time 2020-06-27 11:06:45.134885
Model ind 665 epoch 1482 batch: 600 avg loss -2.940821 avg loss no lamb -2.940821 time 2020-06-27 11:06:55.941488
Model ind 665 epoch 1482 batch: 700 avg loss -2.816236 avg loss no lamb -2.816236 time 2020-06-27 11:07:07.036650
Model ind 665 epoch 1482 batch: 800 avg loss -2.883048 avg loss no lamb -2.883048 time 2020-06-27 11:07:17.833451
last batch sz 10
Pre: time 2020-06-27 11:07:32.022148: 
 	std: 0.0034179543
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9802, 0.9735, 0.9814, 0.9745]
	train_accs: [0.9816667, 0.9810333, 0.97541666, 0.9816667, 0.9767]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97814006
	best: 0.9811

Starting e_i: 1483
Model ind 665 epoch 1483 batch: 0 avg loss -2.993370 avg loss no lamb -2.993370 time 2020-06-27 11:07:33.302635
Model ind 665 epoch 1483 batch: 100 avg loss -2.940968 avg loss no lamb -2.940968 time 2020-06-27 11:07:44.470256
Model ind 665 epoch 1483 batch: 200 avg loss -2.884356 avg loss no lamb -2.884356 time 2020-06-27 11:07:55.502973
Model ind 665 epoch 1483 batch: 300 avg loss -2.853199 avg loss no lamb -2.853199 time 2020-06-27 11:08:06.500761
Model ind 665 epoch 1483 batch: 400 avg loss -2.825333 avg loss no lamb -2.825333 time 2020-06-27 11:08:17.350066
Model ind 665 epoch 1483 batch: 500 avg loss -2.904762 avg loss no lamb -2.904762 time 2020-06-27 11:08:28.398581
Model ind 665 epoch 1483 batch: 600 avg loss -2.893250 avg loss no lamb -2.893250 time 2020-06-27 11:08:39.182523
Model ind 665 epoch 1483 batch: 700 avg loss -2.807678 avg loss no lamb -2.807678 time 2020-06-27 11:08:49.957811
Model ind 665 epoch 1483 batch: 800 avg loss -2.902710 avg loss no lamb -2.902710 time 2020-06-27 11:09:00.872156
last batch sz 10
Pre: time 2020-06-27 11:09:15.370521: 
 	std: 0.003770936
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9811, 0.9742, 0.9819, 0.9738]
	train_accs: [0.9821333, 0.9813167, 0.97543335, 0.9822, 0.976]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.9786
	best: 0.9819

Starting e_i: 1484
Model ind 665 epoch 1484 batch: 0 avg loss -2.954519 avg loss no lamb -2.954519 time 2020-06-27 11:09:16.675502
Model ind 665 epoch 1484 batch: 100 avg loss -2.924987 avg loss no lamb -2.924987 time 2020-06-27 11:09:27.555417
Model ind 665 epoch 1484 batch: 200 avg loss -2.897975 avg loss no lamb -2.897975 time 2020-06-27 11:09:38.295213
Model ind 665 epoch 1484 batch: 300 avg loss -2.917232 avg loss no lamb -2.917232 time 2020-06-27 11:09:49.055018
Model ind 665 epoch 1484 batch: 400 avg loss -2.791979 avg loss no lamb -2.791979 time 2020-06-27 11:10:00.019663
Model ind 665 epoch 1484 batch: 500 avg loss -2.881793 avg loss no lamb -2.881793 time 2020-06-27 11:10:10.918373
Model ind 665 epoch 1484 batch: 600 avg loss -2.881121 avg loss no lamb -2.881121 time 2020-06-27 11:10:21.820295
Model ind 665 epoch 1484 batch: 700 avg loss -2.840003 avg loss no lamb -2.840003 time 2020-06-27 11:10:32.878255
Model ind 665 epoch 1484 batch: 800 avg loss -2.891026 avg loss no lamb -2.891026 time 2020-06-27 11:10:43.888259
last batch sz 10
Pre: time 2020-06-27 11:10:58.283099: 
 	std: 0.0028684454
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9788, 0.9739, 0.9802, 0.9747]
	train_accs: [0.98125, 0.98066664, 0.9755333, 0.98115, 0.9768]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97770005
	best: 0.9809

Starting e_i: 1485
Model ind 665 epoch 1485 batch: 0 avg loss -2.946744 avg loss no lamb -2.946744 time 2020-06-27 11:10:59.552027
Model ind 665 epoch 1485 batch: 100 avg loss -2.955851 avg loss no lamb -2.955851 time 2020-06-27 11:11:10.580708
Model ind 665 epoch 1485 batch: 200 avg loss -2.851844 avg loss no lamb -2.851844 time 2020-06-27 11:11:21.501126
Model ind 665 epoch 1485 batch: 300 avg loss -2.852785 avg loss no lamb -2.852785 time 2020-06-27 11:11:32.424420
Model ind 665 epoch 1485 batch: 400 avg loss -2.817065 avg loss no lamb -2.817065 time 2020-06-27 11:11:43.276889
Model ind 665 epoch 1485 batch: 500 avg loss -2.837325 avg loss no lamb -2.837325 time 2020-06-27 11:11:54.247458
Model ind 665 epoch 1485 batch: 600 avg loss -2.856521 avg loss no lamb -2.856521 time 2020-06-27 11:12:05.319708
Model ind 665 epoch 1485 batch: 700 avg loss -2.800843 avg loss no lamb -2.800843 time 2020-06-27 11:12:16.090605
Model ind 665 epoch 1485 batch: 800 avg loss -2.850674 avg loss no lamb -2.850674 time 2020-06-27 11:12:27.071691
last batch sz 10
Pre: time 2020-06-27 11:12:41.142684: 
 	std: 0.0032798732
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9794, 0.973, 0.9801, 0.9739]
	train_accs: [0.9812833, 0.98083335, 0.9751667, 0.98116666, 0.9763333]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97742
	best: 0.9807

Starting e_i: 1486
Model ind 665 epoch 1486 batch: 0 avg loss -2.935312 avg loss no lamb -2.935312 time 2020-06-27 11:12:42.566616
Model ind 665 epoch 1486 batch: 100 avg loss -2.866798 avg loss no lamb -2.866798 time 2020-06-27 11:12:53.392736
Model ind 665 epoch 1486 batch: 200 avg loss -2.891264 avg loss no lamb -2.891264 time 2020-06-27 11:13:04.269809
Model ind 665 epoch 1486 batch: 300 avg loss -2.885018 avg loss no lamb -2.885018 time 2020-06-27 11:13:15.246892
Model ind 665 epoch 1486 batch: 400 avg loss -2.863441 avg loss no lamb -2.863441 time 2020-06-27 11:13:26.163566
Model ind 665 epoch 1486 batch: 500 avg loss -2.878063 avg loss no lamb -2.878063 time 2020-06-27 11:13:37.196220
Model ind 665 epoch 1486 batch: 600 avg loss -2.894891 avg loss no lamb -2.894891 time 2020-06-27 11:13:48.048566
Model ind 665 epoch 1486 batch: 700 avg loss -2.810054 avg loss no lamb -2.810054 time 2020-06-27 11:13:59.035402
Model ind 665 epoch 1486 batch: 800 avg loss -2.913952 avg loss no lamb -2.913952 time 2020-06-27 11:14:09.999191
last batch sz 10
Pre: time 2020-06-27 11:14:24.134832: 
 	std: 0.0031390574
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9796, 0.974, 0.981, 0.9747]
	train_accs: [0.98181665, 0.98125, 0.97578335, 0.9816, 0.97616667]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97812
	best: 0.9813

Starting e_i: 1487
Model ind 665 epoch 1487 batch: 0 avg loss -2.920656 avg loss no lamb -2.920656 time 2020-06-27 11:14:25.412399
Model ind 665 epoch 1487 batch: 100 avg loss -2.863475 avg loss no lamb -2.863475 time 2020-06-27 11:14:36.403969
Model ind 665 epoch 1487 batch: 200 avg loss -2.879414 avg loss no lamb -2.879414 time 2020-06-27 11:14:47.342604
Model ind 665 epoch 1487 batch: 300 avg loss -2.820503 avg loss no lamb -2.820503 time 2020-06-27 11:14:58.169858
Model ind 665 epoch 1487 batch: 400 avg loss -2.772432 avg loss no lamb -2.772432 time 2020-06-27 11:15:08.883093
Model ind 665 epoch 1487 batch: 500 avg loss -2.874966 avg loss no lamb -2.874966 time 2020-06-27 11:15:19.790293
Model ind 665 epoch 1487 batch: 600 avg loss -2.897230 avg loss no lamb -2.897230 time 2020-06-27 11:15:30.635234
Model ind 665 epoch 1487 batch: 700 avg loss -2.774840 avg loss no lamb -2.774840 time 2020-06-27 11:15:41.432016
Model ind 665 epoch 1487 batch: 800 avg loss -2.914977 avg loss no lamb -2.914977 time 2020-06-27 11:15:52.364731
last batch sz 10
Pre: time 2020-06-27 11:16:06.470241: 
 	std: 0.002792569
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9822, 0.9812, 0.9754, 0.9819, 0.977]
	train_accs: [0.9821333, 0.98191667, 0.9763333, 0.98226666, 0.97748333]
	best_train_sub_head: 3
	worst: 0.9754
	avg: 0.97954
	best: 0.9819

Starting e_i: 1488
Model ind 665 epoch 1488 batch: 0 avg loss -2.945862 avg loss no lamb -2.945862 time 2020-06-27 11:16:07.959674
Model ind 665 epoch 1488 batch: 100 avg loss -2.879305 avg loss no lamb -2.879305 time 2020-06-27 11:16:18.926295
Model ind 665 epoch 1488 batch: 200 avg loss -2.845948 avg loss no lamb -2.845948 time 2020-06-27 11:16:29.735860
Model ind 665 epoch 1488 batch: 300 avg loss -2.870947 avg loss no lamb -2.870947 time 2020-06-27 11:16:40.726126
Model ind 665 epoch 1488 batch: 400 avg loss -2.792463 avg loss no lamb -2.792463 time 2020-06-27 11:16:51.766766
Model ind 665 epoch 1488 batch: 500 avg loss -2.782781 avg loss no lamb -2.782781 time 2020-06-27 11:17:02.904960
Model ind 665 epoch 1488 batch: 600 avg loss -2.855011 avg loss no lamb -2.855011 time 2020-06-27 11:17:13.862021
Model ind 665 epoch 1488 batch: 700 avg loss -2.828069 avg loss no lamb -2.828069 time 2020-06-27 11:17:24.699039
Model ind 665 epoch 1488 batch: 800 avg loss -2.901565 avg loss no lamb -2.901565 time 2020-06-27 11:17:35.649879
last batch sz 10
Pre: time 2020-06-27 11:17:49.885711: 
 	std: 0.002835759
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9798, 0.975, 0.9812, 0.975]
	train_accs: [0.9819833, 0.9813, 0.97566664, 0.98195, 0.97665]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.9784201
	best: 0.9811

Starting e_i: 1489
Model ind 665 epoch 1489 batch: 0 avg loss -2.948021 avg loss no lamb -2.948021 time 2020-06-27 11:17:51.139186
Model ind 665 epoch 1489 batch: 100 avg loss -2.953026 avg loss no lamb -2.953026 time 2020-06-27 11:18:01.919076
Model ind 665 epoch 1489 batch: 200 avg loss -2.907797 avg loss no lamb -2.907797 time 2020-06-27 11:18:12.782562
Model ind 665 epoch 1489 batch: 300 avg loss -2.852976 avg loss no lamb -2.852976 time 2020-06-27 11:18:23.478878
Model ind 665 epoch 1489 batch: 400 avg loss -2.832477 avg loss no lamb -2.832477 time 2020-06-27 11:18:34.740792
Model ind 665 epoch 1489 batch: 500 avg loss -2.840784 avg loss no lamb -2.840784 time 2020-06-27 11:18:45.647956
Model ind 665 epoch 1489 batch: 600 avg loss -2.879114 avg loss no lamb -2.879114 time 2020-06-27 11:18:56.560902
Model ind 665 epoch 1489 batch: 700 avg loss -2.828968 avg loss no lamb -2.828968 time 2020-06-27 11:19:07.353414
Model ind 665 epoch 1489 batch: 800 avg loss -2.851763 avg loss no lamb -2.851763 time 2020-06-27 11:19:18.130894
last batch sz 10
Pre: time 2020-06-27 11:19:32.181285: 
 	std: 0.00293163
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9804, 0.9744, 0.9805, 0.9745]
	train_accs: [0.98148334, 0.9810167, 0.97571665, 0.9816167, 0.97651666]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97804004
	best: 0.9805

Starting e_i: 1490
Model ind 665 epoch 1490 batch: 0 avg loss -2.916558 avg loss no lamb -2.916558 time 2020-06-27 11:19:33.554041
Model ind 665 epoch 1490 batch: 100 avg loss -2.868442 avg loss no lamb -2.868442 time 2020-06-27 11:19:44.372930
Model ind 665 epoch 1490 batch: 200 avg loss -2.901540 avg loss no lamb -2.901540 time 2020-06-27 11:19:55.342395
Model ind 665 epoch 1490 batch: 300 avg loss -2.878037 avg loss no lamb -2.878037 time 2020-06-27 11:20:06.440400
Model ind 665 epoch 1490 batch: 400 avg loss -2.842664 avg loss no lamb -2.842664 time 2020-06-27 11:20:17.514058
Model ind 665 epoch 1490 batch: 500 avg loss -2.803205 avg loss no lamb -2.803205 time 2020-06-27 11:20:28.441033
Model ind 665 epoch 1490 batch: 600 avg loss -2.884346 avg loss no lamb -2.884346 time 2020-06-27 11:20:39.446281
Model ind 665 epoch 1490 batch: 700 avg loss -2.842935 avg loss no lamb -2.842935 time 2020-06-27 11:20:50.430799
Model ind 665 epoch 1490 batch: 800 avg loss -2.877305 avg loss no lamb -2.877305 time 2020-06-27 11:21:01.254688
last batch sz 10
Pre: time 2020-06-27 11:21:15.759187: 
 	std: 0.0032168233
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9795, 0.9738, 0.9809, 0.9738]
	train_accs: [0.98148334, 0.98088336, 0.9752, 0.9816333, 0.9762]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97770005
	best: 0.9809

Starting e_i: 1491
Model ind 665 epoch 1491 batch: 0 avg loss -2.978344 avg loss no lamb -2.978344 time 2020-06-27 11:21:18.607268
Model ind 665 epoch 1491 batch: 100 avg loss -2.886195 avg loss no lamb -2.886195 time 2020-06-27 11:21:29.700183
Model ind 665 epoch 1491 batch: 200 avg loss -2.851512 avg loss no lamb -2.851512 time 2020-06-27 11:21:40.662140
Model ind 665 epoch 1491 batch: 300 avg loss -2.908261 avg loss no lamb -2.908261 time 2020-06-27 11:21:51.470649
Model ind 665 epoch 1491 batch: 400 avg loss -2.808143 avg loss no lamb -2.808143 time 2020-06-27 11:22:02.267706
Model ind 665 epoch 1491 batch: 500 avg loss -2.814148 avg loss no lamb -2.814148 time 2020-06-27 11:22:13.188600
Model ind 665 epoch 1491 batch: 600 avg loss -2.890719 avg loss no lamb -2.890719 time 2020-06-27 11:22:23.973328
Model ind 665 epoch 1491 batch: 700 avg loss -2.808935 avg loss no lamb -2.808935 time 2020-06-27 11:22:34.910940
Model ind 665 epoch 1491 batch: 800 avg loss -2.897763 avg loss no lamb -2.897763 time 2020-06-27 11:22:45.777589
last batch sz 10
Pre: time 2020-06-27 11:22:59.734769: 
 	std: 0.0034652627
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9797, 0.9732, 0.9815, 0.9745]
	train_accs: [0.9813, 0.9810333, 0.9756, 0.98145, 0.9762167]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97800004
	best: 0.9815

Starting e_i: 1492
Model ind 665 epoch 1492 batch: 0 avg loss -2.930429 avg loss no lamb -2.930429 time 2020-06-27 11:23:00.985733
Model ind 665 epoch 1492 batch: 100 avg loss -2.859684 avg loss no lamb -2.859684 time 2020-06-27 11:23:11.543633
Model ind 665 epoch 1492 batch: 200 avg loss -2.892019 avg loss no lamb -2.892019 time 2020-06-27 11:23:22.440613
Model ind 665 epoch 1492 batch: 300 avg loss -2.876479 avg loss no lamb -2.876479 time 2020-06-27 11:23:33.338231
Model ind 665 epoch 1492 batch: 400 avg loss -2.806325 avg loss no lamb -2.806325 time 2020-06-27 11:23:44.168493
Model ind 665 epoch 1492 batch: 500 avg loss -2.899114 avg loss no lamb -2.899114 time 2020-06-27 11:23:55.257136
Model ind 665 epoch 1492 batch: 600 avg loss -2.882778 avg loss no lamb -2.882778 time 2020-06-27 11:24:06.268248
Model ind 665 epoch 1492 batch: 700 avg loss -2.792462 avg loss no lamb -2.792462 time 2020-06-27 11:24:17.240766
Model ind 665 epoch 1492 batch: 800 avg loss -2.845774 avg loss no lamb -2.845774 time 2020-06-27 11:24:28.171148
last batch sz 10
Pre: time 2020-06-27 11:24:42.498776: 
 	std: 0.0028957946
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9792, 0.9738, 0.9803, 0.974]
	train_accs: [0.9810333, 0.98036665, 0.9748833, 0.9809167, 0.97603333]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97742003
	best: 0.9798

Starting e_i: 1493
Model ind 665 epoch 1493 batch: 0 avg loss -2.967516 avg loss no lamb -2.967516 time 2020-06-27 11:24:43.786665
Model ind 665 epoch 1493 batch: 100 avg loss -2.963217 avg loss no lamb -2.963217 time 2020-06-27 11:24:54.641205
Model ind 665 epoch 1493 batch: 200 avg loss -2.911813 avg loss no lamb -2.911813 time 2020-06-27 11:25:05.577461
Model ind 665 epoch 1493 batch: 300 avg loss -2.891495 avg loss no lamb -2.891495 time 2020-06-27 11:25:16.402228
Model ind 665 epoch 1493 batch: 400 avg loss -2.793797 avg loss no lamb -2.793797 time 2020-06-27 11:25:27.102753
Model ind 665 epoch 1493 batch: 500 avg loss -2.866519 avg loss no lamb -2.866519 time 2020-06-27 11:25:37.978050
Model ind 665 epoch 1493 batch: 600 avg loss -2.881969 avg loss no lamb -2.881969 time 2020-06-27 11:25:48.900865
Model ind 665 epoch 1493 batch: 700 avg loss -2.787008 avg loss no lamb -2.787008 time 2020-06-27 11:25:59.824003
Model ind 665 epoch 1493 batch: 800 avg loss -2.861137 avg loss no lamb -2.861137 time 2020-06-27 11:26:10.559762
last batch sz 10
Pre: time 2020-06-27 11:26:24.908167: 
 	std: 0.0032579666
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9801, 0.9731, 0.9809, 0.9752]
	train_accs: [0.9813167, 0.9809833, 0.97545, 0.98148334, 0.9762]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97804004
	best: 0.9809

Starting e_i: 1494
Model ind 665 epoch 1494 batch: 0 avg loss -2.987934 avg loss no lamb -2.987934 time 2020-06-27 11:26:26.255153
Model ind 665 epoch 1494 batch: 100 avg loss -2.900581 avg loss no lamb -2.900581 time 2020-06-27 11:26:37.467633
Model ind 665 epoch 1494 batch: 200 avg loss -2.876322 avg loss no lamb -2.876322 time 2020-06-27 11:26:48.505869
Model ind 665 epoch 1494 batch: 300 avg loss -2.890566 avg loss no lamb -2.890566 time 2020-06-27 11:26:59.397308
Model ind 665 epoch 1494 batch: 400 avg loss -2.774929 avg loss no lamb -2.774929 time 2020-06-27 11:27:10.462527
Model ind 665 epoch 1494 batch: 500 avg loss -2.852503 avg loss no lamb -2.852503 time 2020-06-27 11:27:21.334983
Model ind 665 epoch 1494 batch: 600 avg loss -2.949732 avg loss no lamb -2.949732 time 2020-06-27 11:27:32.102566
Model ind 665 epoch 1494 batch: 700 avg loss -2.800088 avg loss no lamb -2.800088 time 2020-06-27 11:27:43.119409
Model ind 665 epoch 1494 batch: 800 avg loss -2.849479 avg loss no lamb -2.849479 time 2020-06-27 11:27:54.060813
last batch sz 10
Pre: time 2020-06-27 11:28:08.538961: 
 	std: 0.0024904637
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9799, 0.9749, 0.9804, 0.9758]
	train_accs: [0.9817333, 0.9806833, 0.97545, 0.98125, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97836
	best: 0.9808

Starting e_i: 1495
Model ind 665 epoch 1495 batch: 0 avg loss -2.995134 avg loss no lamb -2.995134 time 2020-06-27 11:28:09.812915
Model ind 665 epoch 1495 batch: 100 avg loss -2.972186 avg loss no lamb -2.972186 time 2020-06-27 11:28:20.703585
Model ind 665 epoch 1495 batch: 200 avg loss -2.884752 avg loss no lamb -2.884752 time 2020-06-27 11:28:31.650670
Model ind 665 epoch 1495 batch: 300 avg loss -2.900658 avg loss no lamb -2.900658 time 2020-06-27 11:28:42.633431
Model ind 665 epoch 1495 batch: 400 avg loss -2.814298 avg loss no lamb -2.814298 time 2020-06-27 11:28:53.462812
Model ind 665 epoch 1495 batch: 500 avg loss -2.858895 avg loss no lamb -2.858895 time 2020-06-27 11:29:04.319143
Model ind 665 epoch 1495 batch: 600 avg loss -2.876300 avg loss no lamb -2.876300 time 2020-06-27 11:29:15.426046
Model ind 665 epoch 1495 batch: 700 avg loss -2.712524 avg loss no lamb -2.712524 time 2020-06-27 11:29:26.087753
Model ind 665 epoch 1495 batch: 800 avg loss -2.879760 avg loss no lamb -2.879760 time 2020-06-27 11:29:37.251718
last batch sz 10
Pre: time 2020-06-27 11:29:51.539704: 
 	std: 0.0027895512
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.979, 0.9726, 0.9794, 0.9745]
	train_accs: [0.9810333, 0.9803333, 0.97525, 0.98121667, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97687995
	best: 0.9794

Starting e_i: 1496
Model ind 665 epoch 1496 batch: 0 avg loss -2.957638 avg loss no lamb -2.957638 time 2020-06-27 11:29:52.849904
Model ind 665 epoch 1496 batch: 100 avg loss -2.961521 avg loss no lamb -2.961521 time 2020-06-27 11:30:03.873499
Model ind 665 epoch 1496 batch: 200 avg loss -2.875932 avg loss no lamb -2.875932 time 2020-06-27 11:30:14.732880
Model ind 665 epoch 1496 batch: 300 avg loss -2.865235 avg loss no lamb -2.865235 time 2020-06-27 11:30:25.563897
Model ind 665 epoch 1496 batch: 400 avg loss -2.867831 avg loss no lamb -2.867831 time 2020-06-27 11:30:36.468369
Model ind 665 epoch 1496 batch: 500 avg loss -2.807002 avg loss no lamb -2.807002 time 2020-06-27 11:30:47.336891
Model ind 665 epoch 1496 batch: 600 avg loss -2.909772 avg loss no lamb -2.909772 time 2020-06-27 11:30:58.132549
Model ind 665 epoch 1496 batch: 700 avg loss -2.772133 avg loss no lamb -2.772133 time 2020-06-27 11:31:09.231673
Model ind 665 epoch 1496 batch: 800 avg loss -2.902237 avg loss no lamb -2.902237 time 2020-06-27 11:31:20.044202
last batch sz 10
Pre: time 2020-06-27 11:31:34.290381: 
 	std: 0.0030364424
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9771, 0.9717, 0.979, 0.9729]
	train_accs: [0.98106664, 0.97965, 0.97445, 0.981, 0.97543335]
	best_train_sub_head: 0
	worst: 0.9717
	avg: 0.9759
	best: 0.9788

Starting e_i: 1497
Model ind 665 epoch 1497 batch: 0 avg loss -2.965544 avg loss no lamb -2.965544 time 2020-06-27 11:31:35.576207
Model ind 665 epoch 1497 batch: 100 avg loss -2.855310 avg loss no lamb -2.855310 time 2020-06-27 11:31:46.319700
Model ind 665 epoch 1497 batch: 200 avg loss -2.906832 avg loss no lamb -2.906832 time 2020-06-27 11:31:57.099301
Model ind 665 epoch 1497 batch: 300 avg loss -2.867178 avg loss no lamb -2.867178 time 2020-06-27 11:32:08.178961
Model ind 665 epoch 1497 batch: 400 avg loss -2.754190 avg loss no lamb -2.754190 time 2020-06-27 11:32:19.002708
Model ind 665 epoch 1497 batch: 500 avg loss -2.872346 avg loss no lamb -2.872346 time 2020-06-27 11:32:29.758579
Model ind 665 epoch 1497 batch: 600 avg loss -2.923253 avg loss no lamb -2.923253 time 2020-06-27 11:32:40.486901
Model ind 665 epoch 1497 batch: 700 avg loss -2.795179 avg loss no lamb -2.795179 time 2020-06-27 11:32:51.560856
Model ind 665 epoch 1497 batch: 800 avg loss -2.908370 avg loss no lamb -2.908370 time 2020-06-27 11:33:02.450255
last batch sz 10
Pre: time 2020-06-27 11:33:16.775573: 
 	std: 0.0031997487
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9794, 0.9722, 0.9791, 0.9738]
	train_accs: [0.9810167, 0.9805, 0.9744167, 0.98121667, 0.9758]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.9768599
	best: 0.9791

Starting e_i: 1498
Model ind 665 epoch 1498 batch: 0 avg loss -2.946155 avg loss no lamb -2.946155 time 2020-06-27 11:33:18.056370
Model ind 665 epoch 1498 batch: 100 avg loss -2.906336 avg loss no lamb -2.906336 time 2020-06-27 11:33:28.975536
Model ind 665 epoch 1498 batch: 200 avg loss -2.867430 avg loss no lamb -2.867430 time 2020-06-27 11:33:40.253249
Model ind 665 epoch 1498 batch: 300 avg loss -2.817815 avg loss no lamb -2.817815 time 2020-06-27 11:33:51.241646
Model ind 665 epoch 1498 batch: 400 avg loss -2.798005 avg loss no lamb -2.798005 time 2020-06-27 11:34:01.991948
Model ind 665 epoch 1498 batch: 500 avg loss -2.842390 avg loss no lamb -2.842390 time 2020-06-27 11:34:12.645674
Model ind 665 epoch 1498 batch: 600 avg loss -2.943074 avg loss no lamb -2.943074 time 2020-06-27 11:34:23.437552
Model ind 665 epoch 1498 batch: 700 avg loss -2.811176 avg loss no lamb -2.811176 time 2020-06-27 11:34:34.260235
Model ind 665 epoch 1498 batch: 800 avg loss -2.849069 avg loss no lamb -2.849069 time 2020-06-27 11:34:45.106962
last batch sz 10
Pre: time 2020-06-27 11:34:59.285387: 
 	std: 0.0031183264
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9789, 0.9731, 0.9805, 0.975]
	train_accs: [0.9817333, 0.9808, 0.97463334, 0.98175, 0.9758]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97770005
	best: 0.9805

Starting e_i: 1499
Model ind 665 epoch 1499 batch: 0 avg loss -2.960409 avg loss no lamb -2.960409 time 2020-06-27 11:35:00.792131
Model ind 665 epoch 1499 batch: 100 avg loss -2.907359 avg loss no lamb -2.907359 time 2020-06-27 11:35:11.621205
Model ind 665 epoch 1499 batch: 200 avg loss -2.949617 avg loss no lamb -2.949617 time 2020-06-27 11:35:22.438844
Model ind 665 epoch 1499 batch: 300 avg loss -2.865780 avg loss no lamb -2.865780 time 2020-06-27 11:35:33.255329
Model ind 665 epoch 1499 batch: 400 avg loss -2.790125 avg loss no lamb -2.790125 time 2020-06-27 11:35:44.039167
Model ind 665 epoch 1499 batch: 500 avg loss -2.878853 avg loss no lamb -2.878853 time 2020-06-27 11:35:55.040369
Model ind 665 epoch 1499 batch: 600 avg loss -2.878906 avg loss no lamb -2.878906 time 2020-06-27 11:36:06.023072
Model ind 665 epoch 1499 batch: 700 avg loss -2.755419 avg loss no lamb -2.755419 time 2020-06-27 11:36:17.079513
Model ind 665 epoch 1499 batch: 800 avg loss -2.905082 avg loss no lamb -2.905082 time 2020-06-27 11:36:27.943162
last batch sz 10
Pre: time 2020-06-27 11:36:41.946390: 
 	std: 0.003482171
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9787, 0.9783, 0.9707, 0.9782, 0.972]
	train_accs: [0.98105, 0.9803333, 0.9734333, 0.9808667, 0.97461665]
	best_train_sub_head: 0
	worst: 0.9707
	avg: 0.97558004
	best: 0.9787

Starting e_i: 1500
Model ind 665 epoch 1500 batch: 0 avg loss -2.984834 avg loss no lamb -2.984834 time 2020-06-27 11:36:43.207082
Model ind 665 epoch 1500 batch: 100 avg loss -2.884920 avg loss no lamb -2.884920 time 2020-06-27 11:36:53.951057
Model ind 665 epoch 1500 batch: 200 avg loss -2.860551 avg loss no lamb -2.860551 time 2020-06-27 11:37:04.880448
Model ind 665 epoch 1500 batch: 300 avg loss -2.839071 avg loss no lamb -2.839071 time 2020-06-27 11:37:15.732065
Model ind 665 epoch 1500 batch: 400 avg loss -2.842013 avg loss no lamb -2.842013 time 2020-06-27 11:37:26.512789
Model ind 665 epoch 1500 batch: 500 avg loss -2.864037 avg loss no lamb -2.864037 time 2020-06-27 11:37:37.438070
Model ind 665 epoch 1500 batch: 600 avg loss -2.897618 avg loss no lamb -2.897618 time 2020-06-27 11:37:48.457410
Model ind 665 epoch 1500 batch: 700 avg loss -2.741099 avg loss no lamb -2.741099 time 2020-06-27 11:37:59.304824
Model ind 665 epoch 1500 batch: 800 avg loss -2.820984 avg loss no lamb -2.820984 time 2020-06-27 11:38:10.222583
last batch sz 10
Pre: time 2020-06-27 11:38:24.399125: 
 	std: 0.003164814
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9782, 0.973, 0.9798, 0.973]
	train_accs: [0.9814, 0.9803333, 0.97473335, 0.9816167, 0.97603333]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97679996
	best: 0.9798

Starting e_i: 1501
Model ind 665 epoch 1501 batch: 0 avg loss -2.943973 avg loss no lamb -2.943973 time 2020-06-27 11:38:27.128880
Model ind 665 epoch 1501 batch: 100 avg loss -2.955969 avg loss no lamb -2.955969 time 2020-06-27 11:38:37.893943
Model ind 665 epoch 1501 batch: 200 avg loss -2.853670 avg loss no lamb -2.853670 time 2020-06-27 11:38:48.712164
Model ind 665 epoch 1501 batch: 300 avg loss -2.913330 avg loss no lamb -2.913330 time 2020-06-27 11:38:59.589847
Model ind 665 epoch 1501 batch: 400 avg loss -2.869761 avg loss no lamb -2.869761 time 2020-06-27 11:39:10.587552
Model ind 665 epoch 1501 batch: 500 avg loss -2.909631 avg loss no lamb -2.909631 time 2020-06-27 11:39:21.643503
Model ind 665 epoch 1501 batch: 600 avg loss -2.908861 avg loss no lamb -2.908861 time 2020-06-27 11:39:32.455793
Model ind 665 epoch 1501 batch: 700 avg loss -2.822353 avg loss no lamb -2.822353 time 2020-06-27 11:39:43.388032
Model ind 665 epoch 1501 batch: 800 avg loss -2.834050 avg loss no lamb -2.834050 time 2020-06-27 11:39:54.354845
last batch sz 10
Pre: time 2020-06-27 11:40:08.629946: 
 	std: 0.0032737667
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9779, 0.9718, 0.9794, 0.9729]
	train_accs: [0.9812, 0.98046666, 0.9748167, 0.98073334, 0.97538334]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.97628003
	best: 0.9794

Starting e_i: 1502
Model ind 665 epoch 1502 batch: 0 avg loss -2.958657 avg loss no lamb -2.958657 time 2020-06-27 11:40:09.909576
Model ind 665 epoch 1502 batch: 100 avg loss -2.921925 avg loss no lamb -2.921925 time 2020-06-27 11:40:20.672529
Model ind 665 epoch 1502 batch: 200 avg loss -2.909696 avg loss no lamb -2.909696 time 2020-06-27 11:40:31.547204
Model ind 665 epoch 1502 batch: 300 avg loss -2.850342 avg loss no lamb -2.850342 time 2020-06-27 11:40:42.540811
Model ind 665 epoch 1502 batch: 400 avg loss -2.790935 avg loss no lamb -2.790935 time 2020-06-27 11:40:53.594940
Model ind 665 epoch 1502 batch: 500 avg loss -2.900635 avg loss no lamb -2.900635 time 2020-06-27 11:41:04.555928
Model ind 665 epoch 1502 batch: 600 avg loss -2.933188 avg loss no lamb -2.933188 time 2020-06-27 11:41:15.563061
Model ind 665 epoch 1502 batch: 700 avg loss -2.824394 avg loss no lamb -2.824394 time 2020-06-27 11:41:26.319661
Model ind 665 epoch 1502 batch: 800 avg loss -2.857800 avg loss no lamb -2.857800 time 2020-06-27 11:41:36.942457
last batch sz 10
Pre: time 2020-06-27 11:41:51.265680: 
 	std: 0.0023557742
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9798, 0.9747, 0.9797, 0.9751]
	train_accs: [0.9819667, 0.98141664, 0.9758833, 0.98188335, 0.9763333]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97778
	best: 0.9796

Starting e_i: 1503
Model ind 665 epoch 1503 batch: 0 avg loss -2.947020 avg loss no lamb -2.947020 time 2020-06-27 11:41:52.556614
Model ind 665 epoch 1503 batch: 100 avg loss -2.927315 avg loss no lamb -2.927315 time 2020-06-27 11:42:03.516074
Model ind 665 epoch 1503 batch: 200 avg loss -2.862886 avg loss no lamb -2.862886 time 2020-06-27 11:42:14.408357
Model ind 665 epoch 1503 batch: 300 avg loss -2.825706 avg loss no lamb -2.825706 time 2020-06-27 11:42:25.361535
Model ind 665 epoch 1503 batch: 400 avg loss -2.826813 avg loss no lamb -2.826813 time 2020-06-27 11:42:36.362433
Model ind 665 epoch 1503 batch: 500 avg loss -2.843260 avg loss no lamb -2.843260 time 2020-06-27 11:42:47.462479
Model ind 665 epoch 1503 batch: 600 avg loss -2.882423 avg loss no lamb -2.882423 time 2020-06-27 11:42:58.532271
Model ind 665 epoch 1503 batch: 700 avg loss -2.821088 avg loss no lamb -2.821088 time 2020-06-27 11:43:09.622893
Model ind 665 epoch 1503 batch: 800 avg loss -2.916079 avg loss no lamb -2.916079 time 2020-06-27 11:43:20.348014
last batch sz 10
Pre: time 2020-06-27 11:43:34.715833: 
 	std: 0.0032927739
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9783, 0.9718, 0.9796, 0.9737]
	train_accs: [0.98148334, 0.98036665, 0.9747667, 0.98113334, 0.9758]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.97665995
	best: 0.9799

Starting e_i: 1504
Model ind 665 epoch 1504 batch: 0 avg loss -2.951867 avg loss no lamb -2.951867 time 2020-06-27 11:43:36.009262
Model ind 665 epoch 1504 batch: 100 avg loss -2.935204 avg loss no lamb -2.935204 time 2020-06-27 11:43:46.995495
Model ind 665 epoch 1504 batch: 200 avg loss -2.864180 avg loss no lamb -2.864180 time 2020-06-27 11:43:57.872169
Model ind 665 epoch 1504 batch: 300 avg loss -2.874365 avg loss no lamb -2.874365 time 2020-06-27 11:44:08.844645
Model ind 665 epoch 1504 batch: 400 avg loss -2.823916 avg loss no lamb -2.823916 time 2020-06-27 11:44:19.564858
Model ind 665 epoch 1504 batch: 500 avg loss -2.864991 avg loss no lamb -2.864991 time 2020-06-27 11:44:30.385744
Model ind 665 epoch 1504 batch: 600 avg loss -2.883633 avg loss no lamb -2.883633 time 2020-06-27 11:44:41.344248
Model ind 665 epoch 1504 batch: 700 avg loss -2.801344 avg loss no lamb -2.801344 time 2020-06-27 11:44:52.275259
Model ind 665 epoch 1504 batch: 800 avg loss -2.888988 avg loss no lamb -2.888988 time 2020-06-27 11:45:03.431483
last batch sz 10
Pre: time 2020-06-27 11:45:17.433732: 
 	std: 0.0029427893
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9799, 0.974, 0.9803, 0.9742]
	train_accs: [0.98146665, 0.98143333, 0.9752667, 0.9817, 0.9762]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97770005
	best: 0.9803

Starting e_i: 1505
Model ind 665 epoch 1505 batch: 0 avg loss -2.962702 avg loss no lamb -2.962702 time 2020-06-27 11:45:18.731732
Model ind 665 epoch 1505 batch: 100 avg loss -2.929782 avg loss no lamb -2.929782 time 2020-06-27 11:45:29.560777
Model ind 665 epoch 1505 batch: 200 avg loss -2.894989 avg loss no lamb -2.894989 time 2020-06-27 11:45:40.461860
Model ind 665 epoch 1505 batch: 300 avg loss -2.885385 avg loss no lamb -2.885385 time 2020-06-27 11:45:51.206049
Model ind 665 epoch 1505 batch: 400 avg loss -2.875575 avg loss no lamb -2.875575 time 2020-06-27 11:46:02.195187
Model ind 665 epoch 1505 batch: 500 avg loss -2.874563 avg loss no lamb -2.874563 time 2020-06-27 11:46:13.020101
Model ind 665 epoch 1505 batch: 600 avg loss -2.870919 avg loss no lamb -2.870919 time 2020-06-27 11:46:23.800758
Model ind 665 epoch 1505 batch: 700 avg loss -2.814712 avg loss no lamb -2.814712 time 2020-06-27 11:46:34.558113
Model ind 665 epoch 1505 batch: 800 avg loss -2.925779 avg loss no lamb -2.925779 time 2020-06-27 11:46:45.338581
last batch sz 10
Pre: time 2020-06-27 11:46:59.547563: 
 	std: 0.0031531465
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9808, 0.9738, 0.9805, 0.9748]
	train_accs: [0.9816, 0.98113334, 0.97566664, 0.98145, 0.97651666]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97814
	best: 0.9808

Starting e_i: 1506
Model ind 665 epoch 1506 batch: 0 avg loss -2.967165 avg loss no lamb -2.967165 time 2020-06-27 11:47:00.880284
Model ind 665 epoch 1506 batch: 100 avg loss -2.867303 avg loss no lamb -2.867303 time 2020-06-27 11:47:11.662721
Model ind 665 epoch 1506 batch: 200 avg loss -2.864910 avg loss no lamb -2.864910 time 2020-06-27 11:47:22.395714
Model ind 665 epoch 1506 batch: 300 avg loss -2.884625 avg loss no lamb -2.884625 time 2020-06-27 11:47:33.429238
Model ind 665 epoch 1506 batch: 400 avg loss -2.824146 avg loss no lamb -2.824146 time 2020-06-27 11:47:44.372014
Model ind 665 epoch 1506 batch: 500 avg loss -2.859708 avg loss no lamb -2.859708 time 2020-06-27 11:47:55.019708
Model ind 665 epoch 1506 batch: 600 avg loss -2.882927 avg loss no lamb -2.882927 time 2020-06-27 11:48:06.081977
Model ind 665 epoch 1506 batch: 700 avg loss -2.827684 avg loss no lamb -2.827684 time 2020-06-27 11:48:17.158069
Model ind 665 epoch 1506 batch: 800 avg loss -2.916965 avg loss no lamb -2.916965 time 2020-06-27 11:48:27.882440
last batch sz 10
Pre: time 2020-06-27 11:48:42.168875: 
 	std: 0.002941023
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9796, 0.973, 0.9794, 0.974]
	train_accs: [0.98155, 0.98076665, 0.9745167, 0.9816833, 0.97576666]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97708
	best: 0.9794

Starting e_i: 1507
Model ind 665 epoch 1507 batch: 0 avg loss -2.932383 avg loss no lamb -2.932383 time 2020-06-27 11:48:43.438945
Model ind 665 epoch 1507 batch: 100 avg loss -2.899818 avg loss no lamb -2.899818 time 2020-06-27 11:48:54.184869
Model ind 665 epoch 1507 batch: 200 avg loss -2.919238 avg loss no lamb -2.919238 time 2020-06-27 11:49:05.071012
Model ind 665 epoch 1507 batch: 300 avg loss -2.865264 avg loss no lamb -2.865264 time 2020-06-27 11:49:15.889191
Model ind 665 epoch 1507 batch: 400 avg loss -2.841405 avg loss no lamb -2.841405 time 2020-06-27 11:49:26.688075
Model ind 665 epoch 1507 batch: 500 avg loss -2.839654 avg loss no lamb -2.839654 time 2020-06-27 11:49:37.643003
Model ind 665 epoch 1507 batch: 600 avg loss -2.890322 avg loss no lamb -2.890322 time 2020-06-27 11:49:48.430006
Model ind 665 epoch 1507 batch: 700 avg loss -2.794652 avg loss no lamb -2.794652 time 2020-06-27 11:49:59.247556
Model ind 665 epoch 1507 batch: 800 avg loss -2.897454 avg loss no lamb -2.897454 time 2020-06-27 11:50:10.293798
last batch sz 10
Pre: time 2020-06-27 11:50:24.516846: 
 	std: 0.0030055922
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9794, 0.9728, 0.9803, 0.9748]
	train_accs: [0.9813, 0.9805167, 0.97545, 0.98143333, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97738
	best: 0.9803

Starting e_i: 1508
Model ind 665 epoch 1508 batch: 0 avg loss -2.975076 avg loss no lamb -2.975076 time 2020-06-27 11:50:25.965834
Model ind 665 epoch 1508 batch: 100 avg loss -2.907149 avg loss no lamb -2.907149 time 2020-06-27 11:50:36.872873
Model ind 665 epoch 1508 batch: 200 avg loss -2.828803 avg loss no lamb -2.828803 time 2020-06-27 11:50:47.950324
Model ind 665 epoch 1508 batch: 300 avg loss -2.831681 avg loss no lamb -2.831681 time 2020-06-27 11:50:58.494690
Model ind 665 epoch 1508 batch: 400 avg loss -2.792561 avg loss no lamb -2.792561 time 2020-06-27 11:51:09.559544
Model ind 665 epoch 1508 batch: 500 avg loss -2.863699 avg loss no lamb -2.863699 time 2020-06-27 11:51:20.519478
Model ind 665 epoch 1508 batch: 600 avg loss -2.859641 avg loss no lamb -2.859641 time 2020-06-27 11:51:31.245711
Model ind 665 epoch 1508 batch: 700 avg loss -2.849576 avg loss no lamb -2.849576 time 2020-06-27 11:51:42.251991
Model ind 665 epoch 1508 batch: 800 avg loss -2.895417 avg loss no lamb -2.895417 time 2020-06-27 11:51:53.250132
last batch sz 10
Pre: time 2020-06-27 11:52:07.501070: 
 	std: 0.0032455479
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9796, 0.9732, 0.9801, 0.9733]
	train_accs: [0.9809, 0.98015, 0.97485, 0.9811, 0.9759667]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97721994
	best: 0.9801

Starting e_i: 1509
Model ind 665 epoch 1509 batch: 0 avg loss -2.968377 avg loss no lamb -2.968377 time 2020-06-27 11:52:08.773650
Model ind 665 epoch 1509 batch: 100 avg loss -2.843927 avg loss no lamb -2.843927 time 2020-06-27 11:52:19.725120
Model ind 665 epoch 1509 batch: 200 avg loss -2.860265 avg loss no lamb -2.860265 time 2020-06-27 11:52:30.637378
Model ind 665 epoch 1509 batch: 300 avg loss -2.875827 avg loss no lamb -2.875827 time 2020-06-27 11:52:41.660768
Model ind 665 epoch 1509 batch: 400 avg loss -2.822234 avg loss no lamb -2.822234 time 2020-06-27 11:52:52.534019
Model ind 665 epoch 1509 batch: 500 avg loss -2.861519 avg loss no lamb -2.861519 time 2020-06-27 11:53:03.480757
Model ind 665 epoch 1509 batch: 600 avg loss -2.912742 avg loss no lamb -2.912742 time 2020-06-27 11:53:14.466925
Model ind 665 epoch 1509 batch: 700 avg loss -2.813171 avg loss no lamb -2.813171 time 2020-06-27 11:53:25.485122
Model ind 665 epoch 1509 batch: 800 avg loss -2.880799 avg loss no lamb -2.880799 time 2020-06-27 11:53:36.546484
last batch sz 10
Pre: time 2020-06-27 11:53:50.613520: 
 	std: 0.0030860896
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9786, 0.9725, 0.9796, 0.9732]
	train_accs: [0.9806167, 0.9799833, 0.97456664, 0.98065, 0.97566664]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.9766
	best: 0.9796

Starting e_i: 1510
Model ind 665 epoch 1510 batch: 0 avg loss -2.965593 avg loss no lamb -2.965593 time 2020-06-27 11:53:52.075004
Model ind 665 epoch 1510 batch: 100 avg loss -2.947573 avg loss no lamb -2.947573 time 2020-06-27 11:54:03.007789
Model ind 665 epoch 1510 batch: 200 avg loss -2.907685 avg loss no lamb -2.907685 time 2020-06-27 11:54:14.041411
Model ind 665 epoch 1510 batch: 300 avg loss -2.875711 avg loss no lamb -2.875711 time 2020-06-27 11:54:25.029196
Model ind 665 epoch 1510 batch: 400 avg loss -2.842052 avg loss no lamb -2.842052 time 2020-06-27 11:54:35.943298
Model ind 665 epoch 1510 batch: 500 avg loss -2.859772 avg loss no lamb -2.859772 time 2020-06-27 11:54:46.852939
Model ind 665 epoch 1510 batch: 600 avg loss -2.868038 avg loss no lamb -2.868038 time 2020-06-27 11:54:57.668190
Model ind 665 epoch 1510 batch: 700 avg loss -2.851721 avg loss no lamb -2.851721 time 2020-06-27 11:55:08.578086
Model ind 665 epoch 1510 batch: 800 avg loss -2.886455 avg loss no lamb -2.886455 time 2020-06-27 11:55:19.462216
last batch sz 10
Pre: time 2020-06-27 11:55:33.583302: 
 	std: 0.003530086
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9801, 0.9728, 0.9802, 0.9732]
	train_accs: [0.9813167, 0.9809333, 0.97395, 0.98118335, 0.975]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97732
	best: 0.9803

Starting e_i: 1511
Model ind 665 epoch 1511 batch: 0 avg loss -2.957759 avg loss no lamb -2.957759 time 2020-06-27 11:55:36.080299
Model ind 665 epoch 1511 batch: 100 avg loss -2.943282 avg loss no lamb -2.943282 time 2020-06-27 11:55:46.919465
Model ind 665 epoch 1511 batch: 200 avg loss -2.903486 avg loss no lamb -2.903486 time 2020-06-27 11:55:57.854185
Model ind 665 epoch 1511 batch: 300 avg loss -2.899182 avg loss no lamb -2.899182 time 2020-06-27 11:56:08.990366
Model ind 665 epoch 1511 batch: 400 avg loss -2.845029 avg loss no lamb -2.845029 time 2020-06-27 11:56:20.163550
Model ind 665 epoch 1511 batch: 500 avg loss -2.794983 avg loss no lamb -2.794983 time 2020-06-27 11:56:30.949663
Model ind 665 epoch 1511 batch: 600 avg loss -2.873306 avg loss no lamb -2.873306 time 2020-06-27 11:56:41.891766
Model ind 665 epoch 1511 batch: 700 avg loss -2.806064 avg loss no lamb -2.806064 time 2020-06-27 11:56:52.974455
Model ind 665 epoch 1511 batch: 800 avg loss -2.923892 avg loss no lamb -2.923892 time 2020-06-27 11:57:03.888298
last batch sz 10
Pre: time 2020-06-27 11:57:18.070930: 
 	std: 0.00255812
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9798, 0.9738, 0.9792, 0.9748]
	train_accs: [0.9811, 0.9806833, 0.9754, 0.9812833, 0.9763333]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.9774
	best: 0.9792

Starting e_i: 1512
Model ind 665 epoch 1512 batch: 0 avg loss -2.908415 avg loss no lamb -2.908415 time 2020-06-27 11:57:19.530150
Model ind 665 epoch 1512 batch: 100 avg loss -2.944335 avg loss no lamb -2.944335 time 2020-06-27 11:57:30.436503
Model ind 665 epoch 1512 batch: 200 avg loss -2.885744 avg loss no lamb -2.885744 time 2020-06-27 11:57:41.363644
Model ind 665 epoch 1512 batch: 300 avg loss -2.850101 avg loss no lamb -2.850101 time 2020-06-27 11:57:52.155418
Model ind 665 epoch 1512 batch: 400 avg loss -2.846059 avg loss no lamb -2.846059 time 2020-06-27 11:58:03.177370
Model ind 665 epoch 1512 batch: 500 avg loss -2.886205 avg loss no lamb -2.886205 time 2020-06-27 11:58:14.038413
Model ind 665 epoch 1512 batch: 600 avg loss -2.880540 avg loss no lamb -2.880540 time 2020-06-27 11:58:24.982500
Model ind 665 epoch 1512 batch: 700 avg loss -2.778740 avg loss no lamb -2.778740 time 2020-06-27 11:58:36.050966
Model ind 665 epoch 1512 batch: 800 avg loss -2.837440 avg loss no lamb -2.837440 time 2020-06-27 11:58:46.919673
last batch sz 10
Pre: time 2020-06-27 11:59:01.225720: 
 	std: 0.0031129494
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9795, 0.9729, 0.9795, 0.9738]
	train_accs: [0.9812667, 0.98003334, 0.9742, 0.9809167, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97714007
	best: 0.98

Starting e_i: 1513
Model ind 665 epoch 1513 batch: 0 avg loss -2.976924 avg loss no lamb -2.976924 time 2020-06-27 11:59:02.505807
Model ind 665 epoch 1513 batch: 100 avg loss -2.960240 avg loss no lamb -2.960240 time 2020-06-27 11:59:13.429240
Model ind 665 epoch 1513 batch: 200 avg loss -2.898742 avg loss no lamb -2.898742 time 2020-06-27 11:59:24.476611
Model ind 665 epoch 1513 batch: 300 avg loss -2.891721 avg loss no lamb -2.891721 time 2020-06-27 11:59:35.403929
Model ind 665 epoch 1513 batch: 400 avg loss -2.820979 avg loss no lamb -2.820979 time 2020-06-27 11:59:46.182547
Model ind 665 epoch 1513 batch: 500 avg loss -2.807021 avg loss no lamb -2.807021 time 2020-06-27 11:59:57.043386
Model ind 665 epoch 1513 batch: 600 avg loss -2.908866 avg loss no lamb -2.908866 time 2020-06-27 12:00:07.979618
Model ind 665 epoch 1513 batch: 700 avg loss -2.802630 avg loss no lamb -2.802630 time 2020-06-27 12:00:18.854580
Model ind 665 epoch 1513 batch: 800 avg loss -2.780451 avg loss no lamb -2.780451 time 2020-06-27 12:00:29.784758
last batch sz 10
Pre: time 2020-06-27 12:00:44.061964: 
 	std: 0.0029620116
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9812, 0.9743, 0.9803, 0.9756]
	train_accs: [0.9816167, 0.9809667, 0.9762333, 0.9816667, 0.977]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97852004
	best: 0.9803

Starting e_i: 1514
Model ind 665 epoch 1514 batch: 0 avg loss -2.952443 avg loss no lamb -2.952443 time 2020-06-27 12:00:45.355998
Model ind 665 epoch 1514 batch: 100 avg loss -2.947363 avg loss no lamb -2.947363 time 2020-06-27 12:00:56.737650
Model ind 665 epoch 1514 batch: 200 avg loss -2.873757 avg loss no lamb -2.873757 time 2020-06-27 12:01:07.757575
Model ind 665 epoch 1514 batch: 300 avg loss -2.887651 avg loss no lamb -2.887651 time 2020-06-27 12:01:18.479697
Model ind 665 epoch 1514 batch: 400 avg loss -2.855192 avg loss no lamb -2.855192 time 2020-06-27 12:01:29.468391
Model ind 665 epoch 1514 batch: 500 avg loss -2.891578 avg loss no lamb -2.891578 time 2020-06-27 12:01:40.636954
Model ind 665 epoch 1514 batch: 600 avg loss -2.902307 avg loss no lamb -2.902307 time 2020-06-27 12:01:51.698839
Model ind 665 epoch 1514 batch: 700 avg loss -2.736979 avg loss no lamb -2.736979 time 2020-06-27 12:02:02.706180
Model ind 665 epoch 1514 batch: 800 avg loss -2.870869 avg loss no lamb -2.870869 time 2020-06-27 12:02:13.430568
last batch sz 10
Pre: time 2020-06-27 12:02:27.919785: 
 	std: 0.0034418707
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9807, 0.9741, 0.9811, 0.9738]
	train_accs: [0.98151666, 0.9813167, 0.97565, 0.98176664, 0.97625]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.9781601
	best: 0.9811

Starting e_i: 1515
Model ind 665 epoch 1515 batch: 0 avg loss -2.949543 avg loss no lamb -2.949543 time 2020-06-27 12:02:29.195606
Model ind 665 epoch 1515 batch: 100 avg loss -2.902400 avg loss no lamb -2.902400 time 2020-06-27 12:02:39.881990
Model ind 665 epoch 1515 batch: 200 avg loss -2.918106 avg loss no lamb -2.918106 time 2020-06-27 12:02:51.042519
Model ind 665 epoch 1515 batch: 300 avg loss -2.884669 avg loss no lamb -2.884669 time 2020-06-27 12:03:02.022151
Model ind 665 epoch 1515 batch: 400 avg loss -2.887568 avg loss no lamb -2.887568 time 2020-06-27 12:03:12.959197
Model ind 665 epoch 1515 batch: 500 avg loss -2.851121 avg loss no lamb -2.851121 time 2020-06-27 12:03:23.793760
Model ind 665 epoch 1515 batch: 600 avg loss -2.920551 avg loss no lamb -2.920551 time 2020-06-27 12:03:35.070396
Model ind 665 epoch 1515 batch: 700 avg loss -2.741731 avg loss no lamb -2.741731 time 2020-06-27 12:03:45.871442
Model ind 665 epoch 1515 batch: 800 avg loss -2.890365 avg loss no lamb -2.890365 time 2020-06-27 12:03:56.848021
last batch sz 10
Pre: time 2020-06-27 12:04:11.019022: 
 	std: 0.0035198806
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9795, 0.9721, 0.9798, 0.9734]
	train_accs: [0.98123336, 0.98, 0.9742333, 0.9809833, 0.9759]
	best_train_sub_head: 0
	worst: 0.9721
	avg: 0.97701997
	best: 0.9803

Starting e_i: 1516
Model ind 665 epoch 1516 batch: 0 avg loss -2.937211 avg loss no lamb -2.937211 time 2020-06-27 12:04:12.247921
Model ind 665 epoch 1516 batch: 100 avg loss -2.921139 avg loss no lamb -2.921139 time 2020-06-27 12:04:23.228574
Model ind 665 epoch 1516 batch: 200 avg loss -2.863405 avg loss no lamb -2.863405 time 2020-06-27 12:04:34.253271
Model ind 665 epoch 1516 batch: 300 avg loss -2.867177 avg loss no lamb -2.867177 time 2020-06-27 12:04:45.196229
Model ind 665 epoch 1516 batch: 400 avg loss -2.855450 avg loss no lamb -2.855450 time 2020-06-27 12:04:56.118096
Model ind 665 epoch 1516 batch: 500 avg loss -2.834700 avg loss no lamb -2.834700 time 2020-06-27 12:05:06.829551
Model ind 665 epoch 1516 batch: 600 avg loss -2.883088 avg loss no lamb -2.883088 time 2020-06-27 12:05:17.692199
Model ind 665 epoch 1516 batch: 700 avg loss -2.833670 avg loss no lamb -2.833670 time 2020-06-27 12:05:28.560447
Model ind 665 epoch 1516 batch: 800 avg loss -2.885645 avg loss no lamb -2.885645 time 2020-06-27 12:05:39.636330
last batch sz 10
Pre: time 2020-06-27 12:05:54.029567: 
 	std: 0.002723673
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9787, 0.9724, 0.9789, 0.9742]
	train_accs: [0.981, 0.9804, 0.97508335, 0.9812, 0.97603333]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.97656
	best: 0.9789

Starting e_i: 1517
Model ind 665 epoch 1517 batch: 0 avg loss -2.986680 avg loss no lamb -2.986680 time 2020-06-27 12:05:55.276838
Model ind 665 epoch 1517 batch: 100 avg loss -2.896939 avg loss no lamb -2.896939 time 2020-06-27 12:06:06.224031
Model ind 665 epoch 1517 batch: 200 avg loss -2.920073 avg loss no lamb -2.920073 time 2020-06-27 12:06:17.221365
Model ind 665 epoch 1517 batch: 300 avg loss -2.915072 avg loss no lamb -2.915072 time 2020-06-27 12:06:28.073704
Model ind 665 epoch 1517 batch: 400 avg loss -2.836136 avg loss no lamb -2.836136 time 2020-06-27 12:06:39.062286
Model ind 665 epoch 1517 batch: 500 avg loss -2.845806 avg loss no lamb -2.845806 time 2020-06-27 12:06:49.789367
Model ind 665 epoch 1517 batch: 600 avg loss -2.879063 avg loss no lamb -2.879063 time 2020-06-27 12:07:00.748294
Model ind 665 epoch 1517 batch: 700 avg loss -2.871336 avg loss no lamb -2.871336 time 2020-06-27 12:07:11.462663
Model ind 665 epoch 1517 batch: 800 avg loss -2.881768 avg loss no lamb -2.881768 time 2020-06-27 12:07:22.401307
last batch sz 10
Pre: time 2020-06-27 12:07:36.418910: 
 	std: 0.0029581147
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9797, 0.9737, 0.9799, 0.9742]
	train_accs: [0.98156667, 0.98055, 0.9757, 0.98146665, 0.9763833]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97756004
	best: 0.9803

Starting e_i: 1518
Model ind 665 epoch 1518 batch: 0 avg loss -2.933711 avg loss no lamb -2.933711 time 2020-06-27 12:07:37.691364
Model ind 665 epoch 1518 batch: 100 avg loss -2.884853 avg loss no lamb -2.884853 time 2020-06-27 12:07:48.487003
Model ind 665 epoch 1518 batch: 200 avg loss -2.846654 avg loss no lamb -2.846654 time 2020-06-27 12:07:59.358292
Model ind 665 epoch 1518 batch: 300 avg loss -2.855916 avg loss no lamb -2.855916 time 2020-06-27 12:08:10.248744
Model ind 665 epoch 1518 batch: 400 avg loss -2.778748 avg loss no lamb -2.778748 time 2020-06-27 12:08:21.356516
Model ind 665 epoch 1518 batch: 500 avg loss -2.847785 avg loss no lamb -2.847785 time 2020-06-27 12:08:32.125157
Model ind 665 epoch 1518 batch: 600 avg loss -2.871933 avg loss no lamb -2.871933 time 2020-06-27 12:08:42.966071
Model ind 665 epoch 1518 batch: 700 avg loss -2.802630 avg loss no lamb -2.802630 time 2020-06-27 12:08:53.873178
Model ind 665 epoch 1518 batch: 800 avg loss -2.887393 avg loss no lamb -2.887393 time 2020-06-27 12:09:04.935317
last batch sz 10
Pre: time 2020-06-27 12:09:19.305492: 
 	std: 0.0031632918
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9797, 0.9739, 0.9801, 0.9733]
	train_accs: [0.9810167, 0.98041666, 0.97501665, 0.98108333, 0.97585]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97746
	best: 0.9801

Starting e_i: 1519
Model ind 665 epoch 1519 batch: 0 avg loss -2.984041 avg loss no lamb -2.984041 time 2020-06-27 12:09:20.639816
Model ind 665 epoch 1519 batch: 100 avg loss -2.872548 avg loss no lamb -2.872548 time 2020-06-27 12:09:31.491209
Model ind 665 epoch 1519 batch: 200 avg loss -2.884165 avg loss no lamb -2.884165 time 2020-06-27 12:09:42.119236
Model ind 665 epoch 1519 batch: 300 avg loss -2.881969 avg loss no lamb -2.881969 time 2020-06-27 12:09:53.042576
Model ind 665 epoch 1519 batch: 400 avg loss -2.798637 avg loss no lamb -2.798637 time 2020-06-27 12:10:03.881110
Model ind 665 epoch 1519 batch: 500 avg loss -2.859655 avg loss no lamb -2.859655 time 2020-06-27 12:10:14.487671
Model ind 665 epoch 1519 batch: 600 avg loss -2.955073 avg loss no lamb -2.955073 time 2020-06-27 12:10:25.092840
Model ind 665 epoch 1519 batch: 700 avg loss -2.826073 avg loss no lamb -2.826073 time 2020-06-27 12:10:36.089375
Model ind 665 epoch 1519 batch: 800 avg loss -2.888907 avg loss no lamb -2.888907 time 2020-06-27 12:10:46.887940
last batch sz 10
Pre: time 2020-06-27 12:11:01.081049: 
 	std: 0.002850541
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9804, 0.975, 0.9811, 0.9753]
	train_accs: [0.9813, 0.98046666, 0.976, 0.9817167, 0.9767]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97861993
	best: 0.9811

Starting e_i: 1520
Model ind 665 epoch 1520 batch: 0 avg loss -2.956388 avg loss no lamb -2.956388 time 2020-06-27 12:11:02.328624
Model ind 665 epoch 1520 batch: 100 avg loss -2.887573 avg loss no lamb -2.887573 time 2020-06-27 12:11:12.881162
Model ind 665 epoch 1520 batch: 200 avg loss -2.848900 avg loss no lamb -2.848900 time 2020-06-27 12:11:23.774336
Model ind 665 epoch 1520 batch: 300 avg loss -2.894789 avg loss no lamb -2.894789 time 2020-06-27 12:11:34.643127
Model ind 665 epoch 1520 batch: 400 avg loss -2.801767 avg loss no lamb -2.801767 time 2020-06-27 12:11:45.419616
Model ind 665 epoch 1520 batch: 500 avg loss -2.815478 avg loss no lamb -2.815478 time 2020-06-27 12:11:56.469392
Model ind 665 epoch 1520 batch: 600 avg loss -2.840639 avg loss no lamb -2.840639 time 2020-06-27 12:12:07.173036
Model ind 665 epoch 1520 batch: 700 avg loss -2.859004 avg loss no lamb -2.859004 time 2020-06-27 12:12:18.118020
Model ind 665 epoch 1520 batch: 800 avg loss -2.896654 avg loss no lamb -2.896654 time 2020-06-27 12:12:29.140267
last batch sz 10
Pre: time 2020-06-27 12:12:43.243413: 
 	std: 0.0028449188
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9783, 0.9719, 0.9778, 0.9732]
	train_accs: [0.9806333, 0.98008335, 0.9747667, 0.98075, 0.9762]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97598
	best: 0.9778

Starting e_i: 1521
Model ind 665 epoch 1521 batch: 0 avg loss -2.949605 avg loss no lamb -2.949605 time 2020-06-27 12:12:45.922635
Model ind 665 epoch 1521 batch: 100 avg loss -2.949335 avg loss no lamb -2.949335 time 2020-06-27 12:12:56.588961
Model ind 665 epoch 1521 batch: 200 avg loss -2.889196 avg loss no lamb -2.889196 time 2020-06-27 12:13:07.496151
Model ind 665 epoch 1521 batch: 300 avg loss -2.874369 avg loss no lamb -2.874369 time 2020-06-27 12:13:18.368635
Model ind 665 epoch 1521 batch: 400 avg loss -2.815908 avg loss no lamb -2.815908 time 2020-06-27 12:13:29.354610
Model ind 665 epoch 1521 batch: 500 avg loss -2.872582 avg loss no lamb -2.872582 time 2020-06-27 12:13:40.267816
Model ind 665 epoch 1521 batch: 600 avg loss -2.910172 avg loss no lamb -2.910172 time 2020-06-27 12:13:51.311857
Model ind 665 epoch 1521 batch: 700 avg loss -2.793814 avg loss no lamb -2.793814 time 2020-06-27 12:14:02.195041
Model ind 665 epoch 1521 batch: 800 avg loss -2.867012 avg loss no lamb -2.867012 time 2020-06-27 12:14:12.936428
last batch sz 10
Pre: time 2020-06-27 12:14:27.507711: 
 	std: 0.003384901
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9778, 0.9713, 0.9794, 0.973]
	train_accs: [0.98141664, 0.9798, 0.97438335, 0.98123336, 0.97585]
	best_train_sub_head: 0
	worst: 0.9713
	avg: 0.97617996
	best: 0.9794

Starting e_i: 1522
Model ind 665 epoch 1522 batch: 0 avg loss -2.911642 avg loss no lamb -2.911642 time 2020-06-27 12:14:28.765028
Model ind 665 epoch 1522 batch: 100 avg loss -2.874233 avg loss no lamb -2.874233 time 2020-06-27 12:14:39.490759
Model ind 665 epoch 1522 batch: 200 avg loss -2.900259 avg loss no lamb -2.900259 time 2020-06-27 12:14:50.541874
Model ind 665 epoch 1522 batch: 300 avg loss -2.917323 avg loss no lamb -2.917323 time 2020-06-27 12:15:01.717503
Model ind 665 epoch 1522 batch: 400 avg loss -2.789183 avg loss no lamb -2.789183 time 2020-06-27 12:15:12.546311
Model ind 665 epoch 1522 batch: 500 avg loss -2.865187 avg loss no lamb -2.865187 time 2020-06-27 12:15:23.152244
Model ind 665 epoch 1522 batch: 600 avg loss -2.877125 avg loss no lamb -2.877125 time 2020-06-27 12:15:34.101522
Model ind 665 epoch 1522 batch: 700 avg loss -2.842420 avg loss no lamb -2.842420 time 2020-06-27 12:15:45.346981
Model ind 665 epoch 1522 batch: 800 avg loss -2.886676 avg loss no lamb -2.886676 time 2020-06-27 12:15:56.276613
last batch sz 10
Pre: time 2020-06-27 12:16:10.549928: 
 	std: 0.0030596668
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.9791, 0.9734, 0.9791, 0.9725]
	train_accs: [0.9808667, 0.98053336, 0.97565, 0.98076665, 0.97538334]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97668
	best: 0.9793

Starting e_i: 1523
Model ind 665 epoch 1523 batch: 0 avg loss -3.013826 avg loss no lamb -3.013826 time 2020-06-27 12:16:12.095066
Model ind 665 epoch 1523 batch: 100 avg loss -2.969920 avg loss no lamb -2.969920 time 2020-06-27 12:16:23.250924
Model ind 665 epoch 1523 batch: 200 avg loss -2.879051 avg loss no lamb -2.879051 time 2020-06-27 12:16:34.338872
Model ind 665 epoch 1523 batch: 300 avg loss -2.908288 avg loss no lamb -2.908288 time 2020-06-27 12:16:45.250672
Model ind 665 epoch 1523 batch: 400 avg loss -2.827056 avg loss no lamb -2.827056 time 2020-06-27 12:16:56.173072
Model ind 665 epoch 1523 batch: 500 avg loss -2.876303 avg loss no lamb -2.876303 time 2020-06-27 12:17:07.023787
Model ind 665 epoch 1523 batch: 600 avg loss -2.912706 avg loss no lamb -2.912706 time 2020-06-27 12:17:17.767871
Model ind 665 epoch 1523 batch: 700 avg loss -2.806070 avg loss no lamb -2.806070 time 2020-06-27 12:17:28.628796
Model ind 665 epoch 1523 batch: 800 avg loss -2.839653 avg loss no lamb -2.839653 time 2020-06-27 12:17:39.529126
last batch sz 10
Pre: time 2020-06-27 12:17:53.679883: 
 	std: 0.003361191
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9801, 0.9733, 0.9806, 0.9743]
	train_accs: [0.9816333, 0.9808, 0.97531664, 0.98175, 0.97606665]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97788
	best: 0.9806

Starting e_i: 1524
Model ind 665 epoch 1524 batch: 0 avg loss -2.927940 avg loss no lamb -2.927940 time 2020-06-27 12:17:54.935841
Model ind 665 epoch 1524 batch: 100 avg loss -2.985300 avg loss no lamb -2.985300 time 2020-06-27 12:18:06.041376
Model ind 665 epoch 1524 batch: 200 avg loss -2.901575 avg loss no lamb -2.901575 time 2020-06-27 12:18:16.799284
Model ind 665 epoch 1524 batch: 300 avg loss -2.832443 avg loss no lamb -2.832443 time 2020-06-27 12:18:27.683379
Model ind 665 epoch 1524 batch: 400 avg loss -2.803517 avg loss no lamb -2.803517 time 2020-06-27 12:18:38.898102
Model ind 665 epoch 1524 batch: 500 avg loss -2.823658 avg loss no lamb -2.823658 time 2020-06-27 12:18:49.659967
Model ind 665 epoch 1524 batch: 600 avg loss -2.916359 avg loss no lamb -2.916359 time 2020-06-27 12:19:00.442764
Model ind 665 epoch 1524 batch: 700 avg loss -2.831904 avg loss no lamb -2.831904 time 2020-06-27 12:19:11.332468
Model ind 665 epoch 1524 batch: 800 avg loss -2.904160 avg loss no lamb -2.904160 time 2020-06-27 12:19:22.045886
last batch sz 10
Pre: time 2020-06-27 12:19:36.070723: 
 	std: 0.0034667398
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9794, 0.9725, 0.981, 0.9743]
	train_accs: [0.98113334, 0.98031664, 0.97455, 0.9813, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97754
	best: 0.981

Starting e_i: 1525
Model ind 665 epoch 1525 batch: 0 avg loss -2.956915 avg loss no lamb -2.956915 time 2020-06-27 12:19:37.410274
Model ind 665 epoch 1525 batch: 100 avg loss -2.877713 avg loss no lamb -2.877713 time 2020-06-27 12:19:48.388668
Model ind 665 epoch 1525 batch: 200 avg loss -2.913415 avg loss no lamb -2.913415 time 2020-06-27 12:19:59.280771
Model ind 665 epoch 1525 batch: 300 avg loss -2.878056 avg loss no lamb -2.878056 time 2020-06-27 12:20:10.295821
Model ind 665 epoch 1525 batch: 400 avg loss -2.813202 avg loss no lamb -2.813202 time 2020-06-27 12:20:21.104882
Model ind 665 epoch 1525 batch: 500 avg loss -2.820260 avg loss no lamb -2.820260 time 2020-06-27 12:20:32.153827
Model ind 665 epoch 1525 batch: 600 avg loss -2.902192 avg loss no lamb -2.902192 time 2020-06-27 12:20:43.141020
Model ind 665 epoch 1525 batch: 700 avg loss -2.781115 avg loss no lamb -2.781115 time 2020-06-27 12:20:53.819974
Model ind 665 epoch 1525 batch: 800 avg loss -2.886717 avg loss no lamb -2.886717 time 2020-06-27 12:21:04.888389
last batch sz 10
Pre: time 2020-06-27 12:21:19.220563: 
 	std: 0.0035273714
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9792, 0.9721, 0.9802, 0.9735]
	train_accs: [0.9809833, 0.9802167, 0.97498333, 0.9809833, 0.9752667]
	best_train_sub_head: 0
	worst: 0.9721
	avg: 0.97706
	best: 0.9803

Starting e_i: 1526
Model ind 665 epoch 1526 batch: 0 avg loss -2.974417 avg loss no lamb -2.974417 time 2020-06-27 12:21:20.489567
Model ind 665 epoch 1526 batch: 100 avg loss -2.937909 avg loss no lamb -2.937909 time 2020-06-27 12:21:31.230834
Model ind 665 epoch 1526 batch: 200 avg loss -2.918621 avg loss no lamb -2.918621 time 2020-06-27 12:21:42.285782
Model ind 665 epoch 1526 batch: 300 avg loss -2.889388 avg loss no lamb -2.889388 time 2020-06-27 12:21:53.074659
Model ind 665 epoch 1526 batch: 400 avg loss -2.768319 avg loss no lamb -2.768319 time 2020-06-27 12:22:04.001569
Model ind 665 epoch 1526 batch: 500 avg loss -2.872324 avg loss no lamb -2.872324 time 2020-06-27 12:22:14.633853
Model ind 665 epoch 1526 batch: 600 avg loss -2.895310 avg loss no lamb -2.895310 time 2020-06-27 12:22:25.381752
Model ind 665 epoch 1526 batch: 700 avg loss -2.772564 avg loss no lamb -2.772564 time 2020-06-27 12:22:36.247176
Model ind 665 epoch 1526 batch: 800 avg loss -2.891635 avg loss no lamb -2.891635 time 2020-06-27 12:22:47.108737
last batch sz 10
Pre: time 2020-06-27 12:23:01.046158: 
 	std: 0.0034248573
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9807, 0.9737, 0.9805, 0.9742]
	train_accs: [0.98188335, 0.9813, 0.9758, 0.9815, 0.9762833]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97812
	best: 0.9815

Starting e_i: 1527
Model ind 665 epoch 1527 batch: 0 avg loss -2.899235 avg loss no lamb -2.899235 time 2020-06-27 12:23:02.301576
Model ind 665 epoch 1527 batch: 100 avg loss -2.896227 avg loss no lamb -2.896227 time 2020-06-27 12:23:13.150582
Model ind 665 epoch 1527 batch: 200 avg loss -2.879348 avg loss no lamb -2.879348 time 2020-06-27 12:23:23.978797
Model ind 665 epoch 1527 batch: 300 avg loss -2.828157 avg loss no lamb -2.828157 time 2020-06-27 12:23:34.742772
Model ind 665 epoch 1527 batch: 400 avg loss -2.749152 avg loss no lamb -2.749152 time 2020-06-27 12:23:45.664000
Model ind 665 epoch 1527 batch: 500 avg loss -2.741296 avg loss no lamb -2.741296 time 2020-06-27 12:23:56.541103
Model ind 665 epoch 1527 batch: 600 avg loss -2.910251 avg loss no lamb -2.910251 time 2020-06-27 12:24:07.531255
Model ind 665 epoch 1527 batch: 700 avg loss -2.729598 avg loss no lamb -2.729598 time 2020-06-27 12:24:18.448606
Model ind 665 epoch 1527 batch: 800 avg loss -2.875269 avg loss no lamb -2.875269 time 2020-06-27 12:24:29.318206
last batch sz 10
Pre: time 2020-06-27 12:24:43.698394: 
 	std: 0.003693012
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9792, 0.9716, 0.9801, 0.9732]
	train_accs: [0.98083335, 0.98001665, 0.9741167, 0.98081666, 0.97583336]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.9768599
	best: 0.9802

Starting e_i: 1528
Model ind 665 epoch 1528 batch: 0 avg loss -2.910688 avg loss no lamb -2.910688 time 2020-06-27 12:24:45.045662
Model ind 665 epoch 1528 batch: 100 avg loss -2.883819 avg loss no lamb -2.883819 time 2020-06-27 12:24:55.686626
Model ind 665 epoch 1528 batch: 200 avg loss -2.889806 avg loss no lamb -2.889806 time 2020-06-27 12:25:06.555754
Model ind 665 epoch 1528 batch: 300 avg loss -2.851827 avg loss no lamb -2.851827 time 2020-06-27 12:25:17.261569
Model ind 665 epoch 1528 batch: 400 avg loss -2.787524 avg loss no lamb -2.787524 time 2020-06-27 12:25:28.037657
Model ind 665 epoch 1528 batch: 500 avg loss -2.837034 avg loss no lamb -2.837034 time 2020-06-27 12:25:39.085148
Model ind 665 epoch 1528 batch: 600 avg loss -2.925786 avg loss no lamb -2.925786 time 2020-06-27 12:25:49.891416
Model ind 665 epoch 1528 batch: 700 avg loss -2.809909 avg loss no lamb -2.809909 time 2020-06-27 12:26:00.751229
Model ind 665 epoch 1528 batch: 800 avg loss -2.900176 avg loss no lamb -2.900176 time 2020-06-27 12:26:11.801506
last batch sz 10
Pre: time 2020-06-27 12:26:25.742511: 
 	std: 0.003318198
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.98, 0.9735, 0.9801, 0.9733]
	train_accs: [0.9812, 0.98053336, 0.9752, 0.98146665, 0.9755]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97746
	best: 0.9801

Starting e_i: 1529
Model ind 665 epoch 1529 batch: 0 avg loss -2.972840 avg loss no lamb -2.972840 time 2020-06-27 12:26:27.006375
Model ind 665 epoch 1529 batch: 100 avg loss -2.913932 avg loss no lamb -2.913932 time 2020-06-27 12:26:37.973975
Model ind 665 epoch 1529 batch: 200 avg loss -2.894300 avg loss no lamb -2.894300 time 2020-06-27 12:26:48.794994
Model ind 665 epoch 1529 batch: 300 avg loss -2.881472 avg loss no lamb -2.881472 time 2020-06-27 12:26:59.623329
Model ind 665 epoch 1529 batch: 400 avg loss -2.824928 avg loss no lamb -2.824928 time 2020-06-27 12:27:10.637285
Model ind 665 epoch 1529 batch: 500 avg loss -2.827127 avg loss no lamb -2.827127 time 2020-06-27 12:27:21.379737
Model ind 665 epoch 1529 batch: 600 avg loss -2.879723 avg loss no lamb -2.879723 time 2020-06-27 12:27:32.149118
Model ind 665 epoch 1529 batch: 700 avg loss -2.809065 avg loss no lamb -2.809065 time 2020-06-27 12:27:43.132760
Model ind 665 epoch 1529 batch: 800 avg loss -2.898256 avg loss no lamb -2.898256 time 2020-06-27 12:27:54.003073
last batch sz 10
Pre: time 2020-06-27 12:28:08.410388: 
 	std: 0.002826591
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9815, 0.9752, 0.9817, 0.9763]
	train_accs: [0.98135, 0.98111665, 0.9761, 0.9814, 0.97686666]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97918
	best: 0.9817

Starting e_i: 1530
Model ind 665 epoch 1530 batch: 0 avg loss -2.916543 avg loss no lamb -2.916543 time 2020-06-27 12:28:09.702293
Model ind 665 epoch 1530 batch: 100 avg loss -2.884470 avg loss no lamb -2.884470 time 2020-06-27 12:28:20.383928
Model ind 665 epoch 1530 batch: 200 avg loss -2.864927 avg loss no lamb -2.864927 time 2020-06-27 12:28:31.097400
Model ind 665 epoch 1530 batch: 300 avg loss -2.887036 avg loss no lamb -2.887036 time 2020-06-27 12:28:42.198891
Model ind 665 epoch 1530 batch: 400 avg loss -2.852658 avg loss no lamb -2.852658 time 2020-06-27 12:28:53.207255
Model ind 665 epoch 1530 batch: 500 avg loss -2.875184 avg loss no lamb -2.875184 time 2020-06-27 12:29:04.094190
Model ind 665 epoch 1530 batch: 600 avg loss -2.911832 avg loss no lamb -2.911832 time 2020-06-27 12:29:14.882493
Model ind 665 epoch 1530 batch: 700 avg loss -2.770018 avg loss no lamb -2.770018 time 2020-06-27 12:29:25.672893
Model ind 665 epoch 1530 batch: 800 avg loss -2.909154 avg loss no lamb -2.909154 time 2020-06-27 12:29:36.566970
last batch sz 10
Pre: time 2020-06-27 12:29:50.507937: 
 	std: 0.003349973
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9806, 0.9732, 0.9809, 0.975]
	train_accs: [0.98135, 0.98043334, 0.9745, 0.9813, 0.97568333]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97814006
	best: 0.981

Starting e_i: 1531
Model ind 665 epoch 1531 batch: 0 avg loss -2.951378 avg loss no lamb -2.951378 time 2020-06-27 12:29:52.993964
Model ind 665 epoch 1531 batch: 100 avg loss -2.940767 avg loss no lamb -2.940767 time 2020-06-27 12:30:03.787872
Model ind 665 epoch 1531 batch: 200 avg loss -2.903081 avg loss no lamb -2.903081 time 2020-06-27 12:30:14.778370
Model ind 665 epoch 1531 batch: 300 avg loss -2.936461 avg loss no lamb -2.936461 time 2020-06-27 12:30:25.734854
Model ind 665 epoch 1531 batch: 400 avg loss -2.814723 avg loss no lamb -2.814723 time 2020-06-27 12:30:36.642109
Model ind 665 epoch 1531 batch: 500 avg loss -2.887927 avg loss no lamb -2.887927 time 2020-06-27 12:30:47.426781
Model ind 665 epoch 1531 batch: 600 avg loss -2.908162 avg loss no lamb -2.908162 time 2020-06-27 12:30:58.241624
Model ind 665 epoch 1531 batch: 700 avg loss -2.775333 avg loss no lamb -2.775333 time 2020-06-27 12:31:08.969269
Model ind 665 epoch 1531 batch: 800 avg loss -2.861075 avg loss no lamb -2.861075 time 2020-06-27 12:31:20.195013
last batch sz 10
Pre: time 2020-06-27 12:31:34.466409: 
 	std: 0.0026780535
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9824, 0.9808, 0.9756, 0.9819, 0.9773]
	train_accs: [0.9818, 0.9808, 0.97636664, 0.9816333, 0.97765]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97959995
	best: 0.9824

Starting e_i: 1532
Model ind 665 epoch 1532 batch: 0 avg loss -2.936172 avg loss no lamb -2.936172 time 2020-06-27 12:31:35.992640
Model ind 665 epoch 1532 batch: 100 avg loss -2.863277 avg loss no lamb -2.863277 time 2020-06-27 12:31:46.947802
Model ind 665 epoch 1532 batch: 200 avg loss -2.828325 avg loss no lamb -2.828325 time 2020-06-27 12:31:57.680610
Model ind 665 epoch 1532 batch: 300 avg loss -2.888930 avg loss no lamb -2.888930 time 2020-06-27 12:32:08.548395
Model ind 665 epoch 1532 batch: 400 avg loss -2.798346 avg loss no lamb -2.798346 time 2020-06-27 12:32:19.692155
Model ind 665 epoch 1532 batch: 500 avg loss -2.842640 avg loss no lamb -2.842640 time 2020-06-27 12:32:30.602040
Model ind 665 epoch 1532 batch: 600 avg loss -2.935445 avg loss no lamb -2.935445 time 2020-06-27 12:32:41.494662
Model ind 665 epoch 1532 batch: 700 avg loss -2.795447 avg loss no lamb -2.795447 time 2020-06-27 12:32:52.385372
Model ind 665 epoch 1532 batch: 800 avg loss -2.863994 avg loss no lamb -2.863994 time 2020-06-27 12:33:03.377981
last batch sz 10
Pre: time 2020-06-27 12:33:17.412119: 
 	std: 0.002597991
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9798, 0.9742, 0.9799, 0.9749]
	train_accs: [0.9814, 0.98043334, 0.9759167, 0.98121667, 0.97611666]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.9777201
	best: 0.9798

Starting e_i: 1533
Model ind 665 epoch 1533 batch: 0 avg loss -2.945104 avg loss no lamb -2.945104 time 2020-06-27 12:33:18.721094
Model ind 665 epoch 1533 batch: 100 avg loss -2.955596 avg loss no lamb -2.955596 time 2020-06-27 12:33:29.515285
Model ind 665 epoch 1533 batch: 200 avg loss -2.891508 avg loss no lamb -2.891508 time 2020-06-27 12:33:40.289801
Model ind 665 epoch 1533 batch: 300 avg loss -2.883437 avg loss no lamb -2.883437 time 2020-06-27 12:33:51.157509
Model ind 665 epoch 1533 batch: 400 avg loss -2.747622 avg loss no lamb -2.747622 time 2020-06-27 12:34:02.197004
Model ind 665 epoch 1533 batch: 500 avg loss -2.855513 avg loss no lamb -2.855513 time 2020-06-27 12:34:13.088668
Model ind 665 epoch 1533 batch: 600 avg loss -2.946058 avg loss no lamb -2.946058 time 2020-06-27 12:34:24.134693
Model ind 665 epoch 1533 batch: 700 avg loss -2.707184 avg loss no lamb -2.707184 time 2020-06-27 12:34:35.053648
Model ind 665 epoch 1533 batch: 800 avg loss -2.839381 avg loss no lamb -2.839381 time 2020-06-27 12:34:45.917762
last batch sz 10
Pre: time 2020-06-27 12:35:00.200425: 
 	std: 0.0026507361
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9807, 0.9756, 0.9812, 0.9756]
	train_accs: [0.98155, 0.9806, 0.9759667, 0.98135, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97884005
	best: 0.9811

Starting e_i: 1534
Model ind 665 epoch 1534 batch: 0 avg loss -2.914965 avg loss no lamb -2.914965 time 2020-06-27 12:35:01.643440
Model ind 665 epoch 1534 batch: 100 avg loss -2.946115 avg loss no lamb -2.946115 time 2020-06-27 12:35:12.639286
Model ind 665 epoch 1534 batch: 200 avg loss -2.886632 avg loss no lamb -2.886632 time 2020-06-27 12:35:23.560845
Model ind 665 epoch 1534 batch: 300 avg loss -2.875413 avg loss no lamb -2.875413 time 2020-06-27 12:35:34.596132
Model ind 665 epoch 1534 batch: 400 avg loss -2.823473 avg loss no lamb -2.823473 time 2020-06-27 12:35:45.718559
Model ind 665 epoch 1534 batch: 500 avg loss -2.879990 avg loss no lamb -2.879990 time 2020-06-27 12:35:56.596064
Model ind 665 epoch 1534 batch: 600 avg loss -2.877590 avg loss no lamb -2.877590 time 2020-06-27 12:36:07.733369
Model ind 665 epoch 1534 batch: 700 avg loss -2.805556 avg loss no lamb -2.805556 time 2020-06-27 12:36:18.751218
Model ind 665 epoch 1534 batch: 800 avg loss -2.827697 avg loss no lamb -2.827697 time 2020-06-27 12:36:29.790670
last batch sz 10
Pre: time 2020-06-27 12:36:44.287908: 
 	std: 0.0035801753
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.981, 0.974, 0.9806, 0.9733]
	train_accs: [0.98155, 0.9809833, 0.97508335, 0.9815, 0.9756167]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97802
	best: 0.9812

Starting e_i: 1535
Model ind 665 epoch 1535 batch: 0 avg loss -2.987972 avg loss no lamb -2.987972 time 2020-06-27 12:36:45.629867
Model ind 665 epoch 1535 batch: 100 avg loss -2.933628 avg loss no lamb -2.933628 time 2020-06-27 12:36:56.592912
Model ind 665 epoch 1535 batch: 200 avg loss -2.889033 avg loss no lamb -2.889033 time 2020-06-27 12:37:07.716764
Model ind 665 epoch 1535 batch: 300 avg loss -2.838208 avg loss no lamb -2.838208 time 2020-06-27 12:37:18.649414
Model ind 665 epoch 1535 batch: 400 avg loss -2.795804 avg loss no lamb -2.795804 time 2020-06-27 12:37:29.664915
Model ind 665 epoch 1535 batch: 500 avg loss -2.859136 avg loss no lamb -2.859136 time 2020-06-27 12:37:40.673085
Model ind 665 epoch 1535 batch: 600 avg loss -2.896950 avg loss no lamb -2.896950 time 2020-06-27 12:37:51.539330
Model ind 665 epoch 1535 batch: 700 avg loss -2.803807 avg loss no lamb -2.803807 time 2020-06-27 12:38:02.371355
Model ind 665 epoch 1535 batch: 800 avg loss -2.842263 avg loss no lamb -2.842263 time 2020-06-27 12:38:13.268359
last batch sz 10
Pre: time 2020-06-27 12:38:27.539080: 
 	std: 0.0023397526
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9811, 0.9754, 0.9806, 0.9767]
	train_accs: [0.9809, 0.98085, 0.97576666, 0.98071665, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9754
	avg: 0.97886
	best: 0.9805

Starting e_i: 1536
Model ind 665 epoch 1536 batch: 0 avg loss -2.948972 avg loss no lamb -2.948972 time 2020-06-27 12:38:28.830883
Model ind 665 epoch 1536 batch: 100 avg loss -2.901074 avg loss no lamb -2.901074 time 2020-06-27 12:38:39.651908
Model ind 665 epoch 1536 batch: 200 avg loss -2.831996 avg loss no lamb -2.831996 time 2020-06-27 12:38:50.710444
Model ind 665 epoch 1536 batch: 300 avg loss -2.891727 avg loss no lamb -2.891727 time 2020-06-27 12:39:01.761458
Model ind 665 epoch 1536 batch: 400 avg loss -2.784498 avg loss no lamb -2.784498 time 2020-06-27 12:39:12.605083
Model ind 665 epoch 1536 batch: 500 avg loss -2.831111 avg loss no lamb -2.831111 time 2020-06-27 12:39:23.382537
Model ind 665 epoch 1536 batch: 600 avg loss -2.926300 avg loss no lamb -2.926300 time 2020-06-27 12:39:34.301089
Model ind 665 epoch 1536 batch: 700 avg loss -2.823214 avg loss no lamb -2.823214 time 2020-06-27 12:39:45.357425
Model ind 665 epoch 1536 batch: 800 avg loss -2.874821 avg loss no lamb -2.874821 time 2020-06-27 12:39:56.340987
last batch sz 10
Pre: time 2020-06-27 12:40:10.619118: 
 	std: 0.0032236574
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.981, 0.9739, 0.9817, 0.9765]
	train_accs: [0.98153335, 0.9813833, 0.97533333, 0.9817333, 0.97671664]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.979
	best: 0.9817

Starting e_i: 1537
Model ind 665 epoch 1537 batch: 0 avg loss -2.975912 avg loss no lamb -2.975912 time 2020-06-27 12:40:11.858200
Model ind 665 epoch 1537 batch: 100 avg loss -2.959947 avg loss no lamb -2.959947 time 2020-06-27 12:40:22.636116
Model ind 665 epoch 1537 batch: 200 avg loss -2.888293 avg loss no lamb -2.888293 time 2020-06-27 12:40:33.439766
Model ind 665 epoch 1537 batch: 300 avg loss -2.888705 avg loss no lamb -2.888705 time 2020-06-27 12:40:44.373690
Model ind 665 epoch 1537 batch: 400 avg loss -2.855215 avg loss no lamb -2.855215 time 2020-06-27 12:40:55.248646
Model ind 665 epoch 1537 batch: 500 avg loss -2.866739 avg loss no lamb -2.866739 time 2020-06-27 12:41:06.474172
Model ind 665 epoch 1537 batch: 600 avg loss -2.913063 avg loss no lamb -2.913063 time 2020-06-27 12:41:17.281018
Model ind 665 epoch 1537 batch: 700 avg loss -2.794592 avg loss no lamb -2.794592 time 2020-06-27 12:41:27.989164
Model ind 665 epoch 1537 batch: 800 avg loss -2.915421 avg loss no lamb -2.915421 time 2020-06-27 12:41:38.983824
last batch sz 10
Pre: time 2020-06-27 12:41:52.924430: 
 	std: 0.0027549362
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9804, 0.974, 0.9803, 0.976]
	train_accs: [0.9812, 0.98075, 0.97565, 0.98095, 0.9766833]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97827995
	best: 0.9807

Starting e_i: 1538
Model ind 665 epoch 1538 batch: 0 avg loss -2.990514 avg loss no lamb -2.990514 time 2020-06-27 12:41:54.178386
Model ind 665 epoch 1538 batch: 100 avg loss -2.871852 avg loss no lamb -2.871852 time 2020-06-27 12:42:05.297498
Model ind 665 epoch 1538 batch: 200 avg loss -2.890620 avg loss no lamb -2.890620 time 2020-06-27 12:42:16.224489
Model ind 665 epoch 1538 batch: 300 avg loss -2.872758 avg loss no lamb -2.872758 time 2020-06-27 12:42:27.181550
Model ind 665 epoch 1538 batch: 400 avg loss -2.818214 avg loss no lamb -2.818214 time 2020-06-27 12:42:38.033222
Model ind 665 epoch 1538 batch: 500 avg loss -2.814668 avg loss no lamb -2.814668 time 2020-06-27 12:42:48.912479
Model ind 665 epoch 1538 batch: 600 avg loss -2.859858 avg loss no lamb -2.859858 time 2020-06-27 12:42:59.749507
Model ind 665 epoch 1538 batch: 700 avg loss -2.812916 avg loss no lamb -2.812916 time 2020-06-27 12:43:10.546130
Model ind 665 epoch 1538 batch: 800 avg loss -2.900072 avg loss no lamb -2.900072 time 2020-06-27 12:43:21.361957
last batch sz 10
Pre: time 2020-06-27 12:43:35.830120: 
 	std: 0.0033018629
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9791, 0.9725, 0.9805, 0.9743]
	train_accs: [0.98113334, 0.9802833, 0.9745167, 0.98118335, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97734004
	best: 0.9805

Starting e_i: 1539
Model ind 665 epoch 1539 batch: 0 avg loss -2.900920 avg loss no lamb -2.900920 time 2020-06-27 12:43:37.114465
Model ind 665 epoch 1539 batch: 100 avg loss -2.951788 avg loss no lamb -2.951788 time 2020-06-27 12:43:48.042414
Model ind 665 epoch 1539 batch: 200 avg loss -2.878392 avg loss no lamb -2.878392 time 2020-06-27 12:43:58.960743
Model ind 665 epoch 1539 batch: 300 avg loss -2.856402 avg loss no lamb -2.856402 time 2020-06-27 12:44:09.780184
Model ind 665 epoch 1539 batch: 400 avg loss -2.783444 avg loss no lamb -2.783444 time 2020-06-27 12:44:20.593828
Model ind 665 epoch 1539 batch: 500 avg loss -2.884200 avg loss no lamb -2.884200 time 2020-06-27 12:44:31.537843
Model ind 665 epoch 1539 batch: 600 avg loss -2.923046 avg loss no lamb -2.923046 time 2020-06-27 12:44:42.531296
Model ind 665 epoch 1539 batch: 700 avg loss -2.762421 avg loss no lamb -2.762421 time 2020-06-27 12:44:53.465331
Model ind 665 epoch 1539 batch: 800 avg loss -2.889838 avg loss no lamb -2.889838 time 2020-06-27 12:45:04.419723
last batch sz 10
Pre: time 2020-06-27 12:45:18.361467: 
 	std: 0.0027868156
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.98, 0.9743, 0.9802, 0.9746]
	train_accs: [0.9812667, 0.98081666, 0.9756333, 0.98118335, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.9778601
	best: 0.9802

Starting e_i: 1540
Model ind 665 epoch 1540 batch: 0 avg loss -2.888193 avg loss no lamb -2.888193 time 2020-06-27 12:45:19.629411
Model ind 665 epoch 1540 batch: 100 avg loss -2.903981 avg loss no lamb -2.903981 time 2020-06-27 12:45:30.462516
Model ind 665 epoch 1540 batch: 200 avg loss -2.875051 avg loss no lamb -2.875051 time 2020-06-27 12:45:41.394679
Model ind 665 epoch 1540 batch: 300 avg loss -2.859266 avg loss no lamb -2.859266 time 2020-06-27 12:45:52.187280
Model ind 665 epoch 1540 batch: 400 avg loss -2.803947 avg loss no lamb -2.803947 time 2020-06-27 12:46:03.304781
Model ind 665 epoch 1540 batch: 500 avg loss -2.876836 avg loss no lamb -2.876836 time 2020-06-27 12:46:14.170982
Model ind 665 epoch 1540 batch: 600 avg loss -2.872271 avg loss no lamb -2.872271 time 2020-06-27 12:46:25.089946
Model ind 665 epoch 1540 batch: 700 avg loss -2.814857 avg loss no lamb -2.814857 time 2020-06-27 12:46:35.973172
Model ind 665 epoch 1540 batch: 800 avg loss -2.904184 avg loss no lamb -2.904184 time 2020-06-27 12:46:46.991215
last batch sz 10
Pre: time 2020-06-27 12:47:01.423518: 
 	std: 0.0037359898
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9796, 0.9715, 0.9804, 0.9743]
	train_accs: [0.9809833, 0.98008335, 0.97388333, 0.9809, 0.97583336]
	best_train_sub_head: 0
	worst: 0.9715
	avg: 0.97732
	best: 0.9808

Starting e_i: 1541
Model ind 665 epoch 1541 batch: 0 avg loss -2.953687 avg loss no lamb -2.953687 time 2020-06-27 12:47:03.949676
Model ind 665 epoch 1541 batch: 100 avg loss -2.888783 avg loss no lamb -2.888783 time 2020-06-27 12:47:15.064657
Model ind 665 epoch 1541 batch: 200 avg loss -2.841015 avg loss no lamb -2.841015 time 2020-06-27 12:47:25.835673
Model ind 665 epoch 1541 batch: 300 avg loss -2.876665 avg loss no lamb -2.876665 time 2020-06-27 12:47:36.563726
Model ind 665 epoch 1541 batch: 400 avg loss -2.870035 avg loss no lamb -2.870035 time 2020-06-27 12:47:47.496097
Model ind 665 epoch 1541 batch: 500 avg loss -2.858872 avg loss no lamb -2.858872 time 2020-06-27 12:47:58.341249
Model ind 665 epoch 1541 batch: 600 avg loss -2.909035 avg loss no lamb -2.909035 time 2020-06-27 12:48:09.609601
Model ind 665 epoch 1541 batch: 700 avg loss -2.833086 avg loss no lamb -2.833086 time 2020-06-27 12:48:20.557735
Model ind 665 epoch 1541 batch: 800 avg loss -2.861302 avg loss no lamb -2.861302 time 2020-06-27 12:48:31.464287
last batch sz 10
Pre: time 2020-06-27 12:48:45.446553: 
 	std: 0.00366966
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9804, 0.9724, 0.9805, 0.9738]
	train_accs: [0.98076665, 0.9806, 0.9744167, 0.9809667, 0.9752667]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.97756004
	best: 0.9805

Starting e_i: 1542
Model ind 665 epoch 1542 batch: 0 avg loss -2.950480 avg loss no lamb -2.950480 time 2020-06-27 12:48:46.772519
Model ind 665 epoch 1542 batch: 100 avg loss -2.949423 avg loss no lamb -2.949423 time 2020-06-27 12:48:57.549641
Model ind 665 epoch 1542 batch: 200 avg loss -2.899240 avg loss no lamb -2.899240 time 2020-06-27 12:49:08.672845
Model ind 665 epoch 1542 batch: 300 avg loss -2.905039 avg loss no lamb -2.905039 time 2020-06-27 12:49:19.346859
Model ind 665 epoch 1542 batch: 400 avg loss -2.823727 avg loss no lamb -2.823727 time 2020-06-27 12:49:30.203537
Model ind 665 epoch 1542 batch: 500 avg loss -2.867259 avg loss no lamb -2.867259 time 2020-06-27 12:49:41.187623
Model ind 665 epoch 1542 batch: 600 avg loss -2.862746 avg loss no lamb -2.862746 time 2020-06-27 12:49:52.132786
Model ind 665 epoch 1542 batch: 700 avg loss -2.794391 avg loss no lamb -2.794391 time 2020-06-27 12:50:03.255647
Model ind 665 epoch 1542 batch: 800 avg loss -2.860397 avg loss no lamb -2.860397 time 2020-06-27 12:50:14.262977
last batch sz 10
Pre: time 2020-06-27 12:50:28.654177: 
 	std: 0.003407641
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9804, 0.9731, 0.9806, 0.9742]
	train_accs: [0.9813333, 0.9806833, 0.9748333, 0.98151666, 0.9759333]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.9778
	best: 0.9806

Starting e_i: 1543
Model ind 665 epoch 1543 batch: 0 avg loss -2.984350 avg loss no lamb -2.984350 time 2020-06-27 12:50:30.094523
Model ind 665 epoch 1543 batch: 100 avg loss -2.917137 avg loss no lamb -2.917137 time 2020-06-27 12:50:41.023501
Model ind 665 epoch 1543 batch: 200 avg loss -2.884293 avg loss no lamb -2.884293 time 2020-06-27 12:50:52.125373
Model ind 665 epoch 1543 batch: 300 avg loss -2.876477 avg loss no lamb -2.876477 time 2020-06-27 12:51:03.262595
Model ind 665 epoch 1543 batch: 400 avg loss -2.817581 avg loss no lamb -2.817581 time 2020-06-27 12:51:14.450608
Model ind 665 epoch 1543 batch: 500 avg loss -2.923722 avg loss no lamb -2.923722 time 2020-06-27 12:51:25.322470
Model ind 665 epoch 1543 batch: 600 avg loss -2.910312 avg loss no lamb -2.910312 time 2020-06-27 12:51:36.155579
Model ind 665 epoch 1543 batch: 700 avg loss -2.810677 avg loss no lamb -2.810677 time 2020-06-27 12:51:47.017283
Model ind 665 epoch 1543 batch: 800 avg loss -2.859187 avg loss no lamb -2.859187 time 2020-06-27 12:51:57.942353
last batch sz 10
Pre: time 2020-06-27 12:52:12.144241: 
 	std: 0.0035385874
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9786, 0.9716, 0.9798, 0.9728]
	train_accs: [0.9809167, 0.9799333, 0.97468334, 0.98123336, 0.97565]
	best_train_sub_head: 3
	worst: 0.9716
	avg: 0.97648
	best: 0.9798

Starting e_i: 1544
Model ind 665 epoch 1544 batch: 0 avg loss -2.946908 avg loss no lamb -2.946908 time 2020-06-27 12:52:13.384428
Model ind 665 epoch 1544 batch: 100 avg loss -2.908504 avg loss no lamb -2.908504 time 2020-06-27 12:52:24.390096
Model ind 665 epoch 1544 batch: 200 avg loss -2.924006 avg loss no lamb -2.924006 time 2020-06-27 12:52:35.050541
Model ind 665 epoch 1544 batch: 300 avg loss -2.895495 avg loss no lamb -2.895495 time 2020-06-27 12:52:46.055390
Model ind 665 epoch 1544 batch: 400 avg loss -2.815955 avg loss no lamb -2.815955 time 2020-06-27 12:52:56.885663
Model ind 665 epoch 1544 batch: 500 avg loss -2.779900 avg loss no lamb -2.779900 time 2020-06-27 12:53:07.761564
Model ind 665 epoch 1544 batch: 600 avg loss -2.866124 avg loss no lamb -2.866124 time 2020-06-27 12:53:18.489787
Model ind 665 epoch 1544 batch: 700 avg loss -2.824310 avg loss no lamb -2.824310 time 2020-06-27 12:53:29.452252
Model ind 665 epoch 1544 batch: 800 avg loss -2.929336 avg loss no lamb -2.929336 time 2020-06-27 12:53:40.486444
last batch sz 10
Pre: time 2020-06-27 12:53:54.788604: 
 	std: 0.0029441507
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9789, 0.9726, 0.9794, 0.9741]
	train_accs: [0.9809667, 0.9806, 0.9752833, 0.98121667, 0.97605]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.9769
	best: 0.9794

Starting e_i: 1545
Model ind 665 epoch 1545 batch: 0 avg loss -2.961928 avg loss no lamb -2.961928 time 2020-06-27 12:53:56.346945
Model ind 665 epoch 1545 batch: 100 avg loss -2.884804 avg loss no lamb -2.884804 time 2020-06-27 12:54:07.442780
Model ind 665 epoch 1545 batch: 200 avg loss -2.848629 avg loss no lamb -2.848629 time 2020-06-27 12:54:18.233800
Model ind 665 epoch 1545 batch: 300 avg loss -2.879349 avg loss no lamb -2.879349 time 2020-06-27 12:54:29.031858
Model ind 665 epoch 1545 batch: 400 avg loss -2.823211 avg loss no lamb -2.823211 time 2020-06-27 12:54:40.114138
Model ind 665 epoch 1545 batch: 500 avg loss -2.874787 avg loss no lamb -2.874787 time 2020-06-27 12:54:51.043385
Model ind 665 epoch 1545 batch: 600 avg loss -2.857141 avg loss no lamb -2.857141 time 2020-06-27 12:55:01.842400
Model ind 665 epoch 1545 batch: 700 avg loss -2.810469 avg loss no lamb -2.810469 time 2020-06-27 12:55:12.823048
Model ind 665 epoch 1545 batch: 800 avg loss -2.918808 avg loss no lamb -2.918808 time 2020-06-27 12:55:23.648372
last batch sz 10
Pre: time 2020-06-27 12:55:37.925394: 
 	std: 0.0038034802
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9784, 0.9709, 0.9797, 0.972]
	train_accs: [0.9805167, 0.9797, 0.97375, 0.9806833, 0.97466666]
	best_train_sub_head: 3
	worst: 0.9709
	avg: 0.97606003
	best: 0.9797

Starting e_i: 1546
Model ind 665 epoch 1546 batch: 0 avg loss -2.955915 avg loss no lamb -2.955915 time 2020-06-27 12:55:39.211680
Model ind 665 epoch 1546 batch: 100 avg loss -2.919214 avg loss no lamb -2.919214 time 2020-06-27 12:55:50.030990
Model ind 665 epoch 1546 batch: 200 avg loss -2.897817 avg loss no lamb -2.897817 time 2020-06-27 12:56:01.053341
Model ind 665 epoch 1546 batch: 300 avg loss -2.870672 avg loss no lamb -2.870672 time 2020-06-27 12:56:12.148515
Model ind 665 epoch 1546 batch: 400 avg loss -2.795592 avg loss no lamb -2.795592 time 2020-06-27 12:56:23.219432
Model ind 665 epoch 1546 batch: 500 avg loss -2.853257 avg loss no lamb -2.853257 time 2020-06-27 12:56:34.174808
Model ind 665 epoch 1546 batch: 600 avg loss -2.863388 avg loss no lamb -2.863388 time 2020-06-27 12:56:44.772850
Model ind 665 epoch 1546 batch: 700 avg loss -2.840352 avg loss no lamb -2.840352 time 2020-06-27 12:56:55.842523
Model ind 665 epoch 1546 batch: 800 avg loss -2.922897 avg loss no lamb -2.922897 time 2020-06-27 12:57:06.688619
last batch sz 10
Pre: time 2020-06-27 12:57:20.923054: 
 	std: 0.003708919
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9801, 0.9726, 0.9804, 0.9736]
	train_accs: [0.9820667, 0.9809833, 0.9752667, 0.98186666, 0.9762667]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.9776
	best: 0.9813

Starting e_i: 1547
Model ind 665 epoch 1547 batch: 0 avg loss -2.925716 avg loss no lamb -2.925716 time 2020-06-27 12:57:22.258200
Model ind 665 epoch 1547 batch: 100 avg loss -2.893798 avg loss no lamb -2.893798 time 2020-06-27 12:57:33.045992
Model ind 665 epoch 1547 batch: 200 avg loss -2.868340 avg loss no lamb -2.868340 time 2020-06-27 12:57:43.861300
Model ind 665 epoch 1547 batch: 300 avg loss -2.862235 avg loss no lamb -2.862235 time 2020-06-27 12:57:54.882591
Model ind 665 epoch 1547 batch: 400 avg loss -2.762253 avg loss no lamb -2.762253 time 2020-06-27 12:58:05.706371
Model ind 665 epoch 1547 batch: 500 avg loss -2.846745 avg loss no lamb -2.846745 time 2020-06-27 12:58:16.697209
Model ind 665 epoch 1547 batch: 600 avg loss -2.874906 avg loss no lamb -2.874906 time 2020-06-27 12:58:27.645327
Model ind 665 epoch 1547 batch: 700 avg loss -2.756135 avg loss no lamb -2.756135 time 2020-06-27 12:58:38.890942
Model ind 665 epoch 1547 batch: 800 avg loss -2.865934 avg loss no lamb -2.865934 time 2020-06-27 12:58:49.764106
last batch sz 10
Pre: time 2020-06-27 12:59:04.308128: 
 	std: 0.0029407442
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9804, 0.9743, 0.9803, 0.9745]
	train_accs: [0.98178333, 0.98143333, 0.97618335, 0.98183334, 0.9766]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97800004
	best: 0.9803

Starting e_i: 1548
Model ind 665 epoch 1548 batch: 0 avg loss -2.905240 avg loss no lamb -2.905240 time 2020-06-27 12:59:05.621537
Model ind 665 epoch 1548 batch: 100 avg loss -2.890150 avg loss no lamb -2.890150 time 2020-06-27 12:59:16.542768
Model ind 665 epoch 1548 batch: 200 avg loss -2.879451 avg loss no lamb -2.879451 time 2020-06-27 12:59:27.337987
Model ind 665 epoch 1548 batch: 300 avg loss -2.897333 avg loss no lamb -2.897333 time 2020-06-27 12:59:38.138321
Model ind 665 epoch 1548 batch: 400 avg loss -2.813568 avg loss no lamb -2.813568 time 2020-06-27 12:59:49.115336
Model ind 665 epoch 1548 batch: 500 avg loss -2.886657 avg loss no lamb -2.886657 time 2020-06-27 13:00:00.241855
Model ind 665 epoch 1548 batch: 600 avg loss -2.909299 avg loss no lamb -2.909299 time 2020-06-27 13:00:11.178343
Model ind 665 epoch 1548 batch: 700 avg loss -2.795020 avg loss no lamb -2.795020 time 2020-06-27 13:00:22.031953
Model ind 665 epoch 1548 batch: 800 avg loss -2.906227 avg loss no lamb -2.906227 time 2020-06-27 13:00:33.034131
last batch sz 10
Pre: time 2020-06-27 13:00:47.552440: 
 	std: 0.0035448116
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9811, 0.9733, 0.9808, 0.974]
	train_accs: [0.98118335, 0.98085, 0.97515, 0.9813167, 0.97585]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97798
	best: 0.9808

Starting e_i: 1549
Model ind 665 epoch 1549 batch: 0 avg loss -2.930701 avg loss no lamb -2.930701 time 2020-06-27 13:00:48.946598
Model ind 665 epoch 1549 batch: 100 avg loss -2.920466 avg loss no lamb -2.920466 time 2020-06-27 13:00:59.760356
Model ind 665 epoch 1549 batch: 200 avg loss -2.863266 avg loss no lamb -2.863266 time 2020-06-27 13:01:10.743582
Model ind 665 epoch 1549 batch: 300 avg loss -2.842949 avg loss no lamb -2.842949 time 2020-06-27 13:01:21.523568
Model ind 665 epoch 1549 batch: 400 avg loss -2.807384 avg loss no lamb -2.807384 time 2020-06-27 13:01:32.391805
Model ind 665 epoch 1549 batch: 500 avg loss -2.857663 avg loss no lamb -2.857663 time 2020-06-27 13:01:43.345200
Model ind 665 epoch 1549 batch: 600 avg loss -2.874953 avg loss no lamb -2.874953 time 2020-06-27 13:01:54.290342
Model ind 665 epoch 1549 batch: 700 avg loss -2.768431 avg loss no lamb -2.768431 time 2020-06-27 13:02:05.164627
Model ind 665 epoch 1549 batch: 800 avg loss -2.866598 avg loss no lamb -2.866598 time 2020-06-27 13:02:16.425303
last batch sz 10
Pre: time 2020-06-27 13:02:30.874814: 
 	std: 0.0037725342
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.979, 0.9711, 0.9795, 0.9721]
	train_accs: [0.98085, 0.9801, 0.9741667, 0.98121667, 0.9751]
	best_train_sub_head: 3
	worst: 0.9711
	avg: 0.9762
	best: 0.9795

Starting e_i: 1550
Model ind 665 epoch 1550 batch: 0 avg loss -2.962365 avg loss no lamb -2.962365 time 2020-06-27 13:02:32.230824
Model ind 665 epoch 1550 batch: 100 avg loss -2.890026 avg loss no lamb -2.890026 time 2020-06-27 13:02:43.142956
Model ind 665 epoch 1550 batch: 200 avg loss -2.859855 avg loss no lamb -2.859855 time 2020-06-27 13:02:54.100588
Model ind 665 epoch 1550 batch: 300 avg loss -2.860830 avg loss no lamb -2.860830 time 2020-06-27 13:03:04.921946
Model ind 665 epoch 1550 batch: 400 avg loss -2.816348 avg loss no lamb -2.816348 time 2020-06-27 13:03:15.745122
Model ind 665 epoch 1550 batch: 500 avg loss -2.871160 avg loss no lamb -2.871160 time 2020-06-27 13:03:26.605284
Model ind 665 epoch 1550 batch: 600 avg loss -2.859502 avg loss no lamb -2.859502 time 2020-06-27 13:03:37.503157
Model ind 665 epoch 1550 batch: 700 avg loss -2.785510 avg loss no lamb -2.785510 time 2020-06-27 13:03:48.374393
Model ind 665 epoch 1550 batch: 800 avg loss -2.928129 avg loss no lamb -2.928129 time 2020-06-27 13:03:59.343673
last batch sz 10
Pre: time 2020-06-27 13:04:13.392354: 
 	std: 0.003651082
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9797, 0.9724, 0.9804, 0.9739]
	train_accs: [0.9813667, 0.98046666, 0.9748333, 0.98123336, 0.97605]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97754
	best: 0.9813

Starting e_i: 1551
Model ind 665 epoch 1551 batch: 0 avg loss -2.934464 avg loss no lamb -2.934464 time 2020-06-27 13:04:15.927880
Model ind 665 epoch 1551 batch: 100 avg loss -2.906933 avg loss no lamb -2.906933 time 2020-06-27 13:04:26.523627
Model ind 665 epoch 1551 batch: 200 avg loss -2.877038 avg loss no lamb -2.877038 time 2020-06-27 13:04:37.345605
Model ind 665 epoch 1551 batch: 300 avg loss -2.869184 avg loss no lamb -2.869184 time 2020-06-27 13:04:48.303436
Model ind 665 epoch 1551 batch: 400 avg loss -2.778921 avg loss no lamb -2.778921 time 2020-06-27 13:04:59.103010
Model ind 665 epoch 1551 batch: 500 avg loss -2.850450 avg loss no lamb -2.850450 time 2020-06-27 13:05:09.993456
Model ind 665 epoch 1551 batch: 600 avg loss -2.926237 avg loss no lamb -2.926237 time 2020-06-27 13:05:20.841647
Model ind 665 epoch 1551 batch: 700 avg loss -2.843939 avg loss no lamb -2.843939 time 2020-06-27 13:05:31.706172
Model ind 665 epoch 1551 batch: 800 avg loss -2.871025 avg loss no lamb -2.871025 time 2020-06-27 13:05:42.704481
last batch sz 10
Pre: time 2020-06-27 13:05:56.881983: 
 	std: 0.0033302282
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9817, 0.9799, 0.974, 0.9812, 0.9745]
	train_accs: [0.98145, 0.9811, 0.9762833, 0.9816833, 0.97615]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97826004
	best: 0.9812

Starting e_i: 1552
Model ind 665 epoch 1552 batch: 0 avg loss -2.935043 avg loss no lamb -2.935043 time 2020-06-27 13:05:58.190951
Model ind 665 epoch 1552 batch: 100 avg loss -2.837407 avg loss no lamb -2.837407 time 2020-06-27 13:06:09.143638
Model ind 665 epoch 1552 batch: 200 avg loss -2.910335 avg loss no lamb -2.910335 time 2020-06-27 13:06:20.141547
Model ind 665 epoch 1552 batch: 300 avg loss -2.829018 avg loss no lamb -2.829018 time 2020-06-27 13:06:30.828701
Model ind 665 epoch 1552 batch: 400 avg loss -2.849517 avg loss no lamb -2.849517 time 2020-06-27 13:06:41.854976
Model ind 665 epoch 1552 batch: 500 avg loss -2.898560 avg loss no lamb -2.898560 time 2020-06-27 13:06:52.749171
Model ind 665 epoch 1552 batch: 600 avg loss -2.877221 avg loss no lamb -2.877221 time 2020-06-27 13:07:03.801195
Model ind 665 epoch 1552 batch: 700 avg loss -2.850028 avg loss no lamb -2.850028 time 2020-06-27 13:07:14.763198
Model ind 665 epoch 1552 batch: 800 avg loss -2.873755 avg loss no lamb -2.873755 time 2020-06-27 13:07:25.695553
last batch sz 10
Pre: time 2020-06-27 13:07:39.628216: 
 	std: 0.003523301
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9813, 0.9736, 0.9819, 0.9758]
	train_accs: [0.9818, 0.98108333, 0.97543335, 0.9818, 0.9766]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97892
	best: 0.982

Starting e_i: 1553
Model ind 665 epoch 1553 batch: 0 avg loss -2.962980 avg loss no lamb -2.962980 time 2020-06-27 13:07:40.905693
Model ind 665 epoch 1553 batch: 100 avg loss -2.873912 avg loss no lamb -2.873912 time 2020-06-27 13:07:51.857404
Model ind 665 epoch 1553 batch: 200 avg loss -2.862421 avg loss no lamb -2.862421 time 2020-06-27 13:08:02.959451
Model ind 665 epoch 1553 batch: 300 avg loss -2.885499 avg loss no lamb -2.885499 time 2020-06-27 13:08:13.993800
Model ind 665 epoch 1553 batch: 400 avg loss -2.767787 avg loss no lamb -2.767787 time 2020-06-27 13:08:24.944500
Model ind 665 epoch 1553 batch: 500 avg loss -2.859293 avg loss no lamb -2.859293 time 2020-06-27 13:08:35.758160
Model ind 665 epoch 1553 batch: 600 avg loss -2.878333 avg loss no lamb -2.878333 time 2020-06-27 13:08:46.650253
Model ind 665 epoch 1553 batch: 700 avg loss -2.810486 avg loss no lamb -2.810486 time 2020-06-27 13:08:57.644097
Model ind 665 epoch 1553 batch: 800 avg loss -2.910871 avg loss no lamb -2.910871 time 2020-06-27 13:09:08.656153
last batch sz 10
Pre: time 2020-06-27 13:09:22.991194: 
 	std: 0.003833329
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9787, 0.9705, 0.9796, 0.9731]
	train_accs: [0.981, 0.9802, 0.9735, 0.98081666, 0.97495]
	best_train_sub_head: 0
	worst: 0.9705
	avg: 0.97636
	best: 0.9799

Starting e_i: 1554
Model ind 665 epoch 1554 batch: 0 avg loss -2.974154 avg loss no lamb -2.974154 time 2020-06-27 13:09:24.509293
Model ind 665 epoch 1554 batch: 100 avg loss -2.909565 avg loss no lamb -2.909565 time 2020-06-27 13:09:35.508778
Model ind 665 epoch 1554 batch: 200 avg loss -2.906938 avg loss no lamb -2.906938 time 2020-06-27 13:09:46.425620
Model ind 665 epoch 1554 batch: 300 avg loss -2.927254 avg loss no lamb -2.927254 time 2020-06-27 13:09:57.548574
Model ind 665 epoch 1554 batch: 400 avg loss -2.763138 avg loss no lamb -2.763138 time 2020-06-27 13:10:08.252284
Model ind 665 epoch 1554 batch: 500 avg loss -2.906077 avg loss no lamb -2.906077 time 2020-06-27 13:10:19.151355
Model ind 665 epoch 1554 batch: 600 avg loss -2.938000 avg loss no lamb -2.938000 time 2020-06-27 13:10:30.196080
Model ind 665 epoch 1554 batch: 700 avg loss -2.788403 avg loss no lamb -2.788403 time 2020-06-27 13:10:41.296952
Model ind 665 epoch 1554 batch: 800 avg loss -2.865185 avg loss no lamb -2.865185 time 2020-06-27 13:10:52.666378
last batch sz 10
Pre: time 2020-06-27 13:11:07.050964: 
 	std: 0.0024581368
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9796, 0.9744, 0.9799, 0.9758]
	train_accs: [0.98153335, 0.9802333, 0.9751667, 0.98141664, 0.97671664]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97804004
	best: 0.9805

Starting e_i: 1555
Model ind 665 epoch 1555 batch: 0 avg loss -2.916947 avg loss no lamb -2.916947 time 2020-06-27 13:11:08.367367
Model ind 665 epoch 1555 batch: 100 avg loss -2.843663 avg loss no lamb -2.843663 time 2020-06-27 13:11:19.290769
Model ind 665 epoch 1555 batch: 200 avg loss -2.853137 avg loss no lamb -2.853137 time 2020-06-27 13:11:30.207763
Model ind 665 epoch 1555 batch: 300 avg loss -2.848665 avg loss no lamb -2.848665 time 2020-06-27 13:11:41.241749
Model ind 665 epoch 1555 batch: 400 avg loss -2.816510 avg loss no lamb -2.816510 time 2020-06-27 13:11:51.875573
Model ind 665 epoch 1555 batch: 500 avg loss -2.833263 avg loss no lamb -2.833263 time 2020-06-27 13:12:02.793997
Model ind 665 epoch 1555 batch: 600 avg loss -2.907271 avg loss no lamb -2.907271 time 2020-06-27 13:12:13.583180
Model ind 665 epoch 1555 batch: 700 avg loss -2.799884 avg loss no lamb -2.799884 time 2020-06-27 13:12:24.350611
Model ind 665 epoch 1555 batch: 800 avg loss -2.868390 avg loss no lamb -2.868390 time 2020-06-27 13:12:35.558664
last batch sz 10
Pre: time 2020-06-27 13:12:49.648731: 
 	std: 0.0036096396
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9812, 0.9739, 0.9812, 0.9739]
	train_accs: [0.98118335, 0.9805, 0.975, 0.98121667, 0.97533333]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97832
	best: 0.9812

Starting e_i: 1556
Model ind 665 epoch 1556 batch: 0 avg loss -2.942581 avg loss no lamb -2.942581 time 2020-06-27 13:12:51.161113
Model ind 665 epoch 1556 batch: 100 avg loss -2.900467 avg loss no lamb -2.900467 time 2020-06-27 13:13:02.109019
Model ind 665 epoch 1556 batch: 200 avg loss -2.887161 avg loss no lamb -2.887161 time 2020-06-27 13:13:12.835376
Model ind 665 epoch 1556 batch: 300 avg loss -2.860222 avg loss no lamb -2.860222 time 2020-06-27 13:13:23.876171
Model ind 665 epoch 1556 batch: 400 avg loss -2.809213 avg loss no lamb -2.809213 time 2020-06-27 13:13:34.990178
Model ind 665 epoch 1556 batch: 500 avg loss -2.866688 avg loss no lamb -2.866688 time 2020-06-27 13:13:46.027705
Model ind 665 epoch 1556 batch: 600 avg loss -2.876189 avg loss no lamb -2.876189 time 2020-06-27 13:13:57.193903
Model ind 665 epoch 1556 batch: 700 avg loss -2.822883 avg loss no lamb -2.822883 time 2020-06-27 13:14:08.162773
Model ind 665 epoch 1556 batch: 800 avg loss -2.846792 avg loss no lamb -2.846792 time 2020-06-27 13:14:19.097884
last batch sz 10
Pre: time 2020-06-27 13:14:33.308551: 
 	std: 0.0038400923
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9798, 0.972, 0.9812, 0.9739]
	train_accs: [0.9809333, 0.9799833, 0.97425, 0.98118335, 0.9758833]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97756004
	best: 0.9812

Starting e_i: 1557
Model ind 665 epoch 1557 batch: 0 avg loss -2.977738 avg loss no lamb -2.977738 time 2020-06-27 13:14:34.647490
Model ind 665 epoch 1557 batch: 100 avg loss -2.934266 avg loss no lamb -2.934266 time 2020-06-27 13:14:45.520759
Model ind 665 epoch 1557 batch: 200 avg loss -2.900651 avg loss no lamb -2.900651 time 2020-06-27 13:14:56.379283
Model ind 665 epoch 1557 batch: 300 avg loss -2.881275 avg loss no lamb -2.881275 time 2020-06-27 13:15:07.392483
Model ind 665 epoch 1557 batch: 400 avg loss -2.804544 avg loss no lamb -2.804544 time 2020-06-27 13:15:18.311648
Model ind 665 epoch 1557 batch: 500 avg loss -2.872809 avg loss no lamb -2.872809 time 2020-06-27 13:15:29.144052
Model ind 665 epoch 1557 batch: 600 avg loss -2.883993 avg loss no lamb -2.883993 time 2020-06-27 13:15:39.951177
Model ind 665 epoch 1557 batch: 700 avg loss -2.837222 avg loss no lamb -2.837222 time 2020-06-27 13:15:51.005658
Model ind 665 epoch 1557 batch: 800 avg loss -2.914656 avg loss no lamb -2.914656 time 2020-06-27 13:16:02.149240
last batch sz 10
Pre: time 2020-06-27 13:16:16.366908: 
 	std: 0.0030183415
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.981, 0.9746, 0.9816, 0.9758]
	train_accs: [0.98146665, 0.98105, 0.97605, 0.9816167, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97886
	best: 0.9816

Starting e_i: 1558
Model ind 665 epoch 1558 batch: 0 avg loss -2.963399 avg loss no lamb -2.963399 time 2020-06-27 13:16:17.665979
Model ind 665 epoch 1558 batch: 100 avg loss -2.945238 avg loss no lamb -2.945238 time 2020-06-27 13:16:28.635920
Model ind 665 epoch 1558 batch: 200 avg loss -2.884498 avg loss no lamb -2.884498 time 2020-06-27 13:16:39.381227
Model ind 665 epoch 1558 batch: 300 avg loss -2.814492 avg loss no lamb -2.814492 time 2020-06-27 13:16:50.179577
Model ind 665 epoch 1558 batch: 400 avg loss -2.781619 avg loss no lamb -2.781619 time 2020-06-27 13:17:01.100423
Model ind 665 epoch 1558 batch: 500 avg loss -2.886390 avg loss no lamb -2.886390 time 2020-06-27 13:17:12.044948
Model ind 665 epoch 1558 batch: 600 avg loss -2.925158 avg loss no lamb -2.925158 time 2020-06-27 13:17:23.022980
Model ind 665 epoch 1558 batch: 700 avg loss -2.835886 avg loss no lamb -2.835886 time 2020-06-27 13:17:34.065626
Model ind 665 epoch 1558 batch: 800 avg loss -2.809632 avg loss no lamb -2.809632 time 2020-06-27 13:17:45.035850
last batch sz 10
Pre: time 2020-06-27 13:17:59.484943: 
 	std: 0.0031211504
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9807, 0.9742, 0.9805, 0.9744]
	train_accs: [0.98156667, 0.9807, 0.97568333, 0.9814, 0.9759]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97811997
	best: 0.9808

Starting e_i: 1559
Model ind 665 epoch 1559 batch: 0 avg loss -2.985443 avg loss no lamb -2.985443 time 2020-06-27 13:18:00.826952
Model ind 665 epoch 1559 batch: 100 avg loss -2.925968 avg loss no lamb -2.925968 time 2020-06-27 13:18:11.737584
Model ind 665 epoch 1559 batch: 200 avg loss -2.896271 avg loss no lamb -2.896271 time 2020-06-27 13:18:22.684659
Model ind 665 epoch 1559 batch: 300 avg loss -2.894481 avg loss no lamb -2.894481 time 2020-06-27 13:18:33.441725
Model ind 665 epoch 1559 batch: 400 avg loss -2.795909 avg loss no lamb -2.795909 time 2020-06-27 13:18:44.457756
Model ind 665 epoch 1559 batch: 500 avg loss -2.850062 avg loss no lamb -2.850062 time 2020-06-27 13:18:55.342075
Model ind 665 epoch 1559 batch: 600 avg loss -2.913947 avg loss no lamb -2.913947 time 2020-06-27 13:19:06.351668
Model ind 665 epoch 1559 batch: 700 avg loss -2.732736 avg loss no lamb -2.732736 time 2020-06-27 13:19:17.433109
Model ind 665 epoch 1559 batch: 800 avg loss -2.903904 avg loss no lamb -2.903904 time 2020-06-27 13:19:28.131514
last batch sz 10
Pre: time 2020-06-27 13:19:42.420427: 
 	std: 0.003092316
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9806, 0.9737, 0.9811, 0.9756]
	train_accs: [0.98148334, 0.9809667, 0.97573334, 0.98165, 0.97655]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97836
	best: 0.9811

Starting e_i: 1560
Model ind 665 epoch 1560 batch: 0 avg loss -2.968767 avg loss no lamb -2.968767 time 2020-06-27 13:19:43.746657
Model ind 665 epoch 1560 batch: 100 avg loss -2.947247 avg loss no lamb -2.947247 time 2020-06-27 13:19:54.463583
Model ind 665 epoch 1560 batch: 200 avg loss -2.900891 avg loss no lamb -2.900891 time 2020-06-27 13:20:05.445277
Model ind 665 epoch 1560 batch: 300 avg loss -2.881621 avg loss no lamb -2.881621 time 2020-06-27 13:20:16.391942
Model ind 665 epoch 1560 batch: 400 avg loss -2.789632 avg loss no lamb -2.789632 time 2020-06-27 13:20:27.329559
Model ind 665 epoch 1560 batch: 500 avg loss -2.845858 avg loss no lamb -2.845858 time 2020-06-27 13:20:38.354086
Model ind 665 epoch 1560 batch: 600 avg loss -2.894814 avg loss no lamb -2.894814 time 2020-06-27 13:20:49.267752
Model ind 665 epoch 1560 batch: 700 avg loss -2.791688 avg loss no lamb -2.791688 time 2020-06-27 13:21:00.086451
Model ind 665 epoch 1560 batch: 800 avg loss -2.860249 avg loss no lamb -2.860249 time 2020-06-27 13:21:11.077248
last batch sz 10
Pre: time 2020-06-27 13:21:25.585038: 
 	std: 0.0035170384
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9784, 0.9724, 0.9802, 0.9727]
	train_accs: [0.9806167, 0.9795, 0.97415, 0.98065, 0.97515]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.97678006
	best: 0.9802

Starting e_i: 1561
Model ind 665 epoch 1561 batch: 0 avg loss -2.978927 avg loss no lamb -2.978927 time 2020-06-27 13:21:28.106241
Model ind 665 epoch 1561 batch: 100 avg loss -2.898808 avg loss no lamb -2.898808 time 2020-06-27 13:21:39.011539
Model ind 665 epoch 1561 batch: 200 avg loss -2.882910 avg loss no lamb -2.882910 time 2020-06-27 13:21:49.983863
Model ind 665 epoch 1561 batch: 300 avg loss -2.849834 avg loss no lamb -2.849834 time 2020-06-27 13:22:00.966607
Model ind 665 epoch 1561 batch: 400 avg loss -2.824880 avg loss no lamb -2.824880 time 2020-06-27 13:22:11.867528
Model ind 665 epoch 1561 batch: 500 avg loss -2.822730 avg loss no lamb -2.822730 time 2020-06-27 13:22:22.863016
Model ind 665 epoch 1561 batch: 600 avg loss -2.886021 avg loss no lamb -2.886021 time 2020-06-27 13:22:33.831471
Model ind 665 epoch 1561 batch: 700 avg loss -2.772167 avg loss no lamb -2.772167 time 2020-06-27 13:22:44.581831
Model ind 665 epoch 1561 batch: 800 avg loss -2.895725 avg loss no lamb -2.895725 time 2020-06-27 13:22:55.280249
last batch sz 10
Pre: time 2020-06-27 13:23:09.394349: 
 	std: 0.0033870419
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9805, 0.9737, 0.9814, 0.9749]
	train_accs: [0.98106664, 0.98043334, 0.97568333, 0.98113334, 0.97643334]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97840005
	best: 0.9814

Starting e_i: 1562
Model ind 665 epoch 1562 batch: 0 avg loss -2.950020 avg loss no lamb -2.950020 time 2020-06-27 13:23:10.678552
Model ind 665 epoch 1562 batch: 100 avg loss -2.926238 avg loss no lamb -2.926238 time 2020-06-27 13:23:21.519580
Model ind 665 epoch 1562 batch: 200 avg loss -2.928779 avg loss no lamb -2.928779 time 2020-06-27 13:23:32.735949
Model ind 665 epoch 1562 batch: 300 avg loss -2.876221 avg loss no lamb -2.876221 time 2020-06-27 13:23:43.714477
Model ind 665 epoch 1562 batch: 400 avg loss -2.803257 avg loss no lamb -2.803257 time 2020-06-27 13:23:54.661663
Model ind 665 epoch 1562 batch: 500 avg loss -2.862434 avg loss no lamb -2.862434 time 2020-06-27 13:24:05.622380
Model ind 665 epoch 1562 batch: 600 avg loss -2.901820 avg loss no lamb -2.901820 time 2020-06-27 13:24:16.463860
Model ind 665 epoch 1562 batch: 700 avg loss -2.783067 avg loss no lamb -2.783067 time 2020-06-27 13:24:27.119986
Model ind 665 epoch 1562 batch: 800 avg loss -2.861405 avg loss no lamb -2.861405 time 2020-06-27 13:24:38.189469
last batch sz 10
Pre: time 2020-06-27 13:24:52.560028: 
 	std: 0.0030863436
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9784, 0.9728, 0.9805, 0.9749]
	train_accs: [0.9816667, 0.9804, 0.97545, 0.9817167, 0.9769667]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97742003
	best: 0.9805

Starting e_i: 1563
Model ind 665 epoch 1563 batch: 0 avg loss -2.951700 avg loss no lamb -2.951700 time 2020-06-27 13:24:53.881695
Model ind 665 epoch 1563 batch: 100 avg loss -2.933030 avg loss no lamb -2.933030 time 2020-06-27 13:25:04.795641
Model ind 665 epoch 1563 batch: 200 avg loss -2.886994 avg loss no lamb -2.886994 time 2020-06-27 13:25:15.774977
Model ind 665 epoch 1563 batch: 300 avg loss -2.881559 avg loss no lamb -2.881559 time 2020-06-27 13:25:26.582571
Model ind 665 epoch 1563 batch: 400 avg loss -2.852707 avg loss no lamb -2.852707 time 2020-06-27 13:25:37.473867
Model ind 665 epoch 1563 batch: 500 avg loss -2.903618 avg loss no lamb -2.903618 time 2020-06-27 13:25:48.343198
Model ind 665 epoch 1563 batch: 600 avg loss -2.903059 avg loss no lamb -2.903059 time 2020-06-27 13:25:59.066104
Model ind 665 epoch 1563 batch: 700 avg loss -2.806750 avg loss no lamb -2.806750 time 2020-06-27 13:26:09.876148
Model ind 665 epoch 1563 batch: 800 avg loss -2.821098 avg loss no lamb -2.821098 time 2020-06-27 13:26:20.667207
last batch sz 10
Pre: time 2020-06-27 13:26:34.746175: 
 	std: 0.0033266277
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9801, 0.9729, 0.98, 0.9741]
	train_accs: [0.98151666, 0.98073334, 0.9752, 0.98151666, 0.9763167]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97754
	best: 0.9806

Starting e_i: 1564
Model ind 665 epoch 1564 batch: 0 avg loss -3.001945 avg loss no lamb -3.001945 time 2020-06-27 13:26:36.077179
Model ind 665 epoch 1564 batch: 100 avg loss -2.916963 avg loss no lamb -2.916963 time 2020-06-27 13:26:46.967960
Model ind 665 epoch 1564 batch: 200 avg loss -2.881509 avg loss no lamb -2.881509 time 2020-06-27 13:26:57.735305
Model ind 665 epoch 1564 batch: 300 avg loss -2.799326 avg loss no lamb -2.799326 time 2020-06-27 13:27:08.674099
Model ind 665 epoch 1564 batch: 400 avg loss -2.844136 avg loss no lamb -2.844136 time 2020-06-27 13:27:19.476987
Model ind 665 epoch 1564 batch: 500 avg loss -2.817359 avg loss no lamb -2.817359 time 2020-06-27 13:27:30.202695
Model ind 665 epoch 1564 batch: 600 avg loss -2.866048 avg loss no lamb -2.866048 time 2020-06-27 13:27:41.070904
Model ind 665 epoch 1564 batch: 700 avg loss -2.778049 avg loss no lamb -2.778049 time 2020-06-27 13:27:51.968456
Model ind 665 epoch 1564 batch: 800 avg loss -2.839524 avg loss no lamb -2.839524 time 2020-06-27 13:28:02.661990
last batch sz 10
Pre: time 2020-06-27 13:28:16.791570: 
 	std: 0.0027881125
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.981, 0.9753, 0.9816, 0.9763]
	train_accs: [0.9816167, 0.9806833, 0.976, 0.98155, 0.97693336]
	best_train_sub_head: 0
	worst: 0.9753
	avg: 0.97918
	best: 0.9817

Starting e_i: 1565
Model ind 665 epoch 1565 batch: 0 avg loss -2.972189 avg loss no lamb -2.972189 time 2020-06-27 13:28:18.279997
Model ind 665 epoch 1565 batch: 100 avg loss -2.871192 avg loss no lamb -2.871192 time 2020-06-27 13:28:28.982292
Model ind 665 epoch 1565 batch: 200 avg loss -2.903219 avg loss no lamb -2.903219 time 2020-06-27 13:28:39.725099
Model ind 665 epoch 1565 batch: 300 avg loss -2.865985 avg loss no lamb -2.865985 time 2020-06-27 13:28:50.622035
Model ind 665 epoch 1565 batch: 400 avg loss -2.775522 avg loss no lamb -2.775522 time 2020-06-27 13:29:01.768945
Model ind 665 epoch 1565 batch: 500 avg loss -2.878727 avg loss no lamb -2.878727 time 2020-06-27 13:29:12.872428
Model ind 665 epoch 1565 batch: 600 avg loss -2.870850 avg loss no lamb -2.870850 time 2020-06-27 13:29:23.854197
Model ind 665 epoch 1565 batch: 700 avg loss -2.748755 avg loss no lamb -2.748755 time 2020-06-27 13:29:34.741577
Model ind 665 epoch 1565 batch: 800 avg loss -2.887108 avg loss no lamb -2.887108 time 2020-06-27 13:29:45.596897
last batch sz 10
Pre: time 2020-06-27 13:29:59.683613: 
 	std: 0.0032578632
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9782, 0.9719, 0.9793, 0.973]
	train_accs: [0.98075, 0.98001665, 0.9740833, 0.98065, 0.9751833]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97638
	best: 0.9795

Starting e_i: 1566
Model ind 665 epoch 1566 batch: 0 avg loss -2.933249 avg loss no lamb -2.933249 time 2020-06-27 13:30:01.049792
Model ind 665 epoch 1566 batch: 100 avg loss -2.872549 avg loss no lamb -2.872549 time 2020-06-27 13:30:11.701987
Model ind 665 epoch 1566 batch: 200 avg loss -2.849776 avg loss no lamb -2.849776 time 2020-06-27 13:30:22.604189
Model ind 665 epoch 1566 batch: 300 avg loss -2.905722 avg loss no lamb -2.905722 time 2020-06-27 13:30:33.517907
Model ind 665 epoch 1566 batch: 400 avg loss -2.788660 avg loss no lamb -2.788660 time 2020-06-27 13:30:44.518096
Model ind 665 epoch 1566 batch: 500 avg loss -2.868273 avg loss no lamb -2.868273 time 2020-06-27 13:30:55.470718
Model ind 665 epoch 1566 batch: 600 avg loss -2.884825 avg loss no lamb -2.884825 time 2020-06-27 13:31:06.389321
Model ind 665 epoch 1566 batch: 700 avg loss -2.820737 avg loss no lamb -2.820737 time 2020-06-27 13:31:17.083008
Model ind 665 epoch 1566 batch: 800 avg loss -2.880744 avg loss no lamb -2.880744 time 2020-06-27 13:31:27.939253
last batch sz 10
Pre: time 2020-06-27 13:31:42.023831: 
 	std: 0.0023728397
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9789, 0.9742, 0.9805, 0.9765]
	train_accs: [0.9810333, 0.98003334, 0.97536665, 0.9811, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.9780399
	best: 0.9805

Starting e_i: 1567
Model ind 665 epoch 1567 batch: 0 avg loss -2.980683 avg loss no lamb -2.980683 time 2020-06-27 13:31:43.549622
Model ind 665 epoch 1567 batch: 100 avg loss -2.916828 avg loss no lamb -2.916828 time 2020-06-27 13:31:54.409271
Model ind 665 epoch 1567 batch: 200 avg loss -2.894722 avg loss no lamb -2.894722 time 2020-06-27 13:32:05.221397
Model ind 665 epoch 1567 batch: 300 avg loss -2.897816 avg loss no lamb -2.897816 time 2020-06-27 13:32:16.134549
Model ind 665 epoch 1567 batch: 400 avg loss -2.758873 avg loss no lamb -2.758873 time 2020-06-27 13:32:27.093693
Model ind 665 epoch 1567 batch: 500 avg loss -2.837864 avg loss no lamb -2.837864 time 2020-06-27 13:32:38.274097
Model ind 665 epoch 1567 batch: 600 avg loss -2.900006 avg loss no lamb -2.900006 time 2020-06-27 13:32:49.151623
Model ind 665 epoch 1567 batch: 700 avg loss -2.829120 avg loss no lamb -2.829120 time 2020-06-27 13:32:59.955266
Model ind 665 epoch 1567 batch: 800 avg loss -2.872978 avg loss no lamb -2.872978 time 2020-06-27 13:33:10.789683
last batch sz 10
Pre: time 2020-06-27 13:33:24.935320: 
 	std: 0.0035332649
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9783, 0.9713, 0.9794, 0.9728]
	train_accs: [0.9809667, 0.98005, 0.9744833, 0.98125, 0.9756167]
	best_train_sub_head: 3
	worst: 0.9713
	avg: 0.97629994
	best: 0.9794

Starting e_i: 1568
Model ind 665 epoch 1568 batch: 0 avg loss -2.961970 avg loss no lamb -2.961970 time 2020-06-27 13:33:26.198061
Model ind 665 epoch 1568 batch: 100 avg loss -2.870344 avg loss no lamb -2.870344 time 2020-06-27 13:33:36.981907
Model ind 665 epoch 1568 batch: 200 avg loss -2.871844 avg loss no lamb -2.871844 time 2020-06-27 13:33:47.629542
Model ind 665 epoch 1568 batch: 300 avg loss -2.922458 avg loss no lamb -2.922458 time 2020-06-27 13:33:58.193624
Model ind 665 epoch 1568 batch: 400 avg loss -2.769589 avg loss no lamb -2.769589 time 2020-06-27 13:34:09.131879
Model ind 665 epoch 1568 batch: 500 avg loss -2.881598 avg loss no lamb -2.881598 time 2020-06-27 13:34:19.951932
Model ind 665 epoch 1568 batch: 600 avg loss -2.874134 avg loss no lamb -2.874134 time 2020-06-27 13:34:30.716474
Model ind 665 epoch 1568 batch: 700 avg loss -2.849422 avg loss no lamb -2.849422 time 2020-06-27 13:34:41.538223
Model ind 665 epoch 1568 batch: 800 avg loss -2.912726 avg loss no lamb -2.912726 time 2020-06-27 13:34:52.518851
last batch sz 10
Pre: time 2020-06-27 13:35:06.643021: 
 	std: 0.0026911828
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9793, 0.9748, 0.9811, 0.9758]
	train_accs: [0.98116666, 0.98031664, 0.97545, 0.98125, 0.9769]
	best_train_sub_head: 3
	worst: 0.9748
	avg: 0.97846
	best: 0.9811

Starting e_i: 1569
Model ind 665 epoch 1569 batch: 0 avg loss -2.976218 avg loss no lamb -2.976218 time 2020-06-27 13:35:07.974103
Model ind 665 epoch 1569 batch: 100 avg loss -2.920606 avg loss no lamb -2.920606 time 2020-06-27 13:35:18.689483
Model ind 665 epoch 1569 batch: 200 avg loss -2.838513 avg loss no lamb -2.838513 time 2020-06-27 13:35:29.626045
Model ind 665 epoch 1569 batch: 300 avg loss -2.846400 avg loss no lamb -2.846400 time 2020-06-27 13:35:40.392462
Model ind 665 epoch 1569 batch: 400 avg loss -2.857262 avg loss no lamb -2.857262 time 2020-06-27 13:35:51.160013
Model ind 665 epoch 1569 batch: 500 avg loss -2.840661 avg loss no lamb -2.840661 time 2020-06-27 13:36:02.036345
Model ind 665 epoch 1569 batch: 600 avg loss -2.863639 avg loss no lamb -2.863639 time 2020-06-27 13:36:12.879791
Model ind 665 epoch 1569 batch: 700 avg loss -2.744982 avg loss no lamb -2.744982 time 2020-06-27 13:36:23.635996
Model ind 665 epoch 1569 batch: 800 avg loss -2.909843 avg loss no lamb -2.909843 time 2020-06-27 13:36:34.431398
last batch sz 10
Pre: time 2020-06-27 13:36:48.917042: 
 	std: 0.0034389654
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9818, 0.9794, 0.9737, 0.9817, 0.9747]
	train_accs: [0.9816, 0.98031664, 0.97565, 0.98148334, 0.97653335]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97826004
	best: 0.9818

Starting e_i: 1570
Model ind 665 epoch 1570 batch: 0 avg loss -2.961737 avg loss no lamb -2.961737 time 2020-06-27 13:36:50.214715
Model ind 665 epoch 1570 batch: 100 avg loss -2.921114 avg loss no lamb -2.921114 time 2020-06-27 13:37:00.985443
Model ind 665 epoch 1570 batch: 200 avg loss -2.910503 avg loss no lamb -2.910503 time 2020-06-27 13:37:11.680402
Model ind 665 epoch 1570 batch: 300 avg loss -2.838372 avg loss no lamb -2.838372 time 2020-06-27 13:37:22.434530
Model ind 665 epoch 1570 batch: 400 avg loss -2.804653 avg loss no lamb -2.804653 time 2020-06-27 13:37:33.283039
Model ind 665 epoch 1570 batch: 500 avg loss -2.823005 avg loss no lamb -2.823005 time 2020-06-27 13:37:44.241204
Model ind 665 epoch 1570 batch: 600 avg loss -2.910145 avg loss no lamb -2.910145 time 2020-06-27 13:37:54.945426
Model ind 665 epoch 1570 batch: 700 avg loss -2.825300 avg loss no lamb -2.825300 time 2020-06-27 13:38:05.740734
Model ind 665 epoch 1570 batch: 800 avg loss -2.896695 avg loss no lamb -2.896695 time 2020-06-27 13:38:16.242409
last batch sz 10
Pre: time 2020-06-27 13:38:30.110019: 
 	std: 0.002974153
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9799, 0.9738, 0.9806, 0.9752]
	train_accs: [0.9813167, 0.98055, 0.97546667, 0.9812, 0.97658336]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97808
	best: 0.9809

Starting e_i: 1571
Model ind 665 epoch 1571 batch: 0 avg loss -2.980687 avg loss no lamb -2.980687 time 2020-06-27 13:38:32.540356
Model ind 665 epoch 1571 batch: 100 avg loss -2.870174 avg loss no lamb -2.870174 time 2020-06-27 13:38:43.482288
Model ind 665 epoch 1571 batch: 200 avg loss -2.877156 avg loss no lamb -2.877156 time 2020-06-27 13:38:54.145137
Model ind 665 epoch 1571 batch: 300 avg loss -2.849432 avg loss no lamb -2.849432 time 2020-06-27 13:39:05.065838
Model ind 665 epoch 1571 batch: 400 avg loss -2.792860 avg loss no lamb -2.792860 time 2020-06-27 13:39:15.846763
Model ind 665 epoch 1571 batch: 500 avg loss -2.871228 avg loss no lamb -2.871228 time 2020-06-27 13:39:26.762168
Model ind 665 epoch 1571 batch: 600 avg loss -2.876623 avg loss no lamb -2.876623 time 2020-06-27 13:39:37.316694
Model ind 665 epoch 1571 batch: 700 avg loss -2.771647 avg loss no lamb -2.771647 time 2020-06-27 13:39:48.101437
Model ind 665 epoch 1571 batch: 800 avg loss -2.884744 avg loss no lamb -2.884744 time 2020-06-27 13:39:59.072369
last batch sz 10
Pre: time 2020-06-27 13:40:13.401264: 
 	std: 0.0033380142
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9798, 0.9735, 0.9807, 0.9739]
	train_accs: [0.9813, 0.9802833, 0.97545, 0.98143333, 0.97573334]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97776
	best: 0.9807

Starting e_i: 1572
Model ind 665 epoch 1572 batch: 0 avg loss -2.939734 avg loss no lamb -2.939734 time 2020-06-27 13:40:14.685044
Model ind 665 epoch 1572 batch: 100 avg loss -2.945184 avg loss no lamb -2.945184 time 2020-06-27 13:40:25.227196
Model ind 665 epoch 1572 batch: 200 avg loss -2.891782 avg loss no lamb -2.891782 time 2020-06-27 13:40:36.039729
Model ind 665 epoch 1572 batch: 300 avg loss -2.922161 avg loss no lamb -2.922161 time 2020-06-27 13:40:46.636030
Model ind 665 epoch 1572 batch: 400 avg loss -2.808184 avg loss no lamb -2.808184 time 2020-06-27 13:40:57.560366
Model ind 665 epoch 1572 batch: 500 avg loss -2.858595 avg loss no lamb -2.858595 time 2020-06-27 13:41:08.367415
Model ind 665 epoch 1572 batch: 600 avg loss -2.908052 avg loss no lamb -2.908052 time 2020-06-27 13:41:19.036348
Model ind 665 epoch 1572 batch: 700 avg loss -2.788770 avg loss no lamb -2.788770 time 2020-06-27 13:41:30.003046
Model ind 665 epoch 1572 batch: 800 avg loss -2.847698 avg loss no lamb -2.847698 time 2020-06-27 13:41:40.862858
last batch sz 10
Pre: time 2020-06-27 13:41:54.800012: 
 	std: 0.0025819272
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9796, 0.975, 0.9811, 0.9757]
	train_accs: [0.9813833, 0.98066664, 0.9758, 0.98153335, 0.9766667]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.97844
	best: 0.9811

Starting e_i: 1573
Model ind 665 epoch 1573 batch: 0 avg loss -2.958200 avg loss no lamb -2.958200 time 2020-06-27 13:41:56.113947
Model ind 665 epoch 1573 batch: 100 avg loss -2.900010 avg loss no lamb -2.900010 time 2020-06-27 13:42:06.904955
Model ind 665 epoch 1573 batch: 200 avg loss -2.925308 avg loss no lamb -2.925308 time 2020-06-27 13:42:17.790491
Model ind 665 epoch 1573 batch: 300 avg loss -2.888594 avg loss no lamb -2.888594 time 2020-06-27 13:42:28.664239
Model ind 665 epoch 1573 batch: 400 avg loss -2.852073 avg loss no lamb -2.852073 time 2020-06-27 13:42:39.445245
Model ind 665 epoch 1573 batch: 500 avg loss -2.838781 avg loss no lamb -2.838781 time 2020-06-27 13:42:50.285070
Model ind 665 epoch 1573 batch: 600 avg loss -2.871080 avg loss no lamb -2.871080 time 2020-06-27 13:43:01.024673
Model ind 665 epoch 1573 batch: 700 avg loss -2.852832 avg loss no lamb -2.852832 time 2020-06-27 13:43:11.776523
Model ind 665 epoch 1573 batch: 800 avg loss -2.915097 avg loss no lamb -2.915097 time 2020-06-27 13:43:22.566212
last batch sz 10
Pre: time 2020-06-27 13:43:36.916870: 
 	std: 0.0027330508
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9808, 0.9755, 0.9816, 0.9756]
	train_accs: [0.98115, 0.98083335, 0.97581667, 0.98165, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9755
	avg: 0.97888005
	best: 0.9816

Starting e_i: 1574
Model ind 665 epoch 1574 batch: 0 avg loss -2.967428 avg loss no lamb -2.967428 time 2020-06-27 13:43:38.408057
Model ind 665 epoch 1574 batch: 100 avg loss -2.934840 avg loss no lamb -2.934840 time 2020-06-27 13:43:49.643381
Model ind 665 epoch 1574 batch: 200 avg loss -2.859557 avg loss no lamb -2.859557 time 2020-06-27 13:44:00.641412
Model ind 665 epoch 1574 batch: 300 avg loss -2.921911 avg loss no lamb -2.921911 time 2020-06-27 13:44:11.599178
Model ind 665 epoch 1574 batch: 400 avg loss -2.831600 avg loss no lamb -2.831600 time 2020-06-27 13:44:22.342394
Model ind 665 epoch 1574 batch: 500 avg loss -2.841291 avg loss no lamb -2.841291 time 2020-06-27 13:44:33.279689
Model ind 665 epoch 1574 batch: 600 avg loss -2.938128 avg loss no lamb -2.938128 time 2020-06-27 13:44:44.094598
Model ind 665 epoch 1574 batch: 700 avg loss -2.814656 avg loss no lamb -2.814656 time 2020-06-27 13:44:55.290675
Model ind 665 epoch 1574 batch: 800 avg loss -2.860205 avg loss no lamb -2.860205 time 2020-06-27 13:45:06.398843
last batch sz 10
Pre: time 2020-06-27 13:45:20.674465: 
 	std: 0.0030380264
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9789, 0.973, 0.9796, 0.9739]
	train_accs: [0.9810167, 0.9804, 0.97498333, 0.98088336, 0.9759833]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97712004
	best: 0.9802

Starting e_i: 1575
Model ind 665 epoch 1575 batch: 0 avg loss -2.982948 avg loss no lamb -2.982948 time 2020-06-27 13:45:22.021779
Model ind 665 epoch 1575 batch: 100 avg loss -2.950184 avg loss no lamb -2.950184 time 2020-06-27 13:45:32.880402
Model ind 665 epoch 1575 batch: 200 avg loss -2.877729 avg loss no lamb -2.877729 time 2020-06-27 13:45:43.705375
Model ind 665 epoch 1575 batch: 300 avg loss -2.886156 avg loss no lamb -2.886156 time 2020-06-27 13:45:54.573230
Model ind 665 epoch 1575 batch: 400 avg loss -2.818475 avg loss no lamb -2.818475 time 2020-06-27 13:46:05.558866
Model ind 665 epoch 1575 batch: 500 avg loss -2.855171 avg loss no lamb -2.855171 time 2020-06-27 13:46:16.144722
Model ind 665 epoch 1575 batch: 600 avg loss -2.871555 avg loss no lamb -2.871555 time 2020-06-27 13:46:26.808447
Model ind 665 epoch 1575 batch: 700 avg loss -2.831023 avg loss no lamb -2.831023 time 2020-06-27 13:46:37.651290
Model ind 665 epoch 1575 batch: 800 avg loss -2.871402 avg loss no lamb -2.871402 time 2020-06-27 13:46:48.744808
last batch sz 10
Pre: time 2020-06-27 13:47:02.684800: 
 	std: 0.0027412425
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9796, 0.9741, 0.9807, 0.975]
	train_accs: [0.98135, 0.9806, 0.97555, 0.98153335, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97786
	best: 0.9807

Starting e_i: 1576
Model ind 665 epoch 1576 batch: 0 avg loss -2.948420 avg loss no lamb -2.948420 time 2020-06-27 13:47:04.200903
Model ind 665 epoch 1576 batch: 100 avg loss -2.916763 avg loss no lamb -2.916763 time 2020-06-27 13:47:15.000262
Model ind 665 epoch 1576 batch: 200 avg loss -2.885706 avg loss no lamb -2.885706 time 2020-06-27 13:47:25.806975
Model ind 665 epoch 1576 batch: 300 avg loss -2.877952 avg loss no lamb -2.877952 time 2020-06-27 13:47:36.384743
Model ind 665 epoch 1576 batch: 400 avg loss -2.806201 avg loss no lamb -2.806201 time 2020-06-27 13:47:47.123650
Model ind 665 epoch 1576 batch: 500 avg loss -2.853376 avg loss no lamb -2.853376 time 2020-06-27 13:47:57.890509
Model ind 665 epoch 1576 batch: 600 avg loss -2.905826 avg loss no lamb -2.905826 time 2020-06-27 13:48:08.751287
Model ind 665 epoch 1576 batch: 700 avg loss -2.850235 avg loss no lamb -2.850235 time 2020-06-27 13:48:19.452050
Model ind 665 epoch 1576 batch: 800 avg loss -2.848044 avg loss no lamb -2.848044 time 2020-06-27 13:48:30.490889
last batch sz 10
Pre: time 2020-06-27 13:48:44.643657: 
 	std: 0.0034672297
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9799, 0.973, 0.9811, 0.9744]
	train_accs: [0.98156667, 0.98055, 0.9753, 0.98156667, 0.9765]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97788
	best: 0.981

Starting e_i: 1577
Model ind 665 epoch 1577 batch: 0 avg loss -2.923097 avg loss no lamb -2.923097 time 2020-06-27 13:48:45.978080
Model ind 665 epoch 1577 batch: 100 avg loss -2.892906 avg loss no lamb -2.892906 time 2020-06-27 13:48:56.684138
Model ind 665 epoch 1577 batch: 200 avg loss -2.923752 avg loss no lamb -2.923752 time 2020-06-27 13:49:07.629192
Model ind 665 epoch 1577 batch: 300 avg loss -2.878044 avg loss no lamb -2.878044 time 2020-06-27 13:49:18.359561
Model ind 665 epoch 1577 batch: 400 avg loss -2.838960 avg loss no lamb -2.838960 time 2020-06-27 13:49:29.433781
Model ind 665 epoch 1577 batch: 500 avg loss -2.844489 avg loss no lamb -2.844489 time 2020-06-27 13:49:40.350341
Model ind 665 epoch 1577 batch: 600 avg loss -2.891763 avg loss no lamb -2.891763 time 2020-06-27 13:49:51.163611
Model ind 665 epoch 1577 batch: 700 avg loss -2.814029 avg loss no lamb -2.814029 time 2020-06-27 13:50:02.143508
Model ind 665 epoch 1577 batch: 800 avg loss -2.894285 avg loss no lamb -2.894285 time 2020-06-27 13:50:12.866629
last batch sz 10
Pre: time 2020-06-27 13:50:26.847589: 
 	std: 0.0038455117
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9791, 0.9716, 0.9797, 0.9718]
	train_accs: [0.98115, 0.98036665, 0.97438335, 0.9812667, 0.97473335]
	best_train_sub_head: 3
	worst: 0.9716
	avg: 0.9764
	best: 0.9797

Starting e_i: 1578
Model ind 665 epoch 1578 batch: 0 avg loss -2.961928 avg loss no lamb -2.961928 time 2020-06-27 13:50:28.150124
Model ind 665 epoch 1578 batch: 100 avg loss -2.972590 avg loss no lamb -2.972590 time 2020-06-27 13:50:38.871590
Model ind 665 epoch 1578 batch: 200 avg loss -2.864915 avg loss no lamb -2.864915 time 2020-06-27 13:50:49.788803
Model ind 665 epoch 1578 batch: 300 avg loss -2.900627 avg loss no lamb -2.900627 time 2020-06-27 13:51:00.874636
Model ind 665 epoch 1578 batch: 400 avg loss -2.798487 avg loss no lamb -2.798487 time 2020-06-27 13:51:11.689857
Model ind 665 epoch 1578 batch: 500 avg loss -2.877557 avg loss no lamb -2.877557 time 2020-06-27 13:51:22.589944
Model ind 665 epoch 1578 batch: 600 avg loss -2.907686 avg loss no lamb -2.907686 time 2020-06-27 13:51:33.283511
Model ind 665 epoch 1578 batch: 700 avg loss -2.777748 avg loss no lamb -2.777748 time 2020-06-27 13:51:44.069280
Model ind 665 epoch 1578 batch: 800 avg loss -2.832505 avg loss no lamb -2.832505 time 2020-06-27 13:51:54.856731
last batch sz 10
Pre: time 2020-06-27 13:52:09.145906: 
 	std: 0.0030419715
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9798, 0.9739, 0.9811, 0.9754]
	train_accs: [0.98121667, 0.9809, 0.97568333, 0.98118335, 0.9764]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97828007
	best: 0.9812

Starting e_i: 1579
Model ind 665 epoch 1579 batch: 0 avg loss -2.942273 avg loss no lamb -2.942273 time 2020-06-27 13:52:10.428071
Model ind 665 epoch 1579 batch: 100 avg loss -2.920301 avg loss no lamb -2.920301 time 2020-06-27 13:52:21.238446
Model ind 665 epoch 1579 batch: 200 avg loss -2.956133 avg loss no lamb -2.956133 time 2020-06-27 13:52:32.098896
Model ind 665 epoch 1579 batch: 300 avg loss -2.887354 avg loss no lamb -2.887354 time 2020-06-27 13:52:42.903159
Model ind 665 epoch 1579 batch: 400 avg loss -2.832333 avg loss no lamb -2.832333 time 2020-06-27 13:52:53.921261
Model ind 665 epoch 1579 batch: 500 avg loss -2.892104 avg loss no lamb -2.892104 time 2020-06-27 13:53:04.596306
Model ind 665 epoch 1579 batch: 600 avg loss -2.860096 avg loss no lamb -2.860096 time 2020-06-27 13:53:15.366557
Model ind 665 epoch 1579 batch: 700 avg loss -2.873218 avg loss no lamb -2.873218 time 2020-06-27 13:53:26.191900
Model ind 665 epoch 1579 batch: 800 avg loss -2.926065 avg loss no lamb -2.926065 time 2020-06-27 13:53:36.870989
last batch sz 10
Pre: time 2020-06-27 13:53:50.848640: 
 	std: 0.0034029223
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9819, 0.9802, 0.9735, 0.9826, 0.9768]
	train_accs: [0.98156667, 0.9809667, 0.97571665, 0.98181665, 0.97723335]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.979
	best: 0.9826

Starting e_i: 1580
Model ind 665 epoch 1580 batch: 0 avg loss -2.968691 avg loss no lamb -2.968691 time 2020-06-27 13:53:52.112918
Model ind 665 epoch 1580 batch: 100 avg loss -2.880031 avg loss no lamb -2.880031 time 2020-06-27 13:54:02.791886
Model ind 665 epoch 1580 batch: 200 avg loss -2.896304 avg loss no lamb -2.896304 time 2020-06-27 13:54:13.643183
Model ind 665 epoch 1580 batch: 300 avg loss -2.867870 avg loss no lamb -2.867870 time 2020-06-27 13:54:24.548281
Model ind 665 epoch 1580 batch: 400 avg loss -2.839370 avg loss no lamb -2.839370 time 2020-06-27 13:54:35.363001
Model ind 665 epoch 1580 batch: 500 avg loss -2.846122 avg loss no lamb -2.846122 time 2020-06-27 13:54:46.041996
Model ind 665 epoch 1580 batch: 600 avg loss -2.883094 avg loss no lamb -2.883094 time 2020-06-27 13:54:56.745399
Model ind 665 epoch 1580 batch: 700 avg loss -2.842164 avg loss no lamb -2.842164 time 2020-06-27 13:55:07.635004
Model ind 665 epoch 1580 batch: 800 avg loss -2.906629 avg loss no lamb -2.906629 time 2020-06-27 13:55:18.546695
last batch sz 10
Pre: time 2020-06-27 13:55:32.986722: 
 	std: 0.0033163899
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9785, 0.9729, 0.9804, 0.9735]
	train_accs: [0.98116666, 0.9802333, 0.9745833, 0.9813, 0.9756]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.9771601
	best: 0.9804

Starting e_i: 1581
Model ind 665 epoch 1581 batch: 0 avg loss -2.947847 avg loss no lamb -2.947847 time 2020-06-27 13:55:35.517372
Model ind 665 epoch 1581 batch: 100 avg loss -2.870020 avg loss no lamb -2.870020 time 2020-06-27 13:55:46.276754
Model ind 665 epoch 1581 batch: 200 avg loss -2.911062 avg loss no lamb -2.911062 time 2020-06-27 13:55:57.125460
Model ind 665 epoch 1581 batch: 300 avg loss -2.887707 avg loss no lamb -2.887707 time 2020-06-27 13:56:08.059230
Model ind 665 epoch 1581 batch: 400 avg loss -2.801867 avg loss no lamb -2.801867 time 2020-06-27 13:56:18.825110
Model ind 665 epoch 1581 batch: 500 avg loss -2.822845 avg loss no lamb -2.822845 time 2020-06-27 13:56:29.766863
Model ind 665 epoch 1581 batch: 600 avg loss -2.920660 avg loss no lamb -2.920660 time 2020-06-27 13:56:40.642055
Model ind 665 epoch 1581 batch: 700 avg loss -2.826216 avg loss no lamb -2.826216 time 2020-06-27 13:56:51.303930
Model ind 665 epoch 1581 batch: 800 avg loss -2.856884 avg loss no lamb -2.856884 time 2020-06-27 13:57:02.144723
last batch sz 10
Pre: time 2020-06-27 13:57:16.159862: 
 	std: 0.0029953334
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9815, 0.9806, 0.975, 0.9813, 0.9751]
	train_accs: [0.98156667, 0.98115, 0.97566664, 0.98148334, 0.97646666]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.9787
	best: 0.9815

Starting e_i: 1582
Model ind 665 epoch 1582 batch: 0 avg loss -2.953639 avg loss no lamb -2.953639 time 2020-06-27 13:57:17.452376
Model ind 665 epoch 1582 batch: 100 avg loss -2.941965 avg loss no lamb -2.941965 time 2020-06-27 13:57:28.134690
Model ind 665 epoch 1582 batch: 200 avg loss -2.905397 avg loss no lamb -2.905397 time 2020-06-27 13:57:39.136046
Model ind 665 epoch 1582 batch: 300 avg loss -2.953762 avg loss no lamb -2.953762 time 2020-06-27 13:57:49.973392
Model ind 665 epoch 1582 batch: 400 avg loss -2.790273 avg loss no lamb -2.790273 time 2020-06-27 13:58:00.971433
Model ind 665 epoch 1582 batch: 500 avg loss -2.868136 avg loss no lamb -2.868136 time 2020-06-27 13:58:11.702361
Model ind 665 epoch 1582 batch: 600 avg loss -2.918401 avg loss no lamb -2.918401 time 2020-06-27 13:58:22.457666
Model ind 665 epoch 1582 batch: 700 avg loss -2.818554 avg loss no lamb -2.818554 time 2020-06-27 13:58:33.259997
Model ind 665 epoch 1582 batch: 800 avg loss -2.880615 avg loss no lamb -2.880615 time 2020-06-27 13:58:44.078121
last batch sz 10
Pre: time 2020-06-27 13:58:58.106736: 
 	std: 0.0031224163
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9804, 0.9743, 0.9808, 0.9743]
	train_accs: [0.9813, 0.9813167, 0.97546667, 0.9815, 0.97625]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97812
	best: 0.9808

Starting e_i: 1583
Model ind 665 epoch 1583 batch: 0 avg loss -2.946382 avg loss no lamb -2.946382 time 2020-06-27 13:58:59.648879
Model ind 665 epoch 1583 batch: 100 avg loss -2.922117 avg loss no lamb -2.922117 time 2020-06-27 13:59:10.590946
Model ind 665 epoch 1583 batch: 200 avg loss -2.854290 avg loss no lamb -2.854290 time 2020-06-27 13:59:21.515830
Model ind 665 epoch 1583 batch: 300 avg loss -2.879542 avg loss no lamb -2.879542 time 2020-06-27 13:59:32.344440
Model ind 665 epoch 1583 batch: 400 avg loss -2.817856 avg loss no lamb -2.817856 time 2020-06-27 13:59:43.338895
Model ind 665 epoch 1583 batch: 500 avg loss -2.851449 avg loss no lamb -2.851449 time 2020-06-27 13:59:54.168736
Model ind 665 epoch 1583 batch: 600 avg loss -2.873564 avg loss no lamb -2.873564 time 2020-06-27 14:00:05.374933
Model ind 665 epoch 1583 batch: 700 avg loss -2.818026 avg loss no lamb -2.818026 time 2020-06-27 14:00:16.057575
Model ind 665 epoch 1583 batch: 800 avg loss -2.906442 avg loss no lamb -2.906442 time 2020-06-27 14:00:27.004614
last batch sz 10
Pre: time 2020-06-27 14:00:41.149498: 
 	std: 0.0030473561
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.982, 0.9812, 0.9753, 0.982, 0.9758]
	train_accs: [0.98186666, 0.98121667, 0.9763, 0.9819, 0.9770833]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97925997
	best: 0.982

Starting e_i: 1584
Model ind 665 epoch 1584 batch: 0 avg loss -2.994221 avg loss no lamb -2.994221 time 2020-06-27 14:00:42.447307
Model ind 665 epoch 1584 batch: 100 avg loss -2.888667 avg loss no lamb -2.888667 time 2020-06-27 14:00:53.357373
Model ind 665 epoch 1584 batch: 200 avg loss -2.890016 avg loss no lamb -2.890016 time 2020-06-27 14:01:04.176222
Model ind 665 epoch 1584 batch: 300 avg loss -2.827115 avg loss no lamb -2.827115 time 2020-06-27 14:01:15.105748
Model ind 665 epoch 1584 batch: 400 avg loss -2.799513 avg loss no lamb -2.799513 time 2020-06-27 14:01:26.044811
Model ind 665 epoch 1584 batch: 500 avg loss -2.843267 avg loss no lamb -2.843267 time 2020-06-27 14:01:36.892183
Model ind 665 epoch 1584 batch: 600 avg loss -2.911604 avg loss no lamb -2.911604 time 2020-06-27 14:01:47.732073
Model ind 665 epoch 1584 batch: 700 avg loss -2.798994 avg loss no lamb -2.798994 time 2020-06-27 14:01:58.573618
Model ind 665 epoch 1584 batch: 800 avg loss -2.888239 avg loss no lamb -2.888239 time 2020-06-27 14:02:09.513705
last batch sz 10
Pre: time 2020-06-27 14:02:23.815696: 
 	std: 0.002649838
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9797, 0.9748, 0.9805, 0.9747]
	train_accs: [0.98111665, 0.98076665, 0.97578335, 0.98145, 0.9762333]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97798
	best: 0.9805

Starting e_i: 1585
Model ind 665 epoch 1585 batch: 0 avg loss -2.965524 avg loss no lamb -2.965524 time 2020-06-27 14:02:25.361126
Model ind 665 epoch 1585 batch: 100 avg loss -2.952260 avg loss no lamb -2.952260 time 2020-06-27 14:02:36.169154
Model ind 665 epoch 1585 batch: 200 avg loss -2.862408 avg loss no lamb -2.862408 time 2020-06-27 14:02:46.978077
Model ind 665 epoch 1585 batch: 300 avg loss -2.917159 avg loss no lamb -2.917159 time 2020-06-27 14:02:57.784389
Model ind 665 epoch 1585 batch: 400 avg loss -2.817013 avg loss no lamb -2.817013 time 2020-06-27 14:03:08.728100
Model ind 665 epoch 1585 batch: 500 avg loss -2.893829 avg loss no lamb -2.893829 time 2020-06-27 14:03:19.680980
Model ind 665 epoch 1585 batch: 600 avg loss -2.920022 avg loss no lamb -2.920022 time 2020-06-27 14:03:30.625302
Model ind 665 epoch 1585 batch: 700 avg loss -2.834965 avg loss no lamb -2.834965 time 2020-06-27 14:03:41.575237
Model ind 665 epoch 1585 batch: 800 avg loss -2.859862 avg loss no lamb -2.859862 time 2020-06-27 14:03:52.737322
last batch sz 10
Pre: time 2020-06-27 14:04:06.981308: 
 	std: 0.003227387
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9816, 0.9816, 0.9748, 0.9814, 0.9751]
	train_accs: [0.98158336, 0.98158336, 0.976, 0.98158336, 0.9763833]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97889996
	best: 0.9816

Starting e_i: 1586
Model ind 665 epoch 1586 batch: 0 avg loss -2.949959 avg loss no lamb -2.949959 time 2020-06-27 14:04:08.279874
Model ind 665 epoch 1586 batch: 100 avg loss -2.929337 avg loss no lamb -2.929337 time 2020-06-27 14:04:19.443837
Model ind 665 epoch 1586 batch: 200 avg loss -2.893588 avg loss no lamb -2.893588 time 2020-06-27 14:04:30.209591
Model ind 665 epoch 1586 batch: 300 avg loss -2.841500 avg loss no lamb -2.841500 time 2020-06-27 14:04:41.158741
Model ind 665 epoch 1586 batch: 400 avg loss -2.784961 avg loss no lamb -2.784961 time 2020-06-27 14:04:52.032655
Model ind 665 epoch 1586 batch: 500 avg loss -2.860896 avg loss no lamb -2.860896 time 2020-06-27 14:05:03.032206
Model ind 665 epoch 1586 batch: 600 avg loss -2.909912 avg loss no lamb -2.909912 time 2020-06-27 14:05:14.010591
Model ind 665 epoch 1586 batch: 700 avg loss -2.775099 avg loss no lamb -2.775099 time 2020-06-27 14:05:25.058548
Model ind 665 epoch 1586 batch: 800 avg loss -2.886450 avg loss no lamb -2.886450 time 2020-06-27 14:05:36.174841
last batch sz 10
Pre: time 2020-06-27 14:05:50.449362: 
 	std: 0.003092826
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9785, 0.9725, 0.9796, 0.9737]
	train_accs: [0.9806833, 0.98013335, 0.9741167, 0.98106664, 0.97546667]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97682
	best: 0.9796

Starting e_i: 1587
Model ind 665 epoch 1587 batch: 0 avg loss -2.947579 avg loss no lamb -2.947579 time 2020-06-27 14:05:51.754675
Model ind 665 epoch 1587 batch: 100 avg loss -2.913744 avg loss no lamb -2.913744 time 2020-06-27 14:06:02.543358
Model ind 665 epoch 1587 batch: 200 avg loss -2.938533 avg loss no lamb -2.938533 time 2020-06-27 14:06:13.268542
Model ind 665 epoch 1587 batch: 300 avg loss -2.853801 avg loss no lamb -2.853801 time 2020-06-27 14:06:24.103033
Model ind 665 epoch 1587 batch: 400 avg loss -2.818591 avg loss no lamb -2.818591 time 2020-06-27 14:06:35.095978
Model ind 665 epoch 1587 batch: 500 avg loss -2.826018 avg loss no lamb -2.826018 time 2020-06-27 14:06:46.332492
Model ind 665 epoch 1587 batch: 600 avg loss -2.914612 avg loss no lamb -2.914612 time 2020-06-27 14:06:57.253247
Model ind 665 epoch 1587 batch: 700 avg loss -2.788650 avg loss no lamb -2.788650 time 2020-06-27 14:07:08.238619
Model ind 665 epoch 1587 batch: 800 avg loss -2.878498 avg loss no lamb -2.878498 time 2020-06-27 14:07:19.157826
last batch sz 10
Pre: time 2020-06-27 14:07:33.529640: 
 	std: 0.0034249758
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9786, 0.9719, 0.9797, 0.9727]
	train_accs: [0.98066664, 0.9802833, 0.9741667, 0.9807, 0.97536665]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97646
	best: 0.9797

Starting e_i: 1588
Model ind 665 epoch 1588 batch: 0 avg loss -2.933096 avg loss no lamb -2.933096 time 2020-06-27 14:07:34.805128
Model ind 665 epoch 1588 batch: 100 avg loss -2.906754 avg loss no lamb -2.906754 time 2020-06-27 14:07:45.761415
Model ind 665 epoch 1588 batch: 200 avg loss -2.887569 avg loss no lamb -2.887569 time 2020-06-27 14:07:56.659400
Model ind 665 epoch 1588 batch: 300 avg loss -2.830874 avg loss no lamb -2.830874 time 2020-06-27 14:08:07.653603
Model ind 665 epoch 1588 batch: 400 avg loss -2.806436 avg loss no lamb -2.806436 time 2020-06-27 14:08:18.587624
Model ind 665 epoch 1588 batch: 500 avg loss -2.831289 avg loss no lamb -2.831289 time 2020-06-27 14:08:29.423702
Model ind 665 epoch 1588 batch: 600 avg loss -2.888068 avg loss no lamb -2.888068 time 2020-06-27 14:08:40.399863
Model ind 665 epoch 1588 batch: 700 avg loss -2.841583 avg loss no lamb -2.841583 time 2020-06-27 14:08:51.393183
Model ind 665 epoch 1588 batch: 800 avg loss -2.868329 avg loss no lamb -2.868329 time 2020-06-27 14:09:02.499297
last batch sz 10
Pre: time 2020-06-27 14:09:16.845139: 
 	std: 0.0037839576
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9812, 0.9727, 0.9802, 0.9734]
	train_accs: [0.98073334, 0.9805167, 0.9739, 0.98038334, 0.9748167]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97766
	best: 0.9808

Starting e_i: 1589
Model ind 665 epoch 1589 batch: 0 avg loss -2.937218 avg loss no lamb -2.937218 time 2020-06-27 14:09:18.148766
Model ind 665 epoch 1589 batch: 100 avg loss -2.890359 avg loss no lamb -2.890359 time 2020-06-27 14:09:29.122328
Model ind 665 epoch 1589 batch: 200 avg loss -2.881534 avg loss no lamb -2.881534 time 2020-06-27 14:09:39.920868
Model ind 665 epoch 1589 batch: 300 avg loss -2.870847 avg loss no lamb -2.870847 time 2020-06-27 14:09:51.084559
Model ind 665 epoch 1589 batch: 400 avg loss -2.827651 avg loss no lamb -2.827651 time 2020-06-27 14:10:02.056061
Model ind 665 epoch 1589 batch: 500 avg loss -2.890002 avg loss no lamb -2.890002 time 2020-06-27 14:10:12.999632
Model ind 665 epoch 1589 batch: 600 avg loss -2.911602 avg loss no lamb -2.911602 time 2020-06-27 14:10:23.948901
Model ind 665 epoch 1589 batch: 700 avg loss -2.830218 avg loss no lamb -2.830218 time 2020-06-27 14:10:34.715700
Model ind 665 epoch 1589 batch: 800 avg loss -2.840039 avg loss no lamb -2.840039 time 2020-06-27 14:10:45.765572
last batch sz 10
Pre: time 2020-06-27 14:11:00.211287: 
 	std: 0.0032290085
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.98, 0.9747, 0.9803, 0.973]
	train_accs: [0.9813167, 0.9805, 0.9752, 0.9812, 0.9753]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97774
	best: 0.9807

Starting e_i: 1590
Model ind 665 epoch 1590 batch: 0 avg loss -2.982451 avg loss no lamb -2.982451 time 2020-06-27 14:11:01.542621
Model ind 665 epoch 1590 batch: 100 avg loss -2.922271 avg loss no lamb -2.922271 time 2020-06-27 14:11:12.443270
Model ind 665 epoch 1590 batch: 200 avg loss -2.866215 avg loss no lamb -2.866215 time 2020-06-27 14:11:23.365255
Model ind 665 epoch 1590 batch: 300 avg loss -2.880737 avg loss no lamb -2.880737 time 2020-06-27 14:11:34.071013
Model ind 665 epoch 1590 batch: 400 avg loss -2.783653 avg loss no lamb -2.783653 time 2020-06-27 14:11:44.897020
Model ind 665 epoch 1590 batch: 500 avg loss -2.862571 avg loss no lamb -2.862571 time 2020-06-27 14:11:55.601142
Model ind 665 epoch 1590 batch: 600 avg loss -2.904013 avg loss no lamb -2.904013 time 2020-06-27 14:12:06.498081
Model ind 665 epoch 1590 batch: 700 avg loss -2.757389 avg loss no lamb -2.757389 time 2020-06-27 14:12:17.422109
Model ind 665 epoch 1590 batch: 800 avg loss -2.921934 avg loss no lamb -2.921934 time 2020-06-27 14:12:28.367289
last batch sz 10
Pre: time 2020-06-27 14:12:42.771404: 
 	std: 0.0030642364
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9788, 0.9727, 0.9801, 0.9746]
	train_accs: [0.9809333, 0.97996664, 0.97515, 0.98111665, 0.97643334]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97727996
	best: 0.9801

Starting e_i: 1591
Model ind 665 epoch 1591 batch: 0 avg loss -2.912747 avg loss no lamb -2.912747 time 2020-06-27 14:12:45.313410
Model ind 665 epoch 1591 batch: 100 avg loss -2.917603 avg loss no lamb -2.917603 time 2020-06-27 14:12:56.263237
Model ind 665 epoch 1591 batch: 200 avg loss -2.930316 avg loss no lamb -2.930316 time 2020-06-27 14:13:07.148242
Model ind 665 epoch 1591 batch: 300 avg loss -2.819976 avg loss no lamb -2.819976 time 2020-06-27 14:13:17.983966
Model ind 665 epoch 1591 batch: 400 avg loss -2.795599 avg loss no lamb -2.795599 time 2020-06-27 14:13:28.807240
Model ind 665 epoch 1591 batch: 500 avg loss -2.909083 avg loss no lamb -2.909083 time 2020-06-27 14:13:39.477284
Model ind 665 epoch 1591 batch: 600 avg loss -2.861062 avg loss no lamb -2.861062 time 2020-06-27 14:13:50.442910
Model ind 665 epoch 1591 batch: 700 avg loss -2.875312 avg loss no lamb -2.875312 time 2020-06-27 14:14:01.293668
Model ind 665 epoch 1591 batch: 800 avg loss -2.850378 avg loss no lamb -2.850378 time 2020-06-27 14:14:12.159167
last batch sz 10
Pre: time 2020-06-27 14:14:26.326226: 
 	std: 0.0035090751
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9802, 0.9734, 0.981, 0.9738]
	train_accs: [0.9816833, 0.98035, 0.97515, 0.9813667, 0.9762167]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97788
	best: 0.981

Starting e_i: 1592
Model ind 665 epoch 1592 batch: 0 avg loss -2.965905 avg loss no lamb -2.965905 time 2020-06-27 14:14:27.636732
Model ind 665 epoch 1592 batch: 100 avg loss -2.862561 avg loss no lamb -2.862561 time 2020-06-27 14:14:38.431635
Model ind 665 epoch 1592 batch: 200 avg loss -2.860962 avg loss no lamb -2.860962 time 2020-06-27 14:14:49.154132
Model ind 665 epoch 1592 batch: 300 avg loss -2.882805 avg loss no lamb -2.882805 time 2020-06-27 14:15:00.052643
Model ind 665 epoch 1592 batch: 400 avg loss -2.784883 avg loss no lamb -2.784883 time 2020-06-27 14:15:11.041636
Model ind 665 epoch 1592 batch: 500 avg loss -2.850041 avg loss no lamb -2.850041 time 2020-06-27 14:15:21.752357
Model ind 665 epoch 1592 batch: 600 avg loss -2.869283 avg loss no lamb -2.869283 time 2020-06-27 14:15:32.510000
Model ind 665 epoch 1592 batch: 700 avg loss -2.801032 avg loss no lamb -2.801032 time 2020-06-27 14:15:43.349202
Model ind 665 epoch 1592 batch: 800 avg loss -2.842004 avg loss no lamb -2.842004 time 2020-06-27 14:15:54.290809
last batch sz 10
Pre: time 2020-06-27 14:16:08.508185: 
 	std: 0.0032939895
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9818, 0.9808, 0.9746, 0.9817, 0.9749]
	train_accs: [0.98155, 0.9806333, 0.9756333, 0.98158336, 0.97571665]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97875994
	best: 0.9817

Starting e_i: 1593
Model ind 665 epoch 1593 batch: 0 avg loss -2.952857 avg loss no lamb -2.952857 time 2020-06-27 14:16:09.791719
Model ind 665 epoch 1593 batch: 100 avg loss -2.920432 avg loss no lamb -2.920432 time 2020-06-27 14:16:20.747789
Model ind 665 epoch 1593 batch: 200 avg loss -2.920278 avg loss no lamb -2.920278 time 2020-06-27 14:16:31.621883
Model ind 665 epoch 1593 batch: 300 avg loss -2.935179 avg loss no lamb -2.935179 time 2020-06-27 14:16:42.590415
Model ind 665 epoch 1593 batch: 400 avg loss -2.820948 avg loss no lamb -2.820948 time 2020-06-27 14:16:53.330435
Model ind 665 epoch 1593 batch: 500 avg loss -2.880882 avg loss no lamb -2.880882 time 2020-06-27 14:17:04.275270
Model ind 665 epoch 1593 batch: 600 avg loss -2.881263 avg loss no lamb -2.881263 time 2020-06-27 14:17:15.204018
Model ind 665 epoch 1593 batch: 700 avg loss -2.820207 avg loss no lamb -2.820207 time 2020-06-27 14:17:26.042147
Model ind 665 epoch 1593 batch: 800 avg loss -2.905751 avg loss no lamb -2.905751 time 2020-06-27 14:17:36.989807
last batch sz 10
Pre: time 2020-06-27 14:17:51.381700: 
 	std: 0.0027235227
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.979, 0.9742, 0.9802, 0.9752]
	train_accs: [0.98118335, 0.9803333, 0.97536665, 0.98085, 0.9762]
	best_train_sub_head: 0
	worst: 0.9742
	avg: 0.97792006
	best: 0.981

Starting e_i: 1594
Model ind 665 epoch 1594 batch: 0 avg loss -2.961361 avg loss no lamb -2.961361 time 2020-06-27 14:17:52.891856
Model ind 665 epoch 1594 batch: 100 avg loss -2.874596 avg loss no lamb -2.874596 time 2020-06-27 14:18:03.718193
Model ind 665 epoch 1594 batch: 200 avg loss -2.943868 avg loss no lamb -2.943868 time 2020-06-27 14:18:14.644689
Model ind 665 epoch 1594 batch: 300 avg loss -2.862808 avg loss no lamb -2.862808 time 2020-06-27 14:18:25.942770
Model ind 665 epoch 1594 batch: 400 avg loss -2.831337 avg loss no lamb -2.831337 time 2020-06-27 14:18:39.686001
Model ind 665 epoch 1594 batch: 500 avg loss -2.886021 avg loss no lamb -2.886021 time 2020-06-27 14:18:52.358622
Model ind 665 epoch 1594 batch: 600 avg loss -2.868447 avg loss no lamb -2.868447 time 2020-06-27 14:19:05.000919
Model ind 665 epoch 1594 batch: 700 avg loss -2.820285 avg loss no lamb -2.820285 time 2020-06-27 14:19:17.948106
Model ind 665 epoch 1594 batch: 800 avg loss -2.871157 avg loss no lamb -2.871157 time 2020-06-27 14:19:30.964209
last batch sz 10
Pre: time 2020-06-27 14:19:47.792743: 
 	std: 0.003942904
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9809, 0.9731, 0.9822, 0.974]
	train_accs: [0.9816833, 0.98105, 0.97396666, 0.98175, 0.9749]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97833997
	best: 0.9822

Starting e_i: 1595
Model ind 665 epoch 1595 batch: 0 avg loss -2.966250 avg loss no lamb -2.966250 time 2020-06-27 14:19:49.318635
Model ind 665 epoch 1595 batch: 100 avg loss -2.912947 avg loss no lamb -2.912947 time 2020-06-27 14:20:02.835458
Model ind 665 epoch 1595 batch: 200 avg loss -2.873935 avg loss no lamb -2.873935 time 2020-06-27 14:20:15.994057
Model ind 665 epoch 1595 batch: 300 avg loss -2.884119 avg loss no lamb -2.884119 time 2020-06-27 14:20:29.195779
Model ind 665 epoch 1595 batch: 400 avg loss -2.848088 avg loss no lamb -2.848088 time 2020-06-27 14:20:42.568708
Model ind 665 epoch 1595 batch: 500 avg loss -2.836478 avg loss no lamb -2.836478 time 2020-06-27 14:20:55.858939
Model ind 665 epoch 1595 batch: 600 avg loss -2.921434 avg loss no lamb -2.921434 time 2020-06-27 14:21:09.046748
Model ind 665 epoch 1595 batch: 700 avg loss -2.810832 avg loss no lamb -2.810832 time 2020-06-27 14:21:21.220127
Model ind 665 epoch 1595 batch: 800 avg loss -2.896079 avg loss no lamb -2.896079 time 2020-06-27 14:21:33.880982
last batch sz 10
Pre: time 2020-06-27 14:21:50.195841: 
 	std: 0.0028593582
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9807, 0.975, 0.9808, 0.975]
	train_accs: [0.98141664, 0.98106664, 0.97548336, 0.98156667, 0.97643334]
	best_train_sub_head: 3
	worst: 0.975
	avg: 0.9785
	best: 0.9808

Starting e_i: 1596
Model ind 665 epoch 1596 batch: 0 avg loss -2.928364 avg loss no lamb -2.928364 time 2020-06-27 14:21:51.965201
Model ind 665 epoch 1596 batch: 100 avg loss -2.882575 avg loss no lamb -2.882575 time 2020-06-27 14:22:01.751885
Model ind 665 epoch 1596 batch: 200 avg loss -2.899369 avg loss no lamb -2.899369 time 2020-06-27 14:22:11.188122
Model ind 665 epoch 1596 batch: 300 avg loss -2.863248 avg loss no lamb -2.863248 time 2020-06-27 14:22:20.745386
Model ind 665 epoch 1596 batch: 400 avg loss -2.819122 avg loss no lamb -2.819122 time 2020-06-27 14:22:30.226541
Model ind 665 epoch 1596 batch: 500 avg loss -2.889442 avg loss no lamb -2.889442 time 2020-06-27 14:22:39.732392
Model ind 665 epoch 1596 batch: 600 avg loss -2.909652 avg loss no lamb -2.909652 time 2020-06-27 14:22:49.257874
Model ind 665 epoch 1596 batch: 700 avg loss -2.858222 avg loss no lamb -2.858222 time 2020-06-27 14:23:00.396380
Model ind 665 epoch 1596 batch: 800 avg loss -2.862570 avg loss no lamb -2.862570 time 2020-06-27 14:23:11.381476
last batch sz 10
Pre: time 2020-06-27 14:23:24.053940: 
 	std: 0.003296437
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9803, 0.9747, 0.9822, 0.9753]
	train_accs: [0.98251665, 0.9813, 0.9758667, 0.98245, 0.9770833]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97894
	best: 0.9822

Starting e_i: 1597
Model ind 665 epoch 1597 batch: 0 avg loss -2.890020 avg loss no lamb -2.890020 time 2020-06-27 14:23:25.225371
Model ind 665 epoch 1597 batch: 100 avg loss -2.840694 avg loss no lamb -2.840694 time 2020-06-27 14:23:34.760696
Model ind 665 epoch 1597 batch: 200 avg loss -2.880163 avg loss no lamb -2.880163 time 2020-06-27 14:23:44.256407
Model ind 665 epoch 1597 batch: 300 avg loss -2.849054 avg loss no lamb -2.849054 time 2020-06-27 14:23:53.751418
Model ind 665 epoch 1597 batch: 400 avg loss -2.796720 avg loss no lamb -2.796720 time 2020-06-27 14:24:03.282510
Model ind 665 epoch 1597 batch: 500 avg loss -2.843198 avg loss no lamb -2.843198 time 2020-06-27 14:24:13.255312
Model ind 665 epoch 1597 batch: 600 avg loss -2.903287 avg loss no lamb -2.903287 time 2020-06-27 14:24:22.933832
Model ind 665 epoch 1597 batch: 700 avg loss -2.774796 avg loss no lamb -2.774796 time 2020-06-27 14:24:32.427888
Model ind 665 epoch 1597 batch: 800 avg loss -2.919778 avg loss no lamb -2.919778 time 2020-06-27 14:24:41.950156
last batch sz 10
Pre: time 2020-06-27 14:24:54.640240: 
 	std: 0.003808636
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9804, 0.9729, 0.9814, 0.9738]
	train_accs: [0.9820167, 0.98108333, 0.9748, 0.98188335, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97798
	best: 0.9814

Starting e_i: 1598
Model ind 665 epoch 1598 batch: 0 avg loss -2.924307 avg loss no lamb -2.924307 time 2020-06-27 14:24:55.818603
Model ind 665 epoch 1598 batch: 100 avg loss -2.893677 avg loss no lamb -2.893677 time 2020-06-27 14:25:05.428650
Model ind 665 epoch 1598 batch: 200 avg loss -2.888233 avg loss no lamb -2.888233 time 2020-06-27 14:25:15.001138
Model ind 665 epoch 1598 batch: 300 avg loss -2.855762 avg loss no lamb -2.855762 time 2020-06-27 14:25:24.513135
Model ind 665 epoch 1598 batch: 400 avg loss -2.807776 avg loss no lamb -2.807776 time 2020-06-27 14:25:34.008753
Model ind 665 epoch 1598 batch: 500 avg loss -2.871483 avg loss no lamb -2.871483 time 2020-06-27 14:25:43.427860
Model ind 665 epoch 1598 batch: 600 avg loss -2.880445 avg loss no lamb -2.880445 time 2020-06-27 14:25:52.890472
Model ind 665 epoch 1598 batch: 700 avg loss -2.753512 avg loss no lamb -2.753512 time 2020-06-27 14:26:02.362202
Model ind 665 epoch 1598 batch: 800 avg loss -2.950240 avg loss no lamb -2.950240 time 2020-06-27 14:26:11.856240
last batch sz 10
Pre: time 2020-06-27 14:26:24.714480: 
 	std: 0.0030981407
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9793, 0.9734, 0.9804, 0.974]
	train_accs: [0.9810167, 0.9802167, 0.97473335, 0.98113334, 0.97571665]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97746
	best: 0.9804

Starting e_i: 1599
Model ind 665 epoch 1599 batch: 0 avg loss -2.921349 avg loss no lamb -2.921349 time 2020-06-27 14:26:25.884711
Model ind 665 epoch 1599 batch: 100 avg loss -2.930612 avg loss no lamb -2.930612 time 2020-06-27 14:26:35.247163
Model ind 665 epoch 1599 batch: 200 avg loss -2.893055 avg loss no lamb -2.893055 time 2020-06-27 14:26:44.621493
Model ind 665 epoch 1599 batch: 300 avg loss -2.842775 avg loss no lamb -2.842775 time 2020-06-27 14:26:54.310040
Model ind 665 epoch 1599 batch: 400 avg loss -2.847636 avg loss no lamb -2.847636 time 2020-06-27 14:27:03.856373
Model ind 665 epoch 1599 batch: 500 avg loss -2.915812 avg loss no lamb -2.915812 time 2020-06-27 14:27:13.366461
Model ind 665 epoch 1599 batch: 600 avg loss -2.882084 avg loss no lamb -2.882084 time 2020-06-27 14:27:22.880372
Model ind 665 epoch 1599 batch: 700 avg loss -2.770505 avg loss no lamb -2.770505 time 2020-06-27 14:27:32.323472
Model ind 665 epoch 1599 batch: 800 avg loss -2.907381 avg loss no lamb -2.907381 time 2020-06-27 14:27:41.829756
last batch sz 10
Pre: time 2020-06-27 14:27:54.346755: 
 	std: 0.0024846802
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.978, 0.9782, 0.9723, 0.9781, 0.974]
	train_accs: [0.98055, 0.9799, 0.9748, 0.9806333, 0.9756]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97612
	best: 0.9781

Starting e_i: 1600
Model ind 665 epoch 1600 batch: 0 avg loss -2.986259 avg loss no lamb -2.986259 time 2020-06-27 14:27:55.503760
Model ind 665 epoch 1600 batch: 100 avg loss -2.909565 avg loss no lamb -2.909565 time 2020-06-27 14:28:04.979465
Model ind 665 epoch 1600 batch: 200 avg loss -2.895571 avg loss no lamb -2.895571 time 2020-06-27 14:28:14.498040
Model ind 665 epoch 1600 batch: 300 avg loss -2.878372 avg loss no lamb -2.878372 time 2020-06-27 14:28:24.035501
Model ind 665 epoch 1600 batch: 400 avg loss -2.820752 avg loss no lamb -2.820752 time 2020-06-27 14:28:33.547654
Model ind 665 epoch 1600 batch: 500 avg loss -2.898358 avg loss no lamb -2.898358 time 2020-06-27 14:28:43.089805
Model ind 665 epoch 1600 batch: 600 avg loss -2.899326 avg loss no lamb -2.899326 time 2020-06-27 14:28:52.588644
Model ind 665 epoch 1600 batch: 700 avg loss -2.755812 avg loss no lamb -2.755812 time 2020-06-27 14:29:02.232501
Model ind 665 epoch 1600 batch: 800 avg loss -2.866556 avg loss no lamb -2.866556 time 2020-06-27 14:29:12.338850
last batch sz 10
Pre: time 2020-06-27 14:29:28.768313: 
 	std: 0.0035114705
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9811, 0.973, 0.9805, 0.9744]
	train_accs: [0.9816333, 0.98095, 0.9751667, 0.9814, 0.9755833]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97796
	best: 0.9808

Starting e_i: 1601
Model ind 665 epoch 1601 batch: 0 avg loss -2.964744 avg loss no lamb -2.964744 time 2020-06-27 14:29:31.211273
Model ind 665 epoch 1601 batch: 100 avg loss -2.878632 avg loss no lamb -2.878632 time 2020-06-27 14:29:40.889210
Model ind 665 epoch 1601 batch: 200 avg loss -2.888160 avg loss no lamb -2.888160 time 2020-06-27 14:29:50.601186
Model ind 665 epoch 1601 batch: 300 avg loss -2.878869 avg loss no lamb -2.878869 time 2020-06-27 14:30:00.426588
Model ind 665 epoch 1601 batch: 400 avg loss -2.802105 avg loss no lamb -2.802105 time 2020-06-27 14:30:10.176892
Model ind 665 epoch 1601 batch: 500 avg loss -2.886917 avg loss no lamb -2.886917 time 2020-06-27 14:30:20.217055
Model ind 665 epoch 1601 batch: 600 avg loss -2.930247 avg loss no lamb -2.930247 time 2020-06-27 14:30:30.710028
Model ind 665 epoch 1601 batch: 700 avg loss -2.806601 avg loss no lamb -2.806601 time 2020-06-27 14:30:42.668135
Model ind 665 epoch 1601 batch: 800 avg loss -2.901143 avg loss no lamb -2.901143 time 2020-06-27 14:30:54.704053
last batch sz 10
Pre: time 2020-06-27 14:31:11.655191: 
 	std: 0.003983172
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.981, 0.9726, 0.9812, 0.9733]
	train_accs: [0.98121667, 0.9806, 0.97468334, 0.9813667, 0.97545]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97782004
	best: 0.9812

Starting e_i: 1602
Model ind 665 epoch 1602 batch: 0 avg loss -2.967169 avg loss no lamb -2.967169 time 2020-06-27 14:31:13.170849
Model ind 665 epoch 1602 batch: 100 avg loss -2.959252 avg loss no lamb -2.959252 time 2020-06-27 14:31:25.599870
Model ind 665 epoch 1602 batch: 200 avg loss -2.897265 avg loss no lamb -2.897265 time 2020-06-27 14:31:38.095365
Model ind 665 epoch 1602 batch: 300 avg loss -2.917764 avg loss no lamb -2.917764 time 2020-06-27 14:31:50.631742
Model ind 665 epoch 1602 batch: 400 avg loss -2.772146 avg loss no lamb -2.772146 time 2020-06-27 14:32:02.635805
Model ind 665 epoch 1602 batch: 500 avg loss -2.873039 avg loss no lamb -2.873039 time 2020-06-27 14:32:14.645304
Model ind 665 epoch 1602 batch: 600 avg loss -2.857697 avg loss no lamb -2.857697 time 2020-06-27 14:32:26.317242
Model ind 665 epoch 1602 batch: 700 avg loss -2.812810 avg loss no lamb -2.812810 time 2020-06-27 14:32:38.125733
Model ind 665 epoch 1602 batch: 800 avg loss -2.813551 avg loss no lamb -2.813551 time 2020-06-27 14:32:49.633903
last batch sz 10
Pre: time 2020-06-27 14:33:04.708665: 
 	std: 0.0032745067
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9797, 0.9725, 0.9793, 0.9734]
	train_accs: [0.98155, 0.98085, 0.97498333, 0.9813833, 0.97615]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97694004
	best: 0.9798

Starting e_i: 1603
Model ind 665 epoch 1603 batch: 0 avg loss -2.991891 avg loss no lamb -2.991891 time 2020-06-27 14:33:06.324300
Model ind 665 epoch 1603 batch: 100 avg loss -2.931167 avg loss no lamb -2.931167 time 2020-06-27 14:33:18.005644
Model ind 665 epoch 1603 batch: 200 avg loss -2.886179 avg loss no lamb -2.886179 time 2020-06-27 14:33:29.881310
Model ind 665 epoch 1603 batch: 300 avg loss -2.890909 avg loss no lamb -2.890909 time 2020-06-27 14:33:41.533743
Model ind 665 epoch 1603 batch: 400 avg loss -2.865867 avg loss no lamb -2.865867 time 2020-06-27 14:33:53.186356
Model ind 665 epoch 1603 batch: 500 avg loss -2.877644 avg loss no lamb -2.877644 time 2020-06-27 14:34:05.106656
Model ind 665 epoch 1603 batch: 600 avg loss -2.929976 avg loss no lamb -2.929976 time 2020-06-27 14:34:16.977375
Model ind 665 epoch 1603 batch: 700 avg loss -2.759788 avg loss no lamb -2.759788 time 2020-06-27 14:34:28.393135
Model ind 665 epoch 1603 batch: 800 avg loss -2.886923 avg loss no lamb -2.886923 time 2020-06-27 14:34:39.934693
last batch sz 10
Pre: time 2020-06-27 14:34:55.107543: 
 	std: 0.0027957088
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9791, 0.9731, 0.9793, 0.9741]
	train_accs: [0.9811, 0.9805833, 0.97581667, 0.9813667, 0.97653335]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97699994
	best: 0.9793

Starting e_i: 1604
Model ind 665 epoch 1604 batch: 0 avg loss -2.999061 avg loss no lamb -2.999061 time 2020-06-27 14:34:56.516763
Model ind 665 epoch 1604 batch: 100 avg loss -2.930767 avg loss no lamb -2.930767 time 2020-06-27 14:35:09.427949
Model ind 665 epoch 1604 batch: 200 avg loss -2.851672 avg loss no lamb -2.851672 time 2020-06-27 14:35:22.784187
Model ind 665 epoch 1604 batch: 300 avg loss -2.855295 avg loss no lamb -2.855295 time 2020-06-27 14:35:35.424515
Model ind 665 epoch 1604 batch: 400 avg loss -2.860867 avg loss no lamb -2.860867 time 2020-06-27 14:35:48.113041
Model ind 665 epoch 1604 batch: 500 avg loss -2.846476 avg loss no lamb -2.846476 time 2020-06-27 14:36:01.154346
Model ind 665 epoch 1604 batch: 600 avg loss -2.901790 avg loss no lamb -2.901790 time 2020-06-27 14:36:14.807078
Model ind 665 epoch 1604 batch: 700 avg loss -2.803695 avg loss no lamb -2.803695 time 2020-06-27 14:36:29.292291
Model ind 665 epoch 1604 batch: 800 avg loss -2.905608 avg loss no lamb -2.905608 time 2020-06-27 14:36:43.988943
last batch sz 10
Pre: time 2020-06-27 14:37:01.006449: 
 	std: 0.0030287984
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9807, 0.9737, 0.9798, 0.9745]
	train_accs: [0.98185, 0.98121667, 0.97601664, 0.98193336, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97778004
	best: 0.9798

Starting e_i: 1605
Model ind 665 epoch 1605 batch: 0 avg loss -2.966310 avg loss no lamb -2.966310 time 2020-06-27 14:37:02.845622
Model ind 665 epoch 1605 batch: 100 avg loss -2.911866 avg loss no lamb -2.911866 time 2020-06-27 14:37:17.388551
Model ind 665 epoch 1605 batch: 200 avg loss -2.927856 avg loss no lamb -2.927856 time 2020-06-27 14:37:32.632853
Model ind 665 epoch 1605 batch: 300 avg loss -2.838146 avg loss no lamb -2.838146 time 2020-06-27 14:37:47.415235
Model ind 665 epoch 1605 batch: 400 avg loss -2.821386 avg loss no lamb -2.821386 time 2020-06-27 14:38:02.016629
Model ind 665 epoch 1605 batch: 500 avg loss -2.932409 avg loss no lamb -2.932409 time 2020-06-27 14:38:15.920464
Model ind 665 epoch 1605 batch: 600 avg loss -2.874453 avg loss no lamb -2.874453 time 2020-06-27 14:38:29.590425
Model ind 665 epoch 1605 batch: 700 avg loss -2.855737 avg loss no lamb -2.855737 time 2020-06-27 14:38:42.526674
Model ind 665 epoch 1605 batch: 800 avg loss -2.851820 avg loss no lamb -2.851820 time 2020-06-27 14:38:55.954971
last batch sz 10
Pre: time 2020-06-27 14:39:12.784031: 
 	std: 0.0030083887
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9794, 0.9735, 0.9803, 0.9741]
	train_accs: [0.98151666, 0.98088336, 0.9758667, 0.9817, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9735
	avg: 0.97746
	best: 0.9803

Starting e_i: 1606
Model ind 665 epoch 1606 batch: 0 avg loss -2.967727 avg loss no lamb -2.967727 time 2020-06-27 14:39:14.345206
Model ind 665 epoch 1606 batch: 100 avg loss -2.901444 avg loss no lamb -2.901444 time 2020-06-27 14:39:27.857632
Model ind 665 epoch 1606 batch: 200 avg loss -2.922775 avg loss no lamb -2.922775 time 2020-06-27 14:39:41.466368
Model ind 665 epoch 1606 batch: 300 avg loss -2.886547 avg loss no lamb -2.886547 time 2020-06-27 14:39:55.269935
Model ind 665 epoch 1606 batch: 400 avg loss -2.824715 avg loss no lamb -2.824715 time 2020-06-27 14:40:08.579345
Model ind 665 epoch 1606 batch: 500 avg loss -2.862143 avg loss no lamb -2.862143 time 2020-06-27 14:40:21.302359
Model ind 665 epoch 1606 batch: 600 avg loss -2.847926 avg loss no lamb -2.847926 time 2020-06-27 14:40:34.626278
Model ind 665 epoch 1606 batch: 700 avg loss -2.858489 avg loss no lamb -2.858489 time 2020-06-27 14:40:47.496948
Model ind 665 epoch 1606 batch: 800 avg loss -2.868485 avg loss no lamb -2.868485 time 2020-06-27 14:41:01.155001
last batch sz 10
Pre: time 2020-06-27 14:41:17.432331: 
 	std: 0.0034654865
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9805, 0.9729, 0.9801, 0.9739]
	train_accs: [0.98185, 0.9804, 0.97496665, 0.98153335, 0.9763]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97761995
	best: 0.9807

Starting e_i: 1607
Model ind 665 epoch 1607 batch: 0 avg loss -2.959330 avg loss no lamb -2.959330 time 2020-06-27 14:41:18.951258
Model ind 665 epoch 1607 batch: 100 avg loss -2.962917 avg loss no lamb -2.962917 time 2020-06-27 14:41:31.726621
Model ind 665 epoch 1607 batch: 200 avg loss -2.831746 avg loss no lamb -2.831746 time 2020-06-27 14:41:44.121274
Model ind 665 epoch 1607 batch: 300 avg loss -2.863426 avg loss no lamb -2.863426 time 2020-06-27 14:41:57.071270
Model ind 665 epoch 1607 batch: 400 avg loss -2.780381 avg loss no lamb -2.780381 time 2020-06-27 14:42:09.717968
Model ind 665 epoch 1607 batch: 500 avg loss -2.847723 avg loss no lamb -2.847723 time 2020-06-27 14:42:22.815826
Model ind 665 epoch 1607 batch: 600 avg loss -2.849232 avg loss no lamb -2.849232 time 2020-06-27 14:42:36.194557
Model ind 665 epoch 1607 batch: 700 avg loss -2.761363 avg loss no lamb -2.761363 time 2020-06-27 14:42:49.373734
Model ind 665 epoch 1607 batch: 800 avg loss -2.835656 avg loss no lamb -2.835656 time 2020-06-27 14:43:02.789059
last batch sz 10
Pre: time 2020-06-27 14:43:19.405111: 
 	std: 0.003380764
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9797, 0.9725, 0.9796, 0.9732]
	train_accs: [0.98148334, 0.9806333, 0.97538334, 0.98153335, 0.97636664]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97698003
	best: 0.9796

Starting e_i: 1608
Model ind 665 epoch 1608 batch: 0 avg loss -2.950389 avg loss no lamb -2.950389 time 2020-06-27 14:43:20.894986
Model ind 665 epoch 1608 batch: 100 avg loss -2.913922 avg loss no lamb -2.913922 time 2020-06-27 14:43:34.157953
Model ind 665 epoch 1608 batch: 200 avg loss -2.901023 avg loss no lamb -2.901023 time 2020-06-27 14:43:47.528272
Model ind 665 epoch 1608 batch: 300 avg loss -2.932347 avg loss no lamb -2.932347 time 2020-06-27 14:44:00.385655
Model ind 665 epoch 1608 batch: 400 avg loss -2.826711 avg loss no lamb -2.826711 time 2020-06-27 14:44:12.767185
Model ind 665 epoch 1608 batch: 500 avg loss -2.864425 avg loss no lamb -2.864425 time 2020-06-27 14:44:24.958970
Model ind 665 epoch 1608 batch: 600 avg loss -2.924332 avg loss no lamb -2.924332 time 2020-06-27 14:44:37.411558
Model ind 665 epoch 1608 batch: 700 avg loss -2.865657 avg loss no lamb -2.865657 time 2020-06-27 14:44:50.743096
Model ind 665 epoch 1608 batch: 800 avg loss -2.882978 avg loss no lamb -2.882978 time 2020-06-27 14:45:03.790685
last batch sz 10
Pre: time 2020-06-27 14:45:20.084922: 
 	std: 0.0024943263
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9813, 0.9758, 0.9815, 0.9765]
	train_accs: [0.98181665, 0.9809833, 0.9769, 0.98186666, 0.97761667]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.97918004
	best: 0.9815

Starting e_i: 1609
Model ind 665 epoch 1609 batch: 0 avg loss -2.995292 avg loss no lamb -2.995292 time 2020-06-27 14:45:21.660417
Model ind 665 epoch 1609 batch: 100 avg loss -2.936575 avg loss no lamb -2.936575 time 2020-06-27 14:45:35.050831
Model ind 665 epoch 1609 batch: 200 avg loss -2.801040 avg loss no lamb -2.801040 time 2020-06-27 14:45:48.428039
Model ind 665 epoch 1609 batch: 300 avg loss -2.878571 avg loss no lamb -2.878571 time 2020-06-27 14:46:01.768577
Model ind 665 epoch 1609 batch: 400 avg loss -2.789898 avg loss no lamb -2.789898 time 2020-06-27 14:46:15.318750
Model ind 665 epoch 1609 batch: 500 avg loss -2.835384 avg loss no lamb -2.835384 time 2020-06-27 14:46:29.225464
Model ind 665 epoch 1609 batch: 600 avg loss -2.835329 avg loss no lamb -2.835329 time 2020-06-27 14:46:43.060003
Model ind 665 epoch 1609 batch: 700 avg loss -2.748057 avg loss no lamb -2.748057 time 2020-06-27 14:46:55.935265
Model ind 665 epoch 1609 batch: 800 avg loss -2.932551 avg loss no lamb -2.932551 time 2020-06-27 14:47:10.107091
last batch sz 10
Pre: time 2020-06-27 14:47:26.783171: 
 	std: 0.0034747103
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.982, 0.9806, 0.9738, 0.9818, 0.9752]
	train_accs: [0.98226666, 0.9809, 0.97533333, 0.9819667, 0.9770667]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97868
	best: 0.982

Starting e_i: 1610
Model ind 665 epoch 1610 batch: 0 avg loss -2.990016 avg loss no lamb -2.990016 time 2020-06-27 14:47:28.359901
Model ind 665 epoch 1610 batch: 100 avg loss -2.922978 avg loss no lamb -2.922978 time 2020-06-27 14:47:41.943052
Model ind 665 epoch 1610 batch: 200 avg loss -2.896845 avg loss no lamb -2.896845 time 2020-06-27 14:47:55.454047
Model ind 665 epoch 1610 batch: 300 avg loss -2.886709 avg loss no lamb -2.886709 time 2020-06-27 14:48:08.651803
Model ind 665 epoch 1610 batch: 400 avg loss -2.826445 avg loss no lamb -2.826445 time 2020-06-27 14:48:21.918223
Model ind 665 epoch 1610 batch: 500 avg loss -2.920755 avg loss no lamb -2.920755 time 2020-06-27 14:48:35.474105
Model ind 665 epoch 1610 batch: 600 avg loss -2.911669 avg loss no lamb -2.911669 time 2020-06-27 14:48:49.016837
Model ind 665 epoch 1610 batch: 700 avg loss -2.724683 avg loss no lamb -2.724683 time 2020-06-27 14:49:02.479447
Model ind 665 epoch 1610 batch: 800 avg loss -2.781567 avg loss no lamb -2.781567 time 2020-06-27 14:49:15.407534
last batch sz 10
Pre: time 2020-06-27 14:49:31.655669: 
 	std: 0.0036760934
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9807, 0.9792, 0.9717, 0.9797, 0.9733]
	train_accs: [0.9819, 0.9802167, 0.9751, 0.98151666, 0.97605]
	best_train_sub_head: 0
	worst: 0.9717
	avg: 0.97692
	best: 0.9807

Starting e_i: 1611
Model ind 665 epoch 1611 batch: 0 avg loss -2.954606 avg loss no lamb -2.954606 time 2020-06-27 14:49:34.386123
Model ind 665 epoch 1611 batch: 100 avg loss -2.967523 avg loss no lamb -2.967523 time 2020-06-27 14:49:47.811363
Model ind 665 epoch 1611 batch: 200 avg loss -2.878474 avg loss no lamb -2.878474 time 2020-06-27 14:50:00.440103
Model ind 665 epoch 1611 batch: 300 avg loss -2.881241 avg loss no lamb -2.881241 time 2020-06-27 14:50:12.114147
Model ind 665 epoch 1611 batch: 400 avg loss -2.758963 avg loss no lamb -2.758963 time 2020-06-27 14:50:23.632206
Model ind 665 epoch 1611 batch: 500 avg loss -2.867913 avg loss no lamb -2.867913 time 2020-06-27 14:50:35.219189
Model ind 665 epoch 1611 batch: 600 avg loss -2.871036 avg loss no lamb -2.871036 time 2020-06-27 14:50:46.889032
Model ind 665 epoch 1611 batch: 700 avg loss -2.807316 avg loss no lamb -2.807316 time 2020-06-27 14:50:58.502895
Model ind 665 epoch 1611 batch: 800 avg loss -2.867707 avg loss no lamb -2.867707 time 2020-06-27 14:51:10.197194
last batch sz 10
Pre: time 2020-06-27 14:51:25.267321: 
 	std: 0.0033565448
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9796, 0.9731, 0.9808, 0.9741]
	train_accs: [0.9815, 0.9805667, 0.97501665, 0.98155, 0.9762]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97766
	best: 0.9808

Starting e_i: 1612
Model ind 665 epoch 1612 batch: 0 avg loss -2.961936 avg loss no lamb -2.961936 time 2020-06-27 14:51:26.909815
Model ind 665 epoch 1612 batch: 100 avg loss -2.890496 avg loss no lamb -2.890496 time 2020-06-27 14:51:38.352100
Model ind 665 epoch 1612 batch: 200 avg loss -2.900333 avg loss no lamb -2.900333 time 2020-06-27 14:51:49.848206
Model ind 665 epoch 1612 batch: 300 avg loss -2.897090 avg loss no lamb -2.897090 time 2020-06-27 14:52:01.265831
Model ind 665 epoch 1612 batch: 400 avg loss -2.855830 avg loss no lamb -2.855830 time 2020-06-27 14:52:12.899307
Model ind 665 epoch 1612 batch: 500 avg loss -2.821702 avg loss no lamb -2.821702 time 2020-06-27 14:52:24.923305
Model ind 665 epoch 1612 batch: 600 avg loss -2.916667 avg loss no lamb -2.916667 time 2020-06-27 14:52:36.514413
Model ind 665 epoch 1612 batch: 700 avg loss -2.851079 avg loss no lamb -2.851079 time 2020-06-27 14:52:48.059175
Model ind 665 epoch 1612 batch: 800 avg loss -2.855654 avg loss no lamb -2.855654 time 2020-06-27 14:52:59.630804
last batch sz 10
Pre: time 2020-06-27 14:53:14.735557: 
 	std: 0.003217215
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9822, 0.9812, 0.9751, 0.9822, 0.9756]
	train_accs: [0.9817333, 0.98111665, 0.97575, 0.9816167, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.9792601
	best: 0.9822

Starting e_i: 1613
Model ind 665 epoch 1613 batch: 0 avg loss -2.920684 avg loss no lamb -2.920684 time 2020-06-27 14:53:16.175546
Model ind 665 epoch 1613 batch: 100 avg loss -2.851906 avg loss no lamb -2.851906 time 2020-06-27 14:53:27.854946
Model ind 665 epoch 1613 batch: 200 avg loss -2.868294 avg loss no lamb -2.868294 time 2020-06-27 14:53:39.518215
Model ind 665 epoch 1613 batch: 300 avg loss -2.885191 avg loss no lamb -2.885191 time 2020-06-27 14:53:50.993644
Model ind 665 epoch 1613 batch: 400 avg loss -2.775910 avg loss no lamb -2.775910 time 2020-06-27 14:54:02.617067
Model ind 665 epoch 1613 batch: 500 avg loss -2.886222 avg loss no lamb -2.886222 time 2020-06-27 14:54:14.178730
Model ind 665 epoch 1613 batch: 600 avg loss -2.858601 avg loss no lamb -2.858601 time 2020-06-27 14:54:25.882916
Model ind 665 epoch 1613 batch: 700 avg loss -2.700885 avg loss no lamb -2.700885 time 2020-06-27 14:54:37.463261
Model ind 665 epoch 1613 batch: 800 avg loss -2.922278 avg loss no lamb -2.922278 time 2020-06-27 14:54:48.858525
last batch sz 10
Pre: time 2020-06-27 14:55:04.880293: 
 	std: 0.0029475517
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9815, 0.9818, 0.9753, 0.9815, 0.9759]
	train_accs: [0.9817167, 0.9814, 0.97601664, 0.98178333, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9753
	avg: 0.97920007
	best: 0.9815

Starting e_i: 1614
Model ind 665 epoch 1614 batch: 0 avg loss -2.980490 avg loss no lamb -2.980490 time 2020-06-27 14:55:06.875397
Model ind 665 epoch 1614 batch: 100 avg loss -2.913891 avg loss no lamb -2.913891 time 2020-06-27 14:55:18.454890
Model ind 665 epoch 1614 batch: 200 avg loss -2.910994 avg loss no lamb -2.910994 time 2020-06-27 14:55:29.495927
Model ind 665 epoch 1614 batch: 300 avg loss -2.873019 avg loss no lamb -2.873019 time 2020-06-27 14:55:40.301510
Model ind 665 epoch 1614 batch: 400 avg loss -2.721140 avg loss no lamb -2.721140 time 2020-06-27 14:55:50.960321
Model ind 665 epoch 1614 batch: 500 avg loss -2.882125 avg loss no lamb -2.882125 time 2020-06-27 14:56:01.974846
Model ind 665 epoch 1614 batch: 600 avg loss -2.882559 avg loss no lamb -2.882559 time 2020-06-27 14:56:12.613164
Model ind 665 epoch 1614 batch: 700 avg loss -2.841679 avg loss no lamb -2.841679 time 2020-06-27 14:56:23.113009
Model ind 665 epoch 1614 batch: 800 avg loss -2.913399 avg loss no lamb -2.913399 time 2020-06-27 14:56:33.786581
last batch sz 10
Pre: time 2020-06-27 14:56:47.878487: 
 	std: 0.0031167953
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9801, 0.973, 0.9798, 0.9741]
	train_accs: [0.98088336, 0.98038334, 0.97581667, 0.9813833, 0.97605]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97734004
	best: 0.9798

Starting e_i: 1615
Model ind 665 epoch 1615 batch: 0 avg loss -2.925244 avg loss no lamb -2.925244 time 2020-06-27 14:56:49.292356
Model ind 665 epoch 1615 batch: 100 avg loss -2.950258 avg loss no lamb -2.950258 time 2020-06-27 14:57:00.086167
Model ind 665 epoch 1615 batch: 200 avg loss -2.944257 avg loss no lamb -2.944257 time 2020-06-27 14:57:10.939498
Model ind 665 epoch 1615 batch: 300 avg loss -2.880318 avg loss no lamb -2.880318 time 2020-06-27 14:57:21.963571
Model ind 665 epoch 1615 batch: 400 avg loss -2.839112 avg loss no lamb -2.839112 time 2020-06-27 14:57:32.636882
Model ind 665 epoch 1615 batch: 500 avg loss -2.867491 avg loss no lamb -2.867491 time 2020-06-27 14:57:43.227158
Model ind 665 epoch 1615 batch: 600 avg loss -2.891088 avg loss no lamb -2.891088 time 2020-06-27 14:57:53.953247
Model ind 665 epoch 1615 batch: 700 avg loss -2.740075 avg loss no lamb -2.740075 time 2020-06-27 14:58:04.970442
Model ind 665 epoch 1615 batch: 800 avg loss -2.875955 avg loss no lamb -2.875955 time 2020-06-27 14:58:15.740163
last batch sz 10
Pre: time 2020-06-27 14:58:29.995261: 
 	std: 0.0028288434
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9798, 0.9741, 0.9802, 0.9743]
	train_accs: [0.9809667, 0.98076665, 0.97533333, 0.981, 0.97581667]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97766
	best: 0.9802

Starting e_i: 1616
Model ind 665 epoch 1616 batch: 0 avg loss -2.986024 avg loss no lamb -2.986024 time 2020-06-27 14:58:31.348368
Model ind 665 epoch 1616 batch: 100 avg loss -2.933487 avg loss no lamb -2.933487 time 2020-06-27 14:58:42.199134
Model ind 665 epoch 1616 batch: 200 avg loss -2.892339 avg loss no lamb -2.892339 time 2020-06-27 14:58:52.725913
Model ind 665 epoch 1616 batch: 300 avg loss -2.879714 avg loss no lamb -2.879714 time 2020-06-27 14:59:03.293619
Model ind 665 epoch 1616 batch: 400 avg loss -2.804772 avg loss no lamb -2.804772 time 2020-06-27 14:59:14.125378
Model ind 665 epoch 1616 batch: 500 avg loss -2.907815 avg loss no lamb -2.907815 time 2020-06-27 14:59:24.696753
Model ind 665 epoch 1616 batch: 600 avg loss -2.871923 avg loss no lamb -2.871923 time 2020-06-27 14:59:35.613758
Model ind 665 epoch 1616 batch: 700 avg loss -2.787770 avg loss no lamb -2.787770 time 2020-06-27 14:59:46.563725
Model ind 665 epoch 1616 batch: 800 avg loss -2.901021 avg loss no lamb -2.901021 time 2020-06-27 14:59:57.443063
last batch sz 10
Pre: time 2020-06-27 15:00:11.696048: 
 	std: 0.0028064335
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9818, 0.9751, 0.9818, 0.977]
	train_accs: [0.98195, 0.98135, 0.9763167, 0.98205, 0.97735]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97940004
	best: 0.9818

Starting e_i: 1617
Model ind 665 epoch 1617 batch: 0 avg loss -2.963857 avg loss no lamb -2.963857 time 2020-06-27 15:00:13.063444
Model ind 665 epoch 1617 batch: 100 avg loss -2.895416 avg loss no lamb -2.895416 time 2020-06-27 15:00:23.525909
Model ind 665 epoch 1617 batch: 200 avg loss -2.806803 avg loss no lamb -2.806803 time 2020-06-27 15:00:34.234743
Model ind 665 epoch 1617 batch: 300 avg loss -2.884872 avg loss no lamb -2.884872 time 2020-06-27 15:00:44.928745
Model ind 665 epoch 1617 batch: 400 avg loss -2.798107 avg loss no lamb -2.798107 time 2020-06-27 15:00:55.485166
Model ind 665 epoch 1617 batch: 500 avg loss -2.882107 avg loss no lamb -2.882107 time 2020-06-27 15:01:06.326610
Model ind 665 epoch 1617 batch: 600 avg loss -2.968410 avg loss no lamb -2.968410 time 2020-06-27 15:01:16.895319
Model ind 665 epoch 1617 batch: 700 avg loss -2.800325 avg loss no lamb -2.800325 time 2020-06-27 15:01:27.603630
Model ind 665 epoch 1617 batch: 800 avg loss -2.810361 avg loss no lamb -2.810361 time 2020-06-27 15:01:38.332483
last batch sz 10
Pre: time 2020-06-27 15:01:52.418983: 
 	std: 0.002964176
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9817, 0.9808, 0.9746, 0.9817, 0.9764]
	train_accs: [0.98178333, 0.9808667, 0.9758667, 0.9817, 0.97695]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97903997
	best: 0.9817

Starting e_i: 1618
Model ind 665 epoch 1618 batch: 0 avg loss -2.964864 avg loss no lamb -2.964864 time 2020-06-27 15:01:53.753510
Model ind 665 epoch 1618 batch: 100 avg loss -2.918748 avg loss no lamb -2.918748 time 2020-06-27 15:02:04.508969
Model ind 665 epoch 1618 batch: 200 avg loss -2.917254 avg loss no lamb -2.917254 time 2020-06-27 15:02:15.382514
Model ind 665 epoch 1618 batch: 300 avg loss -2.870918 avg loss no lamb -2.870918 time 2020-06-27 15:02:26.043233
Model ind 665 epoch 1618 batch: 400 avg loss -2.786808 avg loss no lamb -2.786808 time 2020-06-27 15:02:36.742303
Model ind 665 epoch 1618 batch: 500 avg loss -2.861352 avg loss no lamb -2.861352 time 2020-06-27 15:02:47.579046
Model ind 665 epoch 1618 batch: 600 avg loss -2.934939 avg loss no lamb -2.934939 time 2020-06-27 15:02:58.485920
Model ind 665 epoch 1618 batch: 700 avg loss -2.819691 avg loss no lamb -2.819691 time 2020-06-27 15:03:09.289783
Model ind 665 epoch 1618 batch: 800 avg loss -2.918482 avg loss no lamb -2.918482 time 2020-06-27 15:03:20.132143
last batch sz 10
Pre: time 2020-06-27 15:03:34.651870: 
 	std: 0.0026859595
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9811, 0.9757, 0.9816, 0.9765]
	train_accs: [0.9820167, 0.9809833, 0.9763167, 0.98181665, 0.9770167]
	best_train_sub_head: 0
	worst: 0.9757
	avg: 0.97936
	best: 0.9819

Starting e_i: 1619
Model ind 665 epoch 1619 batch: 0 avg loss -2.945885 avg loss no lamb -2.945885 time 2020-06-27 15:03:36.053766
Model ind 665 epoch 1619 batch: 100 avg loss -2.922036 avg loss no lamb -2.922036 time 2020-06-27 15:03:46.989082
Model ind 665 epoch 1619 batch: 200 avg loss -2.920819 avg loss no lamb -2.920819 time 2020-06-27 15:03:57.736598
Model ind 665 epoch 1619 batch: 300 avg loss -2.854449 avg loss no lamb -2.854449 time 2020-06-27 15:04:08.403408
Model ind 665 epoch 1619 batch: 400 avg loss -2.825817 avg loss no lamb -2.825817 time 2020-06-27 15:04:19.247447
Model ind 665 epoch 1619 batch: 500 avg loss -2.831651 avg loss no lamb -2.831651 time 2020-06-27 15:04:30.169420
Model ind 665 epoch 1619 batch: 600 avg loss -2.833034 avg loss no lamb -2.833034 time 2020-06-27 15:04:40.864277
Model ind 665 epoch 1619 batch: 700 avg loss -2.773479 avg loss no lamb -2.773479 time 2020-06-27 15:04:51.446244
Model ind 665 epoch 1619 batch: 800 avg loss -2.845081 avg loss no lamb -2.845081 time 2020-06-27 15:05:02.279204
last batch sz 10
Pre: time 2020-06-27 15:05:16.454472: 
 	std: 0.0028526513
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.98, 0.9741, 0.9801, 0.9747]
	train_accs: [0.98141664, 0.9806833, 0.97546667, 0.9812, 0.97643334]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.9778799
	best: 0.9805

Starting e_i: 1620
Model ind 665 epoch 1620 batch: 0 avg loss -2.941542 avg loss no lamb -2.941542 time 2020-06-27 15:05:17.801413
Model ind 665 epoch 1620 batch: 100 avg loss -2.913479 avg loss no lamb -2.913479 time 2020-06-27 15:05:28.426348
Model ind 665 epoch 1620 batch: 200 avg loss -2.852036 avg loss no lamb -2.852036 time 2020-06-27 15:05:38.962936
Model ind 665 epoch 1620 batch: 300 avg loss -2.851419 avg loss no lamb -2.851419 time 2020-06-27 15:05:49.428784
Model ind 665 epoch 1620 batch: 400 avg loss -2.856987 avg loss no lamb -2.856987 time 2020-06-27 15:06:00.179053
Model ind 665 epoch 1620 batch: 500 avg loss -2.855311 avg loss no lamb -2.855311 time 2020-06-27 15:06:10.784656
Model ind 665 epoch 1620 batch: 600 avg loss -2.870859 avg loss no lamb -2.870859 time 2020-06-27 15:06:21.492053
Model ind 665 epoch 1620 batch: 700 avg loss -2.826897 avg loss no lamb -2.826897 time 2020-06-27 15:06:32.145956
Model ind 665 epoch 1620 batch: 800 avg loss -2.814071 avg loss no lamb -2.814071 time 2020-06-27 15:06:42.719008
last batch sz 10
Pre: time 2020-06-27 15:06:56.736227: 
 	std: 0.002531384
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9783, 0.9791, 0.9743, 0.9791, 0.9732]
	train_accs: [0.98065, 0.9802167, 0.9755, 0.98075, 0.9755]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97679996
	best: 0.9791

Starting e_i: 1621
Model ind 665 epoch 1621 batch: 0 avg loss -2.949399 avg loss no lamb -2.949399 time 2020-06-27 15:06:59.495547
Model ind 665 epoch 1621 batch: 100 avg loss -2.905957 avg loss no lamb -2.905957 time 2020-06-27 15:07:10.348905
Model ind 665 epoch 1621 batch: 200 avg loss -2.901983 avg loss no lamb -2.901983 time 2020-06-27 15:07:21.063223
Model ind 665 epoch 1621 batch: 300 avg loss -2.860020 avg loss no lamb -2.860020 time 2020-06-27 15:07:31.902387
Model ind 665 epoch 1621 batch: 400 avg loss -2.805511 avg loss no lamb -2.805511 time 2020-06-27 15:07:42.707911
Model ind 665 epoch 1621 batch: 500 avg loss -2.901656 avg loss no lamb -2.901656 time 2020-06-27 15:07:53.644136
Model ind 665 epoch 1621 batch: 600 avg loss -2.927542 avg loss no lamb -2.927542 time 2020-06-27 15:08:04.343136
Model ind 665 epoch 1621 batch: 700 avg loss -2.809982 avg loss no lamb -2.809982 time 2020-06-27 15:08:15.180085
Model ind 665 epoch 1621 batch: 800 avg loss -2.899536 avg loss no lamb -2.899536 time 2020-06-27 15:08:26.004613
last batch sz 10
Pre: time 2020-06-27 15:08:39.943530: 
 	std: 0.0028138063
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9798, 0.9743, 0.9801, 0.9746]
	train_accs: [0.9816, 0.98053336, 0.9756167, 0.9813833, 0.97583336]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97788
	best: 0.9806

Starting e_i: 1622
Model ind 665 epoch 1622 batch: 0 avg loss -2.955596 avg loss no lamb -2.955596 time 2020-06-27 15:08:41.267797
Model ind 665 epoch 1622 batch: 100 avg loss -2.902565 avg loss no lamb -2.902565 time 2020-06-27 15:08:52.170550
Model ind 665 epoch 1622 batch: 200 avg loss -2.868747 avg loss no lamb -2.868747 time 2020-06-27 15:09:02.957546
Model ind 665 epoch 1622 batch: 300 avg loss -2.848779 avg loss no lamb -2.848779 time 2020-06-27 15:09:13.679093
Model ind 665 epoch 1622 batch: 400 avg loss -2.824407 avg loss no lamb -2.824407 time 2020-06-27 15:09:24.219709
Model ind 665 epoch 1622 batch: 500 avg loss -2.902240 avg loss no lamb -2.902240 time 2020-06-27 15:09:35.062658
Model ind 665 epoch 1622 batch: 600 avg loss -2.853968 avg loss no lamb -2.853968 time 2020-06-27 15:09:45.960136
Model ind 665 epoch 1622 batch: 700 avg loss -2.821671 avg loss no lamb -2.821671 time 2020-06-27 15:09:56.758798
Model ind 665 epoch 1622 batch: 800 avg loss -2.827920 avg loss no lamb -2.827920 time 2020-06-27 15:10:07.668969
last batch sz 10
Pre: time 2020-06-27 15:10:21.766501: 
 	std: 0.0026244998
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9777, 0.977, 0.9722, 0.9776, 0.972]
	train_accs: [0.9805167, 0.97931665, 0.97466666, 0.9804, 0.97491664]
	best_train_sub_head: 0
	worst: 0.972
	avg: 0.9753
	best: 0.9777

Starting e_i: 1623
Model ind 665 epoch 1623 batch: 0 avg loss -2.923775 avg loss no lamb -2.923775 time 2020-06-27 15:10:23.386281
Model ind 665 epoch 1623 batch: 100 avg loss -2.934108 avg loss no lamb -2.934108 time 2020-06-27 15:10:34.566092
Model ind 665 epoch 1623 batch: 200 avg loss -2.914548 avg loss no lamb -2.914548 time 2020-06-27 15:10:45.344971
Model ind 665 epoch 1623 batch: 300 avg loss -2.837725 avg loss no lamb -2.837725 time 2020-06-27 15:10:56.380056
Model ind 665 epoch 1623 batch: 400 avg loss -2.791331 avg loss no lamb -2.791331 time 2020-06-27 15:11:07.420580
Model ind 665 epoch 1623 batch: 500 avg loss -2.853518 avg loss no lamb -2.853518 time 2020-06-27 15:11:18.524000
Model ind 665 epoch 1623 batch: 600 avg loss -2.878697 avg loss no lamb -2.878697 time 2020-06-27 15:11:29.244107
Model ind 665 epoch 1623 batch: 700 avg loss -2.831175 avg loss no lamb -2.831175 time 2020-06-27 15:11:40.064589
Model ind 665 epoch 1623 batch: 800 avg loss -2.901089 avg loss no lamb -2.901089 time 2020-06-27 15:11:50.706384
last batch sz 10
Pre: time 2020-06-27 15:12:04.933331: 
 	std: 0.002779495
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9788, 0.9734, 0.9795, 0.9741]
	train_accs: [0.9816, 0.98088336, 0.97566664, 0.98141664, 0.9762667]
	best_train_sub_head: 0
	worst: 0.9734
	avg: 0.97712004
	best: 0.9798

Starting e_i: 1624
Model ind 665 epoch 1624 batch: 0 avg loss -2.961890 avg loss no lamb -2.961890 time 2020-06-27 15:12:06.302907
Model ind 665 epoch 1624 batch: 100 avg loss -2.951220 avg loss no lamb -2.951220 time 2020-06-27 15:12:17.091957
Model ind 665 epoch 1624 batch: 200 avg loss -2.888819 avg loss no lamb -2.888819 time 2020-06-27 15:12:27.752749
Model ind 665 epoch 1624 batch: 300 avg loss -2.834630 avg loss no lamb -2.834630 time 2020-06-27 15:12:38.847885
Model ind 665 epoch 1624 batch: 400 avg loss -2.861656 avg loss no lamb -2.861656 time 2020-06-27 15:12:49.698716
Model ind 665 epoch 1624 batch: 500 avg loss -2.864856 avg loss no lamb -2.864856 time 2020-06-27 15:13:00.524095
Model ind 665 epoch 1624 batch: 600 avg loss -2.910935 avg loss no lamb -2.910935 time 2020-06-27 15:13:11.370196
Model ind 665 epoch 1624 batch: 700 avg loss -2.790204 avg loss no lamb -2.790204 time 2020-06-27 15:13:22.176085
Model ind 665 epoch 1624 batch: 800 avg loss -2.897980 avg loss no lamb -2.897980 time 2020-06-27 15:13:32.850222
last batch sz 10
Pre: time 2020-06-27 15:13:46.887485: 
 	std: 0.0031721268
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9795, 0.9731, 0.9803, 0.9739]
	train_accs: [0.9815, 0.9806333, 0.97495, 0.98151666, 0.97616667]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97735995
	best: 0.9803

Starting e_i: 1625
Model ind 665 epoch 1625 batch: 0 avg loss -2.954642 avg loss no lamb -2.954642 time 2020-06-27 15:13:48.210967
Model ind 665 epoch 1625 batch: 100 avg loss -2.876328 avg loss no lamb -2.876328 time 2020-06-27 15:13:58.863681
Model ind 665 epoch 1625 batch: 200 avg loss -2.923575 avg loss no lamb -2.923575 time 2020-06-27 15:14:09.599149
Model ind 665 epoch 1625 batch: 300 avg loss -2.885257 avg loss no lamb -2.885257 time 2020-06-27 15:14:20.367137
Model ind 665 epoch 1625 batch: 400 avg loss -2.827096 avg loss no lamb -2.827096 time 2020-06-27 15:14:31.148688
Model ind 665 epoch 1625 batch: 500 avg loss -2.853304 avg loss no lamb -2.853304 time 2020-06-27 15:14:41.815008
Model ind 665 epoch 1625 batch: 600 avg loss -2.882350 avg loss no lamb -2.882350 time 2020-06-27 15:14:52.462626
Model ind 665 epoch 1625 batch: 700 avg loss -2.788592 avg loss no lamb -2.788592 time 2020-06-27 15:15:03.200950
Model ind 665 epoch 1625 batch: 800 avg loss -2.906322 avg loss no lamb -2.906322 time 2020-06-27 15:15:13.868744
last batch sz 10
Pre: time 2020-06-27 15:15:27.916031: 
 	std: 0.0027789222
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9802, 0.9741, 0.9799, 0.9748]
	train_accs: [0.98153335, 0.98076665, 0.9758667, 0.98135, 0.97635]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97784007
	best: 0.9802

Starting e_i: 1626
Model ind 665 epoch 1626 batch: 0 avg loss -2.910845 avg loss no lamb -2.910845 time 2020-06-27 15:15:29.275659
Model ind 665 epoch 1626 batch: 100 avg loss -2.972927 avg loss no lamb -2.972927 time 2020-06-27 15:15:39.978192
Model ind 665 epoch 1626 batch: 200 avg loss -2.911161 avg loss no lamb -2.911161 time 2020-06-27 15:15:50.863401
Model ind 665 epoch 1626 batch: 300 avg loss -2.842172 avg loss no lamb -2.842172 time 2020-06-27 15:16:01.804933
Model ind 665 epoch 1626 batch: 400 avg loss -2.832039 avg loss no lamb -2.832039 time 2020-06-27 15:16:12.702010
Model ind 665 epoch 1626 batch: 500 avg loss -2.872885 avg loss no lamb -2.872885 time 2020-06-27 15:16:23.644272
Model ind 665 epoch 1626 batch: 600 avg loss -2.881806 avg loss no lamb -2.881806 time 2020-06-27 15:16:34.594521
Model ind 665 epoch 1626 batch: 700 avg loss -2.744737 avg loss no lamb -2.744737 time 2020-06-27 15:16:45.390109
Model ind 665 epoch 1626 batch: 800 avg loss -2.884717 avg loss no lamb -2.884717 time 2020-06-27 15:16:56.134712
last batch sz 10
Pre: time 2020-06-27 15:17:10.338382: 
 	std: 0.0024669103
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9801, 0.9751, 0.9807, 0.9762]
	train_accs: [0.9816667, 0.98081666, 0.97651666, 0.98156667, 0.97683334]
	best_train_sub_head: 0
	worst: 0.9751
	avg: 0.97861993
	best: 0.981

Starting e_i: 1627
Model ind 665 epoch 1627 batch: 0 avg loss -2.959334 avg loss no lamb -2.959334 time 2020-06-27 15:17:11.696840
Model ind 665 epoch 1627 batch: 100 avg loss -2.918109 avg loss no lamb -2.918109 time 2020-06-27 15:17:22.416916
Model ind 665 epoch 1627 batch: 200 avg loss -2.908825 avg loss no lamb -2.908825 time 2020-06-27 15:17:33.227722
Model ind 665 epoch 1627 batch: 300 avg loss -2.872228 avg loss no lamb -2.872228 time 2020-06-27 15:17:44.111112
Model ind 665 epoch 1627 batch: 400 avg loss -2.778576 avg loss no lamb -2.778576 time 2020-06-27 15:17:55.007743
Model ind 665 epoch 1627 batch: 500 avg loss -2.889888 avg loss no lamb -2.889888 time 2020-06-27 15:18:05.876784
Model ind 665 epoch 1627 batch: 600 avg loss -2.896106 avg loss no lamb -2.896106 time 2020-06-27 15:18:16.547158
Model ind 665 epoch 1627 batch: 700 avg loss -2.775532 avg loss no lamb -2.775532 time 2020-06-27 15:18:27.279653
Model ind 665 epoch 1627 batch: 800 avg loss -2.853297 avg loss no lamb -2.853297 time 2020-06-27 15:18:38.109024
last batch sz 10
Pre: time 2020-06-27 15:18:52.716279: 
 	std: 0.0028737502
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.9807, 0.9747, 0.9808, 0.9756]
	train_accs: [0.98175, 0.98065, 0.97583336, 0.98158336, 0.9765]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97863996
	best: 0.9814

Starting e_i: 1628
Model ind 665 epoch 1628 batch: 0 avg loss -2.970602 avg loss no lamb -2.970602 time 2020-06-27 15:18:54.077309
Model ind 665 epoch 1628 batch: 100 avg loss -2.918426 avg loss no lamb -2.918426 time 2020-06-27 15:19:04.817446
Model ind 665 epoch 1628 batch: 200 avg loss -2.910064 avg loss no lamb -2.910064 time 2020-06-27 15:19:15.388734
Model ind 665 epoch 1628 batch: 300 avg loss -2.887919 avg loss no lamb -2.887919 time 2020-06-27 15:19:26.096187
Model ind 665 epoch 1628 batch: 400 avg loss -2.848439 avg loss no lamb -2.848439 time 2020-06-27 15:19:36.861829
Model ind 665 epoch 1628 batch: 500 avg loss -2.866292 avg loss no lamb -2.866292 time 2020-06-27 15:19:47.942291
Model ind 665 epoch 1628 batch: 600 avg loss -2.903197 avg loss no lamb -2.903197 time 2020-06-27 15:19:59.124214
Model ind 665 epoch 1628 batch: 700 avg loss -2.810080 avg loss no lamb -2.810080 time 2020-06-27 15:20:09.823841
Model ind 665 epoch 1628 batch: 800 avg loss -2.946337 avg loss no lamb -2.946337 time 2020-06-27 15:20:20.635190
last batch sz 10
Pre: time 2020-06-27 15:20:34.676674: 
 	std: 0.0035131762
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9812, 0.9736, 0.9818, 0.975]
	train_accs: [0.98176664, 0.98105, 0.9755167, 0.98185, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97856
	best: 0.9818

Starting e_i: 1629
Model ind 665 epoch 1629 batch: 0 avg loss -3.001970 avg loss no lamb -3.001970 time 2020-06-27 15:20:36.059249
Model ind 665 epoch 1629 batch: 100 avg loss -2.952441 avg loss no lamb -2.952441 time 2020-06-27 15:20:46.626276
Model ind 665 epoch 1629 batch: 200 avg loss -2.921314 avg loss no lamb -2.921314 time 2020-06-27 15:20:57.375953
Model ind 665 epoch 1629 batch: 300 avg loss -2.891260 avg loss no lamb -2.891260 time 2020-06-27 15:21:08.394998
Model ind 665 epoch 1629 batch: 400 avg loss -2.819101 avg loss no lamb -2.819101 time 2020-06-27 15:21:19.105037
Model ind 665 epoch 1629 batch: 500 avg loss -2.835490 avg loss no lamb -2.835490 time 2020-06-27 15:21:29.484075
Model ind 665 epoch 1629 batch: 600 avg loss -2.925058 avg loss no lamb -2.925058 time 2020-06-27 15:21:40.278192
Model ind 665 epoch 1629 batch: 700 avg loss -2.805592 avg loss no lamb -2.805592 time 2020-06-27 15:21:51.088250
Model ind 665 epoch 1629 batch: 800 avg loss -2.906998 avg loss no lamb -2.906998 time 2020-06-27 15:22:02.206551
last batch sz 10
Pre: time 2020-06-27 15:22:16.308436: 
 	std: 0.0035969908
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.979, 0.9717, 0.9796, 0.9724]
	train_accs: [0.98073334, 0.97995, 0.97386664, 0.98088336, 0.97495]
	best_train_sub_head: 3
	worst: 0.9717
	avg: 0.97643995
	best: 0.9796

Starting e_i: 1630
Model ind 665 epoch 1630 batch: 0 avg loss -2.942277 avg loss no lamb -2.942277 time 2020-06-27 15:22:17.848878
Model ind 665 epoch 1630 batch: 100 avg loss -2.956284 avg loss no lamb -2.956284 time 2020-06-27 15:22:28.706302
Model ind 665 epoch 1630 batch: 200 avg loss -2.930800 avg loss no lamb -2.930800 time 2020-06-27 15:22:39.390282
Model ind 665 epoch 1630 batch: 300 avg loss -2.937139 avg loss no lamb -2.937139 time 2020-06-27 15:22:50.048088
Model ind 665 epoch 1630 batch: 400 avg loss -2.832370 avg loss no lamb -2.832370 time 2020-06-27 15:23:00.867952
Model ind 665 epoch 1630 batch: 500 avg loss -2.856034 avg loss no lamb -2.856034 time 2020-06-27 15:23:11.529655
Model ind 665 epoch 1630 batch: 600 avg loss -2.925971 avg loss no lamb -2.925971 time 2020-06-27 15:23:22.486648
Model ind 665 epoch 1630 batch: 700 avg loss -2.823374 avg loss no lamb -2.823374 time 2020-06-27 15:23:33.405022
Model ind 665 epoch 1630 batch: 800 avg loss -2.894760 avg loss no lamb -2.894760 time 2020-06-27 15:23:44.300770
last batch sz 10
Pre: time 2020-06-27 15:23:58.452809: 
 	std: 0.0029284763
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9798, 0.9728, 0.9796, 0.9748]
	train_accs: [0.9812833, 0.98031664, 0.97513336, 0.9813833, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.9773
	best: 0.9796

Starting e_i: 1631
Model ind 665 epoch 1631 batch: 0 avg loss -2.957602 avg loss no lamb -2.957602 time 2020-06-27 15:24:01.013200
Model ind 665 epoch 1631 batch: 100 avg loss -2.955370 avg loss no lamb -2.955370 time 2020-06-27 15:24:11.685686
Model ind 665 epoch 1631 batch: 200 avg loss -2.879989 avg loss no lamb -2.879989 time 2020-06-27 15:24:22.485124
Model ind 665 epoch 1631 batch: 300 avg loss -2.862997 avg loss no lamb -2.862997 time 2020-06-27 15:24:33.455435
Model ind 665 epoch 1631 batch: 400 avg loss -2.776036 avg loss no lamb -2.776036 time 2020-06-27 15:24:44.236720
Model ind 665 epoch 1631 batch: 500 avg loss -2.834025 avg loss no lamb -2.834025 time 2020-06-27 15:24:55.041582
Model ind 665 epoch 1631 batch: 600 avg loss -2.916636 avg loss no lamb -2.916636 time 2020-06-27 15:25:05.924970
Model ind 665 epoch 1631 batch: 700 avg loss -2.782257 avg loss no lamb -2.782257 time 2020-06-27 15:25:16.685583
Model ind 665 epoch 1631 batch: 800 avg loss -2.874516 avg loss no lamb -2.874516 time 2020-06-27 15:25:27.582896
last batch sz 10
Pre: time 2020-06-27 15:25:41.899478: 
 	std: 0.002792139
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9806, 0.9748, 0.9815, 0.9763]
	train_accs: [0.98186666, 0.9809333, 0.97603333, 0.98185, 0.9769667]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97889996
	best: 0.9813

Starting e_i: 1632
Model ind 665 epoch 1632 batch: 0 avg loss -2.988098 avg loss no lamb -2.988098 time 2020-06-27 15:25:43.429600
Model ind 665 epoch 1632 batch: 100 avg loss -2.904133 avg loss no lamb -2.904133 time 2020-06-27 15:25:54.140558
Model ind 665 epoch 1632 batch: 200 avg loss -2.891468 avg loss no lamb -2.891468 time 2020-06-27 15:26:04.829427
Model ind 665 epoch 1632 batch: 300 avg loss -2.893034 avg loss no lamb -2.893034 time 2020-06-27 15:26:15.895984
Model ind 665 epoch 1632 batch: 400 avg loss -2.839275 avg loss no lamb -2.839275 time 2020-06-27 15:26:26.596932
Model ind 665 epoch 1632 batch: 500 avg loss -2.868657 avg loss no lamb -2.868657 time 2020-06-27 15:26:37.277577
Model ind 665 epoch 1632 batch: 600 avg loss -2.899742 avg loss no lamb -2.899742 time 2020-06-27 15:26:47.960886
Model ind 665 epoch 1632 batch: 700 avg loss -2.777560 avg loss no lamb -2.777560 time 2020-06-27 15:26:58.755287
Model ind 665 epoch 1632 batch: 800 avg loss -2.858674 avg loss no lamb -2.858674 time 2020-06-27 15:27:09.638800
last batch sz 10
Pre: time 2020-06-27 15:27:23.633283: 
 	std: 0.0035552739
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9812, 0.9736, 0.981, 0.9739]
	train_accs: [0.9815, 0.981, 0.97533333, 0.9815, 0.97658336]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97809994
	best: 0.9808

Starting e_i: 1633
Model ind 665 epoch 1633 batch: 0 avg loss -2.944010 avg loss no lamb -2.944010 time 2020-06-27 15:27:24.977207
Model ind 665 epoch 1633 batch: 100 avg loss -2.911346 avg loss no lamb -2.911346 time 2020-06-27 15:27:35.689535
Model ind 665 epoch 1633 batch: 200 avg loss -2.876761 avg loss no lamb -2.876761 time 2020-06-27 15:27:46.342273
Model ind 665 epoch 1633 batch: 300 avg loss -2.860307 avg loss no lamb -2.860307 time 2020-06-27 15:27:57.437757
Model ind 665 epoch 1633 batch: 400 avg loss -2.838315 avg loss no lamb -2.838315 time 2020-06-27 15:28:08.443380
Model ind 665 epoch 1633 batch: 500 avg loss -2.863528 avg loss no lamb -2.863528 time 2020-06-27 15:28:19.187035
Model ind 665 epoch 1633 batch: 600 avg loss -2.879200 avg loss no lamb -2.879200 time 2020-06-27 15:28:29.716623
Model ind 665 epoch 1633 batch: 700 avg loss -2.789371 avg loss no lamb -2.789371 time 2020-06-27 15:28:40.571936
Model ind 665 epoch 1633 batch: 800 avg loss -2.918283 avg loss no lamb -2.918283 time 2020-06-27 15:28:51.672153
last batch sz 10
Pre: time 2020-06-27 15:29:05.763984: 
 	std: 0.0031757865
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9793, 0.9729, 0.9794, 0.9734]
	train_accs: [0.98115, 0.9802833, 0.9748833, 0.9810167, 0.97546667]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97701997
	best: 0.9801

Starting e_i: 1634
Model ind 665 epoch 1634 batch: 0 avg loss -2.963844 avg loss no lamb -2.963844 time 2020-06-27 15:29:07.185145
Model ind 665 epoch 1634 batch: 100 avg loss -2.943448 avg loss no lamb -2.943448 time 2020-06-27 15:29:18.092286
Model ind 665 epoch 1634 batch: 200 avg loss -2.842355 avg loss no lamb -2.842355 time 2020-06-27 15:29:28.840951
Model ind 665 epoch 1634 batch: 300 avg loss -2.904611 avg loss no lamb -2.904611 time 2020-06-27 15:29:39.720427
Model ind 665 epoch 1634 batch: 400 avg loss -2.825747 avg loss no lamb -2.825747 time 2020-06-27 15:29:50.646029
Model ind 665 epoch 1634 batch: 500 avg loss -2.908023 avg loss no lamb -2.908023 time 2020-06-27 15:30:01.395381
Model ind 665 epoch 1634 batch: 600 avg loss -2.898682 avg loss no lamb -2.898682 time 2020-06-27 15:30:12.207026
Model ind 665 epoch 1634 batch: 700 avg loss -2.762686 avg loss no lamb -2.762686 time 2020-06-27 15:30:22.929735
Model ind 665 epoch 1634 batch: 800 avg loss -2.845136 avg loss no lamb -2.845136 time 2020-06-27 15:30:33.546189
last batch sz 10
Pre: time 2020-06-27 15:30:47.635242: 
 	std: 0.0032071355
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9815, 0.974, 0.9814, 0.9755]
	train_accs: [0.98195, 0.98148334, 0.9759167, 0.9819833, 0.9770833]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97862005
	best: 0.9814

Starting e_i: 1635
Model ind 665 epoch 1635 batch: 0 avg loss -2.946873 avg loss no lamb -2.946873 time 2020-06-27 15:30:48.917326
Model ind 665 epoch 1635 batch: 100 avg loss -2.912084 avg loss no lamb -2.912084 time 2020-06-27 15:30:59.742446
Model ind 665 epoch 1635 batch: 200 avg loss -2.890492 avg loss no lamb -2.890492 time 2020-06-27 15:31:10.964773
Model ind 665 epoch 1635 batch: 300 avg loss -2.896818 avg loss no lamb -2.896818 time 2020-06-27 15:31:21.795802
Model ind 665 epoch 1635 batch: 400 avg loss -2.791082 avg loss no lamb -2.791082 time 2020-06-27 15:31:32.448914
Model ind 665 epoch 1635 batch: 500 avg loss -2.860244 avg loss no lamb -2.860244 time 2020-06-27 15:31:43.271195
Model ind 665 epoch 1635 batch: 600 avg loss -2.938553 avg loss no lamb -2.938553 time 2020-06-27 15:31:54.143752
Model ind 665 epoch 1635 batch: 700 avg loss -2.843822 avg loss no lamb -2.843822 time 2020-06-27 15:32:04.836414
Model ind 665 epoch 1635 batch: 800 avg loss -2.893049 avg loss no lamb -2.893049 time 2020-06-27 15:32:15.302101
last batch sz 10
Pre: time 2020-06-27 15:32:29.374198: 
 	std: 0.0026566053
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9798, 0.9741, 0.9798, 0.975]
	train_accs: [0.98116666, 0.98065, 0.9755667, 0.9813167, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97778004
	best: 0.9798

Starting e_i: 1636
Model ind 665 epoch 1636 batch: 0 avg loss -2.957850 avg loss no lamb -2.957850 time 2020-06-27 15:32:30.672770
Model ind 665 epoch 1636 batch: 100 avg loss -2.932971 avg loss no lamb -2.932971 time 2020-06-27 15:32:41.438648
Model ind 665 epoch 1636 batch: 200 avg loss -2.938128 avg loss no lamb -2.938128 time 2020-06-27 15:32:52.355293
Model ind 665 epoch 1636 batch: 300 avg loss -2.918493 avg loss no lamb -2.918493 time 2020-06-27 15:33:03.136658
Model ind 665 epoch 1636 batch: 400 avg loss -2.846506 avg loss no lamb -2.846506 time 2020-06-27 15:33:13.964843
Model ind 665 epoch 1636 batch: 500 avg loss -2.824357 avg loss no lamb -2.824357 time 2020-06-27 15:33:24.908376
Model ind 665 epoch 1636 batch: 600 avg loss -2.893128 avg loss no lamb -2.893128 time 2020-06-27 15:33:35.652913
Model ind 665 epoch 1636 batch: 700 avg loss -2.845016 avg loss no lamb -2.845016 time 2020-06-27 15:33:47.817689
Model ind 665 epoch 1636 batch: 800 avg loss -2.868827 avg loss no lamb -2.868827 time 2020-06-27 15:34:02.298463
last batch sz 10
Pre: time 2020-06-27 15:34:16.538072: 
 	std: 0.0031007177
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.979, 0.973, 0.9792, 0.9729]
	train_accs: [0.98081666, 0.9804, 0.97503334, 0.98073334, 0.97575]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97674
	best: 0.9796

Starting e_i: 1637
Model ind 665 epoch 1637 batch: 0 avg loss -2.970888 avg loss no lamb -2.970888 time 2020-06-27 15:34:17.867579
Model ind 665 epoch 1637 batch: 100 avg loss -2.897774 avg loss no lamb -2.897774 time 2020-06-27 15:34:28.494314
Model ind 665 epoch 1637 batch: 200 avg loss -2.886646 avg loss no lamb -2.886646 time 2020-06-27 15:34:39.320630
Model ind 665 epoch 1637 batch: 300 avg loss -2.907400 avg loss no lamb -2.907400 time 2020-06-27 15:34:49.966213
Model ind 665 epoch 1637 batch: 400 avg loss -2.847261 avg loss no lamb -2.847261 time 2020-06-27 15:35:00.574666
Model ind 665 epoch 1637 batch: 500 avg loss -2.873983 avg loss no lamb -2.873983 time 2020-06-27 15:35:11.185734
Model ind 665 epoch 1637 batch: 600 avg loss -2.866517 avg loss no lamb -2.866517 time 2020-06-27 15:35:21.631862
Model ind 665 epoch 1637 batch: 700 avg loss -2.816507 avg loss no lamb -2.816507 time 2020-06-27 15:35:32.150985
Model ind 665 epoch 1637 batch: 800 avg loss -2.832374 avg loss no lamb -2.832374 time 2020-06-27 15:35:42.863541
last batch sz 10
Pre: time 2020-06-27 15:35:56.818020: 
 	std: 0.0027832382
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.979, 0.9789, 0.9729, 0.9791, 0.9738]
	train_accs: [0.9809167, 0.9804, 0.97485, 0.98036665, 0.97536665]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97674
	best: 0.979

Starting e_i: 1638
Model ind 665 epoch 1638 batch: 0 avg loss -2.937217 avg loss no lamb -2.937217 time 2020-06-27 15:35:58.206174
Model ind 665 epoch 1638 batch: 100 avg loss -2.852560 avg loss no lamb -2.852560 time 2020-06-27 15:36:09.112872
Model ind 665 epoch 1638 batch: 200 avg loss -2.911448 avg loss no lamb -2.911448 time 2020-06-27 15:36:19.742793
Model ind 665 epoch 1638 batch: 300 avg loss -2.873229 avg loss no lamb -2.873229 time 2020-06-27 15:36:30.458472
Model ind 665 epoch 1638 batch: 400 avg loss -2.851431 avg loss no lamb -2.851431 time 2020-06-27 15:36:41.207647
Model ind 665 epoch 1638 batch: 500 avg loss -2.858887 avg loss no lamb -2.858887 time 2020-06-27 15:36:52.052764
Model ind 665 epoch 1638 batch: 600 avg loss -2.890525 avg loss no lamb -2.890525 time 2020-06-27 15:37:02.945731
Model ind 665 epoch 1638 batch: 700 avg loss -2.795747 avg loss no lamb -2.795747 time 2020-06-27 15:37:13.622326
Model ind 665 epoch 1638 batch: 800 avg loss -2.868412 avg loss no lamb -2.868412 time 2020-06-27 15:37:24.474006
last batch sz 10
Pre: time 2020-06-27 15:37:38.712608: 
 	std: 0.0031814515
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.9793, 0.9723, 0.9785, 0.9727]
	train_accs: [0.9810167, 0.9803, 0.97445, 0.9807, 0.9755]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97638005
	best: 0.9791

Starting e_i: 1639
Model ind 665 epoch 1639 batch: 0 avg loss -2.920631 avg loss no lamb -2.920631 time 2020-06-27 15:37:40.215359
Model ind 665 epoch 1639 batch: 100 avg loss -2.931962 avg loss no lamb -2.931962 time 2020-06-27 15:37:50.821308
Model ind 665 epoch 1639 batch: 200 avg loss -2.895894 avg loss no lamb -2.895894 time 2020-06-27 15:38:01.763836
Model ind 665 epoch 1639 batch: 300 avg loss -2.918941 avg loss no lamb -2.918941 time 2020-06-27 15:38:12.428314
Model ind 665 epoch 1639 batch: 400 avg loss -2.864362 avg loss no lamb -2.864362 time 2020-06-27 15:38:23.341203
Model ind 665 epoch 1639 batch: 500 avg loss -2.862983 avg loss no lamb -2.862983 time 2020-06-27 15:38:33.912006
Model ind 665 epoch 1639 batch: 600 avg loss -2.894778 avg loss no lamb -2.894778 time 2020-06-27 15:38:44.636322
Model ind 665 epoch 1639 batch: 700 avg loss -2.743834 avg loss no lamb -2.743834 time 2020-06-27 15:38:55.435418
Model ind 665 epoch 1639 batch: 800 avg loss -2.895698 avg loss no lamb -2.895698 time 2020-06-27 15:39:06.198514
last batch sz 10
Pre: time 2020-06-27 15:39:20.304882: 
 	std: 0.0029547352
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.9778, 0.9722, 0.978, 0.9723]
	train_accs: [0.98071665, 0.97966665, 0.97485, 0.98013335, 0.97496665]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97584
	best: 0.9789

Starting e_i: 1640
Model ind 665 epoch 1640 batch: 0 avg loss -2.970119 avg loss no lamb -2.970119 time 2020-06-27 15:39:21.597369
Model ind 665 epoch 1640 batch: 100 avg loss -2.911528 avg loss no lamb -2.911528 time 2020-06-27 15:39:32.511167
Model ind 665 epoch 1640 batch: 200 avg loss -2.900444 avg loss no lamb -2.900444 time 2020-06-27 15:39:43.238240
Model ind 665 epoch 1640 batch: 300 avg loss -2.912740 avg loss no lamb -2.912740 time 2020-06-27 15:39:53.877837
Model ind 665 epoch 1640 batch: 400 avg loss -2.760455 avg loss no lamb -2.760455 time 2020-06-27 15:40:04.683038
Model ind 665 epoch 1640 batch: 500 avg loss -2.898702 avg loss no lamb -2.898702 time 2020-06-27 15:40:15.361492
Model ind 665 epoch 1640 batch: 600 avg loss -2.916058 avg loss no lamb -2.916058 time 2020-06-27 15:40:25.977271
Model ind 665 epoch 1640 batch: 700 avg loss -2.743766 avg loss no lamb -2.743766 time 2020-06-27 15:40:36.522415
Model ind 665 epoch 1640 batch: 800 avg loss -2.950536 avg loss no lamb -2.950536 time 2020-06-27 15:40:47.147274
last batch sz 10
Pre: time 2020-06-27 15:41:01.192841: 
 	std: 0.0029493023
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.9802, 0.9744, 0.9805, 0.9743]
	train_accs: [0.9813167, 0.9805167, 0.97505, 0.9812667, 0.97575]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97796
	best: 0.9804

Starting e_i: 1641
Model ind 665 epoch 1641 batch: 0 avg loss -2.963678 avg loss no lamb -2.963678 time 2020-06-27 15:41:03.891375
Model ind 665 epoch 1641 batch: 100 avg loss -2.953667 avg loss no lamb -2.953667 time 2020-06-27 15:41:14.672485
Model ind 665 epoch 1641 batch: 200 avg loss -2.895850 avg loss no lamb -2.895850 time 2020-06-27 15:41:25.311879
Model ind 665 epoch 1641 batch: 300 avg loss -2.897454 avg loss no lamb -2.897454 time 2020-06-27 15:41:36.026449
Model ind 665 epoch 1641 batch: 400 avg loss -2.841905 avg loss no lamb -2.841905 time 2020-06-27 15:41:46.852163
Model ind 665 epoch 1641 batch: 500 avg loss -2.865942 avg loss no lamb -2.865942 time 2020-06-27 15:41:57.450770
Model ind 665 epoch 1641 batch: 600 avg loss -2.868172 avg loss no lamb -2.868172 time 2020-06-27 15:42:08.279369
Model ind 665 epoch 1641 batch: 700 avg loss -2.811468 avg loss no lamb -2.811468 time 2020-06-27 15:42:18.896444
Model ind 665 epoch 1641 batch: 800 avg loss -2.852622 avg loss no lamb -2.852622 time 2020-06-27 15:42:29.682860
last batch sz 10
Pre: time 2020-06-27 15:42:43.758866: 
 	std: 0.0032446806
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9819, 0.9805, 0.9743, 0.9818, 0.9755]
	train_accs: [0.98191667, 0.98105, 0.97571665, 0.98176664, 0.97676665]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.9788
	best: 0.9819

Starting e_i: 1642
Model ind 665 epoch 1642 batch: 0 avg loss -2.997158 avg loss no lamb -2.997158 time 2020-06-27 15:42:45.095236
Model ind 665 epoch 1642 batch: 100 avg loss -2.964588 avg loss no lamb -2.964588 time 2020-06-27 15:42:55.900742
Model ind 665 epoch 1642 batch: 200 avg loss -2.876032 avg loss no lamb -2.876032 time 2020-06-27 15:43:06.922660
Model ind 665 epoch 1642 batch: 300 avg loss -2.862358 avg loss no lamb -2.862358 time 2020-06-27 15:43:17.499735
Model ind 665 epoch 1642 batch: 400 avg loss -2.813870 avg loss no lamb -2.813870 time 2020-06-27 15:43:28.095802
Model ind 665 epoch 1642 batch: 500 avg loss -2.905510 avg loss no lamb -2.905510 time 2020-06-27 15:43:38.952791
Model ind 665 epoch 1642 batch: 600 avg loss -2.948535 avg loss no lamb -2.948535 time 2020-06-27 15:43:49.663259
Model ind 665 epoch 1642 batch: 700 avg loss -2.817411 avg loss no lamb -2.817411 time 2020-06-27 15:44:00.572969
Model ind 665 epoch 1642 batch: 800 avg loss -2.929332 avg loss no lamb -2.929332 time 2020-06-27 15:44:11.217254
last batch sz 10
Pre: time 2020-06-27 15:44:25.008246: 
 	std: 0.0025429237
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.98, 0.9748, 0.9801, 0.9747]
	train_accs: [0.981, 0.9802667, 0.9754, 0.98111665, 0.9761]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.9778601
	best: 0.9801

Starting e_i: 1643
Model ind 665 epoch 1643 batch: 0 avg loss -2.994272 avg loss no lamb -2.994272 time 2020-06-27 15:44:26.304395
Model ind 665 epoch 1643 batch: 100 avg loss -2.930655 avg loss no lamb -2.930655 time 2020-06-27 15:44:36.860987
Model ind 665 epoch 1643 batch: 200 avg loss -2.879335 avg loss no lamb -2.879335 time 2020-06-27 15:44:47.693830
Model ind 665 epoch 1643 batch: 300 avg loss -2.898287 avg loss no lamb -2.898287 time 2020-06-27 15:44:58.289604
Model ind 665 epoch 1643 batch: 400 avg loss -2.829083 avg loss no lamb -2.829083 time 2020-06-27 15:45:08.965715
Model ind 665 epoch 1643 batch: 500 avg loss -2.868512 avg loss no lamb -2.868512 time 2020-06-27 15:45:19.826261
Model ind 665 epoch 1643 batch: 600 avg loss -2.875481 avg loss no lamb -2.875481 time 2020-06-27 15:45:30.418844
Model ind 665 epoch 1643 batch: 700 avg loss -2.751052 avg loss no lamb -2.751052 time 2020-06-27 15:45:41.051356
Model ind 665 epoch 1643 batch: 800 avg loss -2.884856 avg loss no lamb -2.884856 time 2020-06-27 15:45:51.725801
last batch sz 10
Pre: time 2020-06-27 15:46:05.871198: 
 	std: 0.0027594268
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9808, 0.9748, 0.9803, 0.9754]
	train_accs: [0.98111665, 0.98001665, 0.9749333, 0.9809833, 0.9756]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97845995
	best: 0.981

Starting e_i: 1644
Model ind 665 epoch 1644 batch: 0 avg loss -2.997474 avg loss no lamb -2.997474 time 2020-06-27 15:46:07.179862
Model ind 665 epoch 1644 batch: 100 avg loss -2.908901 avg loss no lamb -2.908901 time 2020-06-27 15:46:17.833833
Model ind 665 epoch 1644 batch: 200 avg loss -2.875732 avg loss no lamb -2.875732 time 2020-06-27 15:46:28.425003
Model ind 665 epoch 1644 batch: 300 avg loss -2.890611 avg loss no lamb -2.890611 time 2020-06-27 15:46:39.003774
Model ind 665 epoch 1644 batch: 400 avg loss -2.835533 avg loss no lamb -2.835533 time 2020-06-27 15:46:49.555571
Model ind 665 epoch 1644 batch: 500 avg loss -2.882459 avg loss no lamb -2.882459 time 2020-06-27 15:47:00.154682
Model ind 665 epoch 1644 batch: 600 avg loss -2.911131 avg loss no lamb -2.911131 time 2020-06-27 15:47:10.894195
Model ind 665 epoch 1644 batch: 700 avg loss -2.769485 avg loss no lamb -2.769485 time 2020-06-27 15:47:21.706938
Model ind 665 epoch 1644 batch: 800 avg loss -2.838564 avg loss no lamb -2.838564 time 2020-06-27 15:47:32.393293
last batch sz 10
Pre: time 2020-06-27 15:47:46.231575: 
 	std: 0.0028442214
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9787, 0.973, 0.9791, 0.9735]
	train_accs: [0.9805, 0.97966665, 0.97478336, 0.9805833, 0.97456664]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97672004
	best: 0.9791

Starting e_i: 1645
Model ind 665 epoch 1645 batch: 0 avg loss -2.935147 avg loss no lamb -2.935147 time 2020-06-27 15:47:47.520407
Model ind 665 epoch 1645 batch: 100 avg loss -2.920404 avg loss no lamb -2.920404 time 2020-06-27 15:47:58.117479
Model ind 665 epoch 1645 batch: 200 avg loss -2.906518 avg loss no lamb -2.906518 time 2020-06-27 15:48:08.544687
Model ind 665 epoch 1645 batch: 300 avg loss -2.901155 avg loss no lamb -2.901155 time 2020-06-27 15:48:19.204605
Model ind 665 epoch 1645 batch: 400 avg loss -2.831442 avg loss no lamb -2.831442 time 2020-06-27 15:48:29.858285
Model ind 665 epoch 1645 batch: 500 avg loss -2.864235 avg loss no lamb -2.864235 time 2020-06-27 15:48:40.510286
Model ind 665 epoch 1645 batch: 600 avg loss -2.887803 avg loss no lamb -2.887803 time 2020-06-27 15:48:51.329858
Model ind 665 epoch 1645 batch: 700 avg loss -2.847611 avg loss no lamb -2.847611 time 2020-06-27 15:49:02.168300
Model ind 665 epoch 1645 batch: 800 avg loss -2.848275 avg loss no lamb -2.848275 time 2020-06-27 15:49:12.884075
last batch sz 10
Pre: time 2020-06-27 15:49:26.662257: 
 	std: 0.0021885114
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9801, 0.9756, 0.98, 0.9759]
	train_accs: [0.9817, 0.98075, 0.97601664, 0.98158336, 0.9770833]
	best_train_sub_head: 0
	worst: 0.9756
	avg: 0.97841996
	best: 0.9805

Starting e_i: 1646
Model ind 665 epoch 1646 batch: 0 avg loss -2.945159 avg loss no lamb -2.945159 time 2020-06-27 15:49:28.183960
Model ind 665 epoch 1646 batch: 100 avg loss -2.929963 avg loss no lamb -2.929963 time 2020-06-27 15:49:38.721785
Model ind 665 epoch 1646 batch: 200 avg loss -2.873446 avg loss no lamb -2.873446 time 2020-06-27 15:49:49.544822
Model ind 665 epoch 1646 batch: 300 avg loss -2.824452 avg loss no lamb -2.824452 time 2020-06-27 15:50:00.352010
Model ind 665 epoch 1646 batch: 400 avg loss -2.800623 avg loss no lamb -2.800623 time 2020-06-27 15:50:11.226350
Model ind 665 epoch 1646 batch: 500 avg loss -2.868464 avg loss no lamb -2.868464 time 2020-06-27 15:50:21.903118
Model ind 665 epoch 1646 batch: 600 avg loss -2.878852 avg loss no lamb -2.878852 time 2020-06-27 15:50:32.616520
Model ind 665 epoch 1646 batch: 700 avg loss -2.765455 avg loss no lamb -2.765455 time 2020-06-27 15:50:43.314618
Model ind 665 epoch 1646 batch: 800 avg loss -2.773713 avg loss no lamb -2.773713 time 2020-06-27 15:50:53.950078
last batch sz 10
Pre: time 2020-06-27 15:51:07.727488: 
 	std: 0.0028793134
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9804, 0.9744, 0.9797, 0.9739]
	train_accs: [0.9813167, 0.98081666, 0.97566664, 0.9812, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97766
	best: 0.9799

Starting e_i: 1647
Model ind 665 epoch 1647 batch: 0 avg loss -2.986182 avg loss no lamb -2.986182 time 2020-06-27 15:51:09.041520
Model ind 665 epoch 1647 batch: 100 avg loss -2.890580 avg loss no lamb -2.890580 time 2020-06-27 15:51:19.685266
Model ind 665 epoch 1647 batch: 200 avg loss -2.919354 avg loss no lamb -2.919354 time 2020-06-27 15:51:30.398861
Model ind 665 epoch 1647 batch: 300 avg loss -2.887685 avg loss no lamb -2.887685 time 2020-06-27 15:51:41.352903
Model ind 665 epoch 1647 batch: 400 avg loss -2.836135 avg loss no lamb -2.836135 time 2020-06-27 15:51:52.120249
Model ind 665 epoch 1647 batch: 500 avg loss -2.873663 avg loss no lamb -2.873663 time 2020-06-27 15:52:02.844234
Model ind 665 epoch 1647 batch: 600 avg loss -2.887177 avg loss no lamb -2.887177 time 2020-06-27 15:52:13.646060
Model ind 665 epoch 1647 batch: 700 avg loss -2.845887 avg loss no lamb -2.845887 time 2020-06-27 15:52:24.318318
Model ind 665 epoch 1647 batch: 800 avg loss -2.833422 avg loss no lamb -2.833422 time 2020-06-27 15:52:35.025508
last batch sz 10
Pre: time 2020-06-27 15:52:48.801120: 
 	std: 0.0031378965
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9803, 0.9739, 0.9803, 0.9741]
	train_accs: [0.9814, 0.9806, 0.97566664, 0.98115, 0.9759]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97784007
	best: 0.9806

Starting e_i: 1648
Model ind 665 epoch 1648 batch: 0 avg loss -2.989320 avg loss no lamb -2.989320 time 2020-06-27 15:52:50.321568
Model ind 665 epoch 1648 batch: 100 avg loss -2.869840 avg loss no lamb -2.869840 time 2020-06-27 15:53:01.121474
Model ind 665 epoch 1648 batch: 200 avg loss -2.901150 avg loss no lamb -2.901150 time 2020-06-27 15:53:11.858557
Model ind 665 epoch 1648 batch: 300 avg loss -2.887052 avg loss no lamb -2.887052 time 2020-06-27 15:53:22.650473
Model ind 665 epoch 1648 batch: 400 avg loss -2.781899 avg loss no lamb -2.781899 time 2020-06-27 15:53:33.383746
Model ind 665 epoch 1648 batch: 500 avg loss -2.897024 avg loss no lamb -2.897024 time 2020-06-27 15:53:43.931737
Model ind 665 epoch 1648 batch: 600 avg loss -2.880167 avg loss no lamb -2.880167 time 2020-06-27 15:53:54.641010
Model ind 665 epoch 1648 batch: 700 avg loss -2.810983 avg loss no lamb -2.810983 time 2020-06-27 15:54:05.417444
Model ind 665 epoch 1648 batch: 800 avg loss -2.904678 avg loss no lamb -2.904678 time 2020-06-27 15:54:16.399066
last batch sz 10
Pre: time 2020-06-27 15:54:30.317299: 
 	std: 0.0030741452
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9793, 0.9728, 0.9798, 0.9738]
	train_accs: [0.98106664, 0.98035, 0.97431666, 0.9809667, 0.9755667]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97704
	best: 0.9795

Starting e_i: 1649
Model ind 665 epoch 1649 batch: 0 avg loss -2.954492 avg loss no lamb -2.954492 time 2020-06-27 15:54:31.673608
Model ind 665 epoch 1649 batch: 100 avg loss -2.916645 avg loss no lamb -2.916645 time 2020-06-27 15:54:42.423875
Model ind 665 epoch 1649 batch: 200 avg loss -2.899400 avg loss no lamb -2.899400 time 2020-06-27 15:54:53.145078
Model ind 665 epoch 1649 batch: 300 avg loss -2.818057 avg loss no lamb -2.818057 time 2020-06-27 15:55:03.814063
Model ind 665 epoch 1649 batch: 400 avg loss -2.766309 avg loss no lamb -2.766309 time 2020-06-27 15:55:14.483725
Model ind 665 epoch 1649 batch: 500 avg loss -2.898999 avg loss no lamb -2.898999 time 2020-06-27 15:55:25.233125
Model ind 665 epoch 1649 batch: 600 avg loss -2.860392 avg loss no lamb -2.860392 time 2020-06-27 15:55:35.716860
Model ind 665 epoch 1649 batch: 700 avg loss -2.773283 avg loss no lamb -2.773283 time 2020-06-27 15:55:46.375971
Model ind 665 epoch 1649 batch: 800 avg loss -2.801684 avg loss no lamb -2.801684 time 2020-06-27 15:55:56.993091
last batch sz 10
Pre: time 2020-06-27 15:56:10.636308: 
 	std: 0.00249687
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.9774, 0.9739, 0.9788, 0.9731]
	train_accs: [0.98036665, 0.97925, 0.9744667, 0.98015, 0.9752167]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.9764601
	best: 0.9791

Starting e_i: 1650
Model ind 665 epoch 1650 batch: 0 avg loss -2.958390 avg loss no lamb -2.958390 time 2020-06-27 15:56:11.979040
Model ind 665 epoch 1650 batch: 100 avg loss -2.939355 avg loss no lamb -2.939355 time 2020-06-27 15:56:22.625176
Model ind 665 epoch 1650 batch: 200 avg loss -2.906772 avg loss no lamb -2.906772 time 2020-06-27 15:56:33.375098
Model ind 665 epoch 1650 batch: 300 avg loss -2.910269 avg loss no lamb -2.910269 time 2020-06-27 15:56:44.155041
Model ind 665 epoch 1650 batch: 400 avg loss -2.826801 avg loss no lamb -2.826801 time 2020-06-27 15:56:54.812142
Model ind 665 epoch 1650 batch: 500 avg loss -2.853088 avg loss no lamb -2.853088 time 2020-06-27 15:57:05.753022
Model ind 665 epoch 1650 batch: 600 avg loss -2.930552 avg loss no lamb -2.930552 time 2020-06-27 15:57:16.336334
Model ind 665 epoch 1650 batch: 700 avg loss -2.816114 avg loss no lamb -2.816114 time 2020-06-27 15:57:26.957458
Model ind 665 epoch 1650 batch: 800 avg loss -2.913304 avg loss no lamb -2.913304 time 2020-06-27 15:57:37.763518
last batch sz 10
Pre: time 2020-06-27 15:57:51.977369: 
 	std: 0.0031580979
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9784, 0.9794, 0.9731, 0.9788, 0.9719]
	train_accs: [0.98035, 0.98015, 0.97501665, 0.98036665, 0.9748]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97631997
	best: 0.9788

Starting e_i: 1651
Model ind 665 epoch 1651 batch: 0 avg loss -2.961418 avg loss no lamb -2.961418 time 2020-06-27 15:57:54.515841
Model ind 665 epoch 1651 batch: 100 avg loss -2.966488 avg loss no lamb -2.966488 time 2020-06-27 15:58:05.169790
Model ind 665 epoch 1651 batch: 200 avg loss -2.883793 avg loss no lamb -2.883793 time 2020-06-27 15:58:15.770112
Model ind 665 epoch 1651 batch: 300 avg loss -2.911797 avg loss no lamb -2.911797 time 2020-06-27 15:58:26.397731
Model ind 665 epoch 1651 batch: 400 avg loss -2.832125 avg loss no lamb -2.832125 time 2020-06-27 15:58:36.974993
Model ind 665 epoch 1651 batch: 500 avg loss -2.858704 avg loss no lamb -2.858704 time 2020-06-27 15:58:47.623624
Model ind 665 epoch 1651 batch: 600 avg loss -2.952206 avg loss no lamb -2.952206 time 2020-06-27 15:58:58.382668
Model ind 665 epoch 1651 batch: 700 avg loss -2.799923 avg loss no lamb -2.799923 time 2020-06-27 15:59:09.158605
Model ind 665 epoch 1651 batch: 800 avg loss -2.867769 avg loss no lamb -2.867769 time 2020-06-27 15:59:19.876772
last batch sz 10
Pre: time 2020-06-27 15:59:33.719416: 
 	std: 0.0027117443
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9788, 0.9732, 0.9788, 0.9732]
	train_accs: [0.98083335, 0.9799333, 0.97431666, 0.98085, 0.9755333]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97652006
	best: 0.9788

Starting e_i: 1652
Model ind 665 epoch 1652 batch: 0 avg loss -3.004669 avg loss no lamb -3.004669 time 2020-06-27 15:59:35.035874
Model ind 665 epoch 1652 batch: 100 avg loss -2.948901 avg loss no lamb -2.948901 time 2020-06-27 15:59:45.730700
Model ind 665 epoch 1652 batch: 200 avg loss -2.914157 avg loss no lamb -2.914157 time 2020-06-27 15:59:56.473599
Model ind 665 epoch 1652 batch: 300 avg loss -2.850225 avg loss no lamb -2.850225 time 2020-06-27 16:00:07.414415
Model ind 665 epoch 1652 batch: 400 avg loss -2.804160 avg loss no lamb -2.804160 time 2020-06-27 16:00:18.323380
Model ind 665 epoch 1652 batch: 500 avg loss -2.922390 avg loss no lamb -2.922390 time 2020-06-27 16:00:28.836003
Model ind 665 epoch 1652 batch: 600 avg loss -2.906327 avg loss no lamb -2.906327 time 2020-06-27 16:00:39.430044
Model ind 665 epoch 1652 batch: 700 avg loss -2.796417 avg loss no lamb -2.796417 time 2020-06-27 16:00:50.026626
Model ind 665 epoch 1652 batch: 800 avg loss -2.858636 avg loss no lamb -2.858636 time 2020-06-27 16:01:00.812157
last batch sz 10
Pre: time 2020-06-27 16:01:14.762498: 
 	std: 0.0028617468
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9797, 0.9742, 0.9802, 0.9746]
	train_accs: [0.98158336, 0.98066664, 0.97583336, 0.9816333, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97788
	best: 0.9802

Starting e_i: 1653
Model ind 665 epoch 1653 batch: 0 avg loss -2.945543 avg loss no lamb -2.945543 time 2020-06-27 16:01:16.368192
Model ind 665 epoch 1653 batch: 100 avg loss -2.896617 avg loss no lamb -2.896617 time 2020-06-27 16:01:27.084619
Model ind 665 epoch 1653 batch: 200 avg loss -2.848864 avg loss no lamb -2.848864 time 2020-06-27 16:01:37.943548
Model ind 665 epoch 1653 batch: 300 avg loss -2.891554 avg loss no lamb -2.891554 time 2020-06-27 16:01:48.956590
Model ind 665 epoch 1653 batch: 400 avg loss -2.817290 avg loss no lamb -2.817290 time 2020-06-27 16:01:59.806430
Model ind 665 epoch 1653 batch: 500 avg loss -2.859080 avg loss no lamb -2.859080 time 2020-06-27 16:02:10.590813
Model ind 665 epoch 1653 batch: 600 avg loss -2.875258 avg loss no lamb -2.875258 time 2020-06-27 16:02:21.359563
Model ind 665 epoch 1653 batch: 700 avg loss -2.840186 avg loss no lamb -2.840186 time 2020-06-27 16:02:32.048258
Model ind 665 epoch 1653 batch: 800 avg loss -2.873575 avg loss no lamb -2.873575 time 2020-06-27 16:02:42.746772
last batch sz 10
Pre: time 2020-06-27 16:02:56.691680: 
 	std: 0.0025732517
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9802, 0.9752, 0.9797, 0.9745]
	train_accs: [0.9817167, 0.9811, 0.9758, 0.98183334, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97798
	best: 0.9797

Starting e_i: 1654
Model ind 665 epoch 1654 batch: 0 avg loss -3.000844 avg loss no lamb -3.000844 time 2020-06-27 16:02:58.089166
Model ind 665 epoch 1654 batch: 100 avg loss -2.884937 avg loss no lamb -2.884937 time 2020-06-27 16:03:08.933458
Model ind 665 epoch 1654 batch: 200 avg loss -2.900914 avg loss no lamb -2.900914 time 2020-06-27 16:03:19.789488
Model ind 665 epoch 1654 batch: 300 avg loss -2.899927 avg loss no lamb -2.899927 time 2020-06-27 16:03:30.558924
Model ind 665 epoch 1654 batch: 400 avg loss -2.834015 avg loss no lamb -2.834015 time 2020-06-27 16:03:41.607231
Model ind 665 epoch 1654 batch: 500 avg loss -2.843776 avg loss no lamb -2.843776 time 2020-06-27 16:03:52.356914
Model ind 665 epoch 1654 batch: 600 avg loss -2.936445 avg loss no lamb -2.936445 time 2020-06-27 16:04:03.430684
Model ind 665 epoch 1654 batch: 700 avg loss -2.793751 avg loss no lamb -2.793751 time 2020-06-27 16:04:14.019019
Model ind 665 epoch 1654 batch: 800 avg loss -2.931798 avg loss no lamb -2.931798 time 2020-06-27 16:04:24.843310
last batch sz 10
Pre: time 2020-06-27 16:04:38.593274: 
 	std: 0.0022392925
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9801, 0.9751, 0.9795, 0.9758]
	train_accs: [0.9809833, 0.98066664, 0.97578335, 0.98108333, 0.97685]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97816
	best: 0.9795

Starting e_i: 1655
Model ind 665 epoch 1655 batch: 0 avg loss -2.920162 avg loss no lamb -2.920162 time 2020-06-27 16:04:39.935867
Model ind 665 epoch 1655 batch: 100 avg loss -2.955043 avg loss no lamb -2.955043 time 2020-06-27 16:04:50.636654
Model ind 665 epoch 1655 batch: 200 avg loss -2.905066 avg loss no lamb -2.905066 time 2020-06-27 16:05:01.046789
Model ind 665 epoch 1655 batch: 300 avg loss -2.843187 avg loss no lamb -2.843187 time 2020-06-27 16:05:11.846567
Model ind 665 epoch 1655 batch: 400 avg loss -2.818842 avg loss no lamb -2.818842 time 2020-06-27 16:05:22.616704
Model ind 665 epoch 1655 batch: 500 avg loss -2.847025 avg loss no lamb -2.847025 time 2020-06-27 16:05:33.335866
Model ind 665 epoch 1655 batch: 600 avg loss -2.929228 avg loss no lamb -2.929228 time 2020-06-27 16:05:44.018951
Model ind 665 epoch 1655 batch: 700 avg loss -2.735290 avg loss no lamb -2.735290 time 2020-06-27 16:05:54.585775
Model ind 665 epoch 1655 batch: 800 avg loss -2.886848 avg loss no lamb -2.886848 time 2020-06-27 16:06:05.465624
last batch sz 10
Pre: time 2020-06-27 16:06:19.661379: 
 	std: 0.0024303137
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9806, 0.9751, 0.9806, 0.9763]
	train_accs: [0.9813333, 0.98045, 0.97573334, 0.9816, 0.97685]
	best_train_sub_head: 3
	worst: 0.9751
	avg: 0.97863996
	best: 0.9806

Starting e_i: 1656
Model ind 665 epoch 1656 batch: 0 avg loss -2.948758 avg loss no lamb -2.948758 time 2020-06-27 16:06:21.000678
Model ind 665 epoch 1656 batch: 100 avg loss -2.929507 avg loss no lamb -2.929507 time 2020-06-27 16:06:31.460734
Model ind 665 epoch 1656 batch: 200 avg loss -2.852090 avg loss no lamb -2.852090 time 2020-06-27 16:06:41.926261
Model ind 665 epoch 1656 batch: 300 avg loss -2.884024 avg loss no lamb -2.884024 time 2020-06-27 16:06:52.534529
Model ind 665 epoch 1656 batch: 400 avg loss -2.815606 avg loss no lamb -2.815606 time 2020-06-27 16:07:02.935412
Model ind 665 epoch 1656 batch: 500 avg loss -2.836817 avg loss no lamb -2.836817 time 2020-06-27 16:07:13.668355
Model ind 665 epoch 1656 batch: 600 avg loss -2.925677 avg loss no lamb -2.925677 time 2020-06-27 16:07:24.346163
Model ind 665 epoch 1656 batch: 700 avg loss -2.822055 avg loss no lamb -2.822055 time 2020-06-27 16:07:35.119080
Model ind 665 epoch 1656 batch: 800 avg loss -2.835455 avg loss no lamb -2.835455 time 2020-06-27 16:07:45.661968
last batch sz 10
Pre: time 2020-06-27 16:07:59.576313: 
 	std: 0.0031205053
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9794, 0.9732, 0.9804, 0.9742]
	train_accs: [0.9812, 0.9799167, 0.9749, 0.98075, 0.9758]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97748005
	best: 0.9802

Starting e_i: 1657
Model ind 665 epoch 1657 batch: 0 avg loss -2.967935 avg loss no lamb -2.967935 time 2020-06-27 16:08:01.004826
Model ind 665 epoch 1657 batch: 100 avg loss -2.947408 avg loss no lamb -2.947408 time 2020-06-27 16:08:11.752537
Model ind 665 epoch 1657 batch: 200 avg loss -2.910374 avg loss no lamb -2.910374 time 2020-06-27 16:08:22.613266
Model ind 665 epoch 1657 batch: 300 avg loss -2.882033 avg loss no lamb -2.882033 time 2020-06-27 16:08:33.259410
Model ind 665 epoch 1657 batch: 400 avg loss -2.808698 avg loss no lamb -2.808698 time 2020-06-27 16:08:44.061120
Model ind 665 epoch 1657 batch: 500 avg loss -2.835832 avg loss no lamb -2.835832 time 2020-06-27 16:08:54.659830
Model ind 665 epoch 1657 batch: 600 avg loss -2.893227 avg loss no lamb -2.893227 time 2020-06-27 16:09:05.490037
Model ind 665 epoch 1657 batch: 700 avg loss -2.800046 avg loss no lamb -2.800046 time 2020-06-27 16:09:16.255350
Model ind 665 epoch 1657 batch: 800 avg loss -2.831622 avg loss no lamb -2.831622 time 2020-06-27 16:09:26.916053
last batch sz 10
Pre: time 2020-06-27 16:09:41.286451: 
 	std: 0.0033601369
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9797, 0.9736, 0.9799, 0.9729]
	train_accs: [0.9812, 0.98055, 0.97475, 0.98095, 0.97535]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.9773399
	best: 0.9806

Starting e_i: 1658
Model ind 665 epoch 1658 batch: 0 avg loss -2.948023 avg loss no lamb -2.948023 time 2020-06-27 16:09:42.588522
Model ind 665 epoch 1658 batch: 100 avg loss -2.983387 avg loss no lamb -2.983387 time 2020-06-27 16:09:53.421259
Model ind 665 epoch 1658 batch: 200 avg loss -2.899528 avg loss no lamb -2.899528 time 2020-06-27 16:10:04.075571
Model ind 665 epoch 1658 batch: 300 avg loss -2.893919 avg loss no lamb -2.893919 time 2020-06-27 16:10:14.706385
Model ind 665 epoch 1658 batch: 400 avg loss -2.837503 avg loss no lamb -2.837503 time 2020-06-27 16:10:25.275919
Model ind 665 epoch 1658 batch: 500 avg loss -2.871953 avg loss no lamb -2.871953 time 2020-06-27 16:10:36.041022
Model ind 665 epoch 1658 batch: 600 avg loss -2.909387 avg loss no lamb -2.909387 time 2020-06-27 16:10:46.899517
Model ind 665 epoch 1658 batch: 700 avg loss -2.796585 avg loss no lamb -2.796585 time 2020-06-27 16:10:57.480756
Model ind 665 epoch 1658 batch: 800 avg loss -2.920034 avg loss no lamb -2.920034 time 2020-06-27 16:11:08.261610
last batch sz 10
Pre: time 2020-06-27 16:11:22.294256: 
 	std: 0.002915478
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9794, 0.9727, 0.9793, 0.9745]
	train_accs: [0.98083335, 0.9803, 0.97485, 0.98095, 0.9759667]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.9771
	best: 0.9793

Starting e_i: 1659
Model ind 665 epoch 1659 batch: 0 avg loss -2.999034 avg loss no lamb -2.999034 time 2020-06-27 16:11:23.574804
Model ind 665 epoch 1659 batch: 100 avg loss -2.933006 avg loss no lamb -2.933006 time 2020-06-27 16:11:34.148367
Model ind 665 epoch 1659 batch: 200 avg loss -2.829371 avg loss no lamb -2.829371 time 2020-06-27 16:11:44.845386
Model ind 665 epoch 1659 batch: 300 avg loss -2.894816 avg loss no lamb -2.894816 time 2020-06-27 16:11:55.380616
Model ind 665 epoch 1659 batch: 400 avg loss -2.779964 avg loss no lamb -2.779964 time 2020-06-27 16:12:06.225849
Model ind 665 epoch 1659 batch: 500 avg loss -2.888602 avg loss no lamb -2.888602 time 2020-06-27 16:12:16.894983
Model ind 665 epoch 1659 batch: 600 avg loss -2.903115 avg loss no lamb -2.903115 time 2020-06-27 16:12:27.528521
Model ind 665 epoch 1659 batch: 700 avg loss -2.780117 avg loss no lamb -2.780117 time 2020-06-27 16:12:38.279405
Model ind 665 epoch 1659 batch: 800 avg loss -2.883298 avg loss no lamb -2.883298 time 2020-06-27 16:12:48.781761
last batch sz 10
Pre: time 2020-06-27 16:13:02.575190: 
 	std: 0.0035651517
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9801, 0.9725, 0.98, 0.9731]
	train_accs: [0.98153335, 0.98053336, 0.9748167, 0.9814, 0.9758667]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97716
	best: 0.9801

Starting e_i: 1660
Model ind 665 epoch 1660 batch: 0 avg loss -2.926675 avg loss no lamb -2.926675 time 2020-06-27 16:13:04.120175
Model ind 665 epoch 1660 batch: 100 avg loss -2.977260 avg loss no lamb -2.977260 time 2020-06-27 16:13:14.740210
Model ind 665 epoch 1660 batch: 200 avg loss -2.902263 avg loss no lamb -2.902263 time 2020-06-27 16:13:25.660939
Model ind 665 epoch 1660 batch: 300 avg loss -2.872385 avg loss no lamb -2.872385 time 2020-06-27 16:13:36.389750
Model ind 665 epoch 1660 batch: 400 avg loss -2.771639 avg loss no lamb -2.771639 time 2020-06-27 16:13:46.901771
Model ind 665 epoch 1660 batch: 500 avg loss -2.814322 avg loss no lamb -2.814322 time 2020-06-27 16:13:57.740883
Model ind 665 epoch 1660 batch: 600 avg loss -2.922093 avg loss no lamb -2.922093 time 2020-06-27 16:14:08.415112
Model ind 665 epoch 1660 batch: 700 avg loss -2.778634 avg loss no lamb -2.778634 time 2020-06-27 16:14:19.318711
Model ind 665 epoch 1660 batch: 800 avg loss -2.953615 avg loss no lamb -2.953615 time 2020-06-27 16:14:30.295352
last batch sz 10
Pre: time 2020-06-27 16:14:44.416848: 
 	std: 0.0039565437
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.979, 0.971, 0.9794, 0.9714]
	train_accs: [0.9808, 0.98041666, 0.9737333, 0.98141664, 0.974]
	best_train_sub_head: 3
	worst: 0.971
	avg: 0.97604
	best: 0.9794

Starting e_i: 1661
Model ind 665 epoch 1661 batch: 0 avg loss -2.946576 avg loss no lamb -2.946576 time 2020-06-27 16:14:47.005078
Model ind 665 epoch 1661 batch: 100 avg loss -2.875041 avg loss no lamb -2.875041 time 2020-06-27 16:14:57.730264
Model ind 665 epoch 1661 batch: 200 avg loss -2.887841 avg loss no lamb -2.887841 time 2020-06-27 16:15:08.407102
Model ind 665 epoch 1661 batch: 300 avg loss -2.840629 avg loss no lamb -2.840629 time 2020-06-27 16:15:19.077684
Model ind 665 epoch 1661 batch: 400 avg loss -2.803923 avg loss no lamb -2.803923 time 2020-06-27 16:15:29.876447
Model ind 665 epoch 1661 batch: 500 avg loss -2.878030 avg loss no lamb -2.878030 time 2020-06-27 16:15:40.309467
Model ind 665 epoch 1661 batch: 600 avg loss -2.895650 avg loss no lamb -2.895650 time 2020-06-27 16:15:50.822419
Model ind 665 epoch 1661 batch: 700 avg loss -2.851844 avg loss no lamb -2.851844 time 2020-06-27 16:16:01.731190
Model ind 665 epoch 1661 batch: 800 avg loss -2.913696 avg loss no lamb -2.913696 time 2020-06-27 16:16:12.419404
last batch sz 10
Pre: time 2020-06-27 16:16:26.263486: 
 	std: 0.002446706
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9809, 0.976, 0.9818, 0.9768]
	train_accs: [0.98185, 0.9813333, 0.97658336, 0.98205, 0.9772]
	best_train_sub_head: 3
	worst: 0.976
	avg: 0.97936
	best: 0.9818

Starting e_i: 1662
Model ind 665 epoch 1662 batch: 0 avg loss -2.937673 avg loss no lamb -2.937673 time 2020-06-27 16:16:27.673797
Model ind 665 epoch 1662 batch: 100 avg loss -2.940535 avg loss no lamb -2.940535 time 2020-06-27 16:16:38.418428
Model ind 665 epoch 1662 batch: 200 avg loss -2.919274 avg loss no lamb -2.919274 time 2020-06-27 16:16:48.957112
Model ind 665 epoch 1662 batch: 300 avg loss -2.833919 avg loss no lamb -2.833919 time 2020-06-27 16:16:59.764119
Model ind 665 epoch 1662 batch: 400 avg loss -2.786163 avg loss no lamb -2.786163 time 2020-06-27 16:17:10.783703
Model ind 665 epoch 1662 batch: 500 avg loss -2.827352 avg loss no lamb -2.827352 time 2020-06-27 16:17:22.193675
Model ind 665 epoch 1662 batch: 600 avg loss -2.912177 avg loss no lamb -2.912177 time 2020-06-27 16:17:33.100055
Model ind 665 epoch 1662 batch: 700 avg loss -2.817406 avg loss no lamb -2.817406 time 2020-06-27 16:17:43.908464
Model ind 665 epoch 1662 batch: 800 avg loss -2.881770 avg loss no lamb -2.881770 time 2020-06-27 16:17:54.628836
last batch sz 10
Pre: time 2020-06-27 16:18:09.578532: 
 	std: 0.0028540604
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9797, 0.9736, 0.9794, 0.9742]
	train_accs: [0.9812, 0.98036665, 0.97508335, 0.98085, 0.97646666]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97738
	best: 0.98

Starting e_i: 1663
Model ind 665 epoch 1663 batch: 0 avg loss -2.966107 avg loss no lamb -2.966107 time 2020-06-27 16:18:11.062339
Model ind 665 epoch 1663 batch: 100 avg loss -2.907023 avg loss no lamb -2.907023 time 2020-06-27 16:18:23.076801
Model ind 665 epoch 1663 batch: 200 avg loss -2.916798 avg loss no lamb -2.916798 time 2020-06-27 16:18:34.584556
Model ind 665 epoch 1663 batch: 300 avg loss -2.878748 avg loss no lamb -2.878748 time 2020-06-27 16:18:46.042966
Model ind 665 epoch 1663 batch: 400 avg loss -2.800346 avg loss no lamb -2.800346 time 2020-06-27 16:18:57.583044
Model ind 665 epoch 1663 batch: 500 avg loss -2.860964 avg loss no lamb -2.860964 time 2020-06-27 16:19:09.064982
Model ind 665 epoch 1663 batch: 600 avg loss -2.845813 avg loss no lamb -2.845813 time 2020-06-27 16:19:20.557987
Model ind 665 epoch 1663 batch: 700 avg loss -2.795202 avg loss no lamb -2.795202 time 2020-06-27 16:19:32.276084
Model ind 665 epoch 1663 batch: 800 avg loss -2.885941 avg loss no lamb -2.885941 time 2020-06-27 16:19:43.707499
last batch sz 10
Pre: time 2020-06-27 16:19:59.099027: 
 	std: 0.00313458
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9794, 0.9729, 0.9794, 0.9734]
	train_accs: [0.9812, 0.98036665, 0.9747667, 0.9811, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.9769799
	best: 0.9798

Starting e_i: 1664
Model ind 665 epoch 1664 batch: 0 avg loss -2.950093 avg loss no lamb -2.950093 time 2020-06-27 16:20:00.582560
Model ind 665 epoch 1664 batch: 100 avg loss -2.907826 avg loss no lamb -2.907826 time 2020-06-27 16:20:13.002215
Model ind 665 epoch 1664 batch: 200 avg loss -2.861722 avg loss no lamb -2.861722 time 2020-06-27 16:20:26.401101
Model ind 665 epoch 1664 batch: 300 avg loss -2.905817 avg loss no lamb -2.905817 time 2020-06-27 16:20:39.016896
Model ind 665 epoch 1664 batch: 400 avg loss -2.847414 avg loss no lamb -2.847414 time 2020-06-27 16:20:51.153235
Model ind 665 epoch 1664 batch: 500 avg loss -2.879727 avg loss no lamb -2.879727 time 2020-06-27 16:21:03.730608
Model ind 665 epoch 1664 batch: 600 avg loss -2.886101 avg loss no lamb -2.886101 time 2020-06-27 16:21:15.246744
Model ind 665 epoch 1664 batch: 700 avg loss -2.790036 avg loss no lamb -2.790036 time 2020-06-27 16:21:26.804570
Model ind 665 epoch 1664 batch: 800 avg loss -2.850607 avg loss no lamb -2.850607 time 2020-06-27 16:21:38.277737
last batch sz 10
Pre: time 2020-06-27 16:21:53.524651: 
 	std: 0.0027440267
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9793, 0.9737, 0.9797, 0.9747]
	train_accs: [0.9806, 0.98003334, 0.97478336, 0.9803, 0.97545]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97752
	best: 0.9802

Starting e_i: 1665
Model ind 665 epoch 1665 batch: 0 avg loss -2.976804 avg loss no lamb -2.976804 time 2020-06-27 16:21:54.984251
Model ind 665 epoch 1665 batch: 100 avg loss -2.904901 avg loss no lamb -2.904901 time 2020-06-27 16:22:06.443258
Model ind 665 epoch 1665 batch: 200 avg loss -2.906503 avg loss no lamb -2.906503 time 2020-06-27 16:22:18.020715
Model ind 665 epoch 1665 batch: 300 avg loss -2.815406 avg loss no lamb -2.815406 time 2020-06-27 16:22:29.524764
Model ind 665 epoch 1665 batch: 400 avg loss -2.801371 avg loss no lamb -2.801371 time 2020-06-27 16:22:41.101584
Model ind 665 epoch 1665 batch: 500 avg loss -2.813926 avg loss no lamb -2.813926 time 2020-06-27 16:22:52.633524
Model ind 665 epoch 1665 batch: 600 avg loss -2.940550 avg loss no lamb -2.940550 time 2020-06-27 16:23:04.256793
Model ind 665 epoch 1665 batch: 700 avg loss -2.760652 avg loss no lamb -2.760652 time 2020-06-27 16:23:15.908504
Model ind 665 epoch 1665 batch: 800 avg loss -2.900761 avg loss no lamb -2.900761 time 2020-06-27 16:23:27.531391
last batch sz 10
Pre: time 2020-06-27 16:23:42.432587: 
 	std: 0.0030142409
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.98, 0.9737, 0.98, 0.9743]
	train_accs: [0.9814, 0.98075, 0.97545, 0.98113334, 0.97601664]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97768
	best: 0.9804

Starting e_i: 1666
Model ind 665 epoch 1666 batch: 0 avg loss -2.944850 avg loss no lamb -2.944850 time 2020-06-27 16:23:43.857539
Model ind 665 epoch 1666 batch: 100 avg loss -2.916504 avg loss no lamb -2.916504 time 2020-06-27 16:23:55.402160
Model ind 665 epoch 1666 batch: 200 avg loss -2.790265 avg loss no lamb -2.790265 time 2020-06-27 16:24:07.036756
Model ind 665 epoch 1666 batch: 300 avg loss -2.862405 avg loss no lamb -2.862405 time 2020-06-27 16:24:18.707939
Model ind 665 epoch 1666 batch: 400 avg loss -2.754767 avg loss no lamb -2.754767 time 2020-06-27 16:24:30.107928
Model ind 665 epoch 1666 batch: 500 avg loss -2.824055 avg loss no lamb -2.824055 time 2020-06-27 16:24:41.531134
Model ind 665 epoch 1666 batch: 600 avg loss -2.929357 avg loss no lamb -2.929357 time 2020-06-27 16:24:53.012059
Model ind 665 epoch 1666 batch: 700 avg loss -2.829736 avg loss no lamb -2.829736 time 2020-06-27 16:25:04.538863
Model ind 665 epoch 1666 batch: 800 avg loss -2.886474 avg loss no lamb -2.886474 time 2020-06-27 16:25:16.257386
last batch sz 10
Pre: time 2020-06-27 16:25:31.017648: 
 	std: 0.002691909
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9795, 0.9751, 0.9808, 0.9749]
	train_accs: [0.9813167, 0.98043334, 0.9759833, 0.98106664, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9749
	avg: 0.97824
	best: 0.9809

Starting e_i: 1667
Model ind 665 epoch 1667 batch: 0 avg loss -2.982577 avg loss no lamb -2.982577 time 2020-06-27 16:25:32.641793
Model ind 665 epoch 1667 batch: 100 avg loss -2.944495 avg loss no lamb -2.944495 time 2020-06-27 16:25:44.273261
Model ind 665 epoch 1667 batch: 200 avg loss -2.904320 avg loss no lamb -2.904320 time 2020-06-27 16:25:55.844383
Model ind 665 epoch 1667 batch: 300 avg loss -2.870664 avg loss no lamb -2.870664 time 2020-06-27 16:26:09.367657
Model ind 665 epoch 1667 batch: 400 avg loss -2.751772 avg loss no lamb -2.751772 time 2020-06-27 16:26:21.340327
Model ind 665 epoch 1667 batch: 500 avg loss -2.860198 avg loss no lamb -2.860198 time 2020-06-27 16:26:32.054341
Model ind 665 epoch 1667 batch: 600 avg loss -2.882266 avg loss no lamb -2.882266 time 2020-06-27 16:26:42.767055
Model ind 665 epoch 1667 batch: 700 avg loss -2.795000 avg loss no lamb -2.795000 time 2020-06-27 16:26:53.296280
Model ind 665 epoch 1667 batch: 800 avg loss -2.869858 avg loss no lamb -2.869858 time 2020-06-27 16:27:04.081983
last batch sz 10
Pre: time 2020-06-27 16:27:18.175019: 
 	std: 0.002960127
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9805, 0.9751, 0.9809, 0.9746]
	train_accs: [0.98153335, 0.98123336, 0.9762, 0.98135, 0.97611666]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97846
	best: 0.9812

Starting e_i: 1668
Model ind 665 epoch 1668 batch: 0 avg loss -2.936068 avg loss no lamb -2.936068 time 2020-06-27 16:27:19.514025
Model ind 665 epoch 1668 batch: 100 avg loss -2.944202 avg loss no lamb -2.944202 time 2020-06-27 16:27:29.948472
Model ind 665 epoch 1668 batch: 200 avg loss -2.907671 avg loss no lamb -2.907671 time 2020-06-27 16:27:40.621488
Model ind 665 epoch 1668 batch: 300 avg loss -2.880563 avg loss no lamb -2.880563 time 2020-06-27 16:27:51.276848
Model ind 665 epoch 1668 batch: 400 avg loss -2.772872 avg loss no lamb -2.772872 time 2020-06-27 16:28:01.892966
Model ind 665 epoch 1668 batch: 500 avg loss -2.876520 avg loss no lamb -2.876520 time 2020-06-27 16:28:12.422224
Model ind 665 epoch 1668 batch: 600 avg loss -2.902696 avg loss no lamb -2.902696 time 2020-06-27 16:28:23.062296
Model ind 665 epoch 1668 batch: 700 avg loss -2.812196 avg loss no lamb -2.812196 time 2020-06-27 16:28:33.552543
Model ind 665 epoch 1668 batch: 800 avg loss -2.857624 avg loss no lamb -2.857624 time 2020-06-27 16:28:44.083030
last batch sz 10
Pre: time 2020-06-27 16:28:58.000003: 
 	std: 0.0034828722
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9796, 0.9737, 0.9801, 0.972]
	train_accs: [0.98118335, 0.98036665, 0.97548336, 0.9812, 0.97471666]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97706
	best: 0.9801

Starting e_i: 1669
Model ind 665 epoch 1669 batch: 0 avg loss -2.950755 avg loss no lamb -2.950755 time 2020-06-27 16:28:59.351634
Model ind 665 epoch 1669 batch: 100 avg loss -2.933559 avg loss no lamb -2.933559 time 2020-06-27 16:29:09.884462
Model ind 665 epoch 1669 batch: 200 avg loss -2.884560 avg loss no lamb -2.884560 time 2020-06-27 16:29:20.470813
Model ind 665 epoch 1669 batch: 300 avg loss -2.886051 avg loss no lamb -2.886051 time 2020-06-27 16:29:31.004535
Model ind 665 epoch 1669 batch: 400 avg loss -2.801196 avg loss no lamb -2.801196 time 2020-06-27 16:29:41.592339
Model ind 665 epoch 1669 batch: 500 avg loss -2.862968 avg loss no lamb -2.862968 time 2020-06-27 16:29:52.267393
Model ind 665 epoch 1669 batch: 600 avg loss -2.922507 avg loss no lamb -2.922507 time 2020-06-27 16:30:02.880063
Model ind 665 epoch 1669 batch: 700 avg loss -2.831639 avg loss no lamb -2.831639 time 2020-06-27 16:30:13.616772
Model ind 665 epoch 1669 batch: 800 avg loss -2.912899 avg loss no lamb -2.912899 time 2020-06-27 16:30:24.038018
last batch sz 10
Pre: time 2020-06-27 16:30:38.206239: 
 	std: 0.002689679
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9802, 0.9741, 0.9802, 0.9749]
	train_accs: [0.98156667, 0.98121667, 0.97625, 0.9816, 0.97671664]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97776
	best: 0.9802

Starting e_i: 1670
Model ind 665 epoch 1670 batch: 0 avg loss -2.982607 avg loss no lamb -2.982607 time 2020-06-27 16:30:39.502961
Model ind 665 epoch 1670 batch: 100 avg loss -2.916319 avg loss no lamb -2.916319 time 2020-06-27 16:30:50.146923
Model ind 665 epoch 1670 batch: 200 avg loss -2.912118 avg loss no lamb -2.912118 time 2020-06-27 16:31:00.904847
Model ind 665 epoch 1670 batch: 300 avg loss -2.870085 avg loss no lamb -2.870085 time 2020-06-27 16:31:11.485438
Model ind 665 epoch 1670 batch: 400 avg loss -2.816327 avg loss no lamb -2.816327 time 2020-06-27 16:31:22.127500
Model ind 665 epoch 1670 batch: 500 avg loss -2.841634 avg loss no lamb -2.841634 time 2020-06-27 16:31:32.745060
Model ind 665 epoch 1670 batch: 600 avg loss -2.874210 avg loss no lamb -2.874210 time 2020-06-27 16:31:43.176251
Model ind 665 epoch 1670 batch: 700 avg loss -2.822637 avg loss no lamb -2.822637 time 2020-06-27 16:31:53.729805
Model ind 665 epoch 1670 batch: 800 avg loss -2.921148 avg loss no lamb -2.921148 time 2020-06-27 16:32:04.458746
last batch sz 10
Pre: time 2020-06-27 16:32:18.216161: 
 	std: 0.0038066246
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9796, 0.9719, 0.9798, 0.9721]
	train_accs: [0.98095, 0.98035, 0.97393334, 0.9810167, 0.97495]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97665995
	best: 0.9798

Starting e_i: 1671
Model ind 665 epoch 1671 batch: 0 avg loss -2.994231 avg loss no lamb -2.994231 time 2020-06-27 16:32:20.737757
Model ind 665 epoch 1671 batch: 100 avg loss -2.877512 avg loss no lamb -2.877512 time 2020-06-27 16:32:31.393780
Model ind 665 epoch 1671 batch: 200 avg loss -2.880582 avg loss no lamb -2.880582 time 2020-06-27 16:32:41.941978
Model ind 665 epoch 1671 batch: 300 avg loss -2.877906 avg loss no lamb -2.877906 time 2020-06-27 16:32:52.507968
Model ind 665 epoch 1671 batch: 400 avg loss -2.803900 avg loss no lamb -2.803900 time 2020-06-27 16:33:03.152043
Model ind 665 epoch 1671 batch: 500 avg loss -2.875463 avg loss no lamb -2.875463 time 2020-06-27 16:33:13.731352
Model ind 665 epoch 1671 batch: 600 avg loss -2.859071 avg loss no lamb -2.859071 time 2020-06-27 16:33:24.385972
Model ind 665 epoch 1671 batch: 700 avg loss -2.777566 avg loss no lamb -2.777566 time 2020-06-27 16:33:35.075570
Model ind 665 epoch 1671 batch: 800 avg loss -2.902782 avg loss no lamb -2.902782 time 2020-06-27 16:33:45.626872
last batch sz 10
Pre: time 2020-06-27 16:33:59.816990: 
 	std: 0.0032228043
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9792, 0.9727, 0.98, 0.974]
	train_accs: [0.9813, 0.98011667, 0.97435, 0.9807, 0.97565]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97723997
	best: 0.9803

Starting e_i: 1672
Model ind 665 epoch 1672 batch: 0 avg loss -2.967963 avg loss no lamb -2.967963 time 2020-06-27 16:34:01.133038
Model ind 665 epoch 1672 batch: 100 avg loss -2.914312 avg loss no lamb -2.914312 time 2020-06-27 16:34:11.744135
Model ind 665 epoch 1672 batch: 200 avg loss -2.889260 avg loss no lamb -2.889260 time 2020-06-27 16:34:22.418363
Model ind 665 epoch 1672 batch: 300 avg loss -2.865852 avg loss no lamb -2.865852 time 2020-06-27 16:34:32.965029
Model ind 665 epoch 1672 batch: 400 avg loss -2.821126 avg loss no lamb -2.821126 time 2020-06-27 16:34:43.658287
Model ind 665 epoch 1672 batch: 500 avg loss -2.841531 avg loss no lamb -2.841531 time 2020-06-27 16:34:54.232927
Model ind 665 epoch 1672 batch: 600 avg loss -2.894403 avg loss no lamb -2.894403 time 2020-06-27 16:35:05.101977
Model ind 665 epoch 1672 batch: 700 avg loss -2.814453 avg loss no lamb -2.814453 time 2020-06-27 16:35:15.782142
Model ind 665 epoch 1672 batch: 800 avg loss -2.899910 avg loss no lamb -2.899910 time 2020-06-27 16:35:26.513759
last batch sz 10
Pre: time 2020-06-27 16:35:40.558499: 
 	std: 0.0038091547
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9814, 0.98, 0.9729, 0.9813, 0.9735]
	train_accs: [0.98118335, 0.98008335, 0.97363335, 0.98088336, 0.9745333]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97782004
	best: 0.9814

Starting e_i: 1673
Model ind 665 epoch 1673 batch: 0 avg loss -2.908712 avg loss no lamb -2.908712 time 2020-06-27 16:35:41.967539
Model ind 665 epoch 1673 batch: 100 avg loss -2.908318 avg loss no lamb -2.908318 time 2020-06-27 16:35:52.820453
Model ind 665 epoch 1673 batch: 200 avg loss -2.894131 avg loss no lamb -2.894131 time 2020-06-27 16:36:03.428878
Model ind 665 epoch 1673 batch: 300 avg loss -2.832353 avg loss no lamb -2.832353 time 2020-06-27 16:36:13.911773
Model ind 665 epoch 1673 batch: 400 avg loss -2.861499 avg loss no lamb -2.861499 time 2020-06-27 16:36:24.610640
Model ind 665 epoch 1673 batch: 500 avg loss -2.818690 avg loss no lamb -2.818690 time 2020-06-27 16:36:35.038302
Model ind 665 epoch 1673 batch: 600 avg loss -2.845594 avg loss no lamb -2.845594 time 2020-06-27 16:36:45.592196
Model ind 665 epoch 1673 batch: 700 avg loss -2.760378 avg loss no lamb -2.760378 time 2020-06-27 16:36:56.354408
Model ind 665 epoch 1673 batch: 800 avg loss -2.951490 avg loss no lamb -2.951490 time 2020-06-27 16:37:07.060699
last batch sz 10
Pre: time 2020-06-27 16:37:20.812654: 
 	std: 0.0030895872
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9788, 0.9732, 0.9792, 0.9734]
	train_accs: [0.9811, 0.97985, 0.97428334, 0.9801667, 0.97505]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.9770201
	best: 0.9805

Starting e_i: 1674
Model ind 665 epoch 1674 batch: 0 avg loss -2.958364 avg loss no lamb -2.958364 time 2020-06-27 16:37:22.367747
Model ind 665 epoch 1674 batch: 100 avg loss -2.931233 avg loss no lamb -2.931233 time 2020-06-27 16:37:33.039234
Model ind 665 epoch 1674 batch: 200 avg loss -2.888844 avg loss no lamb -2.888844 time 2020-06-27 16:37:43.744494
Model ind 665 epoch 1674 batch: 300 avg loss -2.879225 avg loss no lamb -2.879225 time 2020-06-27 16:37:54.474245
Model ind 665 epoch 1674 batch: 400 avg loss -2.750688 avg loss no lamb -2.750688 time 2020-06-27 16:38:05.337075
Model ind 665 epoch 1674 batch: 500 avg loss -2.901624 avg loss no lamb -2.901624 time 2020-06-27 16:38:15.966371
Model ind 665 epoch 1674 batch: 600 avg loss -2.916050 avg loss no lamb -2.916050 time 2020-06-27 16:38:26.622619
Model ind 665 epoch 1674 batch: 700 avg loss -2.871452 avg loss no lamb -2.871452 time 2020-06-27 16:38:37.202226
Model ind 665 epoch 1674 batch: 800 avg loss -2.814445 avg loss no lamb -2.814445 time 2020-06-27 16:38:47.844303
last batch sz 10
Pre: time 2020-06-27 16:39:02.075530: 
 	std: 0.002843242
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9797, 0.9737, 0.9802, 0.9748]
	train_accs: [0.9813167, 0.9801667, 0.97463334, 0.98148334, 0.97585]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97770005
	best: 0.9802

Starting e_i: 1675
Model ind 665 epoch 1675 batch: 0 avg loss -2.966303 avg loss no lamb -2.966303 time 2020-06-27 16:39:03.457514
Model ind 665 epoch 1675 batch: 100 avg loss -2.897792 avg loss no lamb -2.897792 time 2020-06-27 16:39:14.078401
Model ind 665 epoch 1675 batch: 200 avg loss -2.856045 avg loss no lamb -2.856045 time 2020-06-27 16:39:24.501932
Model ind 665 epoch 1675 batch: 300 avg loss -2.878583 avg loss no lamb -2.878583 time 2020-06-27 16:39:35.126157
Model ind 665 epoch 1675 batch: 400 avg loss -2.765744 avg loss no lamb -2.765744 time 2020-06-27 16:39:47.047833
Model ind 665 epoch 1675 batch: 500 avg loss -2.831924 avg loss no lamb -2.831924 time 2020-06-27 16:40:01.060064
Model ind 665 epoch 1675 batch: 600 avg loss -2.905663 avg loss no lamb -2.905663 time 2020-06-27 16:40:11.750062
Model ind 665 epoch 1675 batch: 700 avg loss -2.762971 avg loss no lamb -2.762971 time 2020-06-27 16:40:22.483906
Model ind 665 epoch 1675 batch: 800 avg loss -2.890671 avg loss no lamb -2.890671 time 2020-06-27 16:40:33.203522
last batch sz 10
Pre: time 2020-06-27 16:40:46.987417: 
 	std: 0.0023142886
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.979, 0.9783, 0.9735, 0.9792, 0.975]
	train_accs: [0.9810333, 0.97996664, 0.9752, 0.98075, 0.97651666]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97699994
	best: 0.979

Starting e_i: 1676
Model ind 665 epoch 1676 batch: 0 avg loss -2.981505 avg loss no lamb -2.981505 time 2020-06-27 16:40:48.519948
Model ind 665 epoch 1676 batch: 100 avg loss -2.933434 avg loss no lamb -2.933434 time 2020-06-27 16:40:59.116615
Model ind 665 epoch 1676 batch: 200 avg loss -2.893465 avg loss no lamb -2.893465 time 2020-06-27 16:41:09.986883
Model ind 665 epoch 1676 batch: 300 avg loss -2.862086 avg loss no lamb -2.862086 time 2020-06-27 16:41:20.728929
Model ind 665 epoch 1676 batch: 400 avg loss -2.821574 avg loss no lamb -2.821574 time 2020-06-27 16:41:31.399705
Model ind 665 epoch 1676 batch: 500 avg loss -2.845962 avg loss no lamb -2.845962 time 2020-06-27 16:41:42.194935
Model ind 665 epoch 1676 batch: 600 avg loss -2.868250 avg loss no lamb -2.868250 time 2020-06-27 16:41:53.185480
Model ind 665 epoch 1676 batch: 700 avg loss -2.757941 avg loss no lamb -2.757941 time 2020-06-27 16:42:04.009977
Model ind 665 epoch 1676 batch: 800 avg loss -2.898086 avg loss no lamb -2.898086 time 2020-06-27 16:42:14.705351
last batch sz 10
Pre: time 2020-06-27 16:42:28.753887: 
 	std: 0.0029529557
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9801, 0.9747, 0.9805, 0.9743]
	train_accs: [0.9809333, 0.98043334, 0.97538334, 0.9809167, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.9781
	best: 0.9809

Starting e_i: 1677
Model ind 665 epoch 1677 batch: 0 avg loss -2.976370 avg loss no lamb -2.976370 time 2020-06-27 16:42:30.125401
Model ind 665 epoch 1677 batch: 100 avg loss -2.951429 avg loss no lamb -2.951429 time 2020-06-27 16:42:40.911171
Model ind 665 epoch 1677 batch: 200 avg loss -2.866976 avg loss no lamb -2.866976 time 2020-06-27 16:42:51.419021
Model ind 665 epoch 1677 batch: 300 avg loss -2.862427 avg loss no lamb -2.862427 time 2020-06-27 16:43:02.223840
Model ind 665 epoch 1677 batch: 400 avg loss -2.794413 avg loss no lamb -2.794413 time 2020-06-27 16:43:13.105641
Model ind 665 epoch 1677 batch: 500 avg loss -2.874557 avg loss no lamb -2.874557 time 2020-06-27 16:43:23.917982
Model ind 665 epoch 1677 batch: 600 avg loss -2.877327 avg loss no lamb -2.877327 time 2020-06-27 16:43:34.484056
Model ind 665 epoch 1677 batch: 700 avg loss -2.813147 avg loss no lamb -2.813147 time 2020-06-27 16:43:45.017241
Model ind 665 epoch 1677 batch: 800 avg loss -2.905321 avg loss no lamb -2.905321 time 2020-06-27 16:43:55.693821
last batch sz 10
Pre: time 2020-06-27 16:44:09.803803: 
 	std: 0.0029410198
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9783, 0.9723, 0.9789, 0.9731]
	train_accs: [0.98038334, 0.9798833, 0.9748, 0.98038334, 0.97531664]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97628003
	best: 0.9788

Starting e_i: 1678
Model ind 665 epoch 1678 batch: 0 avg loss -2.940160 avg loss no lamb -2.940160 time 2020-06-27 16:44:11.183636
Model ind 665 epoch 1678 batch: 100 avg loss -2.906318 avg loss no lamb -2.906318 time 2020-06-27 16:44:21.902291
Model ind 665 epoch 1678 batch: 200 avg loss -2.891722 avg loss no lamb -2.891722 time 2020-06-27 16:44:32.413291
Model ind 665 epoch 1678 batch: 300 avg loss -2.898223 avg loss no lamb -2.898223 time 2020-06-27 16:44:43.268440
Model ind 665 epoch 1678 batch: 400 avg loss -2.799238 avg loss no lamb -2.799238 time 2020-06-27 16:44:53.984231
Model ind 665 epoch 1678 batch: 500 avg loss -2.903893 avg loss no lamb -2.903893 time 2020-06-27 16:45:04.830666
Model ind 665 epoch 1678 batch: 600 avg loss -2.866673 avg loss no lamb -2.866673 time 2020-06-27 16:45:15.408957
Model ind 665 epoch 1678 batch: 700 avg loss -2.792854 avg loss no lamb -2.792854 time 2020-06-27 16:45:25.906476
Model ind 665 epoch 1678 batch: 800 avg loss -2.868117 avg loss no lamb -2.868117 time 2020-06-27 16:45:36.673661
last batch sz 10
Pre: time 2020-06-27 16:45:50.870758: 
 	std: 0.0031870971
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.978, 0.9713, 0.9787, 0.973]
	train_accs: [0.9806833, 0.97996664, 0.9740667, 0.9806167, 0.9752833]
	best_train_sub_head: 0
	worst: 0.9713
	avg: 0.97598
	best: 0.9789

Starting e_i: 1679
Model ind 665 epoch 1679 batch: 0 avg loss -2.917431 avg loss no lamb -2.917431 time 2020-06-27 16:45:52.182444
Model ind 665 epoch 1679 batch: 100 avg loss -2.939267 avg loss no lamb -2.939267 time 2020-06-27 16:46:02.743295
Model ind 665 epoch 1679 batch: 200 avg loss -2.877369 avg loss no lamb -2.877369 time 2020-06-27 16:46:13.419025
Model ind 665 epoch 1679 batch: 300 avg loss -2.893687 avg loss no lamb -2.893687 time 2020-06-27 16:46:23.854832
Model ind 665 epoch 1679 batch: 400 avg loss -2.789517 avg loss no lamb -2.789517 time 2020-06-27 16:46:34.544515
Model ind 665 epoch 1679 batch: 500 avg loss -2.858100 avg loss no lamb -2.858100 time 2020-06-27 16:46:45.226907
Model ind 665 epoch 1679 batch: 600 avg loss -2.881951 avg loss no lamb -2.881951 time 2020-06-27 16:46:55.672742
Model ind 665 epoch 1679 batch: 700 avg loss -2.780060 avg loss no lamb -2.780060 time 2020-06-27 16:47:06.160897
Model ind 665 epoch 1679 batch: 800 avg loss -2.852648 avg loss no lamb -2.852648 time 2020-06-27 16:47:16.465139
last batch sz 10
Pre: time 2020-06-27 16:47:30.323148: 
 	std: 0.0028876322
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9788, 0.9736, 0.9799, 0.9735]
	train_accs: [0.98116666, 0.9803, 0.97575, 0.9810167, 0.97565]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97705996
	best: 0.9795

Starting e_i: 1680
Model ind 665 epoch 1680 batch: 0 avg loss -2.917812 avg loss no lamb -2.917812 time 2020-06-27 16:47:31.733049
Model ind 665 epoch 1680 batch: 100 avg loss -2.910298 avg loss no lamb -2.910298 time 2020-06-27 16:47:42.386176
Model ind 665 epoch 1680 batch: 200 avg loss -2.891980 avg loss no lamb -2.891980 time 2020-06-27 16:47:53.094353
Model ind 665 epoch 1680 batch: 300 avg loss -2.906050 avg loss no lamb -2.906050 time 2020-06-27 16:48:03.630051
Model ind 665 epoch 1680 batch: 400 avg loss -2.812315 avg loss no lamb -2.812315 time 2020-06-27 16:48:14.208634
Model ind 665 epoch 1680 batch: 500 avg loss -2.883077 avg loss no lamb -2.883077 time 2020-06-27 16:48:24.682231
Model ind 665 epoch 1680 batch: 600 avg loss -2.893986 avg loss no lamb -2.893986 time 2020-06-27 16:48:35.250285
Model ind 665 epoch 1680 batch: 700 avg loss -2.767361 avg loss no lamb -2.767361 time 2020-06-27 16:48:45.786503
Model ind 665 epoch 1680 batch: 800 avg loss -2.898540 avg loss no lamb -2.898540 time 2020-06-27 16:48:56.195363
last batch sz 10
Pre: time 2020-06-27 16:49:10.089741: 
 	std: 0.0038515478
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9795, 0.9719, 0.9806, 0.9726]
	train_accs: [0.98135, 0.98031664, 0.97425, 0.98146665, 0.9752833]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97694
	best: 0.9806

Starting e_i: 1681
Model ind 665 epoch 1681 batch: 0 avg loss -2.952368 avg loss no lamb -2.952368 time 2020-06-27 16:49:12.814681
Model ind 665 epoch 1681 batch: 100 avg loss -2.978545 avg loss no lamb -2.978545 time 2020-06-27 16:49:23.431628
Model ind 665 epoch 1681 batch: 200 avg loss -2.834685 avg loss no lamb -2.834685 time 2020-06-27 16:49:34.334326
Model ind 665 epoch 1681 batch: 300 avg loss -2.872689 avg loss no lamb -2.872689 time 2020-06-27 16:49:45.012834
Model ind 665 epoch 1681 batch: 400 avg loss -2.837951 avg loss no lamb -2.837951 time 2020-06-27 16:49:55.661142
Model ind 665 epoch 1681 batch: 500 avg loss -2.866461 avg loss no lamb -2.866461 time 2020-06-27 16:50:06.425829
Model ind 665 epoch 1681 batch: 600 avg loss -2.899983 avg loss no lamb -2.899983 time 2020-06-27 16:50:17.129349
Model ind 665 epoch 1681 batch: 700 avg loss -2.834179 avg loss no lamb -2.834179 time 2020-06-27 16:50:27.878662
Model ind 665 epoch 1681 batch: 800 avg loss -2.873849 avg loss no lamb -2.873849 time 2020-06-27 16:50:38.456400
last batch sz 10
Pre: time 2020-06-27 16:50:52.107733: 
 	std: 0.002626486
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9771, 0.9786, 0.9728, 0.9779, 0.9724]
	train_accs: [0.98018336, 0.9802667, 0.9745833, 0.9805167, 0.97538334]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.9757601
	best: 0.9779

Starting e_i: 1682
Model ind 665 epoch 1682 batch: 0 avg loss -2.959389 avg loss no lamb -2.959389 time 2020-06-27 16:50:53.543024
Model ind 665 epoch 1682 batch: 100 avg loss -2.891354 avg loss no lamb -2.891354 time 2020-06-27 16:51:04.042336
Model ind 665 epoch 1682 batch: 200 avg loss -2.916818 avg loss no lamb -2.916818 time 2020-06-27 16:51:14.782266
Model ind 665 epoch 1682 batch: 300 avg loss -2.863871 avg loss no lamb -2.863871 time 2020-06-27 16:51:27.527953
Model ind 665 epoch 1682 batch: 400 avg loss -2.834066 avg loss no lamb -2.834066 time 2020-06-27 16:51:41.246840
Model ind 665 epoch 1682 batch: 500 avg loss -2.818487 avg loss no lamb -2.818487 time 2020-06-27 16:51:51.995856
Model ind 665 epoch 1682 batch: 600 avg loss -2.898578 avg loss no lamb -2.898578 time 2020-06-27 16:52:02.787720
Model ind 665 epoch 1682 batch: 700 avg loss -2.784115 avg loss no lamb -2.784115 time 2020-06-27 16:52:13.630467
Model ind 665 epoch 1682 batch: 800 avg loss -2.858017 avg loss no lamb -2.858017 time 2020-06-27 16:52:24.138937
last batch sz 10
Pre: time 2020-06-27 16:52:37.928195: 
 	std: 0.0027949912
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.979, 0.9737, 0.98, 0.9739]
	train_accs: [0.98118335, 0.98081666, 0.97515, 0.9812833, 0.9759667]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97720003
	best: 0.98

Starting e_i: 1683
Model ind 665 epoch 1683 batch: 0 avg loss -2.973382 avg loss no lamb -2.973382 time 2020-06-27 16:52:39.563729
Model ind 665 epoch 1683 batch: 100 avg loss -2.970015 avg loss no lamb -2.970015 time 2020-06-27 16:52:50.335182
Model ind 665 epoch 1683 batch: 200 avg loss -2.846817 avg loss no lamb -2.846817 time 2020-06-27 16:53:01.142039
Model ind 665 epoch 1683 batch: 300 avg loss -2.833291 avg loss no lamb -2.833291 time 2020-06-27 16:53:11.946772
Model ind 665 epoch 1683 batch: 400 avg loss -2.796740 avg loss no lamb -2.796740 time 2020-06-27 16:53:22.839472
Model ind 665 epoch 1683 batch: 500 avg loss -2.844031 avg loss no lamb -2.844031 time 2020-06-27 16:53:33.573783
Model ind 665 epoch 1683 batch: 600 avg loss -2.877406 avg loss no lamb -2.877406 time 2020-06-27 16:53:44.237334
Model ind 665 epoch 1683 batch: 700 avg loss -2.827585 avg loss no lamb -2.827585 time 2020-06-27 16:53:54.729451
Model ind 665 epoch 1683 batch: 800 avg loss -2.892143 avg loss no lamb -2.892143 time 2020-06-27 16:54:05.600487
last batch sz 10
Pre: time 2020-06-27 16:54:19.783959: 
 	std: 0.0035124784
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9801, 0.9732, 0.981, 0.9738]
	train_accs: [0.9812, 0.9802333, 0.97463334, 0.98123336, 0.9751833]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97778
	best: 0.981

Starting e_i: 1684
Model ind 665 epoch 1684 batch: 0 avg loss -2.956494 avg loss no lamb -2.956494 time 2020-06-27 16:54:21.118524
Model ind 665 epoch 1684 batch: 100 avg loss -2.886289 avg loss no lamb -2.886289 time 2020-06-27 16:54:31.829913
Model ind 665 epoch 1684 batch: 200 avg loss -2.852168 avg loss no lamb -2.852168 time 2020-06-27 16:54:42.526865
Model ind 665 epoch 1684 batch: 300 avg loss -2.867236 avg loss no lamb -2.867236 time 2020-06-27 16:54:53.226606
Model ind 665 epoch 1684 batch: 400 avg loss -2.905736 avg loss no lamb -2.905736 time 2020-06-27 16:55:03.918976
Model ind 665 epoch 1684 batch: 500 avg loss -2.935341 avg loss no lamb -2.935341 time 2020-06-27 16:55:14.749010
Model ind 665 epoch 1684 batch: 600 avg loss -2.926061 avg loss no lamb -2.926061 time 2020-06-27 16:55:25.292910
Model ind 665 epoch 1684 batch: 700 avg loss -2.758551 avg loss no lamb -2.758551 time 2020-06-27 16:55:35.981906
Model ind 665 epoch 1684 batch: 800 avg loss -2.860356 avg loss no lamb -2.860356 time 2020-06-27 16:55:46.809770
last batch sz 10
Pre: time 2020-06-27 16:56:01.060381: 
 	std: 0.0029788525
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9805, 0.9738, 0.9807, 0.9757]
	train_accs: [0.98105, 0.9804, 0.9752667, 0.98105, 0.97648335]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97832
	best: 0.9809

Starting e_i: 1685
Model ind 665 epoch 1685 batch: 0 avg loss -2.966846 avg loss no lamb -2.966846 time 2020-06-27 16:56:02.377519
Model ind 665 epoch 1685 batch: 100 avg loss -2.889625 avg loss no lamb -2.889625 time 2020-06-27 16:56:13.429242
Model ind 665 epoch 1685 batch: 200 avg loss -2.909771 avg loss no lamb -2.909771 time 2020-06-27 16:56:24.174350
Model ind 665 epoch 1685 batch: 300 avg loss -2.848727 avg loss no lamb -2.848727 time 2020-06-27 16:56:34.829508
Model ind 665 epoch 1685 batch: 400 avg loss -2.808903 avg loss no lamb -2.808903 time 2020-06-27 16:56:45.606989
Model ind 665 epoch 1685 batch: 500 avg loss -2.881491 avg loss no lamb -2.881491 time 2020-06-27 16:56:56.281445
Model ind 665 epoch 1685 batch: 600 avg loss -2.960437 avg loss no lamb -2.960437 time 2020-06-27 16:57:07.145863
Model ind 665 epoch 1685 batch: 700 avg loss -2.742471 avg loss no lamb -2.742471 time 2020-06-27 16:57:18.007248
Model ind 665 epoch 1685 batch: 800 avg loss -2.876837 avg loss no lamb -2.876837 time 2020-06-27 16:57:29.082808
last batch sz 10
Pre: time 2020-06-27 16:57:43.246563: 
 	std: 0.0036935585
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9796, 0.9718, 0.9807, 0.9741]
	train_accs: [0.98143333, 0.98045, 0.97358334, 0.98146665, 0.97525]
	best_train_sub_head: 3
	worst: 0.9718
	avg: 0.97736007
	best: 0.9807

Starting e_i: 1686
Model ind 665 epoch 1686 batch: 0 avg loss -2.976920 avg loss no lamb -2.976920 time 2020-06-27 16:57:44.653207
Model ind 665 epoch 1686 batch: 100 avg loss -2.951924 avg loss no lamb -2.951924 time 2020-06-27 16:57:55.265495
Model ind 665 epoch 1686 batch: 200 avg loss -2.887648 avg loss no lamb -2.887648 time 2020-06-27 16:58:05.875956
Model ind 665 epoch 1686 batch: 300 avg loss -2.915254 avg loss no lamb -2.915254 time 2020-06-27 16:58:16.457931
Model ind 665 epoch 1686 batch: 400 avg loss -2.822627 avg loss no lamb -2.822627 time 2020-06-27 16:58:27.116005
Model ind 665 epoch 1686 batch: 500 avg loss -2.832804 avg loss no lamb -2.832804 time 2020-06-27 16:58:37.858788
Model ind 665 epoch 1686 batch: 600 avg loss -2.909428 avg loss no lamb -2.909428 time 2020-06-27 16:58:48.526387
Model ind 665 epoch 1686 batch: 700 avg loss -2.838870 avg loss no lamb -2.838870 time 2020-06-27 16:58:59.117886
Model ind 665 epoch 1686 batch: 800 avg loss -2.861201 avg loss no lamb -2.861201 time 2020-06-27 16:59:10.078747
last batch sz 10
Pre: time 2020-06-27 16:59:24.288897: 
 	std: 0.0028740286
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9793, 0.9734, 0.9805, 0.9747]
	train_accs: [0.98066664, 0.98008335, 0.9748, 0.9808667, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97749996
	best: 0.9805

Starting e_i: 1687
Model ind 665 epoch 1687 batch: 0 avg loss -3.007898 avg loss no lamb -3.007898 time 2020-06-27 16:59:25.617136
Model ind 665 epoch 1687 batch: 100 avg loss -2.934055 avg loss no lamb -2.934055 time 2020-06-27 16:59:36.175753
Model ind 665 epoch 1687 batch: 200 avg loss -2.926674 avg loss no lamb -2.926674 time 2020-06-27 16:59:46.818015
Model ind 665 epoch 1687 batch: 300 avg loss -2.929572 avg loss no lamb -2.929572 time 2020-06-27 16:59:57.590760
Model ind 665 epoch 1687 batch: 400 avg loss -2.836798 avg loss no lamb -2.836798 time 2020-06-27 17:00:08.518585
Model ind 665 epoch 1687 batch: 500 avg loss -2.833655 avg loss no lamb -2.833655 time 2020-06-27 17:00:19.088666
Model ind 665 epoch 1687 batch: 600 avg loss -2.900371 avg loss no lamb -2.900371 time 2020-06-27 17:00:29.645155
Model ind 665 epoch 1687 batch: 700 avg loss -2.833057 avg loss no lamb -2.833057 time 2020-06-27 17:00:40.387694
Model ind 665 epoch 1687 batch: 800 avg loss -2.871892 avg loss no lamb -2.871892 time 2020-06-27 17:00:51.044317
last batch sz 10
Pre: time 2020-06-27 17:01:05.317078: 
 	std: 0.0026635395
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9778, 0.9734, 0.9796, 0.9742]
	train_accs: [0.98075, 0.9802, 0.9751, 0.98076665, 0.9759]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97694
	best: 0.9796

Starting e_i: 1688
Model ind 665 epoch 1688 batch: 0 avg loss -2.959481 avg loss no lamb -2.959481 time 2020-06-27 17:01:06.894878
Model ind 665 epoch 1688 batch: 100 avg loss -2.890395 avg loss no lamb -2.890395 time 2020-06-27 17:01:17.412878
Model ind 665 epoch 1688 batch: 200 avg loss -2.891504 avg loss no lamb -2.891504 time 2020-06-27 17:01:28.073194
Model ind 665 epoch 1688 batch: 300 avg loss -2.892547 avg loss no lamb -2.892547 time 2020-06-27 17:01:38.941745
Model ind 665 epoch 1688 batch: 400 avg loss -2.820972 avg loss no lamb -2.820972 time 2020-06-27 17:01:49.406775
Model ind 665 epoch 1688 batch: 500 avg loss -2.862628 avg loss no lamb -2.862628 time 2020-06-27 17:01:59.875665
Model ind 665 epoch 1688 batch: 600 avg loss -2.890987 avg loss no lamb -2.890987 time 2020-06-27 17:02:10.552306
Model ind 665 epoch 1688 batch: 700 avg loss -2.738374 avg loss no lamb -2.738374 time 2020-06-27 17:02:21.397524
Model ind 665 epoch 1688 batch: 800 avg loss -2.918049 avg loss no lamb -2.918049 time 2020-06-27 17:02:32.237157
last batch sz 10
Pre: time 2020-06-27 17:02:46.254528: 
 	std: 0.003169218
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9791, 0.9727, 0.9802, 0.9743]
	train_accs: [0.9806833, 0.9802667, 0.9749333, 0.9809, 0.97585]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97730005
	best: 0.9802

Starting e_i: 1689
Model ind 665 epoch 1689 batch: 0 avg loss -2.967930 avg loss no lamb -2.967930 time 2020-06-27 17:02:47.643793
Model ind 665 epoch 1689 batch: 100 avg loss -2.917314 avg loss no lamb -2.917314 time 2020-06-27 17:02:58.282973
Model ind 665 epoch 1689 batch: 200 avg loss -2.886140 avg loss no lamb -2.886140 time 2020-06-27 17:03:09.031006
Model ind 665 epoch 1689 batch: 300 avg loss -2.942882 avg loss no lamb -2.942882 time 2020-06-27 17:03:19.482548
Model ind 665 epoch 1689 batch: 400 avg loss -2.852173 avg loss no lamb -2.852173 time 2020-06-27 17:03:30.164767
Model ind 665 epoch 1689 batch: 500 avg loss -2.866078 avg loss no lamb -2.866078 time 2020-06-27 17:03:40.817405
Model ind 665 epoch 1689 batch: 600 avg loss -2.861369 avg loss no lamb -2.861369 time 2020-06-27 17:03:54.036894
Model ind 665 epoch 1689 batch: 700 avg loss -2.833783 avg loss no lamb -2.833783 time 2020-06-27 17:04:07.313901
Model ind 665 epoch 1689 batch: 800 avg loss -2.836778 avg loss no lamb -2.836778 time 2020-06-27 17:04:20.428242
last batch sz 10
Pre: time 2020-06-27 17:04:36.793780: 
 	std: 0.0028470387
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9788, 0.9734, 0.981, 0.976]
	train_accs: [0.98116666, 0.98013335, 0.9752333, 0.98141664, 0.97686666]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97791994
	best: 0.981

Starting e_i: 1690
Model ind 665 epoch 1690 batch: 0 avg loss -2.974549 avg loss no lamb -2.974549 time 2020-06-27 17:04:38.644188
Model ind 665 epoch 1690 batch: 100 avg loss -2.941295 avg loss no lamb -2.941295 time 2020-06-27 17:04:51.500886
Model ind 665 epoch 1690 batch: 200 avg loss -2.885367 avg loss no lamb -2.885367 time 2020-06-27 17:05:03.385245
Model ind 665 epoch 1690 batch: 300 avg loss -2.847489 avg loss no lamb -2.847489 time 2020-06-27 17:05:14.099168
Model ind 665 epoch 1690 batch: 400 avg loss -2.839948 avg loss no lamb -2.839948 time 2020-06-27 17:05:24.283172
Model ind 665 epoch 1690 batch: 500 avg loss -2.877080 avg loss no lamb -2.877080 time 2020-06-27 17:05:34.053460
Model ind 665 epoch 1690 batch: 600 avg loss -2.867743 avg loss no lamb -2.867743 time 2020-06-27 17:05:43.800921
Model ind 665 epoch 1690 batch: 700 avg loss -2.764474 avg loss no lamb -2.764474 time 2020-06-27 17:05:53.342932
Model ind 665 epoch 1690 batch: 800 avg loss -2.868736 avg loss no lamb -2.868736 time 2020-06-27 17:06:02.920911
last batch sz 10
Pre: time 2020-06-27 17:06:15.680366: 
 	std: 0.0026346936
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9794, 0.9748, 0.9809, 0.9747]
	train_accs: [0.98158336, 0.9802833, 0.9755833, 0.9817167, 0.9766333]
	best_train_sub_head: 3
	worst: 0.9747
	avg: 0.97791994
	best: 0.9809

Starting e_i: 1691
Model ind 665 epoch 1691 batch: 0 avg loss -2.938236 avg loss no lamb -2.938236 time 2020-06-27 17:06:17.987698
Model ind 665 epoch 1691 batch: 100 avg loss -2.881142 avg loss no lamb -2.881142 time 2020-06-27 17:06:27.426318
Model ind 665 epoch 1691 batch: 200 avg loss -2.889747 avg loss no lamb -2.889747 time 2020-06-27 17:06:36.827539
Model ind 665 epoch 1691 batch: 300 avg loss -2.859360 avg loss no lamb -2.859360 time 2020-06-27 17:06:48.228076
Model ind 665 epoch 1691 batch: 400 avg loss -2.837405 avg loss no lamb -2.837405 time 2020-06-27 17:07:00.604969
Model ind 665 epoch 1691 batch: 500 avg loss -2.896810 avg loss no lamb -2.896810 time 2020-06-27 17:07:13.377371
Model ind 665 epoch 1691 batch: 600 avg loss -2.893284 avg loss no lamb -2.893284 time 2020-06-27 17:07:23.751271
Model ind 665 epoch 1691 batch: 700 avg loss -2.813332 avg loss no lamb -2.813332 time 2020-06-27 17:07:35.620332
Model ind 665 epoch 1691 batch: 800 avg loss -2.809076 avg loss no lamb -2.809076 time 2020-06-27 17:07:47.053405
last batch sz 10
Pre: time 2020-06-27 17:08:00.861233: 
 	std: 0.0028309769
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9788, 0.9733, 0.9795, 0.9739]
	train_accs: [0.9813333, 0.98013335, 0.9749333, 0.98135, 0.97575]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97704
	best: 0.9795

Starting e_i: 1692
Model ind 665 epoch 1692 batch: 0 avg loss -2.957197 avg loss no lamb -2.957197 time 2020-06-27 17:08:02.415148
Model ind 665 epoch 1692 batch: 100 avg loss -2.936822 avg loss no lamb -2.936822 time 2020-06-27 17:08:14.477043
Model ind 665 epoch 1692 batch: 200 avg loss -2.916062 avg loss no lamb -2.916062 time 2020-06-27 17:08:25.727800
Model ind 665 epoch 1692 batch: 300 avg loss -2.878461 avg loss no lamb -2.878461 time 2020-06-27 17:08:36.827945
Model ind 665 epoch 1692 batch: 400 avg loss -2.783910 avg loss no lamb -2.783910 time 2020-06-27 17:08:46.669556
Model ind 665 epoch 1692 batch: 500 avg loss -2.880852 avg loss no lamb -2.880852 time 2020-06-27 17:08:56.442837
Model ind 665 epoch 1692 batch: 600 avg loss -2.870985 avg loss no lamb -2.870985 time 2020-06-27 17:09:06.259321
Model ind 665 epoch 1692 batch: 700 avg loss -2.784714 avg loss no lamb -2.784714 time 2020-06-27 17:09:16.726202
Model ind 665 epoch 1692 batch: 800 avg loss -2.890357 avg loss no lamb -2.890357 time 2020-06-27 17:09:27.185136
last batch sz 10
Pre: time 2020-06-27 17:09:40.401896: 
 	std: 0.002743289
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.9785, 0.974, 0.9799, 0.9743]
	train_accs: [0.9816667, 0.9805667, 0.9759833, 0.9813833, 0.9763833]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97742
	best: 0.9804

Starting e_i: 1693
Model ind 665 epoch 1693 batch: 0 avg loss -2.972898 avg loss no lamb -2.972898 time 2020-06-27 17:09:41.664374
Model ind 665 epoch 1693 batch: 100 avg loss -2.885878 avg loss no lamb -2.885878 time 2020-06-27 17:09:51.533891
Model ind 665 epoch 1693 batch: 200 avg loss -2.884048 avg loss no lamb -2.884048 time 2020-06-27 17:10:01.504661
Model ind 665 epoch 1693 batch: 300 avg loss -2.880193 avg loss no lamb -2.880193 time 2020-06-27 17:10:11.355989
Model ind 665 epoch 1693 batch: 400 avg loss -2.809652 avg loss no lamb -2.809652 time 2020-06-27 17:10:21.280057
Model ind 665 epoch 1693 batch: 500 avg loss -2.837989 avg loss no lamb -2.837989 time 2020-06-27 17:10:32.433525
Model ind 665 epoch 1693 batch: 600 avg loss -2.893623 avg loss no lamb -2.893623 time 2020-06-27 17:10:43.635487
Model ind 665 epoch 1693 batch: 700 avg loss -2.801796 avg loss no lamb -2.801796 time 2020-06-27 17:10:54.065437
Model ind 665 epoch 1693 batch: 800 avg loss -2.900759 avg loss no lamb -2.900759 time 2020-06-27 17:11:04.686652
last batch sz 10
Pre: time 2020-06-27 17:11:18.523284: 
 	std: 0.0029493074
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9797, 0.9739, 0.9799, 0.9738]
	train_accs: [0.9812667, 0.9807, 0.97538334, 0.9814, 0.97615]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.9774601
	best: 0.9799

Starting e_i: 1694
Model ind 665 epoch 1694 batch: 0 avg loss -2.964332 avg loss no lamb -2.964332 time 2020-06-27 17:11:19.874699
Model ind 665 epoch 1694 batch: 100 avg loss -2.933217 avg loss no lamb -2.933217 time 2020-06-27 17:11:29.752680
Model ind 665 epoch 1694 batch: 200 avg loss -2.928752 avg loss no lamb -2.928752 time 2020-06-27 17:11:40.678235
Model ind 665 epoch 1694 batch: 300 avg loss -2.838182 avg loss no lamb -2.838182 time 2020-06-27 17:11:52.410262
Model ind 665 epoch 1694 batch: 400 avg loss -2.776864 avg loss no lamb -2.776864 time 2020-06-27 17:12:03.918182
Model ind 665 epoch 1694 batch: 500 avg loss -2.834330 avg loss no lamb -2.834330 time 2020-06-27 17:12:15.569279
Model ind 665 epoch 1694 batch: 600 avg loss -2.877811 avg loss no lamb -2.877811 time 2020-06-27 17:12:26.832216
Model ind 665 epoch 1694 batch: 700 avg loss -2.758254 avg loss no lamb -2.758254 time 2020-06-27 17:12:38.438095
Model ind 665 epoch 1694 batch: 800 avg loss -2.901563 avg loss no lamb -2.901563 time 2020-06-27 17:12:49.350219
last batch sz 10
Pre: time 2020-06-27 17:13:03.907931: 
 	std: 0.0031230825
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9811, 0.9808, 0.9744, 0.981, 0.9748]
	train_accs: [0.98121667, 0.9810333, 0.9751833, 0.98146665, 0.9762833]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97841996
	best: 0.981

Starting e_i: 1695
Model ind 665 epoch 1695 batch: 0 avg loss -2.937614 avg loss no lamb -2.937614 time 2020-06-27 17:13:05.394907
Model ind 665 epoch 1695 batch: 100 avg loss -2.955592 avg loss no lamb -2.955592 time 2020-06-27 17:13:16.887080
Model ind 665 epoch 1695 batch: 200 avg loss -2.954494 avg loss no lamb -2.954494 time 2020-06-27 17:13:28.027919
Model ind 665 epoch 1695 batch: 300 avg loss -2.865706 avg loss no lamb -2.865706 time 2020-06-27 17:13:39.115265
Model ind 665 epoch 1695 batch: 400 avg loss -2.841012 avg loss no lamb -2.841012 time 2020-06-27 17:13:50.253657
Model ind 665 epoch 1695 batch: 500 avg loss -2.894216 avg loss no lamb -2.894216 time 2020-06-27 17:14:01.362325
Model ind 665 epoch 1695 batch: 600 avg loss -2.892645 avg loss no lamb -2.892645 time 2020-06-27 17:14:12.649135
Model ind 665 epoch 1695 batch: 700 avg loss -2.804240 avg loss no lamb -2.804240 time 2020-06-27 17:14:23.490342
Model ind 665 epoch 1695 batch: 800 avg loss -2.862522 avg loss no lamb -2.862522 time 2020-06-27 17:14:34.505914
last batch sz 10
Pre: time 2020-06-27 17:14:48.647355: 
 	std: 0.0032585983
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9778, 0.9722, 0.9799, 0.9729]
	train_accs: [0.9805833, 0.97985, 0.9749, 0.98076665, 0.9755]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97643995
	best: 0.9799

Starting e_i: 1696
Model ind 665 epoch 1696 batch: 0 avg loss -2.934617 avg loss no lamb -2.934617 time 2020-06-27 17:14:49.857929
Model ind 665 epoch 1696 batch: 100 avg loss -2.923717 avg loss no lamb -2.923717 time 2020-06-27 17:15:00.990188
Model ind 665 epoch 1696 batch: 200 avg loss -2.913921 avg loss no lamb -2.913921 time 2020-06-27 17:15:11.783926
Model ind 665 epoch 1696 batch: 300 avg loss -2.906175 avg loss no lamb -2.906175 time 2020-06-27 17:15:22.504863
Model ind 665 epoch 1696 batch: 400 avg loss -2.858020 avg loss no lamb -2.858020 time 2020-06-27 17:15:32.165777
Model ind 665 epoch 1696 batch: 500 avg loss -2.856134 avg loss no lamb -2.856134 time 2020-06-27 17:15:41.716722
Model ind 665 epoch 1696 batch: 600 avg loss -2.922625 avg loss no lamb -2.922625 time 2020-06-27 17:15:51.158453
Model ind 665 epoch 1696 batch: 700 avg loss -2.765991 avg loss no lamb -2.765991 time 2020-06-27 17:16:00.641571
Model ind 665 epoch 1696 batch: 800 avg loss -2.898461 avg loss no lamb -2.898461 time 2020-06-27 17:16:10.098464
last batch sz 10
Pre: time 2020-06-27 17:16:22.694423: 
 	std: 0.0025822588
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9793, 0.9745, 0.981, 0.9758]
	train_accs: [0.9813, 0.98085, 0.9762, 0.98141664, 0.9770667]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97819996
	best: 0.981

Starting e_i: 1697
Model ind 665 epoch 1697 batch: 0 avg loss -2.897603 avg loss no lamb -2.897603 time 2020-06-27 17:16:24.124361
Model ind 665 epoch 1697 batch: 100 avg loss -2.931322 avg loss no lamb -2.931322 time 2020-06-27 17:16:33.708916
Model ind 665 epoch 1697 batch: 200 avg loss -2.876205 avg loss no lamb -2.876205 time 2020-06-27 17:16:43.295644
Model ind 665 epoch 1697 batch: 300 avg loss -2.905014 avg loss no lamb -2.905014 time 2020-06-27 17:16:52.909379
Model ind 665 epoch 1697 batch: 400 avg loss -2.814007 avg loss no lamb -2.814007 time 2020-06-27 17:17:02.583072
Model ind 665 epoch 1697 batch: 500 avg loss -2.848475 avg loss no lamb -2.848475 time 2020-06-27 17:17:12.215805
Model ind 665 epoch 1697 batch: 600 avg loss -2.878411 avg loss no lamb -2.878411 time 2020-06-27 17:17:21.884538
Model ind 665 epoch 1697 batch: 700 avg loss -2.801822 avg loss no lamb -2.801822 time 2020-06-27 17:17:31.541025
Model ind 665 epoch 1697 batch: 800 avg loss -2.861526 avg loss no lamb -2.861526 time 2020-06-27 17:17:41.155425
last batch sz 10
Pre: time 2020-06-27 17:17:53.965814: 
 	std: 0.0031436374
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9799, 0.9736, 0.9805, 0.974]
	train_accs: [0.98116666, 0.9804, 0.9753, 0.9809833, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97764
	best: 0.9802

Starting e_i: 1698
Model ind 665 epoch 1698 batch: 0 avg loss -2.954781 avg loss no lamb -2.954781 time 2020-06-27 17:17:55.140401
Model ind 665 epoch 1698 batch: 100 avg loss -2.885313 avg loss no lamb -2.885313 time 2020-06-27 17:18:04.725338
Model ind 665 epoch 1698 batch: 200 avg loss -2.903969 avg loss no lamb -2.903969 time 2020-06-27 17:18:14.179302
Model ind 665 epoch 1698 batch: 300 avg loss -2.872216 avg loss no lamb -2.872216 time 2020-06-27 17:18:23.693596
Model ind 665 epoch 1698 batch: 400 avg loss -2.758929 avg loss no lamb -2.758929 time 2020-06-27 17:18:33.218270
Model ind 665 epoch 1698 batch: 500 avg loss -2.860560 avg loss no lamb -2.860560 time 2020-06-27 17:18:42.757617
Model ind 665 epoch 1698 batch: 600 avg loss -2.847796 avg loss no lamb -2.847796 time 2020-06-27 17:18:52.233334
Model ind 665 epoch 1698 batch: 700 avg loss -2.831868 avg loss no lamb -2.831868 time 2020-06-27 17:19:01.747795
Model ind 665 epoch 1698 batch: 800 avg loss -2.935085 avg loss no lamb -2.935085 time 2020-06-27 17:19:11.213876
last batch sz 10
Pre: time 2020-06-27 17:19:23.785645: 
 	std: 0.003040666
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9792, 0.973, 0.98, 0.9747]
	train_accs: [0.9809667, 0.98048335, 0.9753, 0.98073334, 0.976]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97747993
	best: 0.9805

Starting e_i: 1699
Model ind 665 epoch 1699 batch: 0 avg loss -2.940164 avg loss no lamb -2.940164 time 2020-06-27 17:19:25.020735
Model ind 665 epoch 1699 batch: 100 avg loss -2.873536 avg loss no lamb -2.873536 time 2020-06-27 17:19:34.542447
Model ind 665 epoch 1699 batch: 200 avg loss -2.860100 avg loss no lamb -2.860100 time 2020-06-27 17:19:44.057765
Model ind 665 epoch 1699 batch: 300 avg loss -2.878138 avg loss no lamb -2.878138 time 2020-06-27 17:19:53.550270
Model ind 665 epoch 1699 batch: 400 avg loss -2.810217 avg loss no lamb -2.810217 time 2020-06-27 17:20:03.107874
Model ind 665 epoch 1699 batch: 500 avg loss -2.889581 avg loss no lamb -2.889581 time 2020-06-27 17:20:12.770832
Model ind 665 epoch 1699 batch: 600 avg loss -2.900763 avg loss no lamb -2.900763 time 2020-06-27 17:20:24.869024
Model ind 665 epoch 1699 batch: 700 avg loss -2.809182 avg loss no lamb -2.809182 time 2020-06-27 17:20:36.325325
Model ind 665 epoch 1699 batch: 800 avg loss -2.881619 avg loss no lamb -2.881619 time 2020-06-27 17:20:46.094161
last batch sz 10
Pre: time 2020-06-27 17:20:59.228241: 
 	std: 0.0035000646
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9791, 0.9719, 0.9798, 0.9729]
	train_accs: [0.98078334, 0.98015, 0.9741167, 0.98088336, 0.97513336]
	best_train_sub_head: 3
	worst: 0.9719
	avg: 0.97665995
	best: 0.9798

Starting e_i: 1700
Model ind 665 epoch 1700 batch: 0 avg loss -2.969179 avg loss no lamb -2.969179 time 2020-06-27 17:21:00.484299
Model ind 665 epoch 1700 batch: 100 avg loss -2.942199 avg loss no lamb -2.942199 time 2020-06-27 17:21:10.262154
Model ind 665 epoch 1700 batch: 200 avg loss -2.815857 avg loss no lamb -2.815857 time 2020-06-27 17:21:20.084163
Model ind 665 epoch 1700 batch: 300 avg loss -2.887064 avg loss no lamb -2.887064 time 2020-06-27 17:21:29.732874
Model ind 665 epoch 1700 batch: 400 avg loss -2.836448 avg loss no lamb -2.836448 time 2020-06-27 17:21:39.468827
Model ind 665 epoch 1700 batch: 500 avg loss -2.835727 avg loss no lamb -2.835727 time 2020-06-27 17:21:49.326449
Model ind 665 epoch 1700 batch: 600 avg loss -2.897043 avg loss no lamb -2.897043 time 2020-06-27 17:21:59.089514
Model ind 665 epoch 1700 batch: 700 avg loss -2.816223 avg loss no lamb -2.816223 time 2020-06-27 17:22:08.810219
Model ind 665 epoch 1700 batch: 800 avg loss -2.848859 avg loss no lamb -2.848859 time 2020-06-27 17:22:18.749708
last batch sz 10
Pre: time 2020-06-27 17:22:31.884593: 
 	std: 0.0029095802
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9794, 0.973, 0.9796, 0.9744]
	train_accs: [0.9812, 0.9806, 0.9748167, 0.9811, 0.9759667]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97721994
	best: 0.9797

Starting e_i: 1701
Model ind 665 epoch 1701 batch: 0 avg loss -2.935164 avg loss no lamb -2.935164 time 2020-06-27 17:22:34.362980
Model ind 665 epoch 1701 batch: 100 avg loss -2.928498 avg loss no lamb -2.928498 time 2020-06-27 17:22:44.201355
Model ind 665 epoch 1701 batch: 200 avg loss -2.865834 avg loss no lamb -2.865834 time 2020-06-27 17:22:53.902870
Model ind 665 epoch 1701 batch: 300 avg loss -2.878293 avg loss no lamb -2.878293 time 2020-06-27 17:23:03.718054
Model ind 665 epoch 1701 batch: 400 avg loss -2.801389 avg loss no lamb -2.801389 time 2020-06-27 17:23:13.560390
Model ind 665 epoch 1701 batch: 500 avg loss -2.897006 avg loss no lamb -2.897006 time 2020-06-27 17:23:23.339046
Model ind 665 epoch 1701 batch: 600 avg loss -2.894564 avg loss no lamb -2.894564 time 2020-06-27 17:23:33.119414
Model ind 665 epoch 1701 batch: 700 avg loss -2.794374 avg loss no lamb -2.794374 time 2020-06-27 17:23:42.913278
Model ind 665 epoch 1701 batch: 800 avg loss -2.891705 avg loss no lamb -2.891705 time 2020-06-27 17:23:52.758563
last batch sz 10
Pre: time 2020-06-27 17:24:05.715382: 
 	std: 0.0024894902
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9791, 0.9745, 0.9798, 0.9746]
	train_accs: [0.981, 0.9805, 0.9755, 0.9813167, 0.97645]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.97757995
	best: 0.9798

Starting e_i: 1702
Model ind 665 epoch 1702 batch: 0 avg loss -2.960165 avg loss no lamb -2.960165 time 2020-06-27 17:24:07.143767
Model ind 665 epoch 1702 batch: 100 avg loss -2.935209 avg loss no lamb -2.935209 time 2020-06-27 17:24:17.040622
Model ind 665 epoch 1702 batch: 200 avg loss -2.932869 avg loss no lamb -2.932869 time 2020-06-27 17:24:26.932227
Model ind 665 epoch 1702 batch: 300 avg loss -2.907282 avg loss no lamb -2.907282 time 2020-06-27 17:24:36.708696
Model ind 665 epoch 1702 batch: 400 avg loss -2.813356 avg loss no lamb -2.813356 time 2020-06-27 17:24:46.460739
Model ind 665 epoch 1702 batch: 500 avg loss -2.884953 avg loss no lamb -2.884953 time 2020-06-27 17:24:56.327581
Model ind 665 epoch 1702 batch: 600 avg loss -2.899491 avg loss no lamb -2.899491 time 2020-06-27 17:25:06.174203
Model ind 665 epoch 1702 batch: 700 avg loss -2.818956 avg loss no lamb -2.818956 time 2020-06-27 17:25:16.132916
Model ind 665 epoch 1702 batch: 800 avg loss -2.888549 avg loss no lamb -2.888549 time 2020-06-27 17:25:25.904755
last batch sz 10
Pre: time 2020-06-27 17:25:39.152258: 
 	std: 0.0032333345
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9798, 0.9726, 0.9804, 0.9748]
	train_accs: [0.9813167, 0.9805167, 0.97438335, 0.9812833, 0.97611666]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97756004
	best: 0.9802

Starting e_i: 1703
Model ind 665 epoch 1703 batch: 0 avg loss -2.985146 avg loss no lamb -2.985146 time 2020-06-27 17:25:40.731054
Model ind 665 epoch 1703 batch: 100 avg loss -2.916902 avg loss no lamb -2.916902 time 2020-06-27 17:25:53.217052
Model ind 665 epoch 1703 batch: 200 avg loss -2.845734 avg loss no lamb -2.845734 time 2020-06-27 17:26:04.756997
Model ind 665 epoch 1703 batch: 300 avg loss -2.910666 avg loss no lamb -2.910666 time 2020-06-27 17:26:15.464294
Model ind 665 epoch 1703 batch: 400 avg loss -2.796734 avg loss no lamb -2.796734 time 2020-06-27 17:26:25.048012
Model ind 665 epoch 1703 batch: 500 avg loss -2.881941 avg loss no lamb -2.881941 time 2020-06-27 17:26:34.619243
Model ind 665 epoch 1703 batch: 600 avg loss -2.832128 avg loss no lamb -2.832128 time 2020-06-27 17:26:44.179729
Model ind 665 epoch 1703 batch: 700 avg loss -2.838624 avg loss no lamb -2.838624 time 2020-06-27 17:26:53.820621
Model ind 665 epoch 1703 batch: 800 avg loss -2.914552 avg loss no lamb -2.914552 time 2020-06-27 17:27:03.421900
last batch sz 10
Pre: time 2020-06-27 17:27:16.144681: 
 	std: 0.0033937593
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9788, 0.9726, 0.9795, 0.9725]
	train_accs: [0.98123336, 0.9806, 0.97501665, 0.9813167, 0.9753]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97668
	best: 0.9795

Starting e_i: 1704
Model ind 665 epoch 1704 batch: 0 avg loss -2.961694 avg loss no lamb -2.961694 time 2020-06-27 17:27:17.324097
Model ind 665 epoch 1704 batch: 100 avg loss -2.882749 avg loss no lamb -2.882749 time 2020-06-27 17:27:26.837766
Model ind 665 epoch 1704 batch: 200 avg loss -2.866288 avg loss no lamb -2.866288 time 2020-06-27 17:27:36.278800
Model ind 665 epoch 1704 batch: 300 avg loss -2.933586 avg loss no lamb -2.933586 time 2020-06-27 17:27:45.811703
Model ind 665 epoch 1704 batch: 400 avg loss -2.892246 avg loss no lamb -2.892246 time 2020-06-27 17:27:56.436396
Model ind 665 epoch 1704 batch: 500 avg loss -2.842870 avg loss no lamb -2.842870 time 2020-06-27 17:28:06.400918
Model ind 665 epoch 1704 batch: 600 avg loss -2.894658 avg loss no lamb -2.894658 time 2020-06-27 17:28:15.876932
Model ind 665 epoch 1704 batch: 700 avg loss -2.807059 avg loss no lamb -2.807059 time 2020-06-27 17:28:25.347483
Model ind 665 epoch 1704 batch: 800 avg loss -2.923265 avg loss no lamb -2.923265 time 2020-06-27 17:28:34.861857
last batch sz 10
Pre: time 2020-06-27 17:28:47.817056: 
 	std: 0.0025654575
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9782, 0.9741, 0.9799, 0.9746]
	train_accs: [0.9809167, 0.98003334, 0.9752833, 0.9808667, 0.9755167]
	best_train_sub_head: 0
	worst: 0.9741
	avg: 0.97738
	best: 0.9801

Starting e_i: 1705
Model ind 665 epoch 1705 batch: 0 avg loss -2.958691 avg loss no lamb -2.958691 time 2020-06-27 17:28:48.985982
Model ind 665 epoch 1705 batch: 100 avg loss -2.883317 avg loss no lamb -2.883317 time 2020-06-27 17:28:58.514323
Model ind 665 epoch 1705 batch: 200 avg loss -2.881481 avg loss no lamb -2.881481 time 2020-06-27 17:29:07.974502
Model ind 665 epoch 1705 batch: 300 avg loss -2.926963 avg loss no lamb -2.926963 time 2020-06-27 17:29:17.526236
Model ind 665 epoch 1705 batch: 400 avg loss -2.832667 avg loss no lamb -2.832667 time 2020-06-27 17:29:27.071230
Model ind 665 epoch 1705 batch: 500 avg loss -2.885176 avg loss no lamb -2.885176 time 2020-06-27 17:29:36.521366
Model ind 665 epoch 1705 batch: 600 avg loss -2.945460 avg loss no lamb -2.945460 time 2020-06-27 17:29:45.963606
Model ind 665 epoch 1705 batch: 700 avg loss -2.789453 avg loss no lamb -2.789453 time 2020-06-27 17:29:55.435578
Model ind 665 epoch 1705 batch: 800 avg loss -2.892399 avg loss no lamb -2.892399 time 2020-06-27 17:30:05.070106
last batch sz 10
Pre: time 2020-06-27 17:30:17.935513: 
 	std: 0.0029376224
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9796, 0.974, 0.9809, 0.9752]
	train_accs: [0.98108333, 0.9802333, 0.97536665, 0.98125, 0.9766]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97812
	best: 0.9809

Starting e_i: 1706
Model ind 665 epoch 1706 batch: 0 avg loss -2.961383 avg loss no lamb -2.961383 time 2020-06-27 17:30:19.111670
Model ind 665 epoch 1706 batch: 100 avg loss -2.948349 avg loss no lamb -2.948349 time 2020-06-27 17:30:28.606545
Model ind 665 epoch 1706 batch: 200 avg loss -2.888111 avg loss no lamb -2.888111 time 2020-06-27 17:30:38.122045
Model ind 665 epoch 1706 batch: 300 avg loss -2.901265 avg loss no lamb -2.901265 time 2020-06-27 17:30:47.672051
Model ind 665 epoch 1706 batch: 400 avg loss -2.839749 avg loss no lamb -2.839749 time 2020-06-27 17:30:57.223476
Model ind 665 epoch 1706 batch: 500 avg loss -2.839287 avg loss no lamb -2.839287 time 2020-06-27 17:31:06.875423
Model ind 665 epoch 1706 batch: 600 avg loss -2.933366 avg loss no lamb -2.933366 time 2020-06-27 17:31:16.343635
Model ind 665 epoch 1706 batch: 700 avg loss -2.828799 avg loss no lamb -2.828799 time 2020-06-27 17:31:25.801911
Model ind 665 epoch 1706 batch: 800 avg loss -2.893252 avg loss no lamb -2.893252 time 2020-06-27 17:31:35.269661
last batch sz 10
Pre: time 2020-06-27 17:31:48.141241: 
 	std: 0.0034909025
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9791, 0.9728, 0.9805, 0.9726]
	train_accs: [0.9807, 0.98011667, 0.9748667, 0.98146665, 0.97535]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97694
	best: 0.9805

Starting e_i: 1707
Model ind 665 epoch 1707 batch: 0 avg loss -2.966193 avg loss no lamb -2.966193 time 2020-06-27 17:31:49.328774
Model ind 665 epoch 1707 batch: 100 avg loss -2.903264 avg loss no lamb -2.903264 time 2020-06-27 17:31:58.722540
Model ind 665 epoch 1707 batch: 200 avg loss -2.868924 avg loss no lamb -2.868924 time 2020-06-27 17:32:08.154692
Model ind 665 epoch 1707 batch: 300 avg loss -2.869290 avg loss no lamb -2.869290 time 2020-06-27 17:32:17.563052
Model ind 665 epoch 1707 batch: 400 avg loss -2.856471 avg loss no lamb -2.856471 time 2020-06-27 17:32:27.113034
Model ind 665 epoch 1707 batch: 500 avg loss -2.897890 avg loss no lamb -2.897890 time 2020-06-27 17:32:36.644819
Model ind 665 epoch 1707 batch: 600 avg loss -2.921296 avg loss no lamb -2.921296 time 2020-06-27 17:32:46.189255
Model ind 665 epoch 1707 batch: 700 avg loss -2.843677 avg loss no lamb -2.843677 time 2020-06-27 17:32:55.689151
Model ind 665 epoch 1707 batch: 800 avg loss -2.944881 avg loss no lamb -2.944881 time 2020-06-27 17:33:05.217007
last batch sz 10
Pre: time 2020-06-27 17:33:20.672891: 
 	std: 0.00303867
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9816, 0.9806, 0.9746, 0.9816, 0.9757]
	train_accs: [0.9816, 0.98083335, 0.97608334, 0.9817333, 0.9767]
	best_train_sub_head: 3
	worst: 0.9746
	avg: 0.97882
	best: 0.9816

Starting e_i: 1708
Model ind 665 epoch 1708 batch: 0 avg loss -2.948381 avg loss no lamb -2.948381 time 2020-06-27 17:33:22.109079
Model ind 665 epoch 1708 batch: 100 avg loss -2.908145 avg loss no lamb -2.908145 time 2020-06-27 17:33:31.844557
Model ind 665 epoch 1708 batch: 200 avg loss -2.885944 avg loss no lamb -2.885944 time 2020-06-27 17:33:41.754550
Model ind 665 epoch 1708 batch: 300 avg loss -2.864099 avg loss no lamb -2.864099 time 2020-06-27 17:33:51.587240
Model ind 665 epoch 1708 batch: 400 avg loss -2.757454 avg loss no lamb -2.757454 time 2020-06-27 17:34:01.413257
Model ind 665 epoch 1708 batch: 500 avg loss -2.820494 avg loss no lamb -2.820494 time 2020-06-27 17:34:11.230154
Model ind 665 epoch 1708 batch: 600 avg loss -2.831257 avg loss no lamb -2.831257 time 2020-06-27 17:34:21.176246
Model ind 665 epoch 1708 batch: 700 avg loss -2.788857 avg loss no lamb -2.788857 time 2020-06-27 17:34:30.975678
Model ind 665 epoch 1708 batch: 800 avg loss -2.871093 avg loss no lamb -2.871093 time 2020-06-27 17:34:40.795637
last batch sz 10
Pre: time 2020-06-27 17:34:54.815339: 
 	std: 0.002597216
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9787, 0.9727, 0.9787, 0.9743]
	train_accs: [0.98036665, 0.97985, 0.97506666, 0.9806, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97662
	best: 0.9787

Starting e_i: 1709
Model ind 665 epoch 1709 batch: 0 avg loss -2.977086 avg loss no lamb -2.977086 time 2020-06-27 17:34:56.197616
Model ind 665 epoch 1709 batch: 100 avg loss -2.903461 avg loss no lamb -2.903461 time 2020-06-27 17:35:08.395455
Model ind 665 epoch 1709 batch: 200 avg loss -2.902799 avg loss no lamb -2.902799 time 2020-06-27 17:35:18.283174
Model ind 665 epoch 1709 batch: 300 avg loss -2.828883 avg loss no lamb -2.828883 time 2020-06-27 17:35:27.859789
Model ind 665 epoch 1709 batch: 400 avg loss -2.819648 avg loss no lamb -2.819648 time 2020-06-27 17:35:38.189724
Model ind 665 epoch 1709 batch: 500 avg loss -2.888522 avg loss no lamb -2.888522 time 2020-06-27 17:35:47.654886
Model ind 665 epoch 1709 batch: 600 avg loss -2.911029 avg loss no lamb -2.911029 time 2020-06-27 17:35:57.098560
Model ind 665 epoch 1709 batch: 700 avg loss -2.749980 avg loss no lamb -2.749980 time 2020-06-27 17:36:06.846970
Model ind 665 epoch 1709 batch: 800 avg loss -2.865183 avg loss no lamb -2.865183 time 2020-06-27 17:36:16.371041
last batch sz 10
Pre: time 2020-06-27 17:36:28.911799: 
 	std: 0.0030705112
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.9796, 0.9733, 0.9801, 0.9738]
	train_accs: [0.98048335, 0.98015, 0.9747667, 0.9809, 0.97505]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.9773
	best: 0.9801

Starting e_i: 1710
Model ind 665 epoch 1710 batch: 0 avg loss -2.978019 avg loss no lamb -2.978019 time 2020-06-27 17:36:30.119727
Model ind 665 epoch 1710 batch: 100 avg loss -2.947769 avg loss no lamb -2.947769 time 2020-06-27 17:36:39.659468
Model ind 665 epoch 1710 batch: 200 avg loss -2.875803 avg loss no lamb -2.875803 time 2020-06-27 17:36:49.180745
Model ind 665 epoch 1710 batch: 300 avg loss -2.898679 avg loss no lamb -2.898679 time 2020-06-27 17:36:58.712627
Model ind 665 epoch 1710 batch: 400 avg loss -2.818589 avg loss no lamb -2.818589 time 2020-06-27 17:37:08.278069
Model ind 665 epoch 1710 batch: 500 avg loss -2.861552 avg loss no lamb -2.861552 time 2020-06-27 17:37:17.750302
Model ind 665 epoch 1710 batch: 600 avg loss -2.906683 avg loss no lamb -2.906683 time 2020-06-27 17:37:27.333999
Model ind 665 epoch 1710 batch: 700 avg loss -2.784164 avg loss no lamb -2.784164 time 2020-06-27 17:37:36.960623
Model ind 665 epoch 1710 batch: 800 avg loss -2.933403 avg loss no lamb -2.933403 time 2020-06-27 17:37:46.497111
last batch sz 10
Pre: time 2020-06-27 17:37:59.212024: 
 	std: 0.0039824606
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9805, 0.9716, 0.9814, 0.9743]
	train_accs: [0.98135, 0.9811, 0.9740833, 0.98145, 0.9755167]
	best_train_sub_head: 3
	worst: 0.9716
	avg: 0.97770005
	best: 0.9814

Starting e_i: 1711
Model ind 665 epoch 1711 batch: 0 avg loss -2.960839 avg loss no lamb -2.960839 time 2020-06-27 17:38:01.793458
Model ind 665 epoch 1711 batch: 100 avg loss -2.863256 avg loss no lamb -2.863256 time 2020-06-27 17:38:11.308333
Model ind 665 epoch 1711 batch: 200 avg loss -2.853903 avg loss no lamb -2.853903 time 2020-06-27 17:38:20.916039
Model ind 665 epoch 1711 batch: 300 avg loss -2.897423 avg loss no lamb -2.897423 time 2020-06-27 17:38:30.518602
Model ind 665 epoch 1711 batch: 400 avg loss -2.824476 avg loss no lamb -2.824476 time 2020-06-27 17:38:40.111479
Model ind 665 epoch 1711 batch: 500 avg loss -2.815166 avg loss no lamb -2.815166 time 2020-06-27 17:38:49.752483
Model ind 665 epoch 1711 batch: 600 avg loss -2.845073 avg loss no lamb -2.845073 time 2020-06-27 17:38:59.277933
Model ind 665 epoch 1711 batch: 700 avg loss -2.854033 avg loss no lamb -2.854033 time 2020-06-27 17:39:08.939205
Model ind 665 epoch 1711 batch: 800 avg loss -2.847990 avg loss no lamb -2.847990 time 2020-06-27 17:39:18.676625
last batch sz 10
Pre: time 2020-06-27 17:39:31.537305: 
 	std: 0.0033139724
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9783, 0.9713, 0.9803, 0.9747]
	train_accs: [0.98046666, 0.98008335, 0.97345, 0.98083335, 0.97538334]
	best_train_sub_head: 3
	worst: 0.9713
	avg: 0.97676
	best: 0.9803

Starting e_i: 1712
Model ind 665 epoch 1712 batch: 0 avg loss -2.968354 avg loss no lamb -2.968354 time 2020-06-27 17:39:32.764078
Model ind 665 epoch 1712 batch: 100 avg loss -2.946079 avg loss no lamb -2.946079 time 2020-06-27 17:39:42.305318
Model ind 665 epoch 1712 batch: 200 avg loss -2.891543 avg loss no lamb -2.891543 time 2020-06-27 17:39:51.836564
Model ind 665 epoch 1712 batch: 300 avg loss -2.819364 avg loss no lamb -2.819364 time 2020-06-27 17:40:01.376688
Model ind 665 epoch 1712 batch: 400 avg loss -2.809890 avg loss no lamb -2.809890 time 2020-06-27 17:40:10.813669
Model ind 665 epoch 1712 batch: 500 avg loss -2.824699 avg loss no lamb -2.824699 time 2020-06-27 17:40:20.343664
Model ind 665 epoch 1712 batch: 600 avg loss -2.852139 avg loss no lamb -2.852139 time 2020-06-27 17:40:29.821968
Model ind 665 epoch 1712 batch: 700 avg loss -2.727388 avg loss no lamb -2.727388 time 2020-06-27 17:40:41.757650
Model ind 665 epoch 1712 batch: 800 avg loss -2.924708 avg loss no lamb -2.924708 time 2020-06-27 17:40:52.351615
last batch sz 10
Pre: time 2020-06-27 17:41:05.447096: 
 	std: 0.0030146346
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9792, 0.9731, 0.98, 0.9742]
	train_accs: [0.9806333, 0.9806833, 0.97465, 0.9814, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97730005
	best: 0.98

Starting e_i: 1713
Model ind 665 epoch 1713 batch: 0 avg loss -2.949308 avg loss no lamb -2.949308 time 2020-06-27 17:41:06.704435
Model ind 665 epoch 1713 batch: 100 avg loss -2.930295 avg loss no lamb -2.930295 time 2020-06-27 17:41:16.717627
Model ind 665 epoch 1713 batch: 200 avg loss -2.935887 avg loss no lamb -2.935887 time 2020-06-27 17:41:26.509131
Model ind 665 epoch 1713 batch: 300 avg loss -2.850293 avg loss no lamb -2.850293 time 2020-06-27 17:41:36.228140
Model ind 665 epoch 1713 batch: 400 avg loss -2.800464 avg loss no lamb -2.800464 time 2020-06-27 17:41:45.976581
Model ind 665 epoch 1713 batch: 500 avg loss -2.876820 avg loss no lamb -2.876820 time 2020-06-27 17:41:55.675135
Model ind 665 epoch 1713 batch: 600 avg loss -2.941158 avg loss no lamb -2.941158 time 2020-06-27 17:42:05.526229
Model ind 665 epoch 1713 batch: 700 avg loss -2.753593 avg loss no lamb -2.753593 time 2020-06-27 17:42:15.248647
Model ind 665 epoch 1713 batch: 800 avg loss -2.900091 avg loss no lamb -2.900091 time 2020-06-27 17:42:25.143115
last batch sz 10
Pre: time 2020-06-27 17:42:38.574651: 
 	std: 0.0024758
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9781, 0.9771, 0.9717, 0.9784, 0.9751]
	train_accs: [0.98015, 0.9788833, 0.9741667, 0.98015, 0.9763333]
	best_train_sub_head: 0
	worst: 0.9717
	avg: 0.97607994
	best: 0.9781

Starting e_i: 1714
Model ind 665 epoch 1714 batch: 0 avg loss -2.982149 avg loss no lamb -2.982149 time 2020-06-27 17:42:39.823969
Model ind 665 epoch 1714 batch: 100 avg loss -2.843446 avg loss no lamb -2.843446 time 2020-06-27 17:42:49.676564
Model ind 665 epoch 1714 batch: 200 avg loss -2.868542 avg loss no lamb -2.868542 time 2020-06-27 17:42:59.557564
Model ind 665 epoch 1714 batch: 300 avg loss -2.854131 avg loss no lamb -2.854131 time 2020-06-27 17:43:09.368486
Model ind 665 epoch 1714 batch: 400 avg loss -2.847807 avg loss no lamb -2.847807 time 2020-06-27 17:43:19.209602
Model ind 665 epoch 1714 batch: 500 avg loss -2.855927 avg loss no lamb -2.855927 time 2020-06-27 17:43:29.032711
Model ind 665 epoch 1714 batch: 600 avg loss -2.918885 avg loss no lamb -2.918885 time 2020-06-27 17:43:38.905816
Model ind 665 epoch 1714 batch: 700 avg loss -2.824377 avg loss no lamb -2.824377 time 2020-06-27 17:43:48.764273
Model ind 665 epoch 1714 batch: 800 avg loss -2.907919 avg loss no lamb -2.907919 time 2020-06-27 17:43:58.475456
last batch sz 10
Pre: time 2020-06-27 17:44:11.384687: 
 	std: 0.0032946004
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9799, 0.9724, 0.9803, 0.9751]
	train_accs: [0.98111665, 0.9806, 0.9752167, 0.9813, 0.9763]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.97764
	best: 0.9803

Starting e_i: 1715
Model ind 665 epoch 1715 batch: 0 avg loss -2.964652 avg loss no lamb -2.964652 time 2020-06-27 17:44:12.635678
Model ind 665 epoch 1715 batch: 100 avg loss -2.916409 avg loss no lamb -2.916409 time 2020-06-27 17:44:22.526759
Model ind 665 epoch 1715 batch: 200 avg loss -2.931073 avg loss no lamb -2.931073 time 2020-06-27 17:44:32.427309
Model ind 665 epoch 1715 batch: 300 avg loss -2.889065 avg loss no lamb -2.889065 time 2020-06-27 17:44:42.227654
Model ind 665 epoch 1715 batch: 400 avg loss -2.855650 avg loss no lamb -2.855650 time 2020-06-27 17:44:52.102682
Model ind 665 epoch 1715 batch: 500 avg loss -2.896435 avg loss no lamb -2.896435 time 2020-06-27 17:45:01.800796
Model ind 665 epoch 1715 batch: 600 avg loss -2.855504 avg loss no lamb -2.855504 time 2020-06-27 17:45:11.634531
Model ind 665 epoch 1715 batch: 700 avg loss -2.806134 avg loss no lamb -2.806134 time 2020-06-27 17:45:21.257817
Model ind 665 epoch 1715 batch: 800 avg loss -2.907587 avg loss no lamb -2.907587 time 2020-06-27 17:45:31.090978
last batch sz 10
Pre: time 2020-06-27 17:45:43.983551: 
 	std: 0.0035617906
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9765, 0.9703, 0.9793, 0.9732]
	train_accs: [0.9809167, 0.97975, 0.974, 0.98115, 0.9755667]
	best_train_sub_head: 3
	worst: 0.9703
	avg: 0.97576
	best: 0.9793

Starting e_i: 1716
Model ind 665 epoch 1716 batch: 0 avg loss -2.931167 avg loss no lamb -2.931167 time 2020-06-27 17:45:45.452359
Model ind 665 epoch 1716 batch: 100 avg loss -2.924287 avg loss no lamb -2.924287 time 2020-06-27 17:45:55.324641
Model ind 665 epoch 1716 batch: 200 avg loss -2.937620 avg loss no lamb -2.937620 time 2020-06-27 17:46:05.050459
Model ind 665 epoch 1716 batch: 300 avg loss -2.867573 avg loss no lamb -2.867573 time 2020-06-27 17:46:14.919753
Model ind 665 epoch 1716 batch: 400 avg loss -2.797787 avg loss no lamb -2.797787 time 2020-06-27 17:46:24.754536
Model ind 665 epoch 1716 batch: 500 avg loss -2.860554 avg loss no lamb -2.860554 time 2020-06-27 17:46:34.447240
Model ind 665 epoch 1716 batch: 600 avg loss -2.896891 avg loss no lamb -2.896891 time 2020-06-27 17:46:44.317839
Model ind 665 epoch 1716 batch: 700 avg loss -2.836481 avg loss no lamb -2.836481 time 2020-06-27 17:46:54.263540
Model ind 665 epoch 1716 batch: 800 avg loss -2.907332 avg loss no lamb -2.907332 time 2020-06-27 17:47:04.025575
last batch sz 10
Pre: time 2020-06-27 17:47:16.924106: 
 	std: 0.0023366557
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9795, 0.9749, 0.9801, 0.9752]
	train_accs: [0.9809167, 0.9806167, 0.97578335, 0.9810333, 0.9766]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.9779
	best: 0.9801

Starting e_i: 1717
Model ind 665 epoch 1717 batch: 0 avg loss -2.968781 avg loss no lamb -2.968781 time 2020-06-27 17:47:18.132086
Model ind 665 epoch 1717 batch: 100 avg loss -2.914664 avg loss no lamb -2.914664 time 2020-06-27 17:47:27.815623
Model ind 665 epoch 1717 batch: 200 avg loss -2.880888 avg loss no lamb -2.880888 time 2020-06-27 17:47:37.655927
Model ind 665 epoch 1717 batch: 300 avg loss -2.920460 avg loss no lamb -2.920460 time 2020-06-27 17:47:47.380816
Model ind 665 epoch 1717 batch: 400 avg loss -2.783024 avg loss no lamb -2.783024 time 2020-06-27 17:47:57.303299
Model ind 665 epoch 1717 batch: 500 avg loss -2.875339 avg loss no lamb -2.875339 time 2020-06-27 17:48:07.122465
Model ind 665 epoch 1717 batch: 600 avg loss -2.873040 avg loss no lamb -2.873040 time 2020-06-27 17:48:16.826480
Model ind 665 epoch 1717 batch: 700 avg loss -2.830072 avg loss no lamb -2.830072 time 2020-06-27 17:48:26.752231
Model ind 665 epoch 1717 batch: 800 avg loss -2.874608 avg loss no lamb -2.874608 time 2020-06-27 17:48:36.554347
last batch sz 10
Pre: time 2020-06-27 17:48:49.453904: 
 	std: 0.0031103657
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9801, 0.974, 0.9813, 0.975]
	train_accs: [0.98143333, 0.98071665, 0.97573334, 0.9813667, 0.97678334]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.9782599
	best: 0.9809

Starting e_i: 1718
Model ind 665 epoch 1718 batch: 0 avg loss -2.933051 avg loss no lamb -2.933051 time 2020-06-27 17:48:50.697134
Model ind 665 epoch 1718 batch: 100 avg loss -2.955039 avg loss no lamb -2.955039 time 2020-06-27 17:49:00.477867
Model ind 665 epoch 1718 batch: 200 avg loss -2.899365 avg loss no lamb -2.899365 time 2020-06-27 17:49:10.324265
Model ind 665 epoch 1718 batch: 300 avg loss -2.838798 avg loss no lamb -2.838798 time 2020-06-27 17:49:20.173065
Model ind 665 epoch 1718 batch: 400 avg loss -2.809738 avg loss no lamb -2.809738 time 2020-06-27 17:49:29.928024
Model ind 665 epoch 1718 batch: 500 avg loss -2.885731 avg loss no lamb -2.885731 time 2020-06-27 17:49:39.654555
Model ind 665 epoch 1718 batch: 600 avg loss -2.891116 avg loss no lamb -2.891116 time 2020-06-27 17:49:49.435535
Model ind 665 epoch 1718 batch: 700 avg loss -2.800086 avg loss no lamb -2.800086 time 2020-06-27 17:49:59.216564
Model ind 665 epoch 1718 batch: 800 avg loss -2.883322 avg loss no lamb -2.883322 time 2020-06-27 17:50:09.123705
last batch sz 10
Pre: time 2020-06-27 17:50:22.466464: 
 	std: 0.0032077406
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9814, 0.9813, 0.9749, 0.9819, 0.9751]
	train_accs: [0.98156667, 0.9812, 0.97618335, 0.98183334, 0.97658336]
	best_train_sub_head: 3
	worst: 0.9749
	avg: 0.97892
	best: 0.9819

Starting e_i: 1719
Model ind 665 epoch 1719 batch: 0 avg loss -2.937250 avg loss no lamb -2.937250 time 2020-06-27 17:50:23.701848
Model ind 665 epoch 1719 batch: 100 avg loss -2.912101 avg loss no lamb -2.912101 time 2020-06-27 17:50:33.457126
Model ind 665 epoch 1719 batch: 200 avg loss -2.912827 avg loss no lamb -2.912827 time 2020-06-27 17:50:43.281441
Model ind 665 epoch 1719 batch: 300 avg loss -2.896654 avg loss no lamb -2.896654 time 2020-06-27 17:50:53.136883
Model ind 665 epoch 1719 batch: 400 avg loss -2.795805 avg loss no lamb -2.795805 time 2020-06-27 17:51:02.925895
Model ind 665 epoch 1719 batch: 500 avg loss -2.848852 avg loss no lamb -2.848852 time 2020-06-27 17:51:12.810113
Model ind 665 epoch 1719 batch: 600 avg loss -2.841350 avg loss no lamb -2.841350 time 2020-06-27 17:51:22.551810
Model ind 665 epoch 1719 batch: 700 avg loss -2.844303 avg loss no lamb -2.844303 time 2020-06-27 17:51:32.288463
Model ind 665 epoch 1719 batch: 800 avg loss -2.914159 avg loss no lamb -2.914159 time 2020-06-27 17:51:42.032173
last batch sz 10
Pre: time 2020-06-27 17:51:55.268327: 
 	std: 0.0029902451
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9797, 0.9739, 0.9806, 0.9746]
	train_accs: [0.98121667, 0.9805833, 0.97491664, 0.9813833, 0.97581667]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97788
	best: 0.9806

Starting e_i: 1720
Model ind 665 epoch 1720 batch: 0 avg loss -2.992208 avg loss no lamb -2.992208 time 2020-06-27 17:51:56.515758
Model ind 665 epoch 1720 batch: 100 avg loss -2.837148 avg loss no lamb -2.837148 time 2020-06-27 17:52:06.436620
Model ind 665 epoch 1720 batch: 200 avg loss -2.928094 avg loss no lamb -2.928094 time 2020-06-27 17:52:16.247053
Model ind 665 epoch 1720 batch: 300 avg loss -2.877784 avg loss no lamb -2.877784 time 2020-06-27 17:52:26.016121
Model ind 665 epoch 1720 batch: 400 avg loss -2.869214 avg loss no lamb -2.869214 time 2020-06-27 17:52:35.798196
Model ind 665 epoch 1720 batch: 500 avg loss -2.860451 avg loss no lamb -2.860451 time 2020-06-27 17:52:45.581489
Model ind 665 epoch 1720 batch: 600 avg loss -2.921349 avg loss no lamb -2.921349 time 2020-06-27 17:52:55.450778
Model ind 665 epoch 1720 batch: 700 avg loss -2.828839 avg loss no lamb -2.828839 time 2020-06-27 17:53:05.214491
Model ind 665 epoch 1720 batch: 800 avg loss -2.837261 avg loss no lamb -2.837261 time 2020-06-27 17:53:14.960535
last batch sz 10
Pre: time 2020-06-27 17:53:28.127464: 
 	std: 0.0029110834
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9808, 0.9748, 0.9811, 0.9752]
	train_accs: [0.9815, 0.9810333, 0.9759833, 0.98145, 0.97645]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.9785601
	best: 0.9809

Starting e_i: 1721
Model ind 665 epoch 1721 batch: 0 avg loss -2.973996 avg loss no lamb -2.973996 time 2020-06-27 17:53:30.533366
Model ind 665 epoch 1721 batch: 100 avg loss -2.888151 avg loss no lamb -2.888151 time 2020-06-27 17:53:40.305441
Model ind 665 epoch 1721 batch: 200 avg loss -2.897527 avg loss no lamb -2.897527 time 2020-06-27 17:53:50.128295
Model ind 665 epoch 1721 batch: 300 avg loss -2.900442 avg loss no lamb -2.900442 time 2020-06-27 17:53:59.950713
Model ind 665 epoch 1721 batch: 400 avg loss -2.819177 avg loss no lamb -2.819177 time 2020-06-27 17:54:09.759309
Model ind 665 epoch 1721 batch: 500 avg loss -2.899753 avg loss no lamb -2.899753 time 2020-06-27 17:54:19.536453
Model ind 665 epoch 1721 batch: 600 avg loss -2.893722 avg loss no lamb -2.893722 time 2020-06-27 17:54:29.385406
Model ind 665 epoch 1721 batch: 700 avg loss -2.823292 avg loss no lamb -2.823292 time 2020-06-27 17:54:39.222192
Model ind 665 epoch 1721 batch: 800 avg loss -2.828014 avg loss no lamb -2.828014 time 2020-06-27 17:54:49.036265
last batch sz 10
Pre: time 2020-06-27 17:55:01.827835: 
 	std: 0.0026919213
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9804, 0.9802, 0.9746, 0.9804, 0.9751]
	train_accs: [0.98115, 0.98065, 0.97565, 0.98115, 0.9766]
	best_train_sub_head: 0
	worst: 0.9746
	avg: 0.97814
	best: 0.9804

Starting e_i: 1722
Model ind 665 epoch 1722 batch: 0 avg loss -2.948890 avg loss no lamb -2.948890 time 2020-06-27 17:55:03.043754
Model ind 665 epoch 1722 batch: 100 avg loss -2.968410 avg loss no lamb -2.968410 time 2020-06-27 17:55:12.745709
Model ind 665 epoch 1722 batch: 200 avg loss -2.929062 avg loss no lamb -2.929062 time 2020-06-27 17:55:22.505155
Model ind 665 epoch 1722 batch: 300 avg loss -2.819333 avg loss no lamb -2.819333 time 2020-06-27 17:55:32.323921
Model ind 665 epoch 1722 batch: 400 avg loss -2.819970 avg loss no lamb -2.819970 time 2020-06-27 17:55:42.183309
Model ind 665 epoch 1722 batch: 500 avg loss -2.885449 avg loss no lamb -2.885449 time 2020-06-27 17:55:51.893227
Model ind 665 epoch 1722 batch: 600 avg loss -2.874060 avg loss no lamb -2.874060 time 2020-06-27 17:56:01.637316
Model ind 665 epoch 1722 batch: 700 avg loss -2.852616 avg loss no lamb -2.852616 time 2020-06-27 17:56:11.330512
Model ind 665 epoch 1722 batch: 800 avg loss -2.914850 avg loss no lamb -2.914850 time 2020-06-27 17:56:21.288934
last batch sz 10
Pre: time 2020-06-27 17:56:34.286378: 
 	std: 0.0030709116
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9795, 0.9736, 0.9804, 0.974]
	train_accs: [0.98076665, 0.9803, 0.97475, 0.98095, 0.97566664]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97754
	best: 0.9804

Starting e_i: 1723
Model ind 665 epoch 1723 batch: 0 avg loss -2.997674 avg loss no lamb -2.997674 time 2020-06-27 17:56:35.738295
Model ind 665 epoch 1723 batch: 100 avg loss -2.916360 avg loss no lamb -2.916360 time 2020-06-27 17:56:45.547609
Model ind 665 epoch 1723 batch: 200 avg loss -2.900341 avg loss no lamb -2.900341 time 2020-06-27 17:56:55.387112
Model ind 665 epoch 1723 batch: 300 avg loss -2.885171 avg loss no lamb -2.885171 time 2020-06-27 17:57:05.089873
Model ind 665 epoch 1723 batch: 400 avg loss -2.845261 avg loss no lamb -2.845261 time 2020-06-27 17:57:14.950367
Model ind 665 epoch 1723 batch: 500 avg loss -2.831595 avg loss no lamb -2.831595 time 2020-06-27 17:57:24.755522
Model ind 665 epoch 1723 batch: 600 avg loss -2.900820 avg loss no lamb -2.900820 time 2020-06-27 17:57:34.558205
Model ind 665 epoch 1723 batch: 700 avg loss -2.771437 avg loss no lamb -2.771437 time 2020-06-27 17:57:44.413279
Model ind 665 epoch 1723 batch: 800 avg loss -2.853092 avg loss no lamb -2.853092 time 2020-06-27 17:57:54.185200
last batch sz 10
Pre: time 2020-06-27 17:58:06.987131: 
 	std: 0.0029328442
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9796, 0.9738, 0.981, 0.975]
	train_accs: [0.9812833, 0.9806, 0.9752, 0.9814, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97791994
	best: 0.981

Starting e_i: 1724
Model ind 665 epoch 1724 batch: 0 avg loss -2.998930 avg loss no lamb -2.998930 time 2020-06-27 17:58:08.231891
Model ind 665 epoch 1724 batch: 100 avg loss -2.907260 avg loss no lamb -2.907260 time 2020-06-27 17:58:18.245964
Model ind 665 epoch 1724 batch: 200 avg loss -2.861432 avg loss no lamb -2.861432 time 2020-06-27 17:58:28.091025
Model ind 665 epoch 1724 batch: 300 avg loss -2.877810 avg loss no lamb -2.877810 time 2020-06-27 17:58:37.861186
Model ind 665 epoch 1724 batch: 400 avg loss -2.784544 avg loss no lamb -2.784544 time 2020-06-27 17:58:47.661369
Model ind 665 epoch 1724 batch: 500 avg loss -2.829530 avg loss no lamb -2.829530 time 2020-06-27 17:58:57.430193
Model ind 665 epoch 1724 batch: 600 avg loss -2.897706 avg loss no lamb -2.897706 time 2020-06-27 17:59:07.261632
Model ind 665 epoch 1724 batch: 700 avg loss -2.791448 avg loss no lamb -2.791448 time 2020-06-27 17:59:17.042941
Model ind 665 epoch 1724 batch: 800 avg loss -2.917336 avg loss no lamb -2.917336 time 2020-06-27 17:59:26.750135
last batch sz 10
Pre: time 2020-06-27 17:59:39.469554: 
 	std: 0.0025814802
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9785, 0.9791, 0.9736, 0.9791, 0.9737]
	train_accs: [0.98036665, 0.98025, 0.97475, 0.98045, 0.97546667]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97679996
	best: 0.9791

Starting e_i: 1725
Model ind 665 epoch 1725 batch: 0 avg loss -2.985851 avg loss no lamb -2.985851 time 2020-06-27 17:59:40.724576
Model ind 665 epoch 1725 batch: 100 avg loss -2.928807 avg loss no lamb -2.928807 time 2020-06-27 17:59:50.396380
Model ind 665 epoch 1725 batch: 200 avg loss -2.892924 avg loss no lamb -2.892924 time 2020-06-27 18:00:00.386036
Model ind 665 epoch 1725 batch: 300 avg loss -2.885144 avg loss no lamb -2.885144 time 2020-06-27 18:00:10.143489
Model ind 665 epoch 1725 batch: 400 avg loss -2.819573 avg loss no lamb -2.819573 time 2020-06-27 18:00:19.973259
Model ind 665 epoch 1725 batch: 500 avg loss -2.885862 avg loss no lamb -2.885862 time 2020-06-27 18:00:29.736170
Model ind 665 epoch 1725 batch: 600 avg loss -2.903013 avg loss no lamb -2.903013 time 2020-06-27 18:00:39.494787
Model ind 665 epoch 1725 batch: 700 avg loss -2.783904 avg loss no lamb -2.783904 time 2020-06-27 18:00:49.231072
Model ind 665 epoch 1725 batch: 800 avg loss -2.875928 avg loss no lamb -2.875928 time 2020-06-27 18:00:58.932950
last batch sz 10
Pre: time 2020-06-27 18:01:12.162561: 
 	std: 0.0031193686
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9796, 0.9729, 0.9794, 0.973]
	train_accs: [0.98055, 0.9801, 0.97473335, 0.98078334, 0.9755333]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97676
	best: 0.9794

Starting e_i: 1726
Model ind 665 epoch 1726 batch: 0 avg loss -2.891934 avg loss no lamb -2.891934 time 2020-06-27 18:01:13.345785
Model ind 665 epoch 1726 batch: 100 avg loss -2.949006 avg loss no lamb -2.949006 time 2020-06-27 18:01:23.157138
Model ind 665 epoch 1726 batch: 200 avg loss -2.881720 avg loss no lamb -2.881720 time 2020-06-27 18:01:32.963373
Model ind 665 epoch 1726 batch: 300 avg loss -2.922021 avg loss no lamb -2.922021 time 2020-06-27 18:01:42.716287
Model ind 665 epoch 1726 batch: 400 avg loss -2.808236 avg loss no lamb -2.808236 time 2020-06-27 18:01:52.540398
Model ind 665 epoch 1726 batch: 500 avg loss -2.826555 avg loss no lamb -2.826555 time 2020-06-27 18:02:02.464599
Model ind 665 epoch 1726 batch: 600 avg loss -2.918499 avg loss no lamb -2.918499 time 2020-06-27 18:02:12.335110
Model ind 665 epoch 1726 batch: 700 avg loss -2.822271 avg loss no lamb -2.822271 time 2020-06-27 18:02:22.264769
Model ind 665 epoch 1726 batch: 800 avg loss -2.890117 avg loss no lamb -2.890117 time 2020-06-27 18:02:32.161323
last batch sz 10
Pre: time 2020-06-27 18:02:45.182555: 
 	std: 0.0028772096
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9803, 0.9739, 0.9801, 0.9746]
	train_accs: [0.98141664, 0.9809833, 0.97555, 0.98153335, 0.9764]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97776
	best: 0.9801

Starting e_i: 1727
Model ind 665 epoch 1727 batch: 0 avg loss -2.942577 avg loss no lamb -2.942577 time 2020-06-27 18:02:46.443389
Model ind 665 epoch 1727 batch: 100 avg loss -2.885447 avg loss no lamb -2.885447 time 2020-06-27 18:02:56.226962
Model ind 665 epoch 1727 batch: 200 avg loss -2.872048 avg loss no lamb -2.872048 time 2020-06-27 18:03:05.942113
Model ind 665 epoch 1727 batch: 300 avg loss -2.909348 avg loss no lamb -2.909348 time 2020-06-27 18:03:15.792494
Model ind 665 epoch 1727 batch: 400 avg loss -2.880147 avg loss no lamb -2.880147 time 2020-06-27 18:03:25.724707
Model ind 665 epoch 1727 batch: 500 avg loss -2.857691 avg loss no lamb -2.857691 time 2020-06-27 18:03:35.667670
Model ind 665 epoch 1727 batch: 600 avg loss -2.889673 avg loss no lamb -2.889673 time 2020-06-27 18:03:45.584393
Model ind 665 epoch 1727 batch: 700 avg loss -2.802719 avg loss no lamb -2.802719 time 2020-06-27 18:03:55.291609
Model ind 665 epoch 1727 batch: 800 avg loss -2.888686 avg loss no lamb -2.888686 time 2020-06-27 18:04:05.057826
last batch sz 10
Pre: time 2020-06-27 18:04:18.256386: 
 	std: 0.0029903937
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9782, 0.9779, 0.972, 0.9782, 0.972]
	train_accs: [0.98076665, 0.9801, 0.9742333, 0.9806833, 0.9748333]
	best_train_sub_head: 0
	worst: 0.972
	avg: 0.97566
	best: 0.9782

Starting e_i: 1728
Model ind 665 epoch 1728 batch: 0 avg loss -2.931023 avg loss no lamb -2.931023 time 2020-06-27 18:04:19.514916
Model ind 665 epoch 1728 batch: 100 avg loss -2.924276 avg loss no lamb -2.924276 time 2020-06-27 18:04:29.160305
Model ind 665 epoch 1728 batch: 200 avg loss -2.845309 avg loss no lamb -2.845309 time 2020-06-27 18:04:38.879290
Model ind 665 epoch 1728 batch: 300 avg loss -2.887999 avg loss no lamb -2.887999 time 2020-06-27 18:04:48.649606
Model ind 665 epoch 1728 batch: 400 avg loss -2.889780 avg loss no lamb -2.889780 time 2020-06-27 18:04:58.541105
Model ind 665 epoch 1728 batch: 500 avg loss -2.864713 avg loss no lamb -2.864713 time 2020-06-27 18:05:08.474266
Model ind 665 epoch 1728 batch: 600 avg loss -2.906386 avg loss no lamb -2.906386 time 2020-06-27 18:05:18.397696
Model ind 665 epoch 1728 batch: 700 avg loss -2.858112 avg loss no lamb -2.858112 time 2020-06-27 18:05:28.164314
Model ind 665 epoch 1728 batch: 800 avg loss -2.913289 avg loss no lamb -2.913289 time 2020-06-27 18:05:37.911563
last batch sz 10
Pre: time 2020-06-27 18:05:50.741991: 
 	std: 0.002540561
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.978, 0.9767, 0.9722, 0.9778, 0.9726]
	train_accs: [0.98048335, 0.97968334, 0.97496665, 0.98076665, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97545993
	best: 0.9778

Starting e_i: 1729
Model ind 665 epoch 1729 batch: 0 avg loss -2.940509 avg loss no lamb -2.940509 time 2020-06-27 18:05:51.943211
Model ind 665 epoch 1729 batch: 100 avg loss -2.885024 avg loss no lamb -2.885024 time 2020-06-27 18:06:01.889998
Model ind 665 epoch 1729 batch: 200 avg loss -2.850179 avg loss no lamb -2.850179 time 2020-06-27 18:06:11.633525
Model ind 665 epoch 1729 batch: 300 avg loss -2.853047 avg loss no lamb -2.853047 time 2020-06-27 18:06:21.429485
Model ind 665 epoch 1729 batch: 400 avg loss -2.769094 avg loss no lamb -2.769094 time 2020-06-27 18:06:31.086480
Model ind 665 epoch 1729 batch: 500 avg loss -2.908118 avg loss no lamb -2.908118 time 2020-06-27 18:06:40.772909
Model ind 665 epoch 1729 batch: 600 avg loss -2.891469 avg loss no lamb -2.891469 time 2020-06-27 18:06:50.685718
Model ind 665 epoch 1729 batch: 700 avg loss -2.822150 avg loss no lamb -2.822150 time 2020-06-27 18:07:00.614958
Model ind 665 epoch 1729 batch: 800 avg loss -2.910257 avg loss no lamb -2.910257 time 2020-06-27 18:07:10.398173
last batch sz 10
Pre: time 2020-06-27 18:07:23.361020: 
 	std: 0.0023198368
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9782, 0.9781, 0.9738, 0.9785, 0.9733]
	train_accs: [0.98066664, 0.98048335, 0.97543335, 0.98085, 0.97578335]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97638
	best: 0.9785

Starting e_i: 1730
Model ind 665 epoch 1730 batch: 0 avg loss -2.947322 avg loss no lamb -2.947322 time 2020-06-27 18:07:24.807665
Model ind 665 epoch 1730 batch: 100 avg loss -2.907547 avg loss no lamb -2.907547 time 2020-06-27 18:07:34.719715
Model ind 665 epoch 1730 batch: 200 avg loss -2.921973 avg loss no lamb -2.921973 time 2020-06-27 18:07:44.563953
Model ind 665 epoch 1730 batch: 300 avg loss -2.882766 avg loss no lamb -2.882766 time 2020-06-27 18:07:54.338839
Model ind 665 epoch 1730 batch: 400 avg loss -2.818756 avg loss no lamb -2.818756 time 2020-06-27 18:08:04.132705
Model ind 665 epoch 1730 batch: 500 avg loss -2.926425 avg loss no lamb -2.926425 time 2020-06-27 18:08:13.931451
Model ind 665 epoch 1730 batch: 600 avg loss -2.856925 avg loss no lamb -2.856925 time 2020-06-27 18:08:23.806064
Model ind 665 epoch 1730 batch: 700 avg loss -2.802324 avg loss no lamb -2.802324 time 2020-06-27 18:08:33.596521
Model ind 665 epoch 1730 batch: 800 avg loss -2.905154 avg loss no lamb -2.905154 time 2020-06-27 18:08:43.477186
last batch sz 10
Pre: time 2020-06-27 18:08:56.510896: 
 	std: 0.0026328706
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.978, 0.973, 0.9787, 0.9734]
	train_accs: [0.9810333, 0.98035, 0.9755, 0.9809833, 0.9759333]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.9764
	best: 0.9789

Starting e_i: 1731
Model ind 665 epoch 1731 batch: 0 avg loss -2.942786 avg loss no lamb -2.942786 time 2020-06-27 18:08:58.951330
Model ind 665 epoch 1731 batch: 100 avg loss -2.876109 avg loss no lamb -2.876109 time 2020-06-27 18:09:08.632733
Model ind 665 epoch 1731 batch: 200 avg loss -2.898098 avg loss no lamb -2.898098 time 2020-06-27 18:09:18.554203
Model ind 665 epoch 1731 batch: 300 avg loss -2.869757 avg loss no lamb -2.869757 time 2020-06-27 18:09:28.251394
Model ind 665 epoch 1731 batch: 400 avg loss -2.870460 avg loss no lamb -2.870460 time 2020-06-27 18:09:37.893015
Model ind 665 epoch 1731 batch: 500 avg loss -2.901947 avg loss no lamb -2.901947 time 2020-06-27 18:09:47.601251
Model ind 665 epoch 1731 batch: 600 avg loss -2.884883 avg loss no lamb -2.884883 time 2020-06-27 18:09:57.347897
Model ind 665 epoch 1731 batch: 700 avg loss -2.830582 avg loss no lamb -2.830582 time 2020-06-27 18:10:07.164384
Model ind 665 epoch 1731 batch: 800 avg loss -2.878283 avg loss no lamb -2.878283 time 2020-06-27 18:10:17.066313
last batch sz 10
Pre: time 2020-06-27 18:10:29.989991: 
 	std: 0.0026753745
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9788, 0.9775, 0.9727, 0.9792, 0.9737]
	train_accs: [0.98066664, 0.98015, 0.97491664, 0.98078334, 0.9752667]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97638
	best: 0.9792

Starting e_i: 1732
Model ind 665 epoch 1732 batch: 0 avg loss -3.007019 avg loss no lamb -3.007019 time 2020-06-27 18:10:31.220394
Model ind 665 epoch 1732 batch: 100 avg loss -2.938881 avg loss no lamb -2.938881 time 2020-06-27 18:10:41.097411
Model ind 665 epoch 1732 batch: 200 avg loss -2.894619 avg loss no lamb -2.894619 time 2020-06-27 18:10:50.968208
Model ind 665 epoch 1732 batch: 300 avg loss -2.856711 avg loss no lamb -2.856711 time 2020-06-27 18:11:00.734251
Model ind 665 epoch 1732 batch: 400 avg loss -2.849112 avg loss no lamb -2.849112 time 2020-06-27 18:11:10.503976
Model ind 665 epoch 1732 batch: 500 avg loss -2.816830 avg loss no lamb -2.816830 time 2020-06-27 18:11:20.391884
Model ind 665 epoch 1732 batch: 600 avg loss -2.924432 avg loss no lamb -2.924432 time 2020-06-27 18:11:30.072202
Model ind 665 epoch 1732 batch: 700 avg loss -2.831154 avg loss no lamb -2.831154 time 2020-06-27 18:11:39.779075
Model ind 665 epoch 1732 batch: 800 avg loss -2.871898 avg loss no lamb -2.871898 time 2020-06-27 18:11:49.521309
last batch sz 10
Pre: time 2020-06-27 18:12:02.750476: 
 	std: 0.002347258
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9789, 0.9743, 0.9793, 0.9744]
	train_accs: [0.98121667, 0.98073334, 0.97615, 0.9812833, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9743
	avg: 0.97722006
	best: 0.9793

Starting e_i: 1733
Model ind 665 epoch 1733 batch: 0 avg loss -2.989645 avg loss no lamb -2.989645 time 2020-06-27 18:12:03.970862
Model ind 665 epoch 1733 batch: 100 avg loss -2.962237 avg loss no lamb -2.962237 time 2020-06-27 18:12:13.739435
Model ind 665 epoch 1733 batch: 200 avg loss -2.864216 avg loss no lamb -2.864216 time 2020-06-27 18:12:23.345897
Model ind 665 epoch 1733 batch: 300 avg loss -2.907579 avg loss no lamb -2.907579 time 2020-06-27 18:12:33.004417
Model ind 665 epoch 1733 batch: 400 avg loss -2.828406 avg loss no lamb -2.828406 time 2020-06-27 18:12:42.881875
Model ind 665 epoch 1733 batch: 500 avg loss -2.846843 avg loss no lamb -2.846843 time 2020-06-27 18:12:52.663545
Model ind 665 epoch 1733 batch: 600 avg loss -2.916612 avg loss no lamb -2.916612 time 2020-06-27 18:13:02.526380
Model ind 665 epoch 1733 batch: 700 avg loss -2.841549 avg loss no lamb -2.841549 time 2020-06-27 18:13:12.350629
Model ind 665 epoch 1733 batch: 800 avg loss -2.858834 avg loss no lamb -2.858834 time 2020-06-27 18:13:22.207819
last batch sz 10
Pre: time 2020-06-27 18:13:35.219795: 
 	std: 0.0033589147
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9784, 0.9763, 0.9718, 0.9787, 0.9706]
	train_accs: [0.9808667, 0.9796, 0.97395, 0.9809167, 0.97425]
	best_train_sub_head: 3
	worst: 0.9706
	avg: 0.97516
	best: 0.9787

Starting e_i: 1734
Model ind 665 epoch 1734 batch: 0 avg loss -2.968451 avg loss no lamb -2.968451 time 2020-06-27 18:13:36.571163
Model ind 665 epoch 1734 batch: 100 avg loss -2.969549 avg loss no lamb -2.969549 time 2020-06-27 18:13:46.443309
Model ind 665 epoch 1734 batch: 200 avg loss -2.945747 avg loss no lamb -2.945747 time 2020-06-27 18:13:56.407846
Model ind 665 epoch 1734 batch: 300 avg loss -2.879652 avg loss no lamb -2.879652 time 2020-06-27 18:14:06.320803
Model ind 665 epoch 1734 batch: 400 avg loss -2.886143 avg loss no lamb -2.886143 time 2020-06-27 18:14:16.002868
Model ind 665 epoch 1734 batch: 500 avg loss -2.857780 avg loss no lamb -2.857780 time 2020-06-27 18:14:25.830714
Model ind 665 epoch 1734 batch: 600 avg loss -2.889627 avg loss no lamb -2.889627 time 2020-06-27 18:14:35.643846
Model ind 665 epoch 1734 batch: 700 avg loss -2.841441 avg loss no lamb -2.841441 time 2020-06-27 18:14:45.413801
Model ind 665 epoch 1734 batch: 800 avg loss -2.859684 avg loss no lamb -2.859684 time 2020-06-27 18:14:55.076196
last batch sz 10
Pre: time 2020-06-27 18:15:08.574223: 
 	std: 0.003034072
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9808, 0.9744, 0.9813, 0.9756]
	train_accs: [0.98175, 0.9813, 0.97576666, 0.98186666, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.97868
	best: 0.9813

Starting e_i: 1735
Model ind 665 epoch 1735 batch: 0 avg loss -2.973053 avg loss no lamb -2.973053 time 2020-06-27 18:15:09.797861
Model ind 665 epoch 1735 batch: 100 avg loss -2.897011 avg loss no lamb -2.897011 time 2020-06-27 18:15:21.455490
Model ind 665 epoch 1735 batch: 200 avg loss -2.908706 avg loss no lamb -2.908706 time 2020-06-27 18:15:33.304016
Model ind 665 epoch 1735 batch: 300 avg loss -2.897521 avg loss no lamb -2.897521 time 2020-06-27 18:15:43.099101
Model ind 665 epoch 1735 batch: 400 avg loss -2.858375 avg loss no lamb -2.858375 time 2020-06-27 18:15:53.057617
Model ind 665 epoch 1735 batch: 500 avg loss -2.824676 avg loss no lamb -2.824676 time 2020-06-27 18:16:02.999247
Model ind 665 epoch 1735 batch: 600 avg loss -2.905548 avg loss no lamb -2.905548 time 2020-06-27 18:16:12.981422
Model ind 665 epoch 1735 batch: 700 avg loss -2.776299 avg loss no lamb -2.776299 time 2020-06-27 18:16:22.712669
Model ind 665 epoch 1735 batch: 800 avg loss -2.851586 avg loss no lamb -2.851586 time 2020-06-27 18:16:32.578570
last batch sz 10
Pre: time 2020-06-27 18:16:45.774613: 
 	std: 0.0028032851
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9781, 0.9721, 0.9785, 0.9734]
	train_accs: [0.9809333, 0.9802, 0.9741167, 0.98113334, 0.9755333]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97614
	best: 0.9785

Starting e_i: 1736
Model ind 665 epoch 1736 batch: 0 avg loss -2.910864 avg loss no lamb -2.910864 time 2020-06-27 18:16:47.128597
Model ind 665 epoch 1736 batch: 100 avg loss -2.961215 avg loss no lamb -2.961215 time 2020-06-27 18:16:56.865861
Model ind 665 epoch 1736 batch: 200 avg loss -2.938248 avg loss no lamb -2.938248 time 2020-06-27 18:17:06.673198
Model ind 665 epoch 1736 batch: 300 avg loss -2.823711 avg loss no lamb -2.823711 time 2020-06-27 18:17:16.412672
Model ind 665 epoch 1736 batch: 400 avg loss -2.805789 avg loss no lamb -2.805789 time 2020-06-27 18:17:26.153430
Model ind 665 epoch 1736 batch: 500 avg loss -2.860599 avg loss no lamb -2.860599 time 2020-06-27 18:17:35.939457
Model ind 665 epoch 1736 batch: 600 avg loss -2.949016 avg loss no lamb -2.949016 time 2020-06-27 18:17:45.731829
Model ind 665 epoch 1736 batch: 700 avg loss -2.828936 avg loss no lamb -2.828936 time 2020-06-27 18:17:55.643155
Model ind 665 epoch 1736 batch: 800 avg loss -2.916922 avg loss no lamb -2.916922 time 2020-06-27 18:18:05.373745
last batch sz 10
Pre: time 2020-06-27 18:18:18.613224: 
 	std: 0.003386452
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.9778, 0.9708, 0.9786, 0.9727]
	train_accs: [0.98076665, 0.9801, 0.9740667, 0.98055, 0.97531664]
	best_train_sub_head: 0
	worst: 0.9708
	avg: 0.97580004
	best: 0.9791

Starting e_i: 1737
Model ind 665 epoch 1737 batch: 0 avg loss -2.945735 avg loss no lamb -2.945735 time 2020-06-27 18:18:20.100186
Model ind 665 epoch 1737 batch: 100 avg loss -2.880714 avg loss no lamb -2.880714 time 2020-06-27 18:18:29.927626
Model ind 665 epoch 1737 batch: 200 avg loss -2.840730 avg loss no lamb -2.840730 time 2020-06-27 18:18:39.819641
Model ind 665 epoch 1737 batch: 300 avg loss -2.954894 avg loss no lamb -2.954894 time 2020-06-27 18:18:49.678039
Model ind 665 epoch 1737 batch: 400 avg loss -2.861016 avg loss no lamb -2.861016 time 2020-06-27 18:18:59.300553
Model ind 665 epoch 1737 batch: 500 avg loss -2.879514 avg loss no lamb -2.879514 time 2020-06-27 18:19:09.415412
Model ind 665 epoch 1737 batch: 600 avg loss -2.923788 avg loss no lamb -2.923788 time 2020-06-27 18:19:19.326322
Model ind 665 epoch 1737 batch: 700 avg loss -2.841347 avg loss no lamb -2.841347 time 2020-06-27 18:19:29.052301
Model ind 665 epoch 1737 batch: 800 avg loss -2.901937 avg loss no lamb -2.901937 time 2020-06-27 18:19:38.933106
last batch sz 10
Pre: time 2020-06-27 18:19:51.830219: 
 	std: 0.0041004266
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.977, 0.9785, 0.9689, 0.9777, 0.97]
	train_accs: [0.98, 0.9798833, 0.9734333, 0.98038334, 0.9736]
	best_train_sub_head: 3
	worst: 0.9689
	avg: 0.97441995
	best: 0.9777

Starting e_i: 1738
Model ind 665 epoch 1738 batch: 0 avg loss -2.914698 avg loss no lamb -2.914698 time 2020-06-27 18:19:53.051024
Model ind 665 epoch 1738 batch: 100 avg loss -2.906947 avg loss no lamb -2.906947 time 2020-06-27 18:20:02.799656
Model ind 665 epoch 1738 batch: 200 avg loss -2.893374 avg loss no lamb -2.893374 time 2020-06-27 18:20:12.488024
Model ind 665 epoch 1738 batch: 300 avg loss -2.892367 avg loss no lamb -2.892367 time 2020-06-27 18:20:22.215194
Model ind 665 epoch 1738 batch: 400 avg loss -2.805699 avg loss no lamb -2.805699 time 2020-06-27 18:20:32.015836
Model ind 665 epoch 1738 batch: 500 avg loss -2.887010 avg loss no lamb -2.887010 time 2020-06-27 18:20:41.752497
Model ind 665 epoch 1738 batch: 600 avg loss -2.870726 avg loss no lamb -2.870726 time 2020-06-27 18:20:51.672663
Model ind 665 epoch 1738 batch: 700 avg loss -2.809541 avg loss no lamb -2.809541 time 2020-06-27 18:21:01.543880
Model ind 665 epoch 1738 batch: 800 avg loss -2.870048 avg loss no lamb -2.870048 time 2020-06-27 18:21:14.586043
last batch sz 10
Pre: time 2020-06-27 18:21:29.182428: 
 	std: 0.0030062571
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9786, 0.9727, 0.9788, 0.9725]
	train_accs: [0.9813, 0.9802, 0.97546667, 0.98123336, 0.9756]
	best_train_sub_head: 0
	worst: 0.9725
	avg: 0.97628003
	best: 0.9788

Starting e_i: 1739
Model ind 665 epoch 1739 batch: 0 avg loss -2.996442 avg loss no lamb -2.996442 time 2020-06-27 18:21:30.419810
Model ind 665 epoch 1739 batch: 100 avg loss -2.892214 avg loss no lamb -2.892214 time 2020-06-27 18:21:40.357040
Model ind 665 epoch 1739 batch: 200 avg loss -2.871507 avg loss no lamb -2.871507 time 2020-06-27 18:21:50.449653
Model ind 665 epoch 1739 batch: 300 avg loss -2.907557 avg loss no lamb -2.907557 time 2020-06-27 18:22:00.381056
Model ind 665 epoch 1739 batch: 400 avg loss -2.815153 avg loss no lamb -2.815153 time 2020-06-27 18:22:10.036480
Model ind 665 epoch 1739 batch: 500 avg loss -2.899364 avg loss no lamb -2.899364 time 2020-06-27 18:22:19.853600
Model ind 665 epoch 1739 batch: 600 avg loss -2.912155 avg loss no lamb -2.912155 time 2020-06-27 18:22:29.824594
Model ind 665 epoch 1739 batch: 700 avg loss -2.797135 avg loss no lamb -2.797135 time 2020-06-27 18:22:39.673869
Model ind 665 epoch 1739 batch: 800 avg loss -2.905235 avg loss no lamb -2.905235 time 2020-06-27 18:22:49.488152
last batch sz 10
Pre: time 2020-06-27 18:23:02.660117: 
 	std: 0.0026829848
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9785, 0.9775, 0.9735, 0.9789, 0.9724]
	train_accs: [0.98105, 0.9802833, 0.9755833, 0.98083335, 0.97525]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97616005
	best: 0.9785

Starting e_i: 1740
Model ind 665 epoch 1740 batch: 0 avg loss -2.973109 avg loss no lamb -2.973109 time 2020-06-27 18:23:03.896966
Model ind 665 epoch 1740 batch: 100 avg loss -2.939347 avg loss no lamb -2.939347 time 2020-06-27 18:23:13.662870
Model ind 665 epoch 1740 batch: 200 avg loss -2.878350 avg loss no lamb -2.878350 time 2020-06-27 18:23:23.588270
Model ind 665 epoch 1740 batch: 300 avg loss -2.897655 avg loss no lamb -2.897655 time 2020-06-27 18:23:33.595998
Model ind 665 epoch 1740 batch: 400 avg loss -2.820292 avg loss no lamb -2.820292 time 2020-06-27 18:23:43.378850
Model ind 665 epoch 1740 batch: 500 avg loss -2.872088 avg loss no lamb -2.872088 time 2020-06-27 18:23:53.019170
Model ind 665 epoch 1740 batch: 600 avg loss -2.898864 avg loss no lamb -2.898864 time 2020-06-27 18:24:02.799391
Model ind 665 epoch 1740 batch: 700 avg loss -2.740196 avg loss no lamb -2.740196 time 2020-06-27 18:24:12.587284
Model ind 665 epoch 1740 batch: 800 avg loss -2.906626 avg loss no lamb -2.906626 time 2020-06-27 18:24:22.102591
last batch sz 10
Pre: time 2020-06-27 18:24:34.624690: 
 	std: 0.0027104274
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9779, 0.978, 0.972, 0.9782, 0.9731]
	train_accs: [0.9802667, 0.9799167, 0.97495, 0.98095, 0.9755333]
	best_train_sub_head: 3
	worst: 0.972
	avg: 0.97584
	best: 0.9782

Starting e_i: 1741
Model ind 665 epoch 1741 batch: 0 avg loss -2.960689 avg loss no lamb -2.960689 time 2020-06-27 18:24:36.983004
Model ind 665 epoch 1741 batch: 100 avg loss -2.914004 avg loss no lamb -2.914004 time 2020-06-27 18:24:46.354149
Model ind 665 epoch 1741 batch: 200 avg loss -2.915489 avg loss no lamb -2.915489 time 2020-06-27 18:24:55.758308
Model ind 665 epoch 1741 batch: 300 avg loss -2.895583 avg loss no lamb -2.895583 time 2020-06-27 18:25:05.185556
Model ind 665 epoch 1741 batch: 400 avg loss -2.731658 avg loss no lamb -2.731658 time 2020-06-27 18:25:14.672964
Model ind 665 epoch 1741 batch: 500 avg loss -2.870443 avg loss no lamb -2.870443 time 2020-06-27 18:25:24.091790
Model ind 665 epoch 1741 batch: 600 avg loss -2.885134 avg loss no lamb -2.885134 time 2020-06-27 18:25:33.535966
Model ind 665 epoch 1741 batch: 700 avg loss -2.831585 avg loss no lamb -2.831585 time 2020-06-27 18:25:42.923111
Model ind 665 epoch 1741 batch: 800 avg loss -2.926926 avg loss no lamb -2.926926 time 2020-06-27 18:25:52.525958
last batch sz 10
Pre: time 2020-06-27 18:26:05.586722: 
 	std: 0.0034586682
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9776, 0.9788, 0.9706, 0.9777, 0.9715]
	train_accs: [0.9806333, 0.9801, 0.97471666, 0.98076665, 0.97485]
	best_train_sub_head: 3
	worst: 0.9706
	avg: 0.97524005
	best: 0.9777

Starting e_i: 1742
Model ind 665 epoch 1742 batch: 0 avg loss -2.921769 avg loss no lamb -2.921769 time 2020-06-27 18:26:06.805174
Model ind 665 epoch 1742 batch: 100 avg loss -2.913024 avg loss no lamb -2.913024 time 2020-06-27 18:26:16.297200
Model ind 665 epoch 1742 batch: 200 avg loss -2.903523 avg loss no lamb -2.903523 time 2020-06-27 18:26:25.702668
Model ind 665 epoch 1742 batch: 300 avg loss -2.847777 avg loss no lamb -2.847777 time 2020-06-27 18:26:35.277561
Model ind 665 epoch 1742 batch: 400 avg loss -2.782536 avg loss no lamb -2.782536 time 2020-06-27 18:26:44.855775
Model ind 665 epoch 1742 batch: 500 avg loss -2.865349 avg loss no lamb -2.865349 time 2020-06-27 18:26:54.456193
Model ind 665 epoch 1742 batch: 600 avg loss -2.898708 avg loss no lamb -2.898708 time 2020-06-27 18:27:04.064459
Model ind 665 epoch 1742 batch: 700 avg loss -2.759177 avg loss no lamb -2.759177 time 2020-06-27 18:27:13.595682
Model ind 665 epoch 1742 batch: 800 avg loss -2.937948 avg loss no lamb -2.937948 time 2020-06-27 18:27:23.152472
last batch sz 10
Pre: time 2020-06-27 18:27:35.785224: 
 	std: 0.0031607624
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9802, 0.9801, 0.9737, 0.9807, 0.9741]
	train_accs: [0.98151666, 0.98088336, 0.97566664, 0.9816333, 0.97615]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97776
	best: 0.9807

Starting e_i: 1743
Model ind 665 epoch 1743 batch: 0 avg loss -2.984382 avg loss no lamb -2.984382 time 2020-06-27 18:27:36.978700
Model ind 665 epoch 1743 batch: 100 avg loss -2.911540 avg loss no lamb -2.911540 time 2020-06-27 18:27:46.506981
Model ind 665 epoch 1743 batch: 200 avg loss -2.893170 avg loss no lamb -2.893170 time 2020-06-27 18:27:56.017481
Model ind 665 epoch 1743 batch: 300 avg loss -2.893685 avg loss no lamb -2.893685 time 2020-06-27 18:28:05.536766
Model ind 665 epoch 1743 batch: 400 avg loss -2.787162 avg loss no lamb -2.787162 time 2020-06-27 18:28:15.025272
Model ind 665 epoch 1743 batch: 500 avg loss -2.884075 avg loss no lamb -2.884075 time 2020-06-27 18:28:24.486658
Model ind 665 epoch 1743 batch: 600 avg loss -2.872721 avg loss no lamb -2.872721 time 2020-06-27 18:28:33.906207
Model ind 665 epoch 1743 batch: 700 avg loss -2.829909 avg loss no lamb -2.829909 time 2020-06-27 18:28:43.531615
Model ind 665 epoch 1743 batch: 800 avg loss -2.882106 avg loss no lamb -2.882106 time 2020-06-27 18:28:53.096734
last batch sz 10
Pre: time 2020-06-27 18:29:05.654287: 
 	std: 0.0029701274
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.9799, 0.9735, 0.9796, 0.9736]
	train_accs: [0.9809167, 0.98038334, 0.97515, 0.98088336, 0.97541666]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.97718
	best: 0.9793

Starting e_i: 1744
Model ind 665 epoch 1744 batch: 0 avg loss -2.969808 avg loss no lamb -2.969808 time 2020-06-27 18:29:07.059446
Model ind 665 epoch 1744 batch: 100 avg loss -2.862442 avg loss no lamb -2.862442 time 2020-06-27 18:29:18.200909
Model ind 665 epoch 1744 batch: 200 avg loss -2.871076 avg loss no lamb -2.871076 time 2020-06-27 18:29:31.953710
Model ind 665 epoch 1744 batch: 300 avg loss -2.890859 avg loss no lamb -2.890859 time 2020-06-27 18:29:42.144242
Model ind 665 epoch 1744 batch: 400 avg loss -2.846758 avg loss no lamb -2.846758 time 2020-06-27 18:29:51.650903
Model ind 665 epoch 1744 batch: 500 avg loss -2.868185 avg loss no lamb -2.868185 time 2020-06-27 18:30:01.180987
Model ind 665 epoch 1744 batch: 600 avg loss -2.903419 avg loss no lamb -2.903419 time 2020-06-27 18:30:10.677961
Model ind 665 epoch 1744 batch: 700 avg loss -2.804504 avg loss no lamb -2.804504 time 2020-06-27 18:30:20.175518
Model ind 665 epoch 1744 batch: 800 avg loss -2.895471 avg loss no lamb -2.895471 time 2020-06-27 18:30:29.696356
last batch sz 10
Pre: time 2020-06-27 18:30:42.281787: 
 	std: 0.0033511792
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9809, 0.9801, 0.9736, 0.9807, 0.9739]
	train_accs: [0.98155, 0.98066664, 0.9756, 0.9815, 0.9759]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97783995
	best: 0.9809

Starting e_i: 1745
Model ind 665 epoch 1745 batch: 0 avg loss -2.982300 avg loss no lamb -2.982300 time 2020-06-27 18:30:43.521056
Model ind 665 epoch 1745 batch: 100 avg loss -2.956246 avg loss no lamb -2.956246 time 2020-06-27 18:30:52.946414
Model ind 665 epoch 1745 batch: 200 avg loss -2.845642 avg loss no lamb -2.845642 time 2020-06-27 18:31:02.448548
Model ind 665 epoch 1745 batch: 300 avg loss -2.849433 avg loss no lamb -2.849433 time 2020-06-27 18:31:12.020045
Model ind 665 epoch 1745 batch: 400 avg loss -2.893068 avg loss no lamb -2.893068 time 2020-06-27 18:31:21.495991
Model ind 665 epoch 1745 batch: 500 avg loss -2.848032 avg loss no lamb -2.848032 time 2020-06-27 18:31:31.075281
Model ind 665 epoch 1745 batch: 600 avg loss -2.882387 avg loss no lamb -2.882387 time 2020-06-27 18:31:40.685574
Model ind 665 epoch 1745 batch: 700 avg loss -2.802330 avg loss no lamb -2.802330 time 2020-06-27 18:31:50.220234
Model ind 665 epoch 1745 batch: 800 avg loss -2.887403 avg loss no lamb -2.887403 time 2020-06-27 18:31:59.977105
last batch sz 10
Pre: time 2020-06-27 18:32:12.823477: 
 	std: 0.0030413126
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.9784, 0.9724, 0.9794, 0.9734]
	train_accs: [0.9809667, 0.98003334, 0.9744833, 0.9809333, 0.97568333]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97657996
	best: 0.9793

Starting e_i: 1746
Model ind 665 epoch 1746 batch: 0 avg loss -2.967686 avg loss no lamb -2.967686 time 2020-06-27 18:32:14.032647
Model ind 665 epoch 1746 batch: 100 avg loss -2.957200 avg loss no lamb -2.957200 time 2020-06-27 18:32:23.898571
Model ind 665 epoch 1746 batch: 200 avg loss -2.860801 avg loss no lamb -2.860801 time 2020-06-27 18:32:35.671299
Model ind 665 epoch 1746 batch: 300 avg loss -2.896636 avg loss no lamb -2.896636 time 2020-06-27 18:32:45.203018
Model ind 665 epoch 1746 batch: 400 avg loss -2.846479 avg loss no lamb -2.846479 time 2020-06-27 18:32:54.736928
Model ind 665 epoch 1746 batch: 500 avg loss -2.854067 avg loss no lamb -2.854067 time 2020-06-27 18:33:04.276939
Model ind 665 epoch 1746 batch: 600 avg loss -2.904299 avg loss no lamb -2.904299 time 2020-06-27 18:33:13.818858
Model ind 665 epoch 1746 batch: 700 avg loss -2.758149 avg loss no lamb -2.758149 time 2020-06-27 18:33:23.339763
Model ind 665 epoch 1746 batch: 800 avg loss -2.830742 avg loss no lamb -2.830742 time 2020-06-27 18:33:32.842836
last batch sz 10
Pre: time 2020-06-27 18:33:45.619908: 
 	std: 0.0032301203
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9786, 0.9727, 0.9788, 0.9722]
	train_accs: [0.98111665, 0.9802833, 0.975, 0.9806833, 0.97478336]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97638005
	best: 0.9796

Starting e_i: 1747
Model ind 665 epoch 1747 batch: 0 avg loss -2.937601 avg loss no lamb -2.937601 time 2020-06-27 18:33:46.872252
Model ind 665 epoch 1747 batch: 100 avg loss -2.934466 avg loss no lamb -2.934466 time 2020-06-27 18:33:56.343672
Model ind 665 epoch 1747 batch: 200 avg loss -2.931319 avg loss no lamb -2.931319 time 2020-06-27 18:34:05.784391
Model ind 665 epoch 1747 batch: 300 avg loss -2.856465 avg loss no lamb -2.856465 time 2020-06-27 18:34:15.339855
Model ind 665 epoch 1747 batch: 400 avg loss -2.800701 avg loss no lamb -2.800701 time 2020-06-27 18:34:24.907027
Model ind 665 epoch 1747 batch: 500 avg loss -2.847623 avg loss no lamb -2.847623 time 2020-06-27 18:34:34.523731
Model ind 665 epoch 1747 batch: 600 avg loss -2.917786 avg loss no lamb -2.917786 time 2020-06-27 18:34:44.113452
Model ind 665 epoch 1747 batch: 700 avg loss -2.826715 avg loss no lamb -2.826715 time 2020-06-27 18:34:53.561315
Model ind 665 epoch 1747 batch: 800 avg loss -2.838664 avg loss no lamb -2.838664 time 2020-06-27 18:35:02.990843
last batch sz 10
Pre: time 2020-06-27 18:35:15.601449: 
 	std: 0.0032579747
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9782, 0.9722, 0.9791, 0.9724]
	train_accs: [0.981, 0.9799333, 0.9747, 0.9809, 0.9755667]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97626
	best: 0.9794

Starting e_i: 1748
Model ind 665 epoch 1748 batch: 0 avg loss -2.985762 avg loss no lamb -2.985762 time 2020-06-27 18:35:16.840919
Model ind 665 epoch 1748 batch: 100 avg loss -2.955863 avg loss no lamb -2.955863 time 2020-06-27 18:35:26.320475
Model ind 665 epoch 1748 batch: 200 avg loss -2.866973 avg loss no lamb -2.866973 time 2020-06-27 18:35:35.765103
Model ind 665 epoch 1748 batch: 300 avg loss -2.928521 avg loss no lamb -2.928521 time 2020-06-27 18:35:45.156756
Model ind 665 epoch 1748 batch: 400 avg loss -2.765513 avg loss no lamb -2.765513 time 2020-06-27 18:35:54.603236
Model ind 665 epoch 1748 batch: 500 avg loss -2.890606 avg loss no lamb -2.890606 time 2020-06-27 18:36:04.157266
Model ind 665 epoch 1748 batch: 600 avg loss -2.937292 avg loss no lamb -2.937292 time 2020-06-27 18:36:13.656795
Model ind 665 epoch 1748 batch: 700 avg loss -2.774917 avg loss no lamb -2.774917 time 2020-06-27 18:36:23.170915
Model ind 665 epoch 1748 batch: 800 avg loss -2.879376 avg loss no lamb -2.879376 time 2020-06-27 18:36:32.722442
last batch sz 10
Pre: time 2020-06-27 18:36:45.562166: 
 	std: 0.0027875402
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9798, 0.9739, 0.9799, 0.9748]
	train_accs: [0.9818, 0.9810167, 0.97573334, 0.9817167, 0.9766333]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97774
	best: 0.9803

Starting e_i: 1749
Model ind 665 epoch 1749 batch: 0 avg loss -2.968615 avg loss no lamb -2.968615 time 2020-06-27 18:36:46.774597
Model ind 665 epoch 1749 batch: 100 avg loss -2.957427 avg loss no lamb -2.957427 time 2020-06-27 18:36:56.300752
Model ind 665 epoch 1749 batch: 200 avg loss -2.849900 avg loss no lamb -2.849900 time 2020-06-27 18:37:05.949875
Model ind 665 epoch 1749 batch: 300 avg loss -2.900490 avg loss no lamb -2.900490 time 2020-06-27 18:37:15.422690
Model ind 665 epoch 1749 batch: 400 avg loss -2.831255 avg loss no lamb -2.831255 time 2020-06-27 18:37:24.935103
Model ind 665 epoch 1749 batch: 500 avg loss -2.907383 avg loss no lamb -2.907383 time 2020-06-27 18:37:35.192599
Model ind 665 epoch 1749 batch: 600 avg loss -2.898603 avg loss no lamb -2.898603 time 2020-06-27 18:37:47.667692
Model ind 665 epoch 1749 batch: 700 avg loss -2.822503 avg loss no lamb -2.822503 time 2020-06-27 18:37:57.472239
Model ind 665 epoch 1749 batch: 800 avg loss -2.903113 avg loss no lamb -2.903113 time 2020-06-27 18:38:07.323432
last batch sz 10
Pre: time 2020-06-27 18:38:20.404304: 
 	std: 0.0029560856
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9792, 0.9737, 0.9797, 0.9734]
	train_accs: [0.98178333, 0.9806833, 0.97573334, 0.98185, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97716
	best: 0.9797

Starting e_i: 1750
Model ind 665 epoch 1750 batch: 0 avg loss -2.953184 avg loss no lamb -2.953184 time 2020-06-27 18:38:21.678232
Model ind 665 epoch 1750 batch: 100 avg loss -2.869072 avg loss no lamb -2.869072 time 2020-06-27 18:38:31.461985
Model ind 665 epoch 1750 batch: 200 avg loss -2.900570 avg loss no lamb -2.900570 time 2020-06-27 18:38:41.229219
Model ind 665 epoch 1750 batch: 300 avg loss -2.930763 avg loss no lamb -2.930763 time 2020-06-27 18:38:51.027416
Model ind 665 epoch 1750 batch: 400 avg loss -2.811517 avg loss no lamb -2.811517 time 2020-06-27 18:39:00.996348
Model ind 665 epoch 1750 batch: 500 avg loss -2.924030 avg loss no lamb -2.924030 time 2020-06-27 18:39:10.825782
Model ind 665 epoch 1750 batch: 600 avg loss -2.911253 avg loss no lamb -2.911253 time 2020-06-27 18:39:20.599171
Model ind 665 epoch 1750 batch: 700 avg loss -2.761883 avg loss no lamb -2.761883 time 2020-06-27 18:39:30.398614
Model ind 665 epoch 1750 batch: 800 avg loss -2.876383 avg loss no lamb -2.876383 time 2020-06-27 18:39:40.331797
last batch sz 10
Pre: time 2020-06-27 18:39:53.175542: 
 	std: 0.0038359242
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9791, 0.9713, 0.9796, 0.9718]
	train_accs: [0.9815, 0.9808667, 0.97475, 0.9816, 0.9755667]
	best_train_sub_head: 3
	worst: 0.9713
	avg: 0.97624
	best: 0.9796

Starting e_i: 1751
Model ind 665 epoch 1751 batch: 0 avg loss -2.937447 avg loss no lamb -2.937447 time 2020-06-27 18:39:55.823245
Model ind 665 epoch 1751 batch: 100 avg loss -2.927170 avg loss no lamb -2.927170 time 2020-06-27 18:40:05.708311
Model ind 665 epoch 1751 batch: 200 avg loss -2.908895 avg loss no lamb -2.908895 time 2020-06-27 18:40:15.574816
Model ind 665 epoch 1751 batch: 300 avg loss -2.900871 avg loss no lamb -2.900871 time 2020-06-27 18:40:25.336394
Model ind 665 epoch 1751 batch: 400 avg loss -2.865568 avg loss no lamb -2.865568 time 2020-06-27 18:40:35.321925
Model ind 665 epoch 1751 batch: 500 avg loss -2.928819 avg loss no lamb -2.928819 time 2020-06-27 18:40:45.160745
Model ind 665 epoch 1751 batch: 600 avg loss -2.909672 avg loss no lamb -2.909672 time 2020-06-27 18:40:54.980045
Model ind 665 epoch 1751 batch: 700 avg loss -2.745304 avg loss no lamb -2.745304 time 2020-06-27 18:41:04.616883
Model ind 665 epoch 1751 batch: 800 avg loss -2.887537 avg loss no lamb -2.887537 time 2020-06-27 18:41:14.549251
last batch sz 10
Pre: time 2020-06-27 18:41:27.673726: 
 	std: 0.0030070513
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.979, 0.9727, 0.9793, 0.9739]
	train_accs: [0.9813333, 0.98, 0.9745, 0.9813, 0.97608334]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.97694
	best: 0.9798

Starting e_i: 1752
Model ind 665 epoch 1752 batch: 0 avg loss -2.939851 avg loss no lamb -2.939851 time 2020-06-27 18:41:28.916033
Model ind 665 epoch 1752 batch: 100 avg loss -2.916334 avg loss no lamb -2.916334 time 2020-06-27 18:41:38.684237
Model ind 665 epoch 1752 batch: 200 avg loss -2.880948 avg loss no lamb -2.880948 time 2020-06-27 18:41:48.392958
Model ind 665 epoch 1752 batch: 300 avg loss -2.890344 avg loss no lamb -2.890344 time 2020-06-27 18:41:58.101572
Model ind 665 epoch 1752 batch: 400 avg loss -2.817748 avg loss no lamb -2.817748 time 2020-06-27 18:42:07.919531
Model ind 665 epoch 1752 batch: 500 avg loss -2.819711 avg loss no lamb -2.819711 time 2020-06-27 18:42:17.771497
Model ind 665 epoch 1752 batch: 600 avg loss -2.883945 avg loss no lamb -2.883945 time 2020-06-27 18:42:27.532119
Model ind 665 epoch 1752 batch: 700 avg loss -2.735898 avg loss no lamb -2.735898 time 2020-06-27 18:42:37.144969
Model ind 665 epoch 1752 batch: 800 avg loss -2.867160 avg loss no lamb -2.867160 time 2020-06-27 18:42:47.024384
last batch sz 10
Pre: time 2020-06-27 18:42:59.927671: 
 	std: 0.002641523
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9798, 0.974, 0.9797, 0.9748]
	train_accs: [0.9809, 0.9802667, 0.9754, 0.981, 0.9762]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97762
	best: 0.9797

Starting e_i: 1753
Model ind 665 epoch 1753 batch: 0 avg loss -2.955872 avg loss no lamb -2.955872 time 2020-06-27 18:43:01.259320
Model ind 665 epoch 1753 batch: 100 avg loss -2.839007 avg loss no lamb -2.839007 time 2020-06-27 18:43:11.106710
Model ind 665 epoch 1753 batch: 200 avg loss -2.863372 avg loss no lamb -2.863372 time 2020-06-27 18:43:20.949684
Model ind 665 epoch 1753 batch: 300 avg loss -2.933132 avg loss no lamb -2.933132 time 2020-06-27 18:43:30.656267
Model ind 665 epoch 1753 batch: 400 avg loss -2.830469 avg loss no lamb -2.830469 time 2020-06-27 18:43:40.542746
Model ind 665 epoch 1753 batch: 500 avg loss -2.852079 avg loss no lamb -2.852079 time 2020-06-27 18:43:50.425097
Model ind 665 epoch 1753 batch: 600 avg loss -2.893888 avg loss no lamb -2.893888 time 2020-06-27 18:44:00.236679
Model ind 665 epoch 1753 batch: 700 avg loss -2.788882 avg loss no lamb -2.788882 time 2020-06-27 18:44:09.985694
Model ind 665 epoch 1753 batch: 800 avg loss -2.869669 avg loss no lamb -2.869669 time 2020-06-27 18:44:19.714119
last batch sz 10
Pre: time 2020-06-27 18:44:32.914232: 
 	std: 0.0034277705
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9796, 0.9728, 0.9803, 0.9734]
	train_accs: [0.98121667, 0.9805167, 0.9745167, 0.9809333, 0.97545]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.97728
	best: 0.9803

Starting e_i: 1754
Model ind 665 epoch 1754 batch: 0 avg loss -2.988694 avg loss no lamb -2.988694 time 2020-06-27 18:44:34.141527
Model ind 665 epoch 1754 batch: 100 avg loss -2.961909 avg loss no lamb -2.961909 time 2020-06-27 18:44:43.983743
Model ind 665 epoch 1754 batch: 200 avg loss -2.869545 avg loss no lamb -2.869545 time 2020-06-27 18:44:53.963852
Model ind 665 epoch 1754 batch: 300 avg loss -2.876666 avg loss no lamb -2.876666 time 2020-06-27 18:45:03.669489
Model ind 665 epoch 1754 batch: 400 avg loss -2.815261 avg loss no lamb -2.815261 time 2020-06-27 18:45:13.386212
Model ind 665 epoch 1754 batch: 500 avg loss -2.855703 avg loss no lamb -2.855703 time 2020-06-27 18:45:23.148942
Model ind 665 epoch 1754 batch: 600 avg loss -2.923690 avg loss no lamb -2.923690 time 2020-06-27 18:45:32.973661
Model ind 665 epoch 1754 batch: 700 avg loss -2.782844 avg loss no lamb -2.782844 time 2020-06-27 18:45:42.920212
Model ind 665 epoch 1754 batch: 800 avg loss -2.885084 avg loss no lamb -2.885084 time 2020-06-27 18:45:52.619527
last batch sz 10
Pre: time 2020-06-27 18:46:05.423435: 
 	std: 0.0027432851
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9786, 0.9738, 0.9795, 0.9731]
	train_accs: [0.9807, 0.98003334, 0.97568333, 0.98078334, 0.97506666]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97678006
	best: 0.9795

Starting e_i: 1755
Model ind 665 epoch 1755 batch: 0 avg loss -2.966829 avg loss no lamb -2.966829 time 2020-06-27 18:46:06.702560
Model ind 665 epoch 1755 batch: 100 avg loss -2.919858 avg loss no lamb -2.919858 time 2020-06-27 18:46:16.478222
Model ind 665 epoch 1755 batch: 200 avg loss -2.876379 avg loss no lamb -2.876379 time 2020-06-27 18:46:26.305502
Model ind 665 epoch 1755 batch: 300 avg loss -2.878185 avg loss no lamb -2.878185 time 2020-06-27 18:46:36.043333
Model ind 665 epoch 1755 batch: 400 avg loss -2.761728 avg loss no lamb -2.761728 time 2020-06-27 18:46:45.779618
Model ind 665 epoch 1755 batch: 500 avg loss -2.904375 avg loss no lamb -2.904375 time 2020-06-27 18:46:55.561749
Model ind 665 epoch 1755 batch: 600 avg loss -2.909366 avg loss no lamb -2.909366 time 2020-06-27 18:47:05.313413
Model ind 665 epoch 1755 batch: 700 avg loss -2.696898 avg loss no lamb -2.696898 time 2020-06-27 18:47:15.032180
Model ind 665 epoch 1755 batch: 800 avg loss -2.858370 avg loss no lamb -2.858370 time 2020-06-27 18:47:24.845571
last batch sz 10
Pre: time 2020-06-27 18:47:38.042921: 
 	std: 0.002806702
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9798, 0.9747, 0.9802, 0.9743]
	train_accs: [0.98151666, 0.98085, 0.9759167, 0.9815, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.97792006
	best: 0.9806

Starting e_i: 1756
Model ind 665 epoch 1756 batch: 0 avg loss -2.988577 avg loss no lamb -2.988577 time 2020-06-27 18:47:39.258619
Model ind 665 epoch 1756 batch: 100 avg loss -2.923968 avg loss no lamb -2.923968 time 2020-06-27 18:47:48.977676
Model ind 665 epoch 1756 batch: 200 avg loss -2.876276 avg loss no lamb -2.876276 time 2020-06-27 18:47:58.674472
Model ind 665 epoch 1756 batch: 300 avg loss -2.874261 avg loss no lamb -2.874261 time 2020-06-27 18:48:08.442128
Model ind 665 epoch 1756 batch: 400 avg loss -2.825726 avg loss no lamb -2.825726 time 2020-06-27 18:48:18.092311
Model ind 665 epoch 1756 batch: 500 avg loss -2.882706 avg loss no lamb -2.882706 time 2020-06-27 18:48:27.900512
Model ind 665 epoch 1756 batch: 600 avg loss -2.913747 avg loss no lamb -2.913747 time 2020-06-27 18:48:37.888540
Model ind 665 epoch 1756 batch: 700 avg loss -2.785682 avg loss no lamb -2.785682 time 2020-06-27 18:48:47.677679
Model ind 665 epoch 1756 batch: 800 avg loss -2.868219 avg loss no lamb -2.868219 time 2020-06-27 18:48:57.424373
last batch sz 10
Pre: time 2020-06-27 18:49:10.394944: 
 	std: 0.0025779083
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9805, 0.9793, 0.9748, 0.9802, 0.9748]
	train_accs: [0.9816, 0.9809833, 0.97573334, 0.98135, 0.9763]
	best_train_sub_head: 0
	worst: 0.9748
	avg: 0.97791994
	best: 0.9805

Starting e_i: 1757
Model ind 665 epoch 1757 batch: 0 avg loss -2.979493 avg loss no lamb -2.979493 time 2020-06-27 18:49:11.627738
Model ind 665 epoch 1757 batch: 100 avg loss -2.930200 avg loss no lamb -2.930200 time 2020-06-27 18:49:21.431247
Model ind 665 epoch 1757 batch: 200 avg loss -2.891564 avg loss no lamb -2.891564 time 2020-06-27 18:49:31.232368
Model ind 665 epoch 1757 batch: 300 avg loss -2.889889 avg loss no lamb -2.889889 time 2020-06-27 18:49:41.009531
Model ind 665 epoch 1757 batch: 400 avg loss -2.791565 avg loss no lamb -2.791565 time 2020-06-27 18:49:50.657990
Model ind 665 epoch 1757 batch: 500 avg loss -2.858692 avg loss no lamb -2.858692 time 2020-06-27 18:50:00.337153
Model ind 665 epoch 1757 batch: 600 avg loss -2.864677 avg loss no lamb -2.864677 time 2020-06-27 18:50:10.003816
Model ind 665 epoch 1757 batch: 700 avg loss -2.790543 avg loss no lamb -2.790543 time 2020-06-27 18:50:19.831855
Model ind 665 epoch 1757 batch: 800 avg loss -2.813597 avg loss no lamb -2.813597 time 2020-06-27 18:50:29.670659
last batch sz 10
Pre: time 2020-06-27 18:50:42.828315: 
 	std: 0.0030585125
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9789, 0.973, 0.9797, 0.9737]
	train_accs: [0.98156667, 0.9807, 0.9752333, 0.9811, 0.9759667]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97706
	best: 0.98

Starting e_i: 1758
Model ind 665 epoch 1758 batch: 0 avg loss -2.970262 avg loss no lamb -2.970262 time 2020-06-27 18:50:44.268012
Model ind 665 epoch 1758 batch: 100 avg loss -2.977842 avg loss no lamb -2.977842 time 2020-06-27 18:50:54.150565
Model ind 665 epoch 1758 batch: 200 avg loss -2.885937 avg loss no lamb -2.885937 time 2020-06-27 18:51:03.913810
Model ind 665 epoch 1758 batch: 300 avg loss -2.877861 avg loss no lamb -2.877861 time 2020-06-27 18:51:13.566750
Model ind 665 epoch 1758 batch: 400 avg loss -2.846300 avg loss no lamb -2.846300 time 2020-06-27 18:51:23.452763
Model ind 665 epoch 1758 batch: 500 avg loss -2.866605 avg loss no lamb -2.866605 time 2020-06-27 18:51:33.219307
Model ind 665 epoch 1758 batch: 600 avg loss -2.903244 avg loss no lamb -2.903244 time 2020-06-27 18:51:43.069118
Model ind 665 epoch 1758 batch: 700 avg loss -2.799865 avg loss no lamb -2.799865 time 2020-06-27 18:51:52.935977
Model ind 665 epoch 1758 batch: 800 avg loss -2.872297 avg loss no lamb -2.872297 time 2020-06-27 18:52:02.765481
last batch sz 10
Pre: time 2020-06-27 18:52:16.056715: 
 	std: 0.0030434139
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.979, 0.9773, 0.972, 0.9786, 0.9724]
	train_accs: [0.9810167, 0.97966665, 0.97466666, 0.98066664, 0.97525]
	best_train_sub_head: 0
	worst: 0.972
	avg: 0.97586
	best: 0.979

Starting e_i: 1759
Model ind 665 epoch 1759 batch: 0 avg loss -2.974941 avg loss no lamb -2.974941 time 2020-06-27 18:52:17.357278
Model ind 665 epoch 1759 batch: 100 avg loss -2.875726 avg loss no lamb -2.875726 time 2020-06-27 18:52:27.318529
Model ind 665 epoch 1759 batch: 200 avg loss -2.873791 avg loss no lamb -2.873791 time 2020-06-27 18:52:37.127747
Model ind 665 epoch 1759 batch: 300 avg loss -2.863507 avg loss no lamb -2.863507 time 2020-06-27 18:52:46.846617
Model ind 665 epoch 1759 batch: 400 avg loss -2.779342 avg loss no lamb -2.779342 time 2020-06-27 18:52:56.457547
Model ind 665 epoch 1759 batch: 500 avg loss -2.893504 avg loss no lamb -2.893504 time 2020-06-27 18:53:06.204733
Model ind 665 epoch 1759 batch: 600 avg loss -2.941420 avg loss no lamb -2.941420 time 2020-06-27 18:53:15.901906
Model ind 665 epoch 1759 batch: 700 avg loss -2.783910 avg loss no lamb -2.783910 time 2020-06-27 18:53:25.730090
Model ind 665 epoch 1759 batch: 800 avg loss -2.897788 avg loss no lamb -2.897788 time 2020-06-27 18:53:35.379464
last batch sz 10
Pre: time 2020-06-27 18:53:48.253688: 
 	std: 0.0032514564
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9791, 0.9734, 0.9802, 0.9731]
	train_accs: [0.98146665, 0.98048335, 0.9755333, 0.9813667, 0.9757]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97720003
	best: 0.9802

Starting e_i: 1760
Model ind 665 epoch 1760 batch: 0 avg loss -2.985607 avg loss no lamb -2.985607 time 2020-06-27 18:53:49.591217
Model ind 665 epoch 1760 batch: 100 avg loss -2.915688 avg loss no lamb -2.915688 time 2020-06-27 18:53:59.435563
Model ind 665 epoch 1760 batch: 200 avg loss -2.893750 avg loss no lamb -2.893750 time 2020-06-27 18:54:09.318274
Model ind 665 epoch 1760 batch: 300 avg loss -2.893056 avg loss no lamb -2.893056 time 2020-06-27 18:54:19.073141
Model ind 665 epoch 1760 batch: 400 avg loss -2.857529 avg loss no lamb -2.857529 time 2020-06-27 18:54:28.940042
Model ind 665 epoch 1760 batch: 500 avg loss -2.870925 avg loss no lamb -2.870925 time 2020-06-27 18:54:38.871134
Model ind 665 epoch 1760 batch: 600 avg loss -2.920128 avg loss no lamb -2.920128 time 2020-06-27 18:54:48.652563
Model ind 665 epoch 1760 batch: 700 avg loss -2.797933 avg loss no lamb -2.797933 time 2020-06-27 18:54:58.476365
Model ind 665 epoch 1760 batch: 800 avg loss -2.910047 avg loss no lamb -2.910047 time 2020-06-27 18:55:08.290727
last batch sz 10
Pre: time 2020-06-27 18:55:21.766026: 
 	std: 0.003006003
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.979, 0.98, 0.9737, 0.9793, 0.973]
	train_accs: [0.9812833, 0.98121667, 0.9757, 0.9814, 0.97543335]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97699994
	best: 0.9793

Starting e_i: 1761
Model ind 665 epoch 1761 batch: 0 avg loss -2.959740 avg loss no lamb -2.959740 time 2020-06-27 18:55:24.216393
Model ind 665 epoch 1761 batch: 100 avg loss -2.877993 avg loss no lamb -2.877993 time 2020-06-27 18:55:34.109410
Model ind 665 epoch 1761 batch: 200 avg loss -2.898583 avg loss no lamb -2.898583 time 2020-06-27 18:55:43.904376
Model ind 665 epoch 1761 batch: 300 avg loss -2.858094 avg loss no lamb -2.858094 time 2020-06-27 18:55:53.710491
Model ind 665 epoch 1761 batch: 400 avg loss -2.859218 avg loss no lamb -2.859218 time 2020-06-27 18:56:03.417104
Model ind 665 epoch 1761 batch: 500 avg loss -2.856389 avg loss no lamb -2.856389 time 2020-06-27 18:56:13.191536
Model ind 665 epoch 1761 batch: 600 avg loss -2.847226 avg loss no lamb -2.847226 time 2020-06-27 18:56:22.997654
Model ind 665 epoch 1761 batch: 700 avg loss -2.768025 avg loss no lamb -2.768025 time 2020-06-27 18:56:32.783211
Model ind 665 epoch 1761 batch: 800 avg loss -2.914699 avg loss no lamb -2.914699 time 2020-06-27 18:56:42.555234
last batch sz 10
Pre: time 2020-06-27 18:56:55.280602: 
 	std: 0.0030009348
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9793, 0.973, 0.9798, 0.9743]
	train_accs: [0.98135, 0.98036665, 0.97491664, 0.98105, 0.97581667]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97728
	best: 0.98

Starting e_i: 1762
Model ind 665 epoch 1762 batch: 0 avg loss -2.975235 avg loss no lamb -2.975235 time 2020-06-27 18:56:56.541873
Model ind 665 epoch 1762 batch: 100 avg loss -2.992297 avg loss no lamb -2.992297 time 2020-06-27 18:57:06.321546
Model ind 665 epoch 1762 batch: 200 avg loss -2.885369 avg loss no lamb -2.885369 time 2020-06-27 18:57:16.118618
Model ind 665 epoch 1762 batch: 300 avg loss -2.872426 avg loss no lamb -2.872426 time 2020-06-27 18:57:26.087685
Model ind 665 epoch 1762 batch: 400 avg loss -2.823527 avg loss no lamb -2.823527 time 2020-06-27 18:57:35.835396
Model ind 665 epoch 1762 batch: 500 avg loss -2.812455 avg loss no lamb -2.812455 time 2020-06-27 18:57:45.635549
Model ind 665 epoch 1762 batch: 600 avg loss -2.896926 avg loss no lamb -2.896926 time 2020-06-27 18:57:55.410399
Model ind 665 epoch 1762 batch: 700 avg loss -2.791844 avg loss no lamb -2.791844 time 2020-06-27 18:58:05.303278
Model ind 665 epoch 1762 batch: 800 avg loss -2.866403 avg loss no lamb -2.866403 time 2020-06-27 18:58:15.098527
last batch sz 10
Pre: time 2020-06-27 18:58:28.081007: 
 	std: 0.0033111933
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9774, 0.9779, 0.9703, 0.978, 0.9719]
	train_accs: [0.9802333, 0.98013335, 0.97403336, 0.9803333, 0.9751]
	best_train_sub_head: 3
	worst: 0.9703
	avg: 0.9750999
	best: 0.978

Starting e_i: 1763
Model ind 665 epoch 1763 batch: 0 avg loss -2.941462 avg loss no lamb -2.941462 time 2020-06-27 18:58:29.541470
Model ind 665 epoch 1763 batch: 100 avg loss -2.916056 avg loss no lamb -2.916056 time 2020-06-27 18:58:39.379016
Model ind 665 epoch 1763 batch: 200 avg loss -2.909476 avg loss no lamb -2.909476 time 2020-06-27 18:58:49.118848
Model ind 665 epoch 1763 batch: 300 avg loss -2.841747 avg loss no lamb -2.841747 time 2020-06-27 18:58:58.856405
Model ind 665 epoch 1763 batch: 400 avg loss -2.799271 avg loss no lamb -2.799271 time 2020-06-27 18:59:08.783818
Model ind 665 epoch 1763 batch: 500 avg loss -2.834034 avg loss no lamb -2.834034 time 2020-06-27 18:59:18.521748
Model ind 665 epoch 1763 batch: 600 avg loss -2.878349 avg loss no lamb -2.878349 time 2020-06-27 18:59:28.277370
Model ind 665 epoch 1763 batch: 700 avg loss -2.856935 avg loss no lamb -2.856935 time 2020-06-27 18:59:38.031589
Model ind 665 epoch 1763 batch: 800 avg loss -2.856131 avg loss no lamb -2.856131 time 2020-06-27 18:59:47.937726
last batch sz 10
Pre: time 2020-06-27 19:00:01.040173: 
 	std: 0.0026404515
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9804, 0.9742, 0.9799, 0.975]
	train_accs: [0.98123336, 0.9813, 0.97605, 0.98143333, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.9778
	best: 0.9799

Starting e_i: 1764
Model ind 665 epoch 1764 batch: 0 avg loss -2.979289 avg loss no lamb -2.979289 time 2020-06-27 19:00:02.337408
Model ind 665 epoch 1764 batch: 100 avg loss -2.942379 avg loss no lamb -2.942379 time 2020-06-27 19:00:12.076019
Model ind 665 epoch 1764 batch: 200 avg loss -2.887734 avg loss no lamb -2.887734 time 2020-06-27 19:00:21.865578
Model ind 665 epoch 1764 batch: 300 avg loss -2.899072 avg loss no lamb -2.899072 time 2020-06-27 19:00:31.479991
Model ind 665 epoch 1764 batch: 400 avg loss -2.821141 avg loss no lamb -2.821141 time 2020-06-27 19:00:41.394134
Model ind 665 epoch 1764 batch: 500 avg loss -2.897267 avg loss no lamb -2.897267 time 2020-06-27 19:00:51.251724
Model ind 665 epoch 1764 batch: 600 avg loss -2.938148 avg loss no lamb -2.938148 time 2020-06-27 19:01:01.121953
Model ind 665 epoch 1764 batch: 700 avg loss -2.783200 avg loss no lamb -2.783200 time 2020-06-27 19:01:10.921140
Model ind 665 epoch 1764 batch: 800 avg loss -2.902252 avg loss no lamb -2.902252 time 2020-06-27 19:01:20.675020
last batch sz 10
Pre: time 2020-06-27 19:01:33.739234: 
 	std: 0.002082696
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9778, 0.9779, 0.9731, 0.9777, 0.9741]
	train_accs: [0.98048335, 0.9802333, 0.97511667, 0.98053336, 0.97585]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97612
	best: 0.9777

Starting e_i: 1765
Model ind 665 epoch 1765 batch: 0 avg loss -2.982028 avg loss no lamb -2.982028 time 2020-06-27 19:01:35.230010
Model ind 665 epoch 1765 batch: 100 avg loss -2.891500 avg loss no lamb -2.891500 time 2020-06-27 19:01:45.122436
Model ind 665 epoch 1765 batch: 200 avg loss -2.898655 avg loss no lamb -2.898655 time 2020-06-27 19:01:54.967104
Model ind 665 epoch 1765 batch: 300 avg loss -2.909122 avg loss no lamb -2.909122 time 2020-06-27 19:02:04.830603
Model ind 665 epoch 1765 batch: 400 avg loss -2.841945 avg loss no lamb -2.841945 time 2020-06-27 19:02:14.635340
Model ind 665 epoch 1765 batch: 500 avg loss -2.870414 avg loss no lamb -2.870414 time 2020-06-27 19:02:24.307828
Model ind 665 epoch 1765 batch: 600 avg loss -2.914369 avg loss no lamb -2.914369 time 2020-06-27 19:02:34.214211
Model ind 665 epoch 1765 batch: 700 avg loss -2.804090 avg loss no lamb -2.804090 time 2020-06-27 19:02:44.142111
Model ind 665 epoch 1765 batch: 800 avg loss -2.863938 avg loss no lamb -2.863938 time 2020-06-27 19:02:54.055996
last batch sz 10
Pre: time 2020-06-27 19:03:07.040589: 
 	std: 0.0025419795
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9796, 0.9788, 0.9738, 0.9797, 0.9747]
	train_accs: [0.98121667, 0.98041666, 0.975, 0.98118335, 0.97613335]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97732
	best: 0.9796

Starting e_i: 1766
Model ind 665 epoch 1766 batch: 0 avg loss -2.958912 avg loss no lamb -2.958912 time 2020-06-27 19:03:08.292410
Model ind 665 epoch 1766 batch: 100 avg loss -2.926291 avg loss no lamb -2.926291 time 2020-06-27 19:03:17.919184
Model ind 665 epoch 1766 batch: 200 avg loss -2.880792 avg loss no lamb -2.880792 time 2020-06-27 19:03:27.688058
Model ind 665 epoch 1766 batch: 300 avg loss -2.853540 avg loss no lamb -2.853540 time 2020-06-27 19:03:37.464708
Model ind 665 epoch 1766 batch: 400 avg loss -2.863536 avg loss no lamb -2.863536 time 2020-06-27 19:03:47.408357
Model ind 665 epoch 1766 batch: 500 avg loss -2.877324 avg loss no lamb -2.877324 time 2020-06-27 19:03:57.224285
Model ind 665 epoch 1766 batch: 600 avg loss -2.921849 avg loss no lamb -2.921849 time 2020-06-27 19:04:07.012497
Model ind 665 epoch 1766 batch: 700 avg loss -2.767799 avg loss no lamb -2.767799 time 2020-06-27 19:04:16.749093
Model ind 665 epoch 1766 batch: 800 avg loss -2.914111 avg loss no lamb -2.914111 time 2020-06-27 19:04:26.400688
last batch sz 10
Pre: time 2020-06-27 19:04:39.228331: 
 	std: 0.0026007537
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9787, 0.9788, 0.9728, 0.9783, 0.9739]
	train_accs: [0.98046666, 0.9799333, 0.97465, 0.9802, 0.9756333]
	best_train_sub_head: 0
	worst: 0.9728
	avg: 0.9764999
	best: 0.9787

Starting e_i: 1767
Model ind 665 epoch 1767 batch: 0 avg loss -2.964491 avg loss no lamb -2.964491 time 2020-06-27 19:04:40.480812
Model ind 665 epoch 1767 batch: 100 avg loss -2.890198 avg loss no lamb -2.890198 time 2020-06-27 19:04:50.195727
Model ind 665 epoch 1767 batch: 200 avg loss -2.939464 avg loss no lamb -2.939464 time 2020-06-27 19:05:00.161867
Model ind 665 epoch 1767 batch: 300 avg loss -2.903041 avg loss no lamb -2.903041 time 2020-06-27 19:05:10.035670
Model ind 665 epoch 1767 batch: 400 avg loss -2.837427 avg loss no lamb -2.837427 time 2020-06-27 19:05:19.887172
Model ind 665 epoch 1767 batch: 500 avg loss -2.911089 avg loss no lamb -2.911089 time 2020-06-27 19:05:29.638320
Model ind 665 epoch 1767 batch: 600 avg loss -2.903858 avg loss no lamb -2.903858 time 2020-06-27 19:05:39.439615
Model ind 665 epoch 1767 batch: 700 avg loss -2.822467 avg loss no lamb -2.822467 time 2020-06-27 19:05:49.191172
Model ind 665 epoch 1767 batch: 800 avg loss -2.873407 avg loss no lamb -2.873407 time 2020-06-27 19:05:58.979968
last batch sz 10
Pre: time 2020-06-27 19:06:12.039916: 
 	std: 0.002426192
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9807, 0.9806, 0.976, 0.9812, 0.9758]
	train_accs: [0.98155, 0.98123336, 0.97671664, 0.9819667, 0.97713333]
	best_train_sub_head: 3
	worst: 0.9758
	avg: 0.97886
	best: 0.9812

Starting e_i: 1768
Model ind 665 epoch 1768 batch: 0 avg loss -2.976246 avg loss no lamb -2.976246 time 2020-06-27 19:06:13.243982
Model ind 665 epoch 1768 batch: 100 avg loss -2.913148 avg loss no lamb -2.913148 time 2020-06-27 19:06:22.980247
Model ind 665 epoch 1768 batch: 200 avg loss -2.894818 avg loss no lamb -2.894818 time 2020-06-27 19:06:32.779931
Model ind 665 epoch 1768 batch: 300 avg loss -2.865355 avg loss no lamb -2.865355 time 2020-06-27 19:06:42.498563
Model ind 665 epoch 1768 batch: 400 avg loss -2.830480 avg loss no lamb -2.830480 time 2020-06-27 19:06:52.256101
Model ind 665 epoch 1768 batch: 500 avg loss -2.868751 avg loss no lamb -2.868751 time 2020-06-27 19:07:01.961327
Model ind 665 epoch 1768 batch: 600 avg loss -2.947140 avg loss no lamb -2.947140 time 2020-06-27 19:07:11.606628
Model ind 665 epoch 1768 batch: 700 avg loss -2.777110 avg loss no lamb -2.777110 time 2020-06-27 19:07:21.370306
Model ind 665 epoch 1768 batch: 800 avg loss -2.933354 avg loss no lamb -2.933354 time 2020-06-27 19:07:31.113199
last batch sz 10
Pre: time 2020-06-27 19:07:43.940181: 
 	std: 0.0024749902
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9778, 0.9779, 0.9721, 0.9776, 0.9735]
	train_accs: [0.98005, 0.98001665, 0.9752, 0.98045, 0.9756333]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.9757801
	best: 0.9776

Starting e_i: 1769
Model ind 665 epoch 1769 batch: 0 avg loss -2.966941 avg loss no lamb -2.966941 time 2020-06-27 19:07:45.180072
Model ind 665 epoch 1769 batch: 100 avg loss -2.913414 avg loss no lamb -2.913414 time 2020-06-27 19:07:54.987379
Model ind 665 epoch 1769 batch: 200 avg loss -2.900978 avg loss no lamb -2.900978 time 2020-06-27 19:08:04.780741
Model ind 665 epoch 1769 batch: 300 avg loss -2.877056 avg loss no lamb -2.877056 time 2020-06-27 19:08:14.558073
Model ind 665 epoch 1769 batch: 400 avg loss -2.854702 avg loss no lamb -2.854702 time 2020-06-27 19:08:24.311581
Model ind 665 epoch 1769 batch: 500 avg loss -2.914710 avg loss no lamb -2.914710 time 2020-06-27 19:08:34.145384
Model ind 665 epoch 1769 batch: 600 avg loss -2.913979 avg loss no lamb -2.913979 time 2020-06-27 19:08:43.977631
Model ind 665 epoch 1769 batch: 700 avg loss -2.813248 avg loss no lamb -2.813248 time 2020-06-27 19:08:53.799799
Model ind 665 epoch 1769 batch: 800 avg loss -2.871822 avg loss no lamb -2.871822 time 2020-06-27 19:09:03.601106
last batch sz 10
Pre: time 2020-06-27 19:09:16.589794: 
 	std: 0.0027572347
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9813, 0.9806, 0.975, 0.9813, 0.976]
	train_accs: [0.98188335, 0.9814, 0.97611666, 0.9818, 0.97705]
	best_train_sub_head: 0
	worst: 0.975
	avg: 0.97884
	best: 0.9813

Starting e_i: 1770
Model ind 665 epoch 1770 batch: 0 avg loss -2.966586 avg loss no lamb -2.966586 time 2020-06-27 19:09:18.070601
Model ind 665 epoch 1770 batch: 100 avg loss -2.911696 avg loss no lamb -2.911696 time 2020-06-27 19:09:27.794103
Model ind 665 epoch 1770 batch: 200 avg loss -2.891440 avg loss no lamb -2.891440 time 2020-06-27 19:09:37.495037
Model ind 665 epoch 1770 batch: 300 avg loss -2.891408 avg loss no lamb -2.891408 time 2020-06-27 19:09:47.214830
Model ind 665 epoch 1770 batch: 400 avg loss -2.767371 avg loss no lamb -2.767371 time 2020-06-27 19:09:56.979251
Model ind 665 epoch 1770 batch: 500 avg loss -2.869764 avg loss no lamb -2.869764 time 2020-06-27 19:10:06.851511
Model ind 665 epoch 1770 batch: 600 avg loss -2.855607 avg loss no lamb -2.855607 time 2020-06-27 19:10:16.764538
Model ind 665 epoch 1770 batch: 700 avg loss -2.854031 avg loss no lamb -2.854031 time 2020-06-27 19:10:26.754847
Model ind 665 epoch 1770 batch: 800 avg loss -2.873760 avg loss no lamb -2.873760 time 2020-06-27 19:10:36.601238
last batch sz 10
Pre: time 2020-06-27 19:10:49.345394: 
 	std: 0.0029195885
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9793, 0.9728, 0.9792, 0.9739]
	train_accs: [0.9809, 0.98025, 0.97475, 0.9809333, 0.97545]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.9769
	best: 0.9792

Starting e_i: 1771
Model ind 665 epoch 1771 batch: 0 avg loss -2.991953 avg loss no lamb -2.991953 time 2020-06-27 19:10:51.860386
Model ind 665 epoch 1771 batch: 100 avg loss -2.818990 avg loss no lamb -2.818990 time 2020-06-27 19:11:01.636457
Model ind 665 epoch 1771 batch: 200 avg loss -2.931296 avg loss no lamb -2.931296 time 2020-06-27 19:11:11.566522
Model ind 665 epoch 1771 batch: 300 avg loss -2.886483 avg loss no lamb -2.886483 time 2020-06-27 19:11:21.365264
Model ind 665 epoch 1771 batch: 400 avg loss -2.819882 avg loss no lamb -2.819882 time 2020-06-27 19:11:31.125947
Model ind 665 epoch 1771 batch: 500 avg loss -2.807109 avg loss no lamb -2.807109 time 2020-06-27 19:11:40.905179
Model ind 665 epoch 1771 batch: 600 avg loss -2.863353 avg loss no lamb -2.863353 time 2020-06-27 19:11:50.746276
Model ind 665 epoch 1771 batch: 700 avg loss -2.806088 avg loss no lamb -2.806088 time 2020-06-27 19:12:00.543117
Model ind 665 epoch 1771 batch: 800 avg loss -2.917809 avg loss no lamb -2.917809 time 2020-06-27 19:12:10.222127
last batch sz 10
Pre: time 2020-06-27 19:12:23.283792: 
 	std: 0.0037126811
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9773, 0.9774, 0.9692, 0.9778, 0.9708]
	train_accs: [0.9795, 0.9791333, 0.97213334, 0.97965, 0.97305]
	best_train_sub_head: 3
	worst: 0.9692
	avg: 0.9745
	best: 0.9778

Starting e_i: 1772
Model ind 665 epoch 1772 batch: 0 avg loss -2.985699 avg loss no lamb -2.985699 time 2020-06-27 19:12:24.564426
Model ind 665 epoch 1772 batch: 100 avg loss -2.899903 avg loss no lamb -2.899903 time 2020-06-27 19:12:34.225899
Model ind 665 epoch 1772 batch: 200 avg loss -2.940271 avg loss no lamb -2.940271 time 2020-06-27 19:12:44.050023
Model ind 665 epoch 1772 batch: 300 avg loss -2.881363 avg loss no lamb -2.881363 time 2020-06-27 19:12:53.841676
Model ind 665 epoch 1772 batch: 400 avg loss -2.816562 avg loss no lamb -2.816562 time 2020-06-27 19:13:03.612253
Model ind 665 epoch 1772 batch: 500 avg loss -2.857374 avg loss no lamb -2.857374 time 2020-06-27 19:13:13.362217
Model ind 665 epoch 1772 batch: 600 avg loss -2.935863 avg loss no lamb -2.935863 time 2020-06-27 19:13:23.117015
Model ind 665 epoch 1772 batch: 700 avg loss -2.772570 avg loss no lamb -2.772570 time 2020-06-27 19:13:32.814590
Model ind 665 epoch 1772 batch: 800 avg loss -2.895741 avg loss no lamb -2.895741 time 2020-06-27 19:13:42.555213
last batch sz 10
Pre: time 2020-06-27 19:13:55.864848: 
 	std: 0.002876518
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9798, 0.9739, 0.9809, 0.9758]
	train_accs: [0.9816667, 0.98071665, 0.97546667, 0.9816833, 0.9763167]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97826004
	best: 0.9809

Starting e_i: 1773
Model ind 665 epoch 1773 batch: 0 avg loss -2.910255 avg loss no lamb -2.910255 time 2020-06-27 19:13:57.118366
Model ind 665 epoch 1773 batch: 100 avg loss -2.877575 avg loss no lamb -2.877575 time 2020-06-27 19:14:06.858194
Model ind 665 epoch 1773 batch: 200 avg loss -2.898554 avg loss no lamb -2.898554 time 2020-06-27 19:14:16.616384
Model ind 665 epoch 1773 batch: 300 avg loss -2.932223 avg loss no lamb -2.932223 time 2020-06-27 19:14:26.627173
Model ind 665 epoch 1773 batch: 400 avg loss -2.784360 avg loss no lamb -2.784360 time 2020-06-27 19:14:36.497572
Model ind 665 epoch 1773 batch: 500 avg loss -2.877809 avg loss no lamb -2.877809 time 2020-06-27 19:14:46.363116
Model ind 665 epoch 1773 batch: 600 avg loss -2.913903 avg loss no lamb -2.913903 time 2020-06-27 19:14:56.277094
Model ind 665 epoch 1773 batch: 700 avg loss -2.777749 avg loss no lamb -2.777749 time 2020-06-27 19:15:06.081777
Model ind 665 epoch 1773 batch: 800 avg loss -2.939019 avg loss no lamb -2.939019 time 2020-06-27 19:15:15.827724
last batch sz 10
Pre: time 2020-06-27 19:15:28.804224: 
 	std: 0.003858795
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9801, 0.9714, 0.9809, 0.9739]
	train_accs: [0.9816833, 0.98045, 0.97445, 0.98186666, 0.9762]
	best_train_sub_head: 3
	worst: 0.9714
	avg: 0.97726
	best: 0.9809

Starting e_i: 1774
Model ind 665 epoch 1774 batch: 0 avg loss -2.960019 avg loss no lamb -2.960019 time 2020-06-27 19:15:30.071614
Model ind 665 epoch 1774 batch: 100 avg loss -2.957728 avg loss no lamb -2.957728 time 2020-06-27 19:15:39.742769
Model ind 665 epoch 1774 batch: 200 avg loss -2.910695 avg loss no lamb -2.910695 time 2020-06-27 19:15:49.527777
Model ind 665 epoch 1774 batch: 300 avg loss -2.887217 avg loss no lamb -2.887217 time 2020-06-27 19:15:59.434474
Model ind 665 epoch 1774 batch: 400 avg loss -2.868347 avg loss no lamb -2.868347 time 2020-06-27 19:16:09.310622
Model ind 665 epoch 1774 batch: 500 avg loss -2.896321 avg loss no lamb -2.896321 time 2020-06-27 19:16:19.192435
Model ind 665 epoch 1774 batch: 600 avg loss -2.907068 avg loss no lamb -2.907068 time 2020-06-27 19:16:29.036006
Model ind 665 epoch 1774 batch: 700 avg loss -2.829557 avg loss no lamb -2.829557 time 2020-06-27 19:16:38.820065
Model ind 665 epoch 1774 batch: 800 avg loss -2.884383 avg loss no lamb -2.884383 time 2020-06-27 19:16:48.599779
last batch sz 10
Pre: time 2020-06-27 19:17:01.854466: 
 	std: 0.003185362
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9797, 0.9736, 0.9804, 0.9737]
	train_accs: [0.9816667, 0.98066664, 0.9755833, 0.98195, 0.97648335]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97754
	best: 0.9804

Starting e_i: 1775
Model ind 665 epoch 1775 batch: 0 avg loss -2.927060 avg loss no lamb -2.927060 time 2020-06-27 19:17:03.102084
Model ind 665 epoch 1775 batch: 100 avg loss -2.944670 avg loss no lamb -2.944670 time 2020-06-27 19:17:13.020885
Model ind 665 epoch 1775 batch: 200 avg loss -2.867064 avg loss no lamb -2.867064 time 2020-06-27 19:17:22.865802
Model ind 665 epoch 1775 batch: 300 avg loss -2.870990 avg loss no lamb -2.870990 time 2020-06-27 19:17:32.652805
Model ind 665 epoch 1775 batch: 400 avg loss -2.854921 avg loss no lamb -2.854921 time 2020-06-27 19:17:42.461572
Model ind 665 epoch 1775 batch: 500 avg loss -2.914954 avg loss no lamb -2.914954 time 2020-06-27 19:17:52.404838
Model ind 665 epoch 1775 batch: 600 avg loss -2.944715 avg loss no lamb -2.944715 time 2020-06-27 19:18:02.319291
Model ind 665 epoch 1775 batch: 700 avg loss -2.761917 avg loss no lamb -2.761917 time 2020-06-27 19:18:12.090020
Model ind 665 epoch 1775 batch: 800 avg loss -2.912066 avg loss no lamb -2.912066 time 2020-06-27 19:18:21.968121
last batch sz 10
Pre: time 2020-06-27 19:18:34.641066: 
 	std: 0.0034216875
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9794, 0.9717, 0.9797, 0.9739]
	train_accs: [0.98115, 0.98011667, 0.97393334, 0.9813167, 0.9755333]
	best_train_sub_head: 3
	worst: 0.9717
	avg: 0.9769
	best: 0.9797

Starting e_i: 1776
Model ind 665 epoch 1776 batch: 0 avg loss -2.971451 avg loss no lamb -2.971451 time 2020-06-27 19:18:35.869999
Model ind 665 epoch 1776 batch: 100 avg loss -2.899593 avg loss no lamb -2.899593 time 2020-06-27 19:18:45.692732
Model ind 665 epoch 1776 batch: 200 avg loss -2.868983 avg loss no lamb -2.868983 time 2020-06-27 19:18:55.593834
Model ind 665 epoch 1776 batch: 300 avg loss -2.834874 avg loss no lamb -2.834874 time 2020-06-27 19:19:05.463942
Model ind 665 epoch 1776 batch: 400 avg loss -2.844438 avg loss no lamb -2.844438 time 2020-06-27 19:19:15.193454
Model ind 665 epoch 1776 batch: 500 avg loss -2.907086 avg loss no lamb -2.907086 time 2020-06-27 19:19:24.915413
Model ind 665 epoch 1776 batch: 600 avg loss -2.918475 avg loss no lamb -2.918475 time 2020-06-27 19:19:34.803604
Model ind 665 epoch 1776 batch: 700 avg loss -2.813498 avg loss no lamb -2.813498 time 2020-06-27 19:19:44.676781
Model ind 665 epoch 1776 batch: 800 avg loss -2.888757 avg loss no lamb -2.888757 time 2020-06-27 19:19:54.582197
last batch sz 10
Pre: time 2020-06-27 19:20:07.417946: 
 	std: 0.0029143598
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.979, 0.9732, 0.9801, 0.9739]
	train_accs: [0.9812, 0.9809667, 0.9751667, 0.98156667, 0.9758]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97708
	best: 0.9801

Starting e_i: 1777
Model ind 665 epoch 1777 batch: 0 avg loss -2.961516 avg loss no lamb -2.961516 time 2020-06-27 19:20:08.876641
Model ind 665 epoch 1777 batch: 100 avg loss -2.915942 avg loss no lamb -2.915942 time 2020-06-27 19:20:18.629162
Model ind 665 epoch 1777 batch: 200 avg loss -2.897901 avg loss no lamb -2.897901 time 2020-06-27 19:20:28.551440
Model ind 665 epoch 1777 batch: 300 avg loss -2.929328 avg loss no lamb -2.929328 time 2020-06-27 19:20:38.540905
Model ind 665 epoch 1777 batch: 400 avg loss -2.789444 avg loss no lamb -2.789444 time 2020-06-27 19:20:48.436818
Model ind 665 epoch 1777 batch: 500 avg loss -2.903234 avg loss no lamb -2.903234 time 2020-06-27 19:20:58.321121
Model ind 665 epoch 1777 batch: 600 avg loss -2.892925 avg loss no lamb -2.892925 time 2020-06-27 19:21:08.203050
Model ind 665 epoch 1777 batch: 700 avg loss -2.819526 avg loss no lamb -2.819526 time 2020-06-27 19:21:18.046943
Model ind 665 epoch 1777 batch: 800 avg loss -2.930508 avg loss no lamb -2.930508 time 2020-06-27 19:21:28.034438
last batch sz 10
Pre: time 2020-06-27 19:21:41.118560: 
 	std: 0.0026911704
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9796, 0.9755, 0.9812, 0.9752]
	train_accs: [0.9814, 0.9810333, 0.97605, 0.98153335, 0.9762167]
	best_train_sub_head: 3
	worst: 0.9752
	avg: 0.97856
	best: 0.9812

Starting e_i: 1778
Model ind 665 epoch 1778 batch: 0 avg loss -2.941499 avg loss no lamb -2.941499 time 2020-06-27 19:21:42.399747
Model ind 665 epoch 1778 batch: 100 avg loss -2.951368 avg loss no lamb -2.951368 time 2020-06-27 19:21:52.244189
Model ind 665 epoch 1778 batch: 200 avg loss -2.865994 avg loss no lamb -2.865994 time 2020-06-27 19:22:01.876185
Model ind 665 epoch 1778 batch: 300 avg loss -2.872462 avg loss no lamb -2.872462 time 2020-06-27 19:22:11.637437
Model ind 665 epoch 1778 batch: 400 avg loss -2.813274 avg loss no lamb -2.813274 time 2020-06-27 19:22:21.369996
Model ind 665 epoch 1778 batch: 500 avg loss -2.856937 avg loss no lamb -2.856937 time 2020-06-27 19:22:31.187151
Model ind 665 epoch 1778 batch: 600 avg loss -2.901355 avg loss no lamb -2.901355 time 2020-06-27 19:22:40.976163
Model ind 665 epoch 1778 batch: 700 avg loss -2.858720 avg loss no lamb -2.858720 time 2020-06-27 19:22:50.581908
Model ind 665 epoch 1778 batch: 800 avg loss -2.853693 avg loss no lamb -2.853693 time 2020-06-27 19:23:00.427958
last batch sz 10
Pre: time 2020-06-27 19:23:13.470461: 
 	std: 0.0024516103
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9789, 0.9734, 0.9787, 0.9746]
	train_accs: [0.98085, 0.9804, 0.97515, 0.981, 0.97581667]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97696
	best: 0.9787

Starting e_i: 1779
Model ind 665 epoch 1779 batch: 0 avg loss -2.972972 avg loss no lamb -2.972972 time 2020-06-27 19:23:14.747523
Model ind 665 epoch 1779 batch: 100 avg loss -2.944574 avg loss no lamb -2.944574 time 2020-06-27 19:23:24.594552
Model ind 665 epoch 1779 batch: 200 avg loss -2.829745 avg loss no lamb -2.829745 time 2020-06-27 19:23:34.263445
Model ind 665 epoch 1779 batch: 300 avg loss -2.909063 avg loss no lamb -2.909063 time 2020-06-27 19:23:43.993041
Model ind 665 epoch 1779 batch: 400 avg loss -2.858197 avg loss no lamb -2.858197 time 2020-06-27 19:23:53.739240
Model ind 665 epoch 1779 batch: 500 avg loss -2.864168 avg loss no lamb -2.864168 time 2020-06-27 19:24:03.435355
Model ind 665 epoch 1779 batch: 600 avg loss -2.873250 avg loss no lamb -2.873250 time 2020-06-27 19:24:13.063673
Model ind 665 epoch 1779 batch: 700 avg loss -2.889946 avg loss no lamb -2.889946 time 2020-06-27 19:24:22.861767
Model ind 665 epoch 1779 batch: 800 avg loss -2.937953 avg loss no lamb -2.937953 time 2020-06-27 19:24:32.586357
last batch sz 10
Pre: time 2020-06-27 19:24:45.586876: 
 	std: 0.0026687996
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9788, 0.9729, 0.9793, 0.9751]
	train_accs: [0.98121667, 0.98003334, 0.97485, 0.98125, 0.9762167]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97713995
	best: 0.9793

Starting e_i: 1780
Model ind 665 epoch 1780 batch: 0 avg loss -2.962951 avg loss no lamb -2.962951 time 2020-06-27 19:24:46.864167
Model ind 665 epoch 1780 batch: 100 avg loss -2.936327 avg loss no lamb -2.936327 time 2020-06-27 19:24:56.558831
Model ind 665 epoch 1780 batch: 200 avg loss -2.870193 avg loss no lamb -2.870193 time 2020-06-27 19:25:06.342115
Model ind 665 epoch 1780 batch: 300 avg loss -2.844313 avg loss no lamb -2.844313 time 2020-06-27 19:25:16.134327
Model ind 665 epoch 1780 batch: 400 avg loss -2.803561 avg loss no lamb -2.803561 time 2020-06-27 19:25:25.800201
Model ind 665 epoch 1780 batch: 500 avg loss -2.860083 avg loss no lamb -2.860083 time 2020-06-27 19:25:35.487149
Model ind 665 epoch 1780 batch: 600 avg loss -2.892255 avg loss no lamb -2.892255 time 2020-06-27 19:25:45.178614
Model ind 665 epoch 1780 batch: 700 avg loss -2.741430 avg loss no lamb -2.741430 time 2020-06-27 19:25:54.925843
Model ind 665 epoch 1780 batch: 800 avg loss -2.850954 avg loss no lamb -2.850954 time 2020-06-27 19:26:04.930190
last batch sz 10
Pre: time 2020-06-27 19:26:17.779901: 
 	std: 0.0029959842
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9794, 0.9724, 0.9794, 0.9742]
	train_accs: [0.9805167, 0.9802167, 0.9748167, 0.98076665, 0.9755833]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.9768999
	best: 0.9794

Starting e_i: 1781
Model ind 665 epoch 1781 batch: 0 avg loss -2.970378 avg loss no lamb -2.970378 time 2020-06-27 19:26:20.168761
Model ind 665 epoch 1781 batch: 100 avg loss -2.937250 avg loss no lamb -2.937250 time 2020-06-27 19:26:29.937569
Model ind 665 epoch 1781 batch: 200 avg loss -2.885121 avg loss no lamb -2.885121 time 2020-06-27 19:26:39.717709
Model ind 665 epoch 1781 batch: 300 avg loss -2.879109 avg loss no lamb -2.879109 time 2020-06-27 19:26:49.529504
Model ind 665 epoch 1781 batch: 400 avg loss -2.849448 avg loss no lamb -2.849448 time 2020-06-27 19:26:59.209665
Model ind 665 epoch 1781 batch: 500 avg loss -2.894829 avg loss no lamb -2.894829 time 2020-06-27 19:27:09.023686
Model ind 665 epoch 1781 batch: 600 avg loss -2.894914 avg loss no lamb -2.894914 time 2020-06-27 19:27:18.915924
Model ind 665 epoch 1781 batch: 700 avg loss -2.848171 avg loss no lamb -2.848171 time 2020-06-27 19:27:28.854832
Model ind 665 epoch 1781 batch: 800 avg loss -2.927455 avg loss no lamb -2.927455 time 2020-06-27 19:27:38.746013
last batch sz 10
Pre: time 2020-06-27 19:27:51.750111: 
 	std: 0.0028786224
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9808, 0.9747, 0.9804, 0.9744]
	train_accs: [0.9814, 0.9813167, 0.9763167, 0.98135, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9744
	avg: 0.97805995
	best: 0.98

Starting e_i: 1782
Model ind 665 epoch 1782 batch: 0 avg loss -2.954678 avg loss no lamb -2.954678 time 2020-06-27 19:27:53.242697
Model ind 665 epoch 1782 batch: 100 avg loss -2.913499 avg loss no lamb -2.913499 time 2020-06-27 19:28:02.974041
Model ind 665 epoch 1782 batch: 200 avg loss -2.902889 avg loss no lamb -2.902889 time 2020-06-27 19:28:12.739652
Model ind 665 epoch 1782 batch: 300 avg loss -2.870241 avg loss no lamb -2.870241 time 2020-06-27 19:28:22.414030
Model ind 665 epoch 1782 batch: 400 avg loss -2.799642 avg loss no lamb -2.799642 time 2020-06-27 19:28:32.220153
Model ind 665 epoch 1782 batch: 500 avg loss -2.923316 avg loss no lamb -2.923316 time 2020-06-27 19:28:41.954155
Model ind 665 epoch 1782 batch: 600 avg loss -2.878315 avg loss no lamb -2.878315 time 2020-06-27 19:28:51.786563
Model ind 665 epoch 1782 batch: 700 avg loss -2.797768 avg loss no lamb -2.797768 time 2020-06-27 19:29:01.496675
Model ind 665 epoch 1782 batch: 800 avg loss -2.894212 avg loss no lamb -2.894212 time 2020-06-27 19:29:11.332605
last batch sz 10
Pre: time 2020-06-27 19:29:24.261943: 
 	std: 0.0022292596
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.979, 0.9747, 0.9795, 0.9749]
	train_accs: [0.9813, 0.98088336, 0.9761, 0.98125, 0.9764]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97752
	best: 0.9795

Starting e_i: 1783
Model ind 665 epoch 1783 batch: 0 avg loss -2.947805 avg loss no lamb -2.947805 time 2020-06-27 19:29:25.542010
Model ind 665 epoch 1783 batch: 100 avg loss -2.875210 avg loss no lamb -2.875210 time 2020-06-27 19:29:35.312177
Model ind 665 epoch 1783 batch: 200 avg loss -2.897364 avg loss no lamb -2.897364 time 2020-06-27 19:29:45.142777
Model ind 665 epoch 1783 batch: 300 avg loss -2.876511 avg loss no lamb -2.876511 time 2020-06-27 19:29:54.836750
Model ind 665 epoch 1783 batch: 400 avg loss -2.782673 avg loss no lamb -2.782673 time 2020-06-27 19:30:04.576193
Model ind 665 epoch 1783 batch: 500 avg loss -2.888288 avg loss no lamb -2.888288 time 2020-06-27 19:30:14.243792
Model ind 665 epoch 1783 batch: 600 avg loss -2.879602 avg loss no lamb -2.879602 time 2020-06-27 19:30:23.966350
Model ind 665 epoch 1783 batch: 700 avg loss -2.798925 avg loss no lamb -2.798925 time 2020-06-27 19:30:33.800533
Model ind 665 epoch 1783 batch: 800 avg loss -2.891184 avg loss no lamb -2.891184 time 2020-06-27 19:30:43.802523
last batch sz 10
Pre: time 2020-06-27 19:30:56.942484: 
 	std: 0.0026141286
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9786, 0.9734, 0.9796, 0.9747]
	train_accs: [0.98108333, 0.9806333, 0.97546667, 0.98113334, 0.9757]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97718
	best: 0.9796

Starting e_i: 1784
Model ind 665 epoch 1784 batch: 0 avg loss -2.972863 avg loss no lamb -2.972863 time 2020-06-27 19:30:58.183314
Model ind 665 epoch 1784 batch: 100 avg loss -2.904710 avg loss no lamb -2.904710 time 2020-06-27 19:31:07.977080
Model ind 665 epoch 1784 batch: 200 avg loss -2.843922 avg loss no lamb -2.843922 time 2020-06-27 19:31:17.749482
Model ind 665 epoch 1784 batch: 300 avg loss -2.880484 avg loss no lamb -2.880484 time 2020-06-27 19:31:27.552768
Model ind 665 epoch 1784 batch: 400 avg loss -2.838525 avg loss no lamb -2.838525 time 2020-06-27 19:31:37.241198
Model ind 665 epoch 1784 batch: 500 avg loss -2.847932 avg loss no lamb -2.847932 time 2020-06-27 19:31:46.957637
Model ind 665 epoch 1784 batch: 600 avg loss -2.888049 avg loss no lamb -2.888049 time 2020-06-27 19:31:56.743928
Model ind 665 epoch 1784 batch: 700 avg loss -2.796249 avg loss no lamb -2.796249 time 2020-06-27 19:32:06.534168
Model ind 665 epoch 1784 batch: 800 avg loss -2.903687 avg loss no lamb -2.903687 time 2020-06-27 19:32:16.225136
last batch sz 10
Pre: time 2020-06-27 19:32:29.577250: 
 	std: 0.0032909617
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9794, 0.9731, 0.9804, 0.9734]
	train_accs: [0.9813, 0.9805167, 0.97473335, 0.9813667, 0.97571665]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.97726
	best: 0.9804

Starting e_i: 1785
Model ind 665 epoch 1785 batch: 0 avg loss -2.948179 avg loss no lamb -2.948179 time 2020-06-27 19:32:30.796235
Model ind 665 epoch 1785 batch: 100 avg loss -2.953822 avg loss no lamb -2.953822 time 2020-06-27 19:32:40.621893
Model ind 665 epoch 1785 batch: 200 avg loss -2.891632 avg loss no lamb -2.891632 time 2020-06-27 19:32:50.452105
Model ind 665 epoch 1785 batch: 300 avg loss -2.885847 avg loss no lamb -2.885847 time 2020-06-27 19:33:00.181210
Model ind 665 epoch 1785 batch: 400 avg loss -2.781090 avg loss no lamb -2.781090 time 2020-06-27 19:33:09.898366
Model ind 665 epoch 1785 batch: 500 avg loss -2.838360 avg loss no lamb -2.838360 time 2020-06-27 19:33:19.876076
Model ind 665 epoch 1785 batch: 600 avg loss -2.893113 avg loss no lamb -2.893113 time 2020-06-27 19:33:29.593093
Model ind 665 epoch 1785 batch: 700 avg loss -2.805581 avg loss no lamb -2.805581 time 2020-06-27 19:33:39.414098
Model ind 665 epoch 1785 batch: 800 avg loss -2.832932 avg loss no lamb -2.832932 time 2020-06-27 19:33:49.104550
last batch sz 10
Pre: time 2020-06-27 19:34:01.933332: 
 	std: 0.00328511
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9807, 0.9741, 0.9812, 0.9739]
	train_accs: [0.9812667, 0.9809833, 0.9752333, 0.98145, 0.97535]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.978
	best: 0.9812

Starting e_i: 1786
Model ind 665 epoch 1786 batch: 0 avg loss -2.911727 avg loss no lamb -2.911727 time 2020-06-27 19:34:03.215108
Model ind 665 epoch 1786 batch: 100 avg loss -2.898951 avg loss no lamb -2.898951 time 2020-06-27 19:34:13.251465
Model ind 665 epoch 1786 batch: 200 avg loss -2.923962 avg loss no lamb -2.923962 time 2020-06-27 19:34:23.004493
Model ind 665 epoch 1786 batch: 300 avg loss -2.875954 avg loss no lamb -2.875954 time 2020-06-27 19:34:32.684331
Model ind 665 epoch 1786 batch: 400 avg loss -2.776932 avg loss no lamb -2.776932 time 2020-06-27 19:34:42.482663
Model ind 665 epoch 1786 batch: 500 avg loss -2.874542 avg loss no lamb -2.874542 time 2020-06-27 19:34:52.146352
Model ind 665 epoch 1786 batch: 600 avg loss -2.915083 avg loss no lamb -2.915083 time 2020-06-27 19:35:01.906850
Model ind 665 epoch 1786 batch: 700 avg loss -2.821133 avg loss no lamb -2.821133 time 2020-06-27 19:35:11.838400
Model ind 665 epoch 1786 batch: 800 avg loss -2.834244 avg loss no lamb -2.834244 time 2020-06-27 19:35:21.691436
last batch sz 10
Pre: time 2020-06-27 19:35:34.833051: 
 	std: 0.0034113938
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.978, 0.9712, 0.9799, 0.9738]
	train_accs: [0.9813, 0.97971666, 0.97405, 0.98121667, 0.97525]
	best_train_sub_head: 0
	worst: 0.9712
	avg: 0.97648
	best: 0.9795

Starting e_i: 1787
Model ind 665 epoch 1787 batch: 0 avg loss -2.982705 avg loss no lamb -2.982705 time 2020-06-27 19:35:36.045131
Model ind 665 epoch 1787 batch: 100 avg loss -2.873547 avg loss no lamb -2.873547 time 2020-06-27 19:35:45.923822
Model ind 665 epoch 1787 batch: 200 avg loss -2.877676 avg loss no lamb -2.877676 time 2020-06-27 19:35:55.900679
Model ind 665 epoch 1787 batch: 300 avg loss -2.912599 avg loss no lamb -2.912599 time 2020-06-27 19:36:05.879122
Model ind 665 epoch 1787 batch: 400 avg loss -2.839808 avg loss no lamb -2.839808 time 2020-06-27 19:36:15.580692
Model ind 665 epoch 1787 batch: 500 avg loss -2.922636 avg loss no lamb -2.922636 time 2020-06-27 19:36:25.267885
Model ind 665 epoch 1787 batch: 600 avg loss -2.899947 avg loss no lamb -2.899947 time 2020-06-27 19:36:34.964180
Model ind 665 epoch 1787 batch: 700 avg loss -2.766750 avg loss no lamb -2.766750 time 2020-06-27 19:36:44.828442
Model ind 665 epoch 1787 batch: 800 avg loss -2.894113 avg loss no lamb -2.894113 time 2020-06-27 19:36:54.644659
last batch sz 10
Pre: time 2020-06-27 19:37:07.447669: 
 	std: 0.0024490082
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9785, 0.9778, 0.9722, 0.9778, 0.9743]
	train_accs: [0.98118335, 0.98013335, 0.97508335, 0.9809333, 0.9763833]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97612
	best: 0.9785

Starting e_i: 1788
Model ind 665 epoch 1788 batch: 0 avg loss -2.924675 avg loss no lamb -2.924675 time 2020-06-27 19:37:08.712273
Model ind 665 epoch 1788 batch: 100 avg loss -2.924725 avg loss no lamb -2.924725 time 2020-06-27 19:37:18.439639
Model ind 665 epoch 1788 batch: 200 avg loss -2.864799 avg loss no lamb -2.864799 time 2020-06-27 19:37:28.227093
Model ind 665 epoch 1788 batch: 300 avg loss -2.872852 avg loss no lamb -2.872852 time 2020-06-27 19:37:38.073597
Model ind 665 epoch 1788 batch: 400 avg loss -2.770409 avg loss no lamb -2.770409 time 2020-06-27 19:37:47.867807
Model ind 665 epoch 1788 batch: 500 avg loss -2.862890 avg loss no lamb -2.862890 time 2020-06-27 19:37:57.646580
Model ind 665 epoch 1788 batch: 600 avg loss -2.886240 avg loss no lamb -2.886240 time 2020-06-27 19:38:07.351394
Model ind 665 epoch 1788 batch: 700 avg loss -2.780766 avg loss no lamb -2.780766 time 2020-06-27 19:38:17.160664
Model ind 665 epoch 1788 batch: 800 avg loss -2.888413 avg loss no lamb -2.888413 time 2020-06-27 19:38:27.966453
last batch sz 10
Pre: time 2020-06-27 19:38:40.811587: 
 	std: 0.0032388966
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9786, 0.9712, 0.9784, 0.9729]
	train_accs: [0.98095, 0.9805167, 0.97438335, 0.98106664, 0.97578335]
	best_train_sub_head: 3
	worst: 0.9712
	avg: 0.97595996
	best: 0.9784

Starting e_i: 1789
Model ind 665 epoch 1789 batch: 0 avg loss -2.936110 avg loss no lamb -2.936110 time 2020-06-27 19:38:42.255855
Model ind 665 epoch 1789 batch: 100 avg loss -2.899371 avg loss no lamb -2.899371 time 2020-06-27 19:38:52.711756
Model ind 665 epoch 1789 batch: 200 avg loss -2.845196 avg loss no lamb -2.845196 time 2020-06-27 19:39:02.344660
Model ind 665 epoch 1789 batch: 300 avg loss -2.866531 avg loss no lamb -2.866531 time 2020-06-27 19:39:11.953284
Model ind 665 epoch 1789 batch: 400 avg loss -2.878381 avg loss no lamb -2.878381 time 2020-06-27 19:39:21.502243
Model ind 665 epoch 1789 batch: 500 avg loss -2.843621 avg loss no lamb -2.843621 time 2020-06-27 19:39:30.968838
Model ind 665 epoch 1789 batch: 600 avg loss -2.887133 avg loss no lamb -2.887133 time 2020-06-27 19:39:40.455838
Model ind 665 epoch 1789 batch: 700 avg loss -2.776648 avg loss no lamb -2.776648 time 2020-06-27 19:39:49.963300
Model ind 665 epoch 1789 batch: 800 avg loss -2.893469 avg loss no lamb -2.893469 time 2020-06-27 19:39:59.489648
last batch sz 10
Pre: time 2020-06-27 19:40:12.072891: 
 	std: 0.0026027528
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9783, 0.9725, 0.9791, 0.9752]
	train_accs: [0.981, 0.98025, 0.97505, 0.98121667, 0.97613335]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97684
	best: 0.9791

Starting e_i: 1790
Model ind 665 epoch 1790 batch: 0 avg loss -2.979498 avg loss no lamb -2.979498 time 2020-06-27 19:40:13.326673
Model ind 665 epoch 1790 batch: 100 avg loss -2.905971 avg loss no lamb -2.905971 time 2020-06-27 19:40:22.870110
Model ind 665 epoch 1790 batch: 200 avg loss -2.851427 avg loss no lamb -2.851427 time 2020-06-27 19:40:32.339001
Model ind 665 epoch 1790 batch: 300 avg loss -2.898677 avg loss no lamb -2.898677 time 2020-06-27 19:40:41.759607
Model ind 665 epoch 1790 batch: 400 avg loss -2.793066 avg loss no lamb -2.793066 time 2020-06-27 19:40:51.210801
Model ind 665 epoch 1790 batch: 500 avg loss -2.912638 avg loss no lamb -2.912638 time 2020-06-27 19:41:00.697903
Model ind 665 epoch 1790 batch: 600 avg loss -2.928099 avg loss no lamb -2.928099 time 2020-06-27 19:41:10.170683
Model ind 665 epoch 1790 batch: 700 avg loss -2.774856 avg loss no lamb -2.774856 time 2020-06-27 19:41:19.722814
Model ind 665 epoch 1790 batch: 800 avg loss -2.846561 avg loss no lamb -2.846561 time 2020-06-27 19:41:29.203963
last batch sz 10
Pre: time 2020-06-27 19:41:42.126751: 
 	std: 0.0028806995
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9779, 0.9778, 0.9713, 0.9776, 0.9726]
	train_accs: [0.98035, 0.9803, 0.97531664, 0.9803333, 0.9755667]
	best_train_sub_head: 0
	worst: 0.9713
	avg: 0.97544
	best: 0.9779

Starting e_i: 1791
Model ind 665 epoch 1791 batch: 0 avg loss -2.938948 avg loss no lamb -2.938948 time 2020-06-27 19:41:44.537340
Model ind 665 epoch 1791 batch: 100 avg loss -2.922919 avg loss no lamb -2.922919 time 2020-06-27 19:41:54.163016
Model ind 665 epoch 1791 batch: 200 avg loss -2.865709 avg loss no lamb -2.865709 time 2020-06-27 19:42:03.724706
Model ind 665 epoch 1791 batch: 300 avg loss -2.888083 avg loss no lamb -2.888083 time 2020-06-27 19:42:13.287805
Model ind 665 epoch 1791 batch: 400 avg loss -2.821543 avg loss no lamb -2.821543 time 2020-06-27 19:42:22.858957
Model ind 665 epoch 1791 batch: 500 avg loss -2.870578 avg loss no lamb -2.870578 time 2020-06-27 19:42:32.430198
Model ind 665 epoch 1791 batch: 600 avg loss -2.900610 avg loss no lamb -2.900610 time 2020-06-27 19:42:42.070764
Model ind 665 epoch 1791 batch: 700 avg loss -2.854630 avg loss no lamb -2.854630 time 2020-06-27 19:42:51.688034
Model ind 665 epoch 1791 batch: 800 avg loss -2.859208 avg loss no lamb -2.859208 time 2020-06-27 19:43:01.178377
last batch sz 10
Pre: time 2020-06-27 19:43:13.975152: 
 	std: 0.0033126376
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9795, 0.9721, 0.9788, 0.973]
	train_accs: [0.98186666, 0.98111665, 0.97468334, 0.9816333, 0.9767]
	best_train_sub_head: 0
	worst: 0.9721
	avg: 0.97657996
	best: 0.9795

Starting e_i: 1792
Model ind 665 epoch 1792 batch: 0 avg loss -2.945323 avg loss no lamb -2.945323 time 2020-06-27 19:43:15.186389
Model ind 665 epoch 1792 batch: 100 avg loss -2.900769 avg loss no lamb -2.900769 time 2020-06-27 19:43:24.748640
Model ind 665 epoch 1792 batch: 200 avg loss -2.837978 avg loss no lamb -2.837978 time 2020-06-27 19:43:34.140632
Model ind 665 epoch 1792 batch: 300 avg loss -2.902960 avg loss no lamb -2.902960 time 2020-06-27 19:43:43.596429
Model ind 665 epoch 1792 batch: 400 avg loss -2.839913 avg loss no lamb -2.839913 time 2020-06-27 19:43:53.037322
Model ind 665 epoch 1792 batch: 500 avg loss -2.818526 avg loss no lamb -2.818526 time 2020-06-27 19:44:05.886315
Model ind 665 epoch 1792 batch: 600 avg loss -2.880796 avg loss no lamb -2.880796 time 2020-06-27 19:44:16.629127
Model ind 665 epoch 1792 batch: 700 avg loss -2.777016 avg loss no lamb -2.777016 time 2020-06-27 19:44:26.413441
Model ind 665 epoch 1792 batch: 800 avg loss -2.910205 avg loss no lamb -2.910205 time 2020-06-27 19:44:36.286165
last batch sz 10
Pre: time 2020-06-27 19:44:49.077723: 
 	std: 0.0038257062
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.9799, 0.9711, 0.9794, 0.9724]
	train_accs: [0.9813667, 0.98118335, 0.97445, 0.98118335, 0.97573334]
	best_train_sub_head: 0
	worst: 0.9711
	avg: 0.9764
	best: 0.9792

Starting e_i: 1793
Model ind 665 epoch 1793 batch: 0 avg loss -2.941855 avg loss no lamb -2.941855 time 2020-06-27 19:44:50.363175
Model ind 665 epoch 1793 batch: 100 avg loss -2.880731 avg loss no lamb -2.880731 time 2020-06-27 19:45:00.093766
Model ind 665 epoch 1793 batch: 200 avg loss -2.820238 avg loss no lamb -2.820238 time 2020-06-27 19:45:09.881822
Model ind 665 epoch 1793 batch: 300 avg loss -2.885818 avg loss no lamb -2.885818 time 2020-06-27 19:45:19.704699
Model ind 665 epoch 1793 batch: 400 avg loss -2.880528 avg loss no lamb -2.880528 time 2020-06-27 19:45:29.454019
Model ind 665 epoch 1793 batch: 500 avg loss -2.869902 avg loss no lamb -2.869902 time 2020-06-27 19:45:39.345819
Model ind 665 epoch 1793 batch: 600 avg loss -2.894043 avg loss no lamb -2.894043 time 2020-06-27 19:45:49.023096
Model ind 665 epoch 1793 batch: 700 avg loss -2.841352 avg loss no lamb -2.841352 time 2020-06-27 19:45:58.923558
Model ind 665 epoch 1793 batch: 800 avg loss -2.893860 avg loss no lamb -2.893860 time 2020-06-27 19:46:08.706632
last batch sz 10
Pre: time 2020-06-27 19:46:21.780632: 
 	std: 0.0029403477
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9775, 0.9782, 0.9717, 0.9782, 0.9723]
	train_accs: [0.9803333, 0.98031664, 0.97498333, 0.98036665, 0.97608334]
	best_train_sub_head: 3
	worst: 0.9717
	avg: 0.97558004
	best: 0.9782

Starting e_i: 1794
Model ind 665 epoch 1794 batch: 0 avg loss -2.985119 avg loss no lamb -2.985119 time 2020-06-27 19:46:23.229212
Model ind 665 epoch 1794 batch: 100 avg loss -2.956539 avg loss no lamb -2.956539 time 2020-06-27 19:46:33.065491
Model ind 665 epoch 1794 batch: 200 avg loss -2.850206 avg loss no lamb -2.850206 time 2020-06-27 19:46:42.809388
Model ind 665 epoch 1794 batch: 300 avg loss -2.864062 avg loss no lamb -2.864062 time 2020-06-27 19:46:52.769407
Model ind 665 epoch 1794 batch: 400 avg loss -2.804506 avg loss no lamb -2.804506 time 2020-06-27 19:47:02.529690
Model ind 665 epoch 1794 batch: 500 avg loss -2.779566 avg loss no lamb -2.779566 time 2020-06-27 19:47:12.367634
Model ind 665 epoch 1794 batch: 600 avg loss -2.919820 avg loss no lamb -2.919820 time 2020-06-27 19:47:22.175777
Model ind 665 epoch 1794 batch: 700 avg loss -2.742620 avg loss no lamb -2.742620 time 2020-06-27 19:47:31.938752
Model ind 665 epoch 1794 batch: 800 avg loss -2.867108 avg loss no lamb -2.867108 time 2020-06-27 19:47:41.751114
last batch sz 10
Pre: time 2020-06-27 19:47:54.594977: 
 	std: 0.0027485313
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9781, 0.9768, 0.9719, 0.9781, 0.9724]
	train_accs: [0.98053336, 0.97933334, 0.97485, 0.98036665, 0.9756333]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97546005
	best: 0.9781

Starting e_i: 1795
Model ind 665 epoch 1795 batch: 0 avg loss -2.920161 avg loss no lamb -2.920161 time 2020-06-27 19:47:55.863035
Model ind 665 epoch 1795 batch: 100 avg loss -2.892842 avg loss no lamb -2.892842 time 2020-06-27 19:48:05.616521
Model ind 665 epoch 1795 batch: 200 avg loss -2.867897 avg loss no lamb -2.867897 time 2020-06-27 19:48:15.465947
Model ind 665 epoch 1795 batch: 300 avg loss -2.882359 avg loss no lamb -2.882359 time 2020-06-27 19:48:25.239729
Model ind 665 epoch 1795 batch: 400 avg loss -2.748930 avg loss no lamb -2.748930 time 2020-06-27 19:48:35.160801
Model ind 665 epoch 1795 batch: 500 avg loss -2.895864 avg loss no lamb -2.895864 time 2020-06-27 19:48:44.966876
Model ind 665 epoch 1795 batch: 600 avg loss -2.894337 avg loss no lamb -2.894337 time 2020-06-27 19:48:54.688609
Model ind 665 epoch 1795 batch: 700 avg loss -2.811765 avg loss no lamb -2.811765 time 2020-06-27 19:49:04.396477
Model ind 665 epoch 1795 batch: 800 avg loss -2.875067 avg loss no lamb -2.875067 time 2020-06-27 19:49:14.345882
last batch sz 10
Pre: time 2020-06-27 19:49:27.198087: 
 	std: 0.0029403444
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9793, 0.9726, 0.9785, 0.9732]
	train_accs: [0.9810333, 0.9809333, 0.97505, 0.98071665, 0.9755667]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97648
	best: 0.9788

Starting e_i: 1796
Model ind 665 epoch 1796 batch: 0 avg loss -2.952705 avg loss no lamb -2.952705 time 2020-06-27 19:49:28.743875
Model ind 665 epoch 1796 batch: 100 avg loss -2.941212 avg loss no lamb -2.941212 time 2020-06-27 19:49:38.554548
Model ind 665 epoch 1796 batch: 200 avg loss -2.878861 avg loss no lamb -2.878861 time 2020-06-27 19:49:48.442398
Model ind 665 epoch 1796 batch: 300 avg loss -2.888921 avg loss no lamb -2.888921 time 2020-06-27 19:49:58.194205
Model ind 665 epoch 1796 batch: 400 avg loss -2.837288 avg loss no lamb -2.837288 time 2020-06-27 19:50:07.957299
Model ind 665 epoch 1796 batch: 500 avg loss -2.829268 avg loss no lamb -2.829268 time 2020-06-27 19:50:17.716182
Model ind 665 epoch 1796 batch: 600 avg loss -2.951020 avg loss no lamb -2.951020 time 2020-06-27 19:50:27.680830
Model ind 665 epoch 1796 batch: 700 avg loss -2.823246 avg loss no lamb -2.823246 time 2020-06-27 19:50:37.492464
Model ind 665 epoch 1796 batch: 800 avg loss -2.932961 avg loss no lamb -2.932961 time 2020-06-27 19:50:47.270736
last batch sz 10
Pre: time 2020-06-27 19:51:00.198910: 
 	std: 0.0029031015
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9797, 0.9728, 0.9799, 0.9749]
	train_accs: [0.98088336, 0.98075, 0.97471666, 0.9810167, 0.9764]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97730005
	best: 0.9799

Starting e_i: 1797
Model ind 665 epoch 1797 batch: 0 avg loss -2.941067 avg loss no lamb -2.941067 time 2020-06-27 19:51:01.538922
Model ind 665 epoch 1797 batch: 100 avg loss -2.945170 avg loss no lamb -2.945170 time 2020-06-27 19:51:11.357106
Model ind 665 epoch 1797 batch: 200 avg loss -2.905748 avg loss no lamb -2.905748 time 2020-06-27 19:51:21.199107
Model ind 665 epoch 1797 batch: 300 avg loss -2.915110 avg loss no lamb -2.915110 time 2020-06-27 19:51:31.051220
Model ind 665 epoch 1797 batch: 400 avg loss -2.837003 avg loss no lamb -2.837003 time 2020-06-27 19:51:40.898004
Model ind 665 epoch 1797 batch: 500 avg loss -2.928686 avg loss no lamb -2.928686 time 2020-06-27 19:51:50.541969
Model ind 665 epoch 1797 batch: 600 avg loss -2.922747 avg loss no lamb -2.922747 time 2020-06-27 19:52:00.199158
Model ind 665 epoch 1797 batch: 700 avg loss -2.801746 avg loss no lamb -2.801746 time 2020-06-27 19:52:10.172925
Model ind 665 epoch 1797 batch: 800 avg loss -2.862939 avg loss no lamb -2.862939 time 2020-06-27 19:52:19.959136
last batch sz 10
Pre: time 2020-06-27 19:52:32.856115: 
 	std: 0.0025938281
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9799, 0.9739, 0.9791, 0.9746]
	train_accs: [0.9813333, 0.98088336, 0.9759167, 0.9809833, 0.97658336]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.9774
	best: 0.9795

Starting e_i: 1798
Model ind 665 epoch 1798 batch: 0 avg loss -2.950301 avg loss no lamb -2.950301 time 2020-06-27 19:52:34.096270
Model ind 665 epoch 1798 batch: 100 avg loss -2.967407 avg loss no lamb -2.967407 time 2020-06-27 19:52:43.890456
Model ind 665 epoch 1798 batch: 200 avg loss -2.885298 avg loss no lamb -2.885298 time 2020-06-27 19:52:53.628409
Model ind 665 epoch 1798 batch: 300 avg loss -2.908801 avg loss no lamb -2.908801 time 2020-06-27 19:53:03.630066
Model ind 665 epoch 1798 batch: 400 avg loss -2.822489 avg loss no lamb -2.822489 time 2020-06-27 19:53:13.655831
Model ind 665 epoch 1798 batch: 500 avg loss -2.850810 avg loss no lamb -2.850810 time 2020-06-27 19:53:23.456599
Model ind 665 epoch 1798 batch: 600 avg loss -2.884875 avg loss no lamb -2.884875 time 2020-06-27 19:53:33.207663
Model ind 665 epoch 1798 batch: 700 avg loss -2.809171 avg loss no lamb -2.809171 time 2020-06-27 19:53:42.844089
Model ind 665 epoch 1798 batch: 800 avg loss -2.909841 avg loss no lamb -2.909841 time 2020-06-27 19:53:52.744739
last batch sz 10
Pre: time 2020-06-27 19:54:06.206436: 
 	std: 0.0033546432
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9788, 0.9776, 0.9705, 0.9779, 0.9723]
	train_accs: [0.98088336, 0.97978336, 0.9742, 0.9809, 0.97533333]
	best_train_sub_head: 3
	worst: 0.9705
	avg: 0.97542
	best: 0.9779

Starting e_i: 1799
Model ind 665 epoch 1799 batch: 0 avg loss -2.994644 avg loss no lamb -2.994644 time 2020-06-27 19:54:07.461165
Model ind 665 epoch 1799 batch: 100 avg loss -2.946794 avg loss no lamb -2.946794 time 2020-06-27 19:54:17.246291
Model ind 665 epoch 1799 batch: 200 avg loss -2.887141 avg loss no lamb -2.887141 time 2020-06-27 19:54:26.967490
Model ind 665 epoch 1799 batch: 300 avg loss -2.896021 avg loss no lamb -2.896021 time 2020-06-27 19:54:36.686974
Model ind 665 epoch 1799 batch: 400 avg loss -2.839965 avg loss no lamb -2.839965 time 2020-06-27 19:54:46.484278
Model ind 665 epoch 1799 batch: 500 avg loss -2.811217 avg loss no lamb -2.811217 time 2020-06-27 19:54:56.246725
Model ind 665 epoch 1799 batch: 600 avg loss -2.862143 avg loss no lamb -2.862143 time 2020-06-27 19:55:06.095833
Model ind 665 epoch 1799 batch: 700 avg loss -2.838405 avg loss no lamb -2.838405 time 2020-06-27 19:55:15.791360
Model ind 665 epoch 1799 batch: 800 avg loss -2.885521 avg loss no lamb -2.885521 time 2020-06-27 19:55:25.590426
last batch sz 10
Pre: time 2020-06-27 19:55:38.507830: 
 	std: 0.003143247
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.979, 0.9724, 0.9796, 0.9738]
	train_accs: [0.98181665, 0.98105, 0.97505, 0.9818, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.9769
	best: 0.9797

Starting e_i: 1800
Model ind 665 epoch 1800 batch: 0 avg loss -2.940905 avg loss no lamb -2.940905 time 2020-06-27 19:55:39.865852
Model ind 665 epoch 1800 batch: 100 avg loss -2.974179 avg loss no lamb -2.974179 time 2020-06-27 19:55:49.597169
Model ind 665 epoch 1800 batch: 200 avg loss -2.898337 avg loss no lamb -2.898337 time 2020-06-27 19:55:59.251168
Model ind 665 epoch 1800 batch: 300 avg loss -2.853603 avg loss no lamb -2.853603 time 2020-06-27 19:56:09.095498
Model ind 665 epoch 1800 batch: 400 avg loss -2.799465 avg loss no lamb -2.799465 time 2020-06-27 19:56:18.861131
Model ind 665 epoch 1800 batch: 500 avg loss -2.878329 avg loss no lamb -2.878329 time 2020-06-27 19:56:28.502175
Model ind 665 epoch 1800 batch: 600 avg loss -2.923141 avg loss no lamb -2.923141 time 2020-06-27 19:56:38.276725
Model ind 665 epoch 1800 batch: 700 avg loss -2.813864 avg loss no lamb -2.813864 time 2020-06-27 19:56:48.340417
Model ind 665 epoch 1800 batch: 800 avg loss -2.936490 avg loss no lamb -2.936490 time 2020-06-27 19:56:58.203909
last batch sz 10
Pre: time 2020-06-27 19:57:11.083416: 
 	std: 0.0034679193
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.981, 0.9804, 0.9737, 0.9809, 0.9737]
	train_accs: [0.9819667, 0.98121667, 0.9756333, 0.98188335, 0.9759167]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97793996
	best: 0.981

Starting e_i: 1801
Model ind 665 epoch 1801 batch: 0 avg loss -2.987520 avg loss no lamb -2.987520 time 2020-06-27 19:57:13.780259
Model ind 665 epoch 1801 batch: 100 avg loss -2.965381 avg loss no lamb -2.965381 time 2020-06-27 19:57:23.455653
Model ind 665 epoch 1801 batch: 200 avg loss -2.908712 avg loss no lamb -2.908712 time 2020-06-27 19:57:33.302684
Model ind 665 epoch 1801 batch: 300 avg loss -2.902006 avg loss no lamb -2.902006 time 2020-06-27 19:57:43.172748
Model ind 665 epoch 1801 batch: 400 avg loss -2.802884 avg loss no lamb -2.802884 time 2020-06-27 19:57:53.040343
Model ind 665 epoch 1801 batch: 500 avg loss -2.857372 avg loss no lamb -2.857372 time 2020-06-27 19:58:02.800278
Model ind 665 epoch 1801 batch: 600 avg loss -2.916885 avg loss no lamb -2.916885 time 2020-06-27 19:58:12.663493
Model ind 665 epoch 1801 batch: 700 avg loss -2.763991 avg loss no lamb -2.763991 time 2020-06-27 19:58:22.539692
Model ind 665 epoch 1801 batch: 800 avg loss -2.848712 avg loss no lamb -2.848712 time 2020-06-27 19:58:32.345492
last batch sz 10
Pre: time 2020-06-27 19:58:45.264365: 
 	std: 0.0030042604
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9783, 0.9727, 0.9795, 0.9734]
	train_accs: [0.98111665, 0.9798833, 0.97546667, 0.98116666, 0.97611666]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97668
	best: 0.9795

Starting e_i: 1802
Model ind 665 epoch 1802 batch: 0 avg loss -2.962470 avg loss no lamb -2.962470 time 2020-06-27 19:58:46.494568
Model ind 665 epoch 1802 batch: 100 avg loss -2.887155 avg loss no lamb -2.887155 time 2020-06-27 19:58:56.269589
Model ind 665 epoch 1802 batch: 200 avg loss -2.908872 avg loss no lamb -2.908872 time 2020-06-27 19:59:06.005171
Model ind 665 epoch 1802 batch: 300 avg loss -2.906317 avg loss no lamb -2.906317 time 2020-06-27 19:59:15.692062
Model ind 665 epoch 1802 batch: 400 avg loss -2.833008 avg loss no lamb -2.833008 time 2020-06-27 19:59:25.584421
Model ind 665 epoch 1802 batch: 500 avg loss -2.856505 avg loss no lamb -2.856505 time 2020-06-27 19:59:35.590218
Model ind 665 epoch 1802 batch: 600 avg loss -2.906443 avg loss no lamb -2.906443 time 2020-06-27 19:59:45.456281
Model ind 665 epoch 1802 batch: 700 avg loss -2.768242 avg loss no lamb -2.768242 time 2020-06-27 19:59:55.179762
Model ind 665 epoch 1802 batch: 800 avg loss -2.917912 avg loss no lamb -2.917912 time 2020-06-27 20:00:05.014777
last batch sz 10
Pre: time 2020-06-27 20:00:17.995697: 
 	std: 0.0028701117
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9787, 0.9728, 0.9802, 0.9749]
	train_accs: [0.9810167, 0.98053336, 0.9751833, 0.98116666, 0.9767333]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97721994
	best: 0.9802

Starting e_i: 1803
Model ind 665 epoch 1803 batch: 0 avg loss -2.955592 avg loss no lamb -2.955592 time 2020-06-27 20:00:19.299192
Model ind 665 epoch 1803 batch: 100 avg loss -2.847469 avg loss no lamb -2.847469 time 2020-06-27 20:00:29.138522
Model ind 665 epoch 1803 batch: 200 avg loss -2.841419 avg loss no lamb -2.841419 time 2020-06-27 20:00:39.048447
Model ind 665 epoch 1803 batch: 300 avg loss -2.871277 avg loss no lamb -2.871277 time 2020-06-27 20:00:48.893518
Model ind 665 epoch 1803 batch: 400 avg loss -2.807256 avg loss no lamb -2.807256 time 2020-06-27 20:00:58.838832
Model ind 665 epoch 1803 batch: 500 avg loss -2.861727 avg loss no lamb -2.861727 time 2020-06-27 20:01:08.709351
Model ind 665 epoch 1803 batch: 600 avg loss -2.907611 avg loss no lamb -2.907611 time 2020-06-27 20:01:18.758881
Model ind 665 epoch 1803 batch: 700 avg loss -2.833378 avg loss no lamb -2.833378 time 2020-06-27 20:01:28.711668
Model ind 665 epoch 1803 batch: 800 avg loss -2.892188 avg loss no lamb -2.892188 time 2020-06-27 20:01:38.605157
last batch sz 10
Pre: time 2020-06-27 20:01:51.720957: 
 	std: 0.0031601381
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9789, 0.9726, 0.9795, 0.973]
	train_accs: [0.9812833, 0.98036665, 0.97508335, 0.98141664, 0.97585]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97665995
	best: 0.9795

Starting e_i: 1804
Model ind 665 epoch 1804 batch: 0 avg loss -2.966577 avg loss no lamb -2.966577 time 2020-06-27 20:01:52.986274
Model ind 665 epoch 1804 batch: 100 avg loss -2.918928 avg loss no lamb -2.918928 time 2020-06-27 20:02:02.890003
Model ind 665 epoch 1804 batch: 200 avg loss -2.930762 avg loss no lamb -2.930762 time 2020-06-27 20:02:12.861549
Model ind 665 epoch 1804 batch: 300 avg loss -2.878565 avg loss no lamb -2.878565 time 2020-06-27 20:02:22.544992
Model ind 665 epoch 1804 batch: 400 avg loss -2.836750 avg loss no lamb -2.836750 time 2020-06-27 20:02:32.355073
Model ind 665 epoch 1804 batch: 500 avg loss -2.911057 avg loss no lamb -2.911057 time 2020-06-27 20:02:42.309230
Model ind 665 epoch 1804 batch: 600 avg loss -2.915729 avg loss no lamb -2.915729 time 2020-06-27 20:02:52.254192
Model ind 665 epoch 1804 batch: 700 avg loss -2.849733 avg loss no lamb -2.849733 time 2020-06-27 20:03:02.166392
Model ind 665 epoch 1804 batch: 800 avg loss -2.898807 avg loss no lamb -2.898807 time 2020-06-27 20:03:11.919594
last batch sz 10
Pre: time 2020-06-27 20:03:24.778853: 
 	std: 0.0032239067
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9791, 0.9721, 0.9799, 0.9744]
	train_accs: [0.98108333, 0.98041666, 0.9752333, 0.9810333, 0.9762667]
	best_train_sub_head: 0
	worst: 0.9721
	avg: 0.97708
	best: 0.9799

Starting e_i: 1805
Model ind 665 epoch 1805 batch: 0 avg loss -2.953886 avg loss no lamb -2.953886 time 2020-06-27 20:03:26.098022
Model ind 665 epoch 1805 batch: 100 avg loss -2.918419 avg loss no lamb -2.918419 time 2020-06-27 20:03:35.844007
Model ind 665 epoch 1805 batch: 200 avg loss -2.904559 avg loss no lamb -2.904559 time 2020-06-27 20:03:45.710363
Model ind 665 epoch 1805 batch: 300 avg loss -2.911378 avg loss no lamb -2.911378 time 2020-06-27 20:03:55.593471
Model ind 665 epoch 1805 batch: 400 avg loss -2.886584 avg loss no lamb -2.886584 time 2020-06-27 20:04:05.475286
Model ind 665 epoch 1805 batch: 500 avg loss -2.885066 avg loss no lamb -2.885066 time 2020-06-27 20:04:15.335218
Model ind 665 epoch 1805 batch: 600 avg loss -2.916850 avg loss no lamb -2.916850 time 2020-06-27 20:04:25.068994
Model ind 665 epoch 1805 batch: 700 avg loss -2.815183 avg loss no lamb -2.815183 time 2020-06-27 20:04:34.901407
Model ind 665 epoch 1805 batch: 800 avg loss -2.868669 avg loss no lamb -2.868669 time 2020-06-27 20:04:44.750016
last batch sz 10
Pre: time 2020-06-27 20:04:57.868198: 
 	std: 0.0029014575
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9786, 0.9787, 0.9719, 0.9784, 0.9736]
	train_accs: [0.9812833, 0.98041666, 0.97473335, 0.9812, 0.9759333]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97624
	best: 0.9786

Starting e_i: 1806
Model ind 665 epoch 1806 batch: 0 avg loss -2.965956 avg loss no lamb -2.965956 time 2020-06-27 20:04:59.126801
Model ind 665 epoch 1806 batch: 100 avg loss -2.906453 avg loss no lamb -2.906453 time 2020-06-27 20:05:08.931247
Model ind 665 epoch 1806 batch: 200 avg loss -2.873927 avg loss no lamb -2.873927 time 2020-06-27 20:05:18.702130
Model ind 665 epoch 1806 batch: 300 avg loss -2.872020 avg loss no lamb -2.872020 time 2020-06-27 20:05:28.661153
Model ind 665 epoch 1806 batch: 400 avg loss -2.801096 avg loss no lamb -2.801096 time 2020-06-27 20:05:38.536913
Model ind 665 epoch 1806 batch: 500 avg loss -2.840380 avg loss no lamb -2.840380 time 2020-06-27 20:05:48.357799
Model ind 665 epoch 1806 batch: 600 avg loss -2.946014 avg loss no lamb -2.946014 time 2020-06-27 20:05:58.139138
Model ind 665 epoch 1806 batch: 700 avg loss -2.787041 avg loss no lamb -2.787041 time 2020-06-27 20:06:07.926134
Model ind 665 epoch 1806 batch: 800 avg loss -2.893307 avg loss no lamb -2.893307 time 2020-06-27 20:06:17.675353
last batch sz 10
Pre: time 2020-06-27 20:06:30.739487: 
 	std: 0.0030472146
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9795, 0.9794, 0.9725, 0.9794, 0.9741]
	train_accs: [0.98085, 0.98048335, 0.97533333, 0.9809333, 0.97555]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97698003
	best: 0.9794

Starting e_i: 1807
Model ind 665 epoch 1807 batch: 0 avg loss -2.941630 avg loss no lamb -2.941630 time 2020-06-27 20:06:32.010548
Model ind 665 epoch 1807 batch: 100 avg loss -2.878561 avg loss no lamb -2.878561 time 2020-06-27 20:06:42.000625
Model ind 665 epoch 1807 batch: 200 avg loss -2.909761 avg loss no lamb -2.909761 time 2020-06-27 20:06:51.850775
Model ind 665 epoch 1807 batch: 300 avg loss -2.933021 avg loss no lamb -2.933021 time 2020-06-27 20:07:01.687977
Model ind 665 epoch 1807 batch: 400 avg loss -2.861858 avg loss no lamb -2.861858 time 2020-06-27 20:07:11.604221
Model ind 665 epoch 1807 batch: 500 avg loss -2.856474 avg loss no lamb -2.856474 time 2020-06-27 20:07:21.418641
Model ind 665 epoch 1807 batch: 600 avg loss -2.861096 avg loss no lamb -2.861096 time 2020-06-27 20:07:31.353828
Model ind 665 epoch 1807 batch: 700 avg loss -2.801907 avg loss no lamb -2.801907 time 2020-06-27 20:07:41.170489
Model ind 665 epoch 1807 batch: 800 avg loss -2.896915 avg loss no lamb -2.896915 time 2020-06-27 20:07:50.922273
last batch sz 10
Pre: time 2020-06-27 20:08:03.923871: 
 	std: 0.0037860735
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9795, 0.9714, 0.9807, 0.9739]
	train_accs: [0.98113334, 0.9806333, 0.97425, 0.9813167, 0.9759667]
	best_train_sub_head: 3
	worst: 0.9714
	avg: 0.97716
	best: 0.9807

Starting e_i: 1808
Model ind 665 epoch 1808 batch: 0 avg loss -2.958398 avg loss no lamb -2.958398 time 2020-06-27 20:08:05.440979
Model ind 665 epoch 1808 batch: 100 avg loss -2.848663 avg loss no lamb -2.848663 time 2020-06-27 20:08:15.331598
Model ind 665 epoch 1808 batch: 200 avg loss -2.916182 avg loss no lamb -2.916182 time 2020-06-27 20:08:25.295406
Model ind 665 epoch 1808 batch: 300 avg loss -2.888131 avg loss no lamb -2.888131 time 2020-06-27 20:08:35.253679
Model ind 665 epoch 1808 batch: 400 avg loss -2.810374 avg loss no lamb -2.810374 time 2020-06-27 20:08:45.238604
Model ind 665 epoch 1808 batch: 500 avg loss -2.846867 avg loss no lamb -2.846867 time 2020-06-27 20:08:55.010787
Model ind 665 epoch 1808 batch: 600 avg loss -2.886302 avg loss no lamb -2.886302 time 2020-06-27 20:09:04.998830
Model ind 665 epoch 1808 batch: 700 avg loss -2.787663 avg loss no lamb -2.787663 time 2020-06-27 20:09:14.855171
Model ind 665 epoch 1808 batch: 800 avg loss -2.943893 avg loss no lamb -2.943893 time 2020-06-27 20:09:24.786939
last batch sz 10
Pre: time 2020-06-27 20:09:37.808344: 
 	std: 0.003095417
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9795, 0.9734, 0.9804, 0.9739]
	train_accs: [0.9813333, 0.9806, 0.9754, 0.9813667, 0.97606665]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97742
	best: 0.9804

Starting e_i: 1809
Model ind 665 epoch 1809 batch: 0 avg loss -2.994035 avg loss no lamb -2.994035 time 2020-06-27 20:09:39.070884
Model ind 665 epoch 1809 batch: 100 avg loss -2.903155 avg loss no lamb -2.903155 time 2020-06-27 20:09:49.008010
Model ind 665 epoch 1809 batch: 200 avg loss -2.885701 avg loss no lamb -2.885701 time 2020-06-27 20:09:58.887345
Model ind 665 epoch 1809 batch: 300 avg loss -2.901561 avg loss no lamb -2.901561 time 2020-06-27 20:10:08.661468
Model ind 665 epoch 1809 batch: 400 avg loss -2.839538 avg loss no lamb -2.839538 time 2020-06-27 20:10:18.500855
Model ind 665 epoch 1809 batch: 500 avg loss -2.803187 avg loss no lamb -2.803187 time 2020-06-27 20:10:28.276548
Model ind 665 epoch 1809 batch: 600 avg loss -2.930635 avg loss no lamb -2.930635 time 2020-06-27 20:10:38.050564
Model ind 665 epoch 1809 batch: 700 avg loss -2.789007 avg loss no lamb -2.789007 time 2020-06-27 20:10:48.031578
Model ind 665 epoch 1809 batch: 800 avg loss -2.877112 avg loss no lamb -2.877112 time 2020-06-27 20:10:57.923702
last batch sz 10
Pre: time 2020-06-27 20:11:11.108079: 
 	std: 0.0031908618
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9777, 0.9716, 0.9787, 0.9732]
	train_accs: [0.98135, 0.98011667, 0.97525, 0.98123336, 0.97606665]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.97617996
	best: 0.9797

Starting e_i: 1810
Model ind 665 epoch 1810 batch: 0 avg loss -2.941643 avg loss no lamb -2.941643 time 2020-06-27 20:11:12.389912
Model ind 665 epoch 1810 batch: 100 avg loss -2.912369 avg loss no lamb -2.912369 time 2020-06-27 20:11:22.253475
Model ind 665 epoch 1810 batch: 200 avg loss -2.873668 avg loss no lamb -2.873668 time 2020-06-27 20:11:32.176737
Model ind 665 epoch 1810 batch: 300 avg loss -2.912171 avg loss no lamb -2.912171 time 2020-06-27 20:11:42.008305
Model ind 665 epoch 1810 batch: 400 avg loss -2.892050 avg loss no lamb -2.892050 time 2020-06-27 20:11:51.979555
Model ind 665 epoch 1810 batch: 500 avg loss -2.865106 avg loss no lamb -2.865106 time 2020-06-27 20:12:01.898730
Model ind 665 epoch 1810 batch: 600 avg loss -2.881638 avg loss no lamb -2.881638 time 2020-06-27 20:12:11.877861
Model ind 665 epoch 1810 batch: 700 avg loss -2.823653 avg loss no lamb -2.823653 time 2020-06-27 20:12:21.816205
Model ind 665 epoch 1810 batch: 800 avg loss -2.892214 avg loss no lamb -2.892214 time 2020-06-27 20:12:31.721370
last batch sz 10
Pre: time 2020-06-27 20:12:45.015487: 
 	std: 0.0028373147
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9766, 0.9771, 0.9707, 0.9766, 0.9713]
	train_accs: [0.97971666, 0.97963333, 0.97401667, 0.97961664, 0.9740667]
	best_train_sub_head: 0
	worst: 0.9707
	avg: 0.97446
	best: 0.9766

Starting e_i: 1811
Model ind 665 epoch 1811 batch: 0 avg loss -2.962734 avg loss no lamb -2.962734 time 2020-06-27 20:12:47.438575
Model ind 665 epoch 1811 batch: 100 avg loss -2.896667 avg loss no lamb -2.896667 time 2020-06-27 20:12:57.162967
Model ind 665 epoch 1811 batch: 200 avg loss -2.933653 avg loss no lamb -2.933653 time 2020-06-27 20:13:06.992426
Model ind 665 epoch 1811 batch: 300 avg loss -2.914953 avg loss no lamb -2.914953 time 2020-06-27 20:13:17.012222
Model ind 665 epoch 1811 batch: 400 avg loss -2.773511 avg loss no lamb -2.773511 time 2020-06-27 20:13:26.835724
Model ind 665 epoch 1811 batch: 500 avg loss -2.867634 avg loss no lamb -2.867634 time 2020-06-27 20:13:36.512248
Model ind 665 epoch 1811 batch: 600 avg loss -2.916691 avg loss no lamb -2.916691 time 2020-06-27 20:13:46.230975
Model ind 665 epoch 1811 batch: 700 avg loss -2.786936 avg loss no lamb -2.786936 time 2020-06-27 20:13:56.188320
Model ind 665 epoch 1811 batch: 800 avg loss -2.948648 avg loss no lamb -2.948648 time 2020-06-27 20:14:06.049425
last batch sz 10
Pre: time 2020-06-27 20:14:19.038145: 
 	std: 0.0035130556
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.979, 0.9781, 0.9713, 0.9792, 0.972]
	train_accs: [0.9813833, 0.9805667, 0.97456664, 0.9812833, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9713
	avg: 0.97592
	best: 0.979

Starting e_i: 1812
Model ind 665 epoch 1812 batch: 0 avg loss -2.952228 avg loss no lamb -2.952228 time 2020-06-27 20:14:20.274689
Model ind 665 epoch 1812 batch: 100 avg loss -2.958154 avg loss no lamb -2.958154 time 2020-06-27 20:14:30.005763
Model ind 665 epoch 1812 batch: 200 avg loss -2.902750 avg loss no lamb -2.902750 time 2020-06-27 20:14:39.952069
Model ind 665 epoch 1812 batch: 300 avg loss -2.885152 avg loss no lamb -2.885152 time 2020-06-27 20:14:49.751234
Model ind 665 epoch 1812 batch: 400 avg loss -2.804549 avg loss no lamb -2.804549 time 2020-06-27 20:14:59.823889
Model ind 665 epoch 1812 batch: 500 avg loss -2.877901 avg loss no lamb -2.877901 time 2020-06-27 20:15:09.755520
Model ind 665 epoch 1812 batch: 600 avg loss -2.937825 avg loss no lamb -2.937825 time 2020-06-27 20:15:19.551398
Model ind 665 epoch 1812 batch: 700 avg loss -2.838980 avg loss no lamb -2.838980 time 2020-06-27 20:15:29.327539
Model ind 665 epoch 1812 batch: 800 avg loss -2.859699 avg loss no lamb -2.859699 time 2020-06-27 20:15:39.077723
last batch sz 10
Pre: time 2020-06-27 20:15:52.171312: 
 	std: 0.002787552
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9777, 0.9722, 0.978, 0.9727]
	train_accs: [0.98045, 0.97941667, 0.9746, 0.98046666, 0.9752667]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97584
	best: 0.978

Starting e_i: 1813
Model ind 665 epoch 1813 batch: 0 avg loss -2.985873 avg loss no lamb -2.985873 time 2020-06-27 20:15:53.699984
Model ind 665 epoch 1813 batch: 100 avg loss -2.925736 avg loss no lamb -2.925736 time 2020-06-27 20:16:03.678563
Model ind 665 epoch 1813 batch: 200 avg loss -2.924309 avg loss no lamb -2.924309 time 2020-06-27 20:16:13.580082
Model ind 665 epoch 1813 batch: 300 avg loss -2.853786 avg loss no lamb -2.853786 time 2020-06-27 20:16:23.345424
Model ind 665 epoch 1813 batch: 400 avg loss -2.805904 avg loss no lamb -2.805904 time 2020-06-27 20:16:33.248104
Model ind 665 epoch 1813 batch: 500 avg loss -2.897966 avg loss no lamb -2.897966 time 2020-06-27 20:16:43.174379
Model ind 665 epoch 1813 batch: 600 avg loss -2.930308 avg loss no lamb -2.930308 time 2020-06-27 20:16:53.075438
Model ind 665 epoch 1813 batch: 700 avg loss -2.828025 avg loss no lamb -2.828025 time 2020-06-27 20:17:02.980124
Model ind 665 epoch 1813 batch: 800 avg loss -2.883034 avg loss no lamb -2.883034 time 2020-06-27 20:17:12.895709
last batch sz 10
Pre: time 2020-06-27 20:17:25.754940: 
 	std: 0.003480582
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9784, 0.9712, 0.9793, 0.9729]
	train_accs: [0.9810167, 0.98043334, 0.97433335, 0.98143333, 0.97566664]
	best_train_sub_head: 3
	worst: 0.9712
	avg: 0.97624
	best: 0.9793

Starting e_i: 1814
Model ind 665 epoch 1814 batch: 0 avg loss -2.950743 avg loss no lamb -2.950743 time 2020-06-27 20:17:27.044201
Model ind 665 epoch 1814 batch: 100 avg loss -2.936667 avg loss no lamb -2.936667 time 2020-06-27 20:17:36.825627
Model ind 665 epoch 1814 batch: 200 avg loss -2.876374 avg loss no lamb -2.876374 time 2020-06-27 20:17:46.597782
Model ind 665 epoch 1814 batch: 300 avg loss -2.887792 avg loss no lamb -2.887792 time 2020-06-27 20:17:56.462306
Model ind 665 epoch 1814 batch: 400 avg loss -2.848082 avg loss no lamb -2.848082 time 2020-06-27 20:18:06.428902
Model ind 665 epoch 1814 batch: 500 avg loss -2.874959 avg loss no lamb -2.874959 time 2020-06-27 20:18:16.283829
Model ind 665 epoch 1814 batch: 600 avg loss -2.889838 avg loss no lamb -2.889838 time 2020-06-27 20:18:26.104674
Model ind 665 epoch 1814 batch: 700 avg loss -2.813174 avg loss no lamb -2.813174 time 2020-06-27 20:18:36.080043
Model ind 665 epoch 1814 batch: 800 avg loss -2.906728 avg loss no lamb -2.906728 time 2020-06-27 20:18:46.005301
last batch sz 10
Pre: time 2020-06-27 20:18:58.913936: 
 	std: 0.0033310694
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9783, 0.9713, 0.9795, 0.9736]
	train_accs: [0.98106664, 0.9798, 0.9744667, 0.9812833, 0.97575]
	best_train_sub_head: 3
	worst: 0.9713
	avg: 0.9764
	best: 0.9795

Starting e_i: 1815
Model ind 665 epoch 1815 batch: 0 avg loss -2.925429 avg loss no lamb -2.925429 time 2020-06-27 20:19:00.243479
Model ind 665 epoch 1815 batch: 100 avg loss -2.905452 avg loss no lamb -2.905452 time 2020-06-27 20:19:10.079942
Model ind 665 epoch 1815 batch: 200 avg loss -2.863582 avg loss no lamb -2.863582 time 2020-06-27 20:19:19.819736
Model ind 665 epoch 1815 batch: 300 avg loss -2.863578 avg loss no lamb -2.863578 time 2020-06-27 20:19:29.716582
Model ind 665 epoch 1815 batch: 400 avg loss -2.784364 avg loss no lamb -2.784364 time 2020-06-27 20:19:39.556206
Model ind 665 epoch 1815 batch: 500 avg loss -2.890314 avg loss no lamb -2.890314 time 2020-06-27 20:19:49.409077
Model ind 665 epoch 1815 batch: 600 avg loss -2.901074 avg loss no lamb -2.901074 time 2020-06-27 20:19:59.066428
Model ind 665 epoch 1815 batch: 700 avg loss -2.810830 avg loss no lamb -2.810830 time 2020-06-27 20:20:08.817503
Model ind 665 epoch 1815 batch: 800 avg loss -2.882818 avg loss no lamb -2.882818 time 2020-06-27 20:20:18.733446
last batch sz 10
Pre: time 2020-06-27 20:20:31.865151: 
 	std: 0.0034102108
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.978, 0.9762, 0.9695, 0.9777, 0.9717]
	train_accs: [0.98038334, 0.9787667, 0.97275, 0.98031664, 0.9740833]
	best_train_sub_head: 0
	worst: 0.9695
	avg: 0.97462004
	best: 0.978

Starting e_i: 1816
Model ind 665 epoch 1816 batch: 0 avg loss -2.951127 avg loss no lamb -2.951127 time 2020-06-27 20:20:33.087874
Model ind 665 epoch 1816 batch: 100 avg loss -2.935561 avg loss no lamb -2.935561 time 2020-06-27 20:20:42.890920
Model ind 665 epoch 1816 batch: 200 avg loss -2.904728 avg loss no lamb -2.904728 time 2020-06-27 20:20:52.752269
Model ind 665 epoch 1816 batch: 300 avg loss -2.883670 avg loss no lamb -2.883670 time 2020-06-27 20:21:02.833801
Model ind 665 epoch 1816 batch: 400 avg loss -2.835384 avg loss no lamb -2.835384 time 2020-06-27 20:21:12.667490
Model ind 665 epoch 1816 batch: 500 avg loss -2.881826 avg loss no lamb -2.881826 time 2020-06-27 20:21:22.538364
Model ind 665 epoch 1816 batch: 600 avg loss -2.895472 avg loss no lamb -2.895472 time 2020-06-27 20:21:32.391706
Model ind 665 epoch 1816 batch: 700 avg loss -2.780372 avg loss no lamb -2.780372 time 2020-06-27 20:21:42.323434
Model ind 665 epoch 1816 batch: 800 avg loss -2.863177 avg loss no lamb -2.863177 time 2020-06-27 20:21:52.377362
last batch sz 10
Pre: time 2020-06-27 20:22:05.498028: 
 	std: 0.0026184053
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9775, 0.9773, 0.9715, 0.9777, 0.973]
	train_accs: [0.98048335, 0.97966665, 0.9743, 0.98045, 0.9756333]
	best_train_sub_head: 0
	worst: 0.9715
	avg: 0.9754
	best: 0.9775

Starting e_i: 1817
Model ind 665 epoch 1817 batch: 0 avg loss -2.983291 avg loss no lamb -2.983291 time 2020-06-27 20:22:06.759628
Model ind 665 epoch 1817 batch: 100 avg loss -2.836706 avg loss no lamb -2.836706 time 2020-06-27 20:22:16.383523
Model ind 665 epoch 1817 batch: 200 avg loss -2.894796 avg loss no lamb -2.894796 time 2020-06-27 20:22:26.118548
Model ind 665 epoch 1817 batch: 300 avg loss -2.826241 avg loss no lamb -2.826241 time 2020-06-27 20:22:35.962791
Model ind 665 epoch 1817 batch: 400 avg loss -2.828875 avg loss no lamb -2.828875 time 2020-06-27 20:22:45.828809
Model ind 665 epoch 1817 batch: 500 avg loss -2.878493 avg loss no lamb -2.878493 time 2020-06-27 20:22:55.841089
Model ind 665 epoch 1817 batch: 600 avg loss -2.910319 avg loss no lamb -2.910319 time 2020-06-27 20:23:05.737370
Model ind 665 epoch 1817 batch: 700 avg loss -2.806797 avg loss no lamb -2.806797 time 2020-06-27 20:23:15.606935
Model ind 665 epoch 1817 batch: 800 avg loss -2.953757 avg loss no lamb -2.953757 time 2020-06-27 20:23:25.515472
last batch sz 10
Pre: time 2020-06-27 20:23:38.880506: 
 	std: 0.003114749
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9778, 0.9716, 0.9792, 0.9737]
	train_accs: [0.98073334, 0.9801667, 0.9741333, 0.98088336, 0.97566664]
	best_train_sub_head: 3
	worst: 0.9716
	avg: 0.97631997
	best: 0.9792

Starting e_i: 1818
Model ind 665 epoch 1818 batch: 0 avg loss -2.958386 avg loss no lamb -2.958386 time 2020-06-27 20:23:40.166819
Model ind 665 epoch 1818 batch: 100 avg loss -2.900691 avg loss no lamb -2.900691 time 2020-06-27 20:23:52.689736
Model ind 665 epoch 1818 batch: 200 avg loss -2.857277 avg loss no lamb -2.857277 time 2020-06-27 20:24:06.216673
Model ind 665 epoch 1818 batch: 300 avg loss -2.918517 avg loss no lamb -2.918517 time 2020-06-27 20:24:20.265237
Model ind 665 epoch 1818 batch: 400 avg loss -2.794418 avg loss no lamb -2.794418 time 2020-06-27 20:24:33.478505
Model ind 665 epoch 1818 batch: 500 avg loss -2.862007 avg loss no lamb -2.862007 time 2020-06-27 20:24:44.486792
Model ind 665 epoch 1818 batch: 600 avg loss -2.855425 avg loss no lamb -2.855425 time 2020-06-27 20:24:56.539101
Model ind 665 epoch 1818 batch: 700 avg loss -2.783833 avg loss no lamb -2.783833 time 2020-06-27 20:25:11.152206
Model ind 665 epoch 1818 batch: 800 avg loss -2.890774 avg loss no lamb -2.890774 time 2020-06-27 20:25:21.741270
last batch sz 10
Pre: time 2020-06-27 20:25:34.443711: 
 	std: 0.003471381
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9782, 0.9711, 0.9792, 0.9726]
	train_accs: [0.9808, 0.9805667, 0.97386664, 0.98088336, 0.97466666]
	best_train_sub_head: 3
	worst: 0.9711
	avg: 0.97604
	best: 0.9792

Starting e_i: 1819
Model ind 665 epoch 1819 batch: 0 avg loss -2.938210 avg loss no lamb -2.938210 time 2020-06-27 20:25:35.735311
Model ind 665 epoch 1819 batch: 100 avg loss -2.910264 avg loss no lamb -2.910264 time 2020-06-27 20:25:45.340037
Model ind 665 epoch 1819 batch: 200 avg loss -2.888402 avg loss no lamb -2.888402 time 2020-06-27 20:25:55.107739
Model ind 665 epoch 1819 batch: 300 avg loss -2.882214 avg loss no lamb -2.882214 time 2020-06-27 20:26:04.901478
Model ind 665 epoch 1819 batch: 400 avg loss -2.847865 avg loss no lamb -2.847865 time 2020-06-27 20:26:14.552656
Model ind 665 epoch 1819 batch: 500 avg loss -2.859516 avg loss no lamb -2.859516 time 2020-06-27 20:26:24.187525
Model ind 665 epoch 1819 batch: 600 avg loss -2.911564 avg loss no lamb -2.911564 time 2020-06-27 20:26:33.791586
Model ind 665 epoch 1819 batch: 700 avg loss -2.824849 avg loss no lamb -2.824849 time 2020-06-27 20:26:43.274847
Model ind 665 epoch 1819 batch: 800 avg loss -2.858506 avg loss no lamb -2.858506 time 2020-06-27 20:26:52.853826
last batch sz 10
Pre: time 2020-06-27 20:27:05.730250: 
 	std: 0.0029430618
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9795, 0.9734, 0.9807, 0.975]
	train_accs: [0.9809333, 0.9808667, 0.97573334, 0.98156667, 0.97636664]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97771996
	best: 0.9807

Starting e_i: 1820
Model ind 665 epoch 1820 batch: 0 avg loss -2.957818 avg loss no lamb -2.957818 time 2020-06-27 20:27:07.260372
Model ind 665 epoch 1820 batch: 100 avg loss -2.916525 avg loss no lamb -2.916525 time 2020-06-27 20:27:19.664166
Model ind 665 epoch 1820 batch: 200 avg loss -2.886564 avg loss no lamb -2.886564 time 2020-06-27 20:27:32.856224
Model ind 665 epoch 1820 batch: 300 avg loss -2.849240 avg loss no lamb -2.849240 time 2020-06-27 20:27:45.721573
Model ind 665 epoch 1820 batch: 400 avg loss -2.818067 avg loss no lamb -2.818067 time 2020-06-27 20:27:58.487617
Model ind 665 epoch 1820 batch: 500 avg loss -2.843879 avg loss no lamb -2.843879 time 2020-06-27 20:28:11.339365
Model ind 665 epoch 1820 batch: 600 avg loss -2.925467 avg loss no lamb -2.925467 time 2020-06-27 20:28:24.331883
Model ind 665 epoch 1820 batch: 700 avg loss -2.848620 avg loss no lamb -2.848620 time 2020-06-27 20:28:37.972041
Model ind 665 epoch 1820 batch: 800 avg loss -2.894582 avg loss no lamb -2.894582 time 2020-06-27 20:28:48.504393
last batch sz 10
Pre: time 2020-06-27 20:29:01.293860: 
 	std: 0.0030715417
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9809, 0.9784, 0.9734, 0.9805, 0.9745]
	train_accs: [0.9816667, 0.98071665, 0.97616667, 0.9817167, 0.97681665]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97754
	best: 0.9805

Starting e_i: 1821
Model ind 665 epoch 1821 batch: 0 avg loss -2.940042 avg loss no lamb -2.940042 time 2020-06-27 20:29:03.740255
Model ind 665 epoch 1821 batch: 100 avg loss -2.910313 avg loss no lamb -2.910313 time 2020-06-27 20:29:13.282850
Model ind 665 epoch 1821 batch: 200 avg loss -2.898264 avg loss no lamb -2.898264 time 2020-06-27 20:29:22.794044
Model ind 665 epoch 1821 batch: 300 avg loss -2.869049 avg loss no lamb -2.869049 time 2020-06-27 20:29:32.308294
Model ind 665 epoch 1821 batch: 400 avg loss -2.811214 avg loss no lamb -2.811214 time 2020-06-27 20:29:41.806550
Model ind 665 epoch 1821 batch: 500 avg loss -2.806204 avg loss no lamb -2.806204 time 2020-06-27 20:29:51.355524
Model ind 665 epoch 1821 batch: 600 avg loss -2.917698 avg loss no lamb -2.917698 time 2020-06-27 20:30:00.936876
Model ind 665 epoch 1821 batch: 700 avg loss -2.753125 avg loss no lamb -2.753125 time 2020-06-27 20:30:10.411161
Model ind 665 epoch 1821 batch: 800 avg loss -2.837927 avg loss no lamb -2.837927 time 2020-06-27 20:30:19.900496
last batch sz 10
Pre: time 2020-06-27 20:30:32.664421: 
 	std: 0.002972815
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9801, 0.9737, 0.9802, 0.9744]
	train_accs: [0.98108333, 0.9806167, 0.97541666, 0.98115, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97768
	best: 0.9802

Starting e_i: 1822
Model ind 665 epoch 1822 batch: 0 avg loss -2.983214 avg loss no lamb -2.983214 time 2020-06-27 20:30:33.948441
Model ind 665 epoch 1822 batch: 100 avg loss -2.937946 avg loss no lamb -2.937946 time 2020-06-27 20:30:43.499847
Model ind 665 epoch 1822 batch: 200 avg loss -2.951613 avg loss no lamb -2.951613 time 2020-06-27 20:30:53.043084
Model ind 665 epoch 1822 batch: 300 avg loss -2.871342 avg loss no lamb -2.871342 time 2020-06-27 20:31:02.659070
Model ind 665 epoch 1822 batch: 400 avg loss -2.805795 avg loss no lamb -2.805795 time 2020-06-27 20:31:12.301369
Model ind 665 epoch 1822 batch: 500 avg loss -2.881006 avg loss no lamb -2.881006 time 2020-06-27 20:31:21.976473
Model ind 665 epoch 1822 batch: 600 avg loss -2.933787 avg loss no lamb -2.933787 time 2020-06-27 20:31:31.617187
Model ind 665 epoch 1822 batch: 700 avg loss -2.811090 avg loss no lamb -2.811090 time 2020-06-27 20:31:41.164922
Model ind 665 epoch 1822 batch: 800 avg loss -2.882773 avg loss no lamb -2.882773 time 2020-06-27 20:31:50.790435
last batch sz 10
Pre: time 2020-06-27 20:32:03.644286: 
 	std: 0.002744455
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.98, 0.9741, 0.9801, 0.9748]
	train_accs: [0.98106664, 0.98073334, 0.97581667, 0.98121667, 0.9762]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.9778
	best: 0.9801

Starting e_i: 1823
Model ind 665 epoch 1823 batch: 0 avg loss -2.977772 avg loss no lamb -2.977772 time 2020-06-27 20:32:04.904673
Model ind 665 epoch 1823 batch: 100 avg loss -2.956190 avg loss no lamb -2.956190 time 2020-06-27 20:32:14.395380
Model ind 665 epoch 1823 batch: 200 avg loss -2.833054 avg loss no lamb -2.833054 time 2020-06-27 20:32:24.005910
Model ind 665 epoch 1823 batch: 300 avg loss -2.881565 avg loss no lamb -2.881565 time 2020-06-27 20:32:33.415656
Model ind 665 epoch 1823 batch: 400 avg loss -2.797049 avg loss no lamb -2.797049 time 2020-06-27 20:32:42.823819
Model ind 665 epoch 1823 batch: 500 avg loss -2.783348 avg loss no lamb -2.783348 time 2020-06-27 20:32:52.305196
Model ind 665 epoch 1823 batch: 600 avg loss -2.862062 avg loss no lamb -2.862062 time 2020-06-27 20:33:01.892613
Model ind 665 epoch 1823 batch: 700 avg loss -2.865008 avg loss no lamb -2.865008 time 2020-06-27 20:33:11.483003
Model ind 665 epoch 1823 batch: 800 avg loss -2.875321 avg loss no lamb -2.875321 time 2020-06-27 20:33:21.053488
last batch sz 10
Pre: time 2020-06-27 20:33:33.612105: 
 	std: 0.0027885449
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9803, 0.9743, 0.9805, 0.9751]
	train_accs: [0.9815, 0.9809667, 0.97611666, 0.9814, 0.9771]
	best_train_sub_head: 0
	worst: 0.9743
	avg: 0.9781
	best: 0.9803

Starting e_i: 1824
Model ind 665 epoch 1824 batch: 0 avg loss -3.001039 avg loss no lamb -3.001039 time 2020-06-27 20:33:34.825859
Model ind 665 epoch 1824 batch: 100 avg loss -2.963061 avg loss no lamb -2.963061 time 2020-06-27 20:33:44.325715
Model ind 665 epoch 1824 batch: 200 avg loss -2.913297 avg loss no lamb -2.913297 time 2020-06-27 20:33:56.786479
Model ind 665 epoch 1824 batch: 300 avg loss -2.876229 avg loss no lamb -2.876229 time 2020-06-27 20:34:07.052216
Model ind 665 epoch 1824 batch: 400 avg loss -2.835032 avg loss no lamb -2.835032 time 2020-06-27 20:34:16.791572
Model ind 665 epoch 1824 batch: 500 avg loss -2.926373 avg loss no lamb -2.926373 time 2020-06-27 20:34:26.569853
Model ind 665 epoch 1824 batch: 600 avg loss -2.899422 avg loss no lamb -2.899422 time 2020-06-27 20:34:36.263389
Model ind 665 epoch 1824 batch: 700 avg loss -2.861482 avg loss no lamb -2.861482 time 2020-06-27 20:34:46.173002
Model ind 665 epoch 1824 batch: 800 avg loss -2.866164 avg loss no lamb -2.866164 time 2020-06-27 20:34:55.915576
last batch sz 10
Pre: time 2020-06-27 20:35:08.899855: 
 	std: 0.0036246886
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9803, 0.9796, 0.9718, 0.9803, 0.9738]
	train_accs: [0.98111665, 0.9802167, 0.97485, 0.98108333, 0.9762333]
	best_train_sub_head: 0
	worst: 0.9718
	avg: 0.97716
	best: 0.9803

Starting e_i: 1825
Model ind 665 epoch 1825 batch: 0 avg loss -2.933875 avg loss no lamb -2.933875 time 2020-06-27 20:35:10.417854
Model ind 665 epoch 1825 batch: 100 avg loss -2.921113 avg loss no lamb -2.921113 time 2020-06-27 20:35:20.245914
Model ind 665 epoch 1825 batch: 200 avg loss -2.923777 avg loss no lamb -2.923777 time 2020-06-27 20:35:30.036161
Model ind 665 epoch 1825 batch: 300 avg loss -2.884072 avg loss no lamb -2.884072 time 2020-06-27 20:35:40.046143
Model ind 665 epoch 1825 batch: 400 avg loss -2.842577 avg loss no lamb -2.842577 time 2020-06-27 20:35:49.897697
Model ind 665 epoch 1825 batch: 500 avg loss -2.868963 avg loss no lamb -2.868963 time 2020-06-27 20:35:59.707865
Model ind 665 epoch 1825 batch: 600 avg loss -2.913770 avg loss no lamb -2.913770 time 2020-06-27 20:36:09.647979
Model ind 665 epoch 1825 batch: 700 avg loss -2.831908 avg loss no lamb -2.831908 time 2020-06-27 20:36:19.636238
Model ind 665 epoch 1825 batch: 800 avg loss -2.904725 avg loss no lamb -2.904725 time 2020-06-27 20:36:29.524223
last batch sz 10
Pre: time 2020-06-27 20:36:42.460422: 
 	std: 0.0027712882
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.9785, 0.9727, 0.9781, 0.9734]
	train_accs: [0.9814, 0.9806333, 0.9755, 0.98113334, 0.97655]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.9764
	best: 0.9793

Starting e_i: 1826
Model ind 665 epoch 1826 batch: 0 avg loss -2.991151 avg loss no lamb -2.991151 time 2020-06-27 20:36:43.746870
Model ind 665 epoch 1826 batch: 100 avg loss -2.938266 avg loss no lamb -2.938266 time 2020-06-27 20:36:53.522587
Model ind 665 epoch 1826 batch: 200 avg loss -2.899632 avg loss no lamb -2.899632 time 2020-06-27 20:37:03.375202
Model ind 665 epoch 1826 batch: 300 avg loss -2.867443 avg loss no lamb -2.867443 time 2020-06-27 20:37:13.389339
Model ind 665 epoch 1826 batch: 400 avg loss -2.859618 avg loss no lamb -2.859618 time 2020-06-27 20:37:23.211699
Model ind 665 epoch 1826 batch: 500 avg loss -2.865036 avg loss no lamb -2.865036 time 2020-06-27 20:37:32.953428
Model ind 665 epoch 1826 batch: 600 avg loss -2.911766 avg loss no lamb -2.911766 time 2020-06-27 20:37:42.803224
Model ind 665 epoch 1826 batch: 700 avg loss -2.886522 avg loss no lamb -2.886522 time 2020-06-27 20:37:52.651735
Model ind 665 epoch 1826 batch: 800 avg loss -2.834336 avg loss no lamb -2.834336 time 2020-06-27 20:38:02.407393
last batch sz 10
Pre: time 2020-06-27 20:38:15.493809: 
 	std: 0.003232715
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9797, 0.9723, 0.9798, 0.9742]
	train_accs: [0.98095, 0.9801, 0.9745, 0.9809167, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97714007
	best: 0.9797

Starting e_i: 1827
Model ind 665 epoch 1827 batch: 0 avg loss -2.932025 avg loss no lamb -2.932025 time 2020-06-27 20:38:16.797171
Model ind 665 epoch 1827 batch: 100 avg loss -2.879734 avg loss no lamb -2.879734 time 2020-06-27 20:38:26.693540
Model ind 665 epoch 1827 batch: 200 avg loss -2.852567 avg loss no lamb -2.852567 time 2020-06-27 20:38:36.745928
Model ind 665 epoch 1827 batch: 300 avg loss -2.886147 avg loss no lamb -2.886147 time 2020-06-27 20:38:46.706100
Model ind 665 epoch 1827 batch: 400 avg loss -2.815123 avg loss no lamb -2.815123 time 2020-06-27 20:38:56.590486
Model ind 665 epoch 1827 batch: 500 avg loss -2.854426 avg loss no lamb -2.854426 time 2020-06-27 20:39:06.557237
Model ind 665 epoch 1827 batch: 600 avg loss -2.860413 avg loss no lamb -2.860413 time 2020-06-27 20:39:16.424368
Model ind 665 epoch 1827 batch: 700 avg loss -2.808359 avg loss no lamb -2.808359 time 2020-06-27 20:39:26.340483
Model ind 665 epoch 1827 batch: 800 avg loss -2.863139 avg loss no lamb -2.863139 time 2020-06-27 20:39:36.401561
last batch sz 10
Pre: time 2020-06-27 20:39:49.889799: 
 	std: 0.002857971
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9813, 0.9805, 0.9745, 0.9809, 0.9758]
	train_accs: [0.9817, 0.9812, 0.97606665, 0.9817333, 0.97683334]
	best_train_sub_head: 3
	worst: 0.9745
	avg: 0.9786
	best: 0.9809

Starting e_i: 1828
Model ind 665 epoch 1828 batch: 0 avg loss -2.956803 avg loss no lamb -2.956803 time 2020-06-27 20:39:51.181331
Model ind 665 epoch 1828 batch: 100 avg loss -2.921054 avg loss no lamb -2.921054 time 2020-06-27 20:40:01.103733
Model ind 665 epoch 1828 batch: 200 avg loss -2.910359 avg loss no lamb -2.910359 time 2020-06-27 20:40:10.717450
Model ind 665 epoch 1828 batch: 300 avg loss -2.877087 avg loss no lamb -2.877087 time 2020-06-27 20:40:20.554565
Model ind 665 epoch 1828 batch: 400 avg loss -2.799324 avg loss no lamb -2.799324 time 2020-06-27 20:40:30.423505
Model ind 665 epoch 1828 batch: 500 avg loss -2.877351 avg loss no lamb -2.877351 time 2020-06-27 20:40:40.447300
Model ind 665 epoch 1828 batch: 600 avg loss -2.896610 avg loss no lamb -2.896610 time 2020-06-27 20:40:50.389994
Model ind 665 epoch 1828 batch: 700 avg loss -2.807113 avg loss no lamb -2.807113 time 2020-06-27 20:41:00.249598
Model ind 665 epoch 1828 batch: 800 avg loss -2.923322 avg loss no lamb -2.923322 time 2020-06-27 20:41:09.900468
last batch sz 10
Pre: time 2020-06-27 20:41:22.668969: 
 	std: 0.0032700999
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.979, 0.9733, 0.9805, 0.9738]
	train_accs: [0.98176664, 0.98075, 0.9752333, 0.9816667, 0.97585]
	best_train_sub_head: 0
	worst: 0.9733
	avg: 0.97748005
	best: 0.9808

Starting e_i: 1829
Model ind 665 epoch 1829 batch: 0 avg loss -2.936173 avg loss no lamb -2.936173 time 2020-06-27 20:41:23.989460
Model ind 665 epoch 1829 batch: 100 avg loss -2.886813 avg loss no lamb -2.886813 time 2020-06-27 20:41:33.918167
Model ind 665 epoch 1829 batch: 200 avg loss -2.891923 avg loss no lamb -2.891923 time 2020-06-27 20:41:43.736004
Model ind 665 epoch 1829 batch: 300 avg loss -2.868737 avg loss no lamb -2.868737 time 2020-06-27 20:41:53.590496
Model ind 665 epoch 1829 batch: 400 avg loss -2.855632 avg loss no lamb -2.855632 time 2020-06-27 20:42:03.330425
Model ind 665 epoch 1829 batch: 500 avg loss -2.869860 avg loss no lamb -2.869860 time 2020-06-27 20:42:13.377853
Model ind 665 epoch 1829 batch: 600 avg loss -2.933813 avg loss no lamb -2.933813 time 2020-06-27 20:42:23.196896
Model ind 665 epoch 1829 batch: 700 avg loss -2.775379 avg loss no lamb -2.775379 time 2020-06-27 20:42:33.117119
Model ind 665 epoch 1829 batch: 800 avg loss -2.840208 avg loss no lamb -2.840208 time 2020-06-27 20:42:42.989988
last batch sz 10
Pre: time 2020-06-27 20:42:56.264108: 
 	std: 0.0031619116
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.9793, 0.9726, 0.9791, 0.9729]
	train_accs: [0.9817, 0.98141664, 0.97503334, 0.9816833, 0.9758833]
	best_train_sub_head: 0
	worst: 0.9726
	avg: 0.97662
	best: 0.9792

Starting e_i: 1830
Model ind 665 epoch 1830 batch: 0 avg loss -2.969877 avg loss no lamb -2.969877 time 2020-06-27 20:42:57.553099
Model ind 665 epoch 1830 batch: 100 avg loss -2.867033 avg loss no lamb -2.867033 time 2020-06-27 20:43:07.440930
Model ind 665 epoch 1830 batch: 200 avg loss -2.932445 avg loss no lamb -2.932445 time 2020-06-27 20:43:17.204760
Model ind 665 epoch 1830 batch: 300 avg loss -2.800270 avg loss no lamb -2.800270 time 2020-06-27 20:43:27.022263
Model ind 665 epoch 1830 batch: 400 avg loss -2.827029 avg loss no lamb -2.827029 time 2020-06-27 20:43:36.847943
Model ind 665 epoch 1830 batch: 500 avg loss -2.842812 avg loss no lamb -2.842812 time 2020-06-27 20:43:46.904398
Model ind 665 epoch 1830 batch: 600 avg loss -2.822454 avg loss no lamb -2.822454 time 2020-06-27 20:43:56.987510
Model ind 665 epoch 1830 batch: 700 avg loss -2.807073 avg loss no lamb -2.807073 time 2020-06-27 20:44:06.887309
Model ind 665 epoch 1830 batch: 800 avg loss -2.891499 avg loss no lamb -2.891499 time 2020-06-27 20:44:16.696872
last batch sz 10
Pre: time 2020-06-27 20:44:29.631209: 
 	std: 0.0030903716
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9787, 0.9722, 0.9795, 0.9739]
	train_accs: [0.9816, 0.98081666, 0.97461665, 0.98153335, 0.97603333]
	best_train_sub_head: 0
	worst: 0.9722
	avg: 0.97676
	best: 0.9795

Starting e_i: 1831
Model ind 665 epoch 1831 batch: 0 avg loss -2.954635 avg loss no lamb -2.954635 time 2020-06-27 20:44:32.115965
Model ind 665 epoch 1831 batch: 100 avg loss -2.892532 avg loss no lamb -2.892532 time 2020-06-27 20:44:42.001822
Model ind 665 epoch 1831 batch: 200 avg loss -2.901289 avg loss no lamb -2.901289 time 2020-06-27 20:44:51.704346
Model ind 665 epoch 1831 batch: 300 avg loss -2.865201 avg loss no lamb -2.865201 time 2020-06-27 20:45:01.542192
Model ind 665 epoch 1831 batch: 400 avg loss -2.845889 avg loss no lamb -2.845889 time 2020-06-27 20:45:11.382092
Model ind 665 epoch 1831 batch: 500 avg loss -2.887715 avg loss no lamb -2.887715 time 2020-06-27 20:45:21.111894
Model ind 665 epoch 1831 batch: 600 avg loss -2.884519 avg loss no lamb -2.884519 time 2020-06-27 20:45:30.916821
Model ind 665 epoch 1831 batch: 700 avg loss -2.760613 avg loss no lamb -2.760613 time 2020-06-27 20:45:40.718396
Model ind 665 epoch 1831 batch: 800 avg loss -2.866171 avg loss no lamb -2.866171 time 2020-06-27 20:45:50.519524
last batch sz 10
Pre: time 2020-06-27 20:46:03.445217: 
 	std: 0.0026020056
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.9789, 0.973, 0.979, 0.9744]
	train_accs: [0.981, 0.98001665, 0.9751, 0.9809, 0.97585]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97684
	best: 0.9789

Starting e_i: 1832
Model ind 665 epoch 1832 batch: 0 avg loss -2.992157 avg loss no lamb -2.992157 time 2020-06-27 20:46:05.003730
Model ind 665 epoch 1832 batch: 100 avg loss -2.887439 avg loss no lamb -2.887439 time 2020-06-27 20:46:14.970373
Model ind 665 epoch 1832 batch: 200 avg loss -2.848745 avg loss no lamb -2.848745 time 2020-06-27 20:46:24.761415
Model ind 665 epoch 1832 batch: 300 avg loss -2.891863 avg loss no lamb -2.891863 time 2020-06-27 20:46:34.527050
Model ind 665 epoch 1832 batch: 400 avg loss -2.891839 avg loss no lamb -2.891839 time 2020-06-27 20:46:44.470622
Model ind 665 epoch 1832 batch: 500 avg loss -2.855129 avg loss no lamb -2.855129 time 2020-06-27 20:46:54.281514
Model ind 665 epoch 1832 batch: 600 avg loss -2.900592 avg loss no lamb -2.900592 time 2020-06-27 20:47:04.099027
Model ind 665 epoch 1832 batch: 700 avg loss -2.885725 avg loss no lamb -2.885725 time 2020-06-27 20:47:13.975362
Model ind 665 epoch 1832 batch: 800 avg loss -2.853720 avg loss no lamb -2.853720 time 2020-06-27 20:47:23.836528
last batch sz 10
Pre: time 2020-06-27 20:47:36.942092: 
 	std: 0.0030374865
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9808, 0.9797, 0.9743, 0.9801, 0.9738]
	train_accs: [0.98178333, 0.98116666, 0.97585, 0.9813333, 0.97568333]
	best_train_sub_head: 0
	worst: 0.9738
	avg: 0.97774
	best: 0.9808

Starting e_i: 1833
Model ind 665 epoch 1833 batch: 0 avg loss -2.969677 avg loss no lamb -2.969677 time 2020-06-27 20:47:38.240503
Model ind 665 epoch 1833 batch: 100 avg loss -2.951131 avg loss no lamb -2.951131 time 2020-06-27 20:47:47.991004
Model ind 665 epoch 1833 batch: 200 avg loss -2.892398 avg loss no lamb -2.892398 time 2020-06-27 20:47:57.727419
Model ind 665 epoch 1833 batch: 300 avg loss -2.892951 avg loss no lamb -2.892951 time 2020-06-27 20:48:07.649969
Model ind 665 epoch 1833 batch: 400 avg loss -2.823823 avg loss no lamb -2.823823 time 2020-06-27 20:48:17.302282
Model ind 665 epoch 1833 batch: 500 avg loss -2.851625 avg loss no lamb -2.851625 time 2020-06-27 20:48:27.044442
Model ind 665 epoch 1833 batch: 600 avg loss -2.864295 avg loss no lamb -2.864295 time 2020-06-27 20:48:36.848217
Model ind 665 epoch 1833 batch: 700 avg loss -2.806420 avg loss no lamb -2.806420 time 2020-06-27 20:48:46.722425
Model ind 665 epoch 1833 batch: 800 avg loss -2.867888 avg loss no lamb -2.867888 time 2020-06-27 20:48:56.524114
last batch sz 10
Pre: time 2020-06-27 20:49:09.390170: 
 	std: 0.003490207
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9801, 0.9793, 0.9725, 0.9806, 0.9734]
	train_accs: [0.98113334, 0.98011667, 0.97471666, 0.98135, 0.97573334]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97718
	best: 0.9806

Starting e_i: 1834
Model ind 665 epoch 1834 batch: 0 avg loss -2.954047 avg loss no lamb -2.954047 time 2020-06-27 20:49:10.688831
Model ind 665 epoch 1834 batch: 100 avg loss -2.915199 avg loss no lamb -2.915199 time 2020-06-27 20:49:20.596564
Model ind 665 epoch 1834 batch: 200 avg loss -2.885002 avg loss no lamb -2.885002 time 2020-06-27 20:49:30.576052
Model ind 665 epoch 1834 batch: 300 avg loss -2.874983 avg loss no lamb -2.874983 time 2020-06-27 20:49:40.602193
Model ind 665 epoch 1834 batch: 400 avg loss -2.864936 avg loss no lamb -2.864936 time 2020-06-27 20:49:50.432481
Model ind 665 epoch 1834 batch: 500 avg loss -2.835301 avg loss no lamb -2.835301 time 2020-06-27 20:50:00.293968
Model ind 665 epoch 1834 batch: 600 avg loss -2.904307 avg loss no lamb -2.904307 time 2020-06-27 20:50:09.977606
Model ind 665 epoch 1834 batch: 700 avg loss -2.781789 avg loss no lamb -2.781789 time 2020-06-27 20:50:19.849365
Model ind 665 epoch 1834 batch: 800 avg loss -2.904064 avg loss no lamb -2.904064 time 2020-06-27 20:50:29.927737
last batch sz 10
Pre: time 2020-06-27 20:50:43.244083: 
 	std: 0.003043952
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9803, 0.9788, 0.9736, 0.9798, 0.9734]
	train_accs: [0.98078334, 0.9799333, 0.9752167, 0.9809, 0.9755667]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97718
	best: 0.9798

Starting e_i: 1835
Model ind 665 epoch 1835 batch: 0 avg loss -2.969315 avg loss no lamb -2.969315 time 2020-06-27 20:50:44.499277
Model ind 665 epoch 1835 batch: 100 avg loss -2.937907 avg loss no lamb -2.937907 time 2020-06-27 20:50:54.354967
Model ind 665 epoch 1835 batch: 200 avg loss -2.886633 avg loss no lamb -2.886633 time 2020-06-27 20:51:04.099001
Model ind 665 epoch 1835 batch: 300 avg loss -2.865262 avg loss no lamb -2.865262 time 2020-06-27 20:51:13.789382
Model ind 665 epoch 1835 batch: 400 avg loss -2.826473 avg loss no lamb -2.826473 time 2020-06-27 20:51:23.382945
Model ind 665 epoch 1835 batch: 500 avg loss -2.889292 avg loss no lamb -2.889292 time 2020-06-27 20:51:32.963409
Model ind 665 epoch 1835 batch: 600 avg loss -2.842804 avg loss no lamb -2.842804 time 2020-06-27 20:51:42.479581
Model ind 665 epoch 1835 batch: 700 avg loss -2.791012 avg loss no lamb -2.791012 time 2020-06-27 20:51:51.980187
Model ind 665 epoch 1835 batch: 800 avg loss -2.880410 avg loss no lamb -2.880410 time 2020-06-27 20:52:01.507165
last batch sz 10
Pre: time 2020-06-27 20:52:14.261120: 
 	std: 0.003138531
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.9789, 0.9731, 0.9802, 0.9738]
	train_accs: [0.9815, 0.98036665, 0.9755667, 0.98148334, 0.9759333]
	best_train_sub_head: 0
	worst: 0.9731
	avg: 0.97723997
	best: 0.9802

Starting e_i: 1836
Model ind 665 epoch 1836 batch: 0 avg loss -2.900215 avg loss no lamb -2.900215 time 2020-06-27 20:52:15.539459
Model ind 665 epoch 1836 batch: 100 avg loss -2.951633 avg loss no lamb -2.951633 time 2020-06-27 20:52:25.283377
Model ind 665 epoch 1836 batch: 200 avg loss -2.902097 avg loss no lamb -2.902097 time 2020-06-27 20:52:34.869278
Model ind 665 epoch 1836 batch: 300 avg loss -2.953287 avg loss no lamb -2.953287 time 2020-06-27 20:52:44.427270
Model ind 665 epoch 1836 batch: 400 avg loss -2.798287 avg loss no lamb -2.798287 time 2020-06-27 20:52:53.994407
Model ind 665 epoch 1836 batch: 500 avg loss -2.899110 avg loss no lamb -2.899110 time 2020-06-27 20:53:03.592484
Model ind 665 epoch 1836 batch: 600 avg loss -2.895776 avg loss no lamb -2.895776 time 2020-06-27 20:53:13.213627
Model ind 665 epoch 1836 batch: 700 avg loss -2.807812 avg loss no lamb -2.807812 time 2020-06-27 20:53:22.871615
Model ind 665 epoch 1836 batch: 800 avg loss -2.891839 avg loss no lamb -2.891839 time 2020-06-27 20:53:32.484241
last batch sz 10
Pre: time 2020-06-27 20:53:45.205437: 
 	std: 0.0033398091
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9794, 0.9783, 0.9716, 0.9791, 0.9728]
	train_accs: [0.9809667, 0.97996664, 0.9738333, 0.9809, 0.9744667]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.97624
	best: 0.9794

Starting e_i: 1837
Model ind 665 epoch 1837 batch: 0 avg loss -2.987509 avg loss no lamb -2.987509 time 2020-06-27 20:53:46.642343
Model ind 665 epoch 1837 batch: 100 avg loss -2.912912 avg loss no lamb -2.912912 time 2020-06-27 20:53:56.219222
Model ind 665 epoch 1837 batch: 200 avg loss -2.891078 avg loss no lamb -2.891078 time 2020-06-27 20:54:05.687713
Model ind 665 epoch 1837 batch: 300 avg loss -2.868301 avg loss no lamb -2.868301 time 2020-06-27 20:54:15.162827
Model ind 665 epoch 1837 batch: 400 avg loss -2.782157 avg loss no lamb -2.782157 time 2020-06-27 20:54:24.703627
Model ind 665 epoch 1837 batch: 500 avg loss -2.886776 avg loss no lamb -2.886776 time 2020-06-27 20:54:34.269509
Model ind 665 epoch 1837 batch: 600 avg loss -2.902900 avg loss no lamb -2.902900 time 2020-06-27 20:54:43.727087
Model ind 665 epoch 1837 batch: 700 avg loss -2.792948 avg loss no lamb -2.792948 time 2020-06-27 20:54:53.202562
Model ind 665 epoch 1837 batch: 800 avg loss -2.866384 avg loss no lamb -2.866384 time 2020-06-27 20:55:02.757630
last batch sz 10
Pre: time 2020-06-27 20:55:15.320174: 
 	std: 0.0030796167
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9799, 0.9795, 0.9731, 0.98, 0.974]
	train_accs: [0.98095, 0.9802833, 0.97478336, 0.98116666, 0.9757]
	best_train_sub_head: 3
	worst: 0.9731
	avg: 0.9773
	best: 0.98

Starting e_i: 1838
Model ind 665 epoch 1838 batch: 0 avg loss -2.945931 avg loss no lamb -2.945931 time 2020-06-27 20:55:16.563699
Model ind 665 epoch 1838 batch: 100 avg loss -2.961632 avg loss no lamb -2.961632 time 2020-06-27 20:55:26.272469
Model ind 665 epoch 1838 batch: 200 avg loss -2.885943 avg loss no lamb -2.885943 time 2020-06-27 20:55:36.051927
Model ind 665 epoch 1838 batch: 300 avg loss -2.912524 avg loss no lamb -2.912524 time 2020-06-27 20:55:45.770931
Model ind 665 epoch 1838 batch: 400 avg loss -2.852305 avg loss no lamb -2.852305 time 2020-06-27 20:55:55.563800
Model ind 665 epoch 1838 batch: 500 avg loss -2.882890 avg loss no lamb -2.882890 time 2020-06-27 20:56:05.531213
Model ind 665 epoch 1838 batch: 600 avg loss -2.867769 avg loss no lamb -2.867769 time 2020-06-27 20:56:15.268504
Model ind 665 epoch 1838 batch: 700 avg loss -2.786646 avg loss no lamb -2.786646 time 2020-06-27 20:56:25.003965
Model ind 665 epoch 1838 batch: 800 avg loss -2.863214 avg loss no lamb -2.863214 time 2020-06-27 20:56:34.708919
last batch sz 10
Pre: time 2020-06-27 20:56:47.651543: 
 	std: 0.0026672853
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9812, 0.9801, 0.9747, 0.9805, 0.9758]
	train_accs: [0.98115, 0.9808, 0.97538334, 0.98115, 0.9762667]
	best_train_sub_head: 0
	worst: 0.9747
	avg: 0.97845995
	best: 0.9812

Starting e_i: 1839
Model ind 665 epoch 1839 batch: 0 avg loss -2.962640 avg loss no lamb -2.962640 time 2020-06-27 20:56:48.972759
Model ind 665 epoch 1839 batch: 100 avg loss -2.906823 avg loss no lamb -2.906823 time 2020-06-27 20:56:58.609543
Model ind 665 epoch 1839 batch: 200 avg loss -2.827005 avg loss no lamb -2.827005 time 2020-06-27 20:57:08.360281
Model ind 665 epoch 1839 batch: 300 avg loss -2.862386 avg loss no lamb -2.862386 time 2020-06-27 20:57:18.130615
Model ind 665 epoch 1839 batch: 400 avg loss -2.804170 avg loss no lamb -2.804170 time 2020-06-27 20:57:28.110267
Model ind 665 epoch 1839 batch: 500 avg loss -2.883234 avg loss no lamb -2.883234 time 2020-06-27 20:57:37.800151
Model ind 665 epoch 1839 batch: 600 avg loss -2.932214 avg loss no lamb -2.932214 time 2020-06-27 20:57:47.591084
Model ind 665 epoch 1839 batch: 700 avg loss -2.830074 avg loss no lamb -2.830074 time 2020-06-27 20:57:57.417305
Model ind 665 epoch 1839 batch: 800 avg loss -2.869837 avg loss no lamb -2.869837 time 2020-06-27 20:58:07.196814
last batch sz 10
Pre: time 2020-06-27 20:58:20.298538: 
 	std: 0.0038194265
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.981, 0.973, 0.9817, 0.9741]
	train_accs: [0.98135, 0.98088336, 0.97473335, 0.98158336, 0.97546667]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97819996
	best: 0.9817

Starting e_i: 1840
Model ind 665 epoch 1840 batch: 0 avg loss -2.907352 avg loss no lamb -2.907352 time 2020-06-27 20:58:21.613339
Model ind 665 epoch 1840 batch: 100 avg loss -2.870382 avg loss no lamb -2.870382 time 2020-06-27 20:58:31.543874
Model ind 665 epoch 1840 batch: 200 avg loss -2.837102 avg loss no lamb -2.837102 time 2020-06-27 20:58:41.529201
Model ind 665 epoch 1840 batch: 300 avg loss -2.874510 avg loss no lamb -2.874510 time 2020-06-27 20:58:51.318228
Model ind 665 epoch 1840 batch: 400 avg loss -2.795002 avg loss no lamb -2.795002 time 2020-06-27 20:59:01.267609
Model ind 665 epoch 1840 batch: 500 avg loss -2.856089 avg loss no lamb -2.856089 time 2020-06-27 20:59:11.153122
Model ind 665 epoch 1840 batch: 600 avg loss -2.895824 avg loss no lamb -2.895824 time 2020-06-27 20:59:21.060519
Model ind 665 epoch 1840 batch: 700 avg loss -2.804006 avg loss no lamb -2.804006 time 2020-06-27 20:59:30.899233
Model ind 665 epoch 1840 batch: 800 avg loss -2.826765 avg loss no lamb -2.826765 time 2020-06-27 20:59:40.758034
last batch sz 10
Pre: time 2020-06-27 20:59:53.735744: 
 	std: 0.0026407547
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9779, 0.9782, 0.9725, 0.9784, 0.9731]
	train_accs: [0.9807, 0.9802333, 0.97425, 0.9811, 0.97538334]
	best_train_sub_head: 3
	worst: 0.9725
	avg: 0.97602004
	best: 0.9784

Starting e_i: 1841
Model ind 665 epoch 1841 batch: 0 avg loss -2.971043 avg loss no lamb -2.971043 time 2020-06-27 20:59:56.201036
Model ind 665 epoch 1841 batch: 100 avg loss -2.945150 avg loss no lamb -2.945150 time 2020-06-27 21:00:06.058025
Model ind 665 epoch 1841 batch: 200 avg loss -2.840268 avg loss no lamb -2.840268 time 2020-06-27 21:00:15.813034
Model ind 665 epoch 1841 batch: 300 avg loss -2.928563 avg loss no lamb -2.928563 time 2020-06-27 21:00:26.455174
Model ind 665 epoch 1841 batch: 400 avg loss -2.834186 avg loss no lamb -2.834186 time 2020-06-27 21:00:39.560823
Model ind 665 epoch 1841 batch: 500 avg loss -2.811503 avg loss no lamb -2.811503 time 2020-06-27 21:00:49.906932
Model ind 665 epoch 1841 batch: 600 avg loss -2.877782 avg loss no lamb -2.877782 time 2020-06-27 21:00:59.822412
Model ind 665 epoch 1841 batch: 700 avg loss -2.829948 avg loss no lamb -2.829948 time 2020-06-27 21:01:09.735375
Model ind 665 epoch 1841 batch: 800 avg loss -2.867004 avg loss no lamb -2.867004 time 2020-06-27 21:01:19.487858
last batch sz 10
Pre: time 2020-06-27 21:01:32.552944: 
 	std: 0.0031479588
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9797, 0.98, 0.9732, 0.9804, 0.9741]
	train_accs: [0.9806833, 0.9806167, 0.97505, 0.9809833, 0.9756]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97748005
	best: 0.9804

Starting e_i: 1842
Model ind 665 epoch 1842 batch: 0 avg loss -2.973975 avg loss no lamb -2.973975 time 2020-06-27 21:01:33.910896
Model ind 665 epoch 1842 batch: 100 avg loss -2.959730 avg loss no lamb -2.959730 time 2020-06-27 21:01:43.722486
Model ind 665 epoch 1842 batch: 200 avg loss -2.922550 avg loss no lamb -2.922550 time 2020-06-27 21:01:53.690658
Model ind 665 epoch 1842 batch: 300 avg loss -2.918515 avg loss no lamb -2.918515 time 2020-06-27 21:02:03.650764
Model ind 665 epoch 1842 batch: 400 avg loss -2.854984 avg loss no lamb -2.854984 time 2020-06-27 21:02:13.697599
Model ind 665 epoch 1842 batch: 500 avg loss -2.882887 avg loss no lamb -2.882887 time 2020-06-27 21:02:23.535713
Model ind 665 epoch 1842 batch: 600 avg loss -2.890422 avg loss no lamb -2.890422 time 2020-06-27 21:02:33.353131
Model ind 665 epoch 1842 batch: 700 avg loss -2.885172 avg loss no lamb -2.885172 time 2020-06-27 21:02:43.363424
Model ind 665 epoch 1842 batch: 800 avg loss -2.921991 avg loss no lamb -2.921991 time 2020-06-27 21:02:53.220408
last batch sz 10
Pre: time 2020-06-27 21:03:06.173169: 
 	std: 0.0032933922
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9792, 0.9722, 0.9801, 0.9742]
	train_accs: [0.9805833, 0.98013335, 0.9742, 0.9806833, 0.9756]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97714007
	best: 0.9801

Starting e_i: 1843
Model ind 665 epoch 1843 batch: 0 avg loss -2.979242 avg loss no lamb -2.979242 time 2020-06-27 21:03:07.512884
Model ind 665 epoch 1843 batch: 100 avg loss -2.923709 avg loss no lamb -2.923709 time 2020-06-27 21:03:17.208508
Model ind 665 epoch 1843 batch: 200 avg loss -2.819145 avg loss no lamb -2.819145 time 2020-06-27 21:03:27.117403
Model ind 665 epoch 1843 batch: 300 avg loss -2.910766 avg loss no lamb -2.910766 time 2020-06-27 21:03:37.179944
Model ind 665 epoch 1843 batch: 400 avg loss -2.805136 avg loss no lamb -2.805136 time 2020-06-27 21:03:47.018361
Model ind 665 epoch 1843 batch: 500 avg loss -2.894646 avg loss no lamb -2.894646 time 2020-06-27 21:03:56.884599
Model ind 665 epoch 1843 batch: 600 avg loss -2.967869 avg loss no lamb -2.967869 time 2020-06-27 21:04:06.668642
Model ind 665 epoch 1843 batch: 700 avg loss -2.810583 avg loss no lamb -2.810583 time 2020-06-27 21:04:16.521178
Model ind 665 epoch 1843 batch: 800 avg loss -2.877110 avg loss no lamb -2.877110 time 2020-06-27 21:04:26.361339
last batch sz 10
Pre: time 2020-06-27 21:04:39.263814: 
 	std: 0.0033755621
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9806, 0.9801, 0.9729, 0.9799, 0.9738]
	train_accs: [0.9813, 0.98106664, 0.97545, 0.98151666, 0.97585]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.97746
	best: 0.9799

Starting e_i: 1844
Model ind 665 epoch 1844 batch: 0 avg loss -2.979889 avg loss no lamb -2.979889 time 2020-06-27 21:04:40.794862
Model ind 665 epoch 1844 batch: 100 avg loss -2.892566 avg loss no lamb -2.892566 time 2020-06-27 21:04:50.567925
Model ind 665 epoch 1844 batch: 200 avg loss -2.904294 avg loss no lamb -2.904294 time 2020-06-27 21:05:00.585080
Model ind 665 epoch 1844 batch: 300 avg loss -2.895078 avg loss no lamb -2.895078 time 2020-06-27 21:05:10.334515
Model ind 665 epoch 1844 batch: 400 avg loss -2.821238 avg loss no lamb -2.821238 time 2020-06-27 21:05:20.183446
Model ind 665 epoch 1844 batch: 500 avg loss -2.849377 avg loss no lamb -2.849377 time 2020-06-27 21:05:30.007093
Model ind 665 epoch 1844 batch: 600 avg loss -2.948435 avg loss no lamb -2.948435 time 2020-06-27 21:05:40.029667
Model ind 665 epoch 1844 batch: 700 avg loss -2.850949 avg loss no lamb -2.850949 time 2020-06-27 21:05:49.964338
Model ind 665 epoch 1844 batch: 800 avg loss -2.821488 avg loss no lamb -2.821488 time 2020-06-27 21:05:59.897566
last batch sz 10
Pre: time 2020-06-27 21:06:12.988304: 
 	std: 0.0031581116
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9804, 0.9737, 0.9811, 0.9751]
	train_accs: [0.9816333, 0.98108333, 0.97555, 0.98185, 0.97646666]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97822
	best: 0.9811

Starting e_i: 1845
Model ind 665 epoch 1845 batch: 0 avg loss -2.947638 avg loss no lamb -2.947638 time 2020-06-27 21:06:14.256714
Model ind 665 epoch 1845 batch: 100 avg loss -2.914427 avg loss no lamb -2.914427 time 2020-06-27 21:06:24.000376
Model ind 665 epoch 1845 batch: 200 avg loss -2.925730 avg loss no lamb -2.925730 time 2020-06-27 21:06:33.768637
Model ind 665 epoch 1845 batch: 300 avg loss -2.877166 avg loss no lamb -2.877166 time 2020-06-27 21:06:43.620552
Model ind 665 epoch 1845 batch: 400 avg loss -2.794755 avg loss no lamb -2.794755 time 2020-06-27 21:06:53.427016
Model ind 665 epoch 1845 batch: 500 avg loss -2.933491 avg loss no lamb -2.933491 time 2020-06-27 21:07:03.299520
Model ind 665 epoch 1845 batch: 600 avg loss -2.910408 avg loss no lamb -2.910408 time 2020-06-27 21:07:13.064270
Model ind 665 epoch 1845 batch: 700 avg loss -2.825636 avg loss no lamb -2.825636 time 2020-06-27 21:07:22.794734
Model ind 665 epoch 1845 batch: 800 avg loss -2.930672 avg loss no lamb -2.930672 time 2020-06-27 21:07:32.623612
last batch sz 10
Pre: time 2020-06-27 21:07:45.733713: 
 	std: 0.0028373213
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9785, 0.9721, 0.9788, 0.9738]
	train_accs: [0.98055, 0.9802, 0.97478336, 0.98066664, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97636
	best: 0.9788

Starting e_i: 1846
Model ind 665 epoch 1846 batch: 0 avg loss -2.993887 avg loss no lamb -2.993887 time 2020-06-27 21:07:47.017873
Model ind 665 epoch 1846 batch: 100 avg loss -2.919768 avg loss no lamb -2.919768 time 2020-06-27 21:07:56.784845
Model ind 665 epoch 1846 batch: 200 avg loss -2.899916 avg loss no lamb -2.899916 time 2020-06-27 21:08:06.483897
Model ind 665 epoch 1846 batch: 300 avg loss -2.899573 avg loss no lamb -2.899573 time 2020-06-27 21:08:16.235046
Model ind 665 epoch 1846 batch: 400 avg loss -2.864678 avg loss no lamb -2.864678 time 2020-06-27 21:08:26.125858
Model ind 665 epoch 1846 batch: 500 avg loss -2.913698 avg loss no lamb -2.913698 time 2020-06-27 21:08:35.885086
Model ind 665 epoch 1846 batch: 600 avg loss -2.920224 avg loss no lamb -2.920224 time 2020-06-27 21:08:45.684819
Model ind 665 epoch 1846 batch: 700 avg loss -2.849157 avg loss no lamb -2.849157 time 2020-06-27 21:08:55.398943
Model ind 665 epoch 1846 batch: 800 avg loss -2.900661 avg loss no lamb -2.900661 time 2020-06-27 21:09:05.315356
last batch sz 10
Pre: time 2020-06-27 21:09:18.476153: 
 	std: 0.0028436647
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.981, 0.9793, 0.9741, 0.9809, 0.9755]
	train_accs: [0.9813167, 0.98075, 0.97581667, 0.9813667, 0.97625]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97816
	best: 0.9809

Starting e_i: 1847
Model ind 665 epoch 1847 batch: 0 avg loss -2.979036 avg loss no lamb -2.979036 time 2020-06-27 21:09:19.734728
Model ind 665 epoch 1847 batch: 100 avg loss -2.894315 avg loss no lamb -2.894315 time 2020-06-27 21:09:29.611937
Model ind 665 epoch 1847 batch: 200 avg loss -2.898101 avg loss no lamb -2.898101 time 2020-06-27 21:09:39.450713
Model ind 665 epoch 1847 batch: 300 avg loss -2.900510 avg loss no lamb -2.900510 time 2020-06-27 21:09:49.418963
Model ind 665 epoch 1847 batch: 400 avg loss -2.798227 avg loss no lamb -2.798227 time 2020-06-27 21:09:59.248607
Model ind 665 epoch 1847 batch: 500 avg loss -2.888780 avg loss no lamb -2.888780 time 2020-06-27 21:10:09.162253
Model ind 665 epoch 1847 batch: 600 avg loss -2.835281 avg loss no lamb -2.835281 time 2020-06-27 21:10:18.915780
Model ind 665 epoch 1847 batch: 700 avg loss -2.800172 avg loss no lamb -2.800172 time 2020-06-27 21:10:28.842405
Model ind 665 epoch 1847 batch: 800 avg loss -2.846919 avg loss no lamb -2.846919 time 2020-06-27 21:10:38.707099
last batch sz 10
Pre: time 2020-06-27 21:10:51.784482: 
 	std: 0.002736137
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.9783, 0.9727, 0.9789, 0.974]
	train_accs: [0.981, 0.98018336, 0.97463334, 0.9809833, 0.9758667]
	best_train_sub_head: 0
	worst: 0.9727
	avg: 0.9766399
	best: 0.9793

Starting e_i: 1848
Model ind 665 epoch 1848 batch: 0 avg loss -2.897510 avg loss no lamb -2.897510 time 2020-06-27 21:10:53.034905
Model ind 665 epoch 1848 batch: 100 avg loss -2.934720 avg loss no lamb -2.934720 time 2020-06-27 21:11:02.902183
Model ind 665 epoch 1848 batch: 200 avg loss -2.879369 avg loss no lamb -2.879369 time 2020-06-27 21:11:14.041685
Model ind 665 epoch 1848 batch: 300 avg loss -2.852971 avg loss no lamb -2.852971 time 2020-06-27 21:11:27.466974
Model ind 665 epoch 1848 batch: 400 avg loss -2.777871 avg loss no lamb -2.777871 time 2020-06-27 21:11:37.364498
Model ind 665 epoch 1848 batch: 500 avg loss -2.810453 avg loss no lamb -2.810453 time 2020-06-27 21:11:47.183113
Model ind 665 epoch 1848 batch: 600 avg loss -2.894303 avg loss no lamb -2.894303 time 2020-06-27 21:11:57.028842
Model ind 665 epoch 1848 batch: 700 avg loss -2.788231 avg loss no lamb -2.788231 time 2020-06-27 21:12:07.091670
Model ind 665 epoch 1848 batch: 800 avg loss -2.865047 avg loss no lamb -2.865047 time 2020-06-27 21:12:16.949324
last batch sz 10
Pre: time 2020-06-27 21:12:30.016400: 
 	std: 0.0028872134
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9778, 0.9728, 0.9796, 0.974]
	train_accs: [0.98106664, 0.98035, 0.9755667, 0.9812, 0.97641665]
	best_train_sub_head: 3
	worst: 0.9728
	avg: 0.97679996
	best: 0.9796

Starting e_i: 1849
Model ind 665 epoch 1849 batch: 0 avg loss -2.942860 avg loss no lamb -2.942860 time 2020-06-27 21:12:31.527949
Model ind 665 epoch 1849 batch: 100 avg loss -2.926965 avg loss no lamb -2.926965 time 2020-06-27 21:12:41.311935
Model ind 665 epoch 1849 batch: 200 avg loss -2.883303 avg loss no lamb -2.883303 time 2020-06-27 21:12:51.202879
Model ind 665 epoch 1849 batch: 300 avg loss -2.901983 avg loss no lamb -2.901983 time 2020-06-27 21:13:01.225789
Model ind 665 epoch 1849 batch: 400 avg loss -2.821478 avg loss no lamb -2.821478 time 2020-06-27 21:13:11.145069
Model ind 665 epoch 1849 batch: 500 avg loss -2.875975 avg loss no lamb -2.875975 time 2020-06-27 21:13:21.038408
Model ind 665 epoch 1849 batch: 600 avg loss -2.914374 avg loss no lamb -2.914374 time 2020-06-27 21:13:30.918217
Model ind 665 epoch 1849 batch: 700 avg loss -2.861994 avg loss no lamb -2.861994 time 2020-06-27 21:13:40.962757
Model ind 665 epoch 1849 batch: 800 avg loss -2.870793 avg loss no lamb -2.870793 time 2020-06-27 21:13:50.805129
last batch sz 10
Pre: time 2020-06-27 21:14:04.133450: 
 	std: 0.0031474454
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.9793, 0.9736, 0.9807, 0.9743]
	train_accs: [0.9813333, 0.98085, 0.9759667, 0.9816333, 0.9758833]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97774
	best: 0.9807

Starting e_i: 1850
Model ind 665 epoch 1850 batch: 0 avg loss -2.992627 avg loss no lamb -2.992627 time 2020-06-27 21:14:05.435163
Model ind 665 epoch 1850 batch: 100 avg loss -2.923567 avg loss no lamb -2.923567 time 2020-06-27 21:14:15.508964
Model ind 665 epoch 1850 batch: 200 avg loss -2.886247 avg loss no lamb -2.886247 time 2020-06-27 21:14:25.397490
Model ind 665 epoch 1850 batch: 300 avg loss -2.898485 avg loss no lamb -2.898485 time 2020-06-27 21:14:35.211240
Model ind 665 epoch 1850 batch: 400 avg loss -2.809215 avg loss no lamb -2.809215 time 2020-06-27 21:14:45.013301
Model ind 665 epoch 1850 batch: 500 avg loss -2.864787 avg loss no lamb -2.864787 time 2020-06-27 21:14:54.940143
Model ind 665 epoch 1850 batch: 600 avg loss -2.883107 avg loss no lamb -2.883107 time 2020-06-27 21:15:04.831260
Model ind 665 epoch 1850 batch: 700 avg loss -2.807118 avg loss no lamb -2.807118 time 2020-06-27 21:15:14.721531
Model ind 665 epoch 1850 batch: 800 avg loss -2.899779 avg loss no lamb -2.899779 time 2020-06-27 21:15:24.721757
last batch sz 10
Pre: time 2020-06-27 21:15:37.785096: 
 	std: 0.0038717561
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9789, 0.9708, 0.9793, 0.9723]
	train_accs: [0.981, 0.98013335, 0.97375, 0.981, 0.9743]
	best_train_sub_head: 0
	worst: 0.9708
	avg: 0.97624
	best: 0.9799

Starting e_i: 1851
Model ind 665 epoch 1851 batch: 0 avg loss -2.914023 avg loss no lamb -2.914023 time 2020-06-27 21:15:40.320953
Model ind 665 epoch 1851 batch: 100 avg loss -2.856820 avg loss no lamb -2.856820 time 2020-06-27 21:15:50.073290
Model ind 665 epoch 1851 batch: 200 avg loss -2.896358 avg loss no lamb -2.896358 time 2020-06-27 21:15:59.796078
Model ind 665 epoch 1851 batch: 300 avg loss -2.895192 avg loss no lamb -2.895192 time 2020-06-27 21:16:09.722633
Model ind 665 epoch 1851 batch: 400 avg loss -2.851469 avg loss no lamb -2.851469 time 2020-06-27 21:16:19.587572
Model ind 665 epoch 1851 batch: 500 avg loss -2.872660 avg loss no lamb -2.872660 time 2020-06-27 21:16:29.467416
Model ind 665 epoch 1851 batch: 600 avg loss -2.900556 avg loss no lamb -2.900556 time 2020-06-27 21:16:39.372713
Model ind 665 epoch 1851 batch: 700 avg loss -2.754591 avg loss no lamb -2.754591 time 2020-06-27 21:16:49.227700
Model ind 665 epoch 1851 batch: 800 avg loss -2.882765 avg loss no lamb -2.882765 time 2020-06-27 21:16:59.131550
last batch sz 10
Pre: time 2020-06-27 21:17:12.711812: 
 	std: 0.002480801
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9789, 0.9742, 0.9801, 0.9749]
	train_accs: [0.9813333, 0.98041666, 0.9754, 0.98141664, 0.97625]
	best_train_sub_head: 3
	worst: 0.9742
	avg: 0.97754
	best: 0.9801

Starting e_i: 1852
Model ind 665 epoch 1852 batch: 0 avg loss -2.945864 avg loss no lamb -2.945864 time 2020-06-27 21:17:14.021340
Model ind 665 epoch 1852 batch: 100 avg loss -2.930865 avg loss no lamb -2.930865 time 2020-06-27 21:17:23.747224
Model ind 665 epoch 1852 batch: 200 avg loss -2.918796 avg loss no lamb -2.918796 time 2020-06-27 21:17:33.619838
Model ind 665 epoch 1852 batch: 300 avg loss -2.899098 avg loss no lamb -2.899098 time 2020-06-27 21:17:43.371399
Model ind 665 epoch 1852 batch: 400 avg loss -2.738506 avg loss no lamb -2.738506 time 2020-06-27 21:17:53.108429
Model ind 665 epoch 1852 batch: 500 avg loss -2.903834 avg loss no lamb -2.903834 time 2020-06-27 21:18:02.977625
Model ind 665 epoch 1852 batch: 600 avg loss -2.833754 avg loss no lamb -2.833754 time 2020-06-27 21:18:12.959113
Model ind 665 epoch 1852 batch: 700 avg loss -2.806313 avg loss no lamb -2.806313 time 2020-06-27 21:18:22.776685
Model ind 665 epoch 1852 batch: 800 avg loss -2.874500 avg loss no lamb -2.874500 time 2020-06-27 21:18:32.586938
last batch sz 10
Pre: time 2020-06-27 21:18:45.587008: 
 	std: 0.0022312428
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9796, 0.9788, 0.9741, 0.9793, 0.9755]
	train_accs: [0.9811, 0.98073334, 0.9759833, 0.9812833, 0.9762667]
	best_train_sub_head: 3
	worst: 0.9741
	avg: 0.97746
	best: 0.9793

Starting e_i: 1853
Model ind 665 epoch 1853 batch: 0 avg loss -2.965570 avg loss no lamb -2.965570 time 2020-06-27 21:18:46.876723
Model ind 665 epoch 1853 batch: 100 avg loss -2.890617 avg loss no lamb -2.890617 time 2020-06-27 21:18:57.001160
Model ind 665 epoch 1853 batch: 200 avg loss -2.858969 avg loss no lamb -2.858969 time 2020-06-27 21:19:06.828230
Model ind 665 epoch 1853 batch: 300 avg loss -2.888776 avg loss no lamb -2.888776 time 2020-06-27 21:19:16.575750
Model ind 665 epoch 1853 batch: 400 avg loss -2.852589 avg loss no lamb -2.852589 time 2020-06-27 21:19:26.276797
Model ind 665 epoch 1853 batch: 500 avg loss -2.852967 avg loss no lamb -2.852967 time 2020-06-27 21:19:36.016574
Model ind 665 epoch 1853 batch: 600 avg loss -2.871104 avg loss no lamb -2.871104 time 2020-06-27 21:19:46.067471
Model ind 665 epoch 1853 batch: 700 avg loss -2.787776 avg loss no lamb -2.787776 time 2020-06-27 21:19:55.994839
Model ind 665 epoch 1853 batch: 800 avg loss -2.905308 avg loss no lamb -2.905308 time 2020-06-27 21:20:05.902509
last batch sz 10
Pre: time 2020-06-27 21:20:19.185629: 
 	std: 0.0030681703
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9792, 0.9779, 0.9722, 0.9787, 0.9726]
	train_accs: [0.9806333, 0.9801, 0.97491664, 0.98076665, 0.9751]
	best_train_sub_head: 3
	worst: 0.9722
	avg: 0.97612
	best: 0.9787

Starting e_i: 1854
Model ind 665 epoch 1854 batch: 0 avg loss -2.964005 avg loss no lamb -2.964005 time 2020-06-27 21:20:20.444324
Model ind 665 epoch 1854 batch: 100 avg loss -2.918998 avg loss no lamb -2.918998 time 2020-06-27 21:20:30.304199
Model ind 665 epoch 1854 batch: 200 avg loss -2.852524 avg loss no lamb -2.852524 time 2020-06-27 21:20:40.165294
Model ind 665 epoch 1854 batch: 300 avg loss -2.899651 avg loss no lamb -2.899651 time 2020-06-27 21:20:50.044079
Model ind 665 epoch 1854 batch: 400 avg loss -2.823638 avg loss no lamb -2.823638 time 2020-06-27 21:20:59.930710
Model ind 665 epoch 1854 batch: 500 avg loss -2.880570 avg loss no lamb -2.880570 time 2020-06-27 21:21:09.794145
Model ind 665 epoch 1854 batch: 600 avg loss -2.889126 avg loss no lamb -2.889126 time 2020-06-27 21:21:19.607195
Model ind 665 epoch 1854 batch: 700 avg loss -2.831027 avg loss no lamb -2.831027 time 2020-06-27 21:21:29.469017
Model ind 665 epoch 1854 batch: 800 avg loss -2.887269 avg loss no lamb -2.887269 time 2020-06-27 21:21:39.436131
last batch sz 10
Pre: time 2020-06-27 21:21:52.617518: 
 	std: 0.0023380318
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9784, 0.9739, 0.9787, 0.9737]
	train_accs: [0.9809333, 0.98078334, 0.97581667, 0.98095, 0.97625]
	best_train_sub_head: 3
	worst: 0.9737
	avg: 0.97665995
	best: 0.9787

Starting e_i: 1855
Model ind 665 epoch 1855 batch: 0 avg loss -3.001418 avg loss no lamb -3.001418 time 2020-06-27 21:21:53.907420
Model ind 665 epoch 1855 batch: 100 avg loss -2.930193 avg loss no lamb -2.930193 time 2020-06-27 21:22:03.799585
Model ind 665 epoch 1855 batch: 200 avg loss -2.913023 avg loss no lamb -2.913023 time 2020-06-27 21:22:13.654951
Model ind 665 epoch 1855 batch: 300 avg loss -2.885181 avg loss no lamb -2.885181 time 2020-06-27 21:22:23.676930
Model ind 665 epoch 1855 batch: 400 avg loss -2.803809 avg loss no lamb -2.803809 time 2020-06-27 21:22:33.466012
Model ind 665 epoch 1855 batch: 500 avg loss -2.843663 avg loss no lamb -2.843663 time 2020-06-27 21:22:43.383264
Model ind 665 epoch 1855 batch: 600 avg loss -2.904972 avg loss no lamb -2.904972 time 2020-06-27 21:22:53.190444
Model ind 665 epoch 1855 batch: 700 avg loss -2.832713 avg loss no lamb -2.832713 time 2020-06-27 21:23:03.057579
Model ind 665 epoch 1855 batch: 800 avg loss -2.894975 avg loss no lamb -2.894975 time 2020-06-27 21:23:13.113922
last batch sz 10
Pre: time 2020-06-27 21:23:26.171441: 
 	std: 0.003454848
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9786, 0.9776, 0.9706, 0.9778, 0.9714]
	train_accs: [0.98008335, 0.97976667, 0.9741167, 0.98005, 0.97465]
	best_train_sub_head: 0
	worst: 0.9706
	avg: 0.97520006
	best: 0.9786

Starting e_i: 1856
Model ind 665 epoch 1856 batch: 0 avg loss -2.943691 avg loss no lamb -2.943691 time 2020-06-27 21:23:27.678747
Model ind 665 epoch 1856 batch: 100 avg loss -2.936824 avg loss no lamb -2.936824 time 2020-06-27 21:23:37.550122
Model ind 665 epoch 1856 batch: 200 avg loss -2.881218 avg loss no lamb -2.881218 time 2020-06-27 21:23:47.431927
Model ind 665 epoch 1856 batch: 300 avg loss -2.854839 avg loss no lamb -2.854839 time 2020-06-27 21:23:57.470643
Model ind 665 epoch 1856 batch: 400 avg loss -2.849416 avg loss no lamb -2.849416 time 2020-06-27 21:24:07.555445
Model ind 665 epoch 1856 batch: 500 avg loss -2.906883 avg loss no lamb -2.906883 time 2020-06-27 21:24:17.448916
Model ind 665 epoch 1856 batch: 600 avg loss -2.894885 avg loss no lamb -2.894885 time 2020-06-27 21:24:27.316064
Model ind 665 epoch 1856 batch: 700 avg loss -2.799247 avg loss no lamb -2.799247 time 2020-06-27 21:24:37.238162
Model ind 665 epoch 1856 batch: 800 avg loss -2.878536 avg loss no lamb -2.878536 time 2020-06-27 21:24:47.355035
last batch sz 10
Pre: time 2020-06-27 21:25:00.594141: 
 	std: 0.003437096
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.9791, 0.9712, 0.9791, 0.9733]
	train_accs: [0.98095, 0.9805167, 0.97468334, 0.98073334, 0.97543335]
	best_train_sub_head: 0
	worst: 0.9712
	avg: 0.97638
	best: 0.9792

Starting e_i: 1857
Model ind 665 epoch 1857 batch: 0 avg loss -2.955174 avg loss no lamb -2.955174 time 2020-06-27 21:25:01.915417
Model ind 665 epoch 1857 batch: 100 avg loss -2.940384 avg loss no lamb -2.940384 time 2020-06-27 21:25:11.718802
Model ind 665 epoch 1857 batch: 200 avg loss -2.899269 avg loss no lamb -2.899269 time 2020-06-27 21:25:21.380286
Model ind 665 epoch 1857 batch: 300 avg loss -2.914966 avg loss no lamb -2.914966 time 2020-06-27 21:25:31.336756
Model ind 665 epoch 1857 batch: 400 avg loss -2.860323 avg loss no lamb -2.860323 time 2020-06-27 21:25:41.242275
Model ind 665 epoch 1857 batch: 500 avg loss -2.915085 avg loss no lamb -2.915085 time 2020-06-27 21:25:51.115398
Model ind 665 epoch 1857 batch: 600 avg loss -2.898107 avg loss no lamb -2.898107 time 2020-06-27 21:26:00.905483
Model ind 665 epoch 1857 batch: 700 avg loss -2.787086 avg loss no lamb -2.787086 time 2020-06-27 21:26:10.802384
Model ind 665 epoch 1857 batch: 800 avg loss -2.862000 avg loss no lamb -2.862000 time 2020-06-27 21:26:20.734472
last batch sz 10
Pre: time 2020-06-27 21:26:33.889712: 
 	std: 0.0026099929
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9797, 0.9744, 0.9805, 0.9755]
	train_accs: [0.9812, 0.98055, 0.97585, 0.9812833, 0.9763333]
	best_train_sub_head: 3
	worst: 0.9744
	avg: 0.9781
	best: 0.9805

Starting e_i: 1858
Model ind 665 epoch 1858 batch: 0 avg loss -2.974231 avg loss no lamb -2.974231 time 2020-06-27 21:26:35.201914
Model ind 665 epoch 1858 batch: 100 avg loss -2.957018 avg loss no lamb -2.957018 time 2020-06-27 21:26:44.943109
Model ind 665 epoch 1858 batch: 200 avg loss -2.927377 avg loss no lamb -2.927377 time 2020-06-27 21:26:54.715096
Model ind 665 epoch 1858 batch: 300 avg loss -2.885312 avg loss no lamb -2.885312 time 2020-06-27 21:27:04.628195
Model ind 665 epoch 1858 batch: 400 avg loss -2.825553 avg loss no lamb -2.825553 time 2020-06-27 21:27:14.518088
Model ind 665 epoch 1858 batch: 500 avg loss -2.871644 avg loss no lamb -2.871644 time 2020-06-27 21:27:24.514146
Model ind 665 epoch 1858 batch: 600 avg loss -2.916013 avg loss no lamb -2.916013 time 2020-06-27 21:27:34.320826
Model ind 665 epoch 1858 batch: 700 avg loss -2.823879 avg loss no lamb -2.823879 time 2020-06-27 21:27:44.310801
Model ind 665 epoch 1858 batch: 800 avg loss -2.947082 avg loss no lamb -2.947082 time 2020-06-27 21:27:54.126291
last batch sz 10
Pre: time 2020-06-27 21:28:07.378530: 
 	std: 0.0030873967
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9798, 0.9736, 0.9806, 0.9745]
	train_accs: [0.9813333, 0.98055, 0.9749, 0.98155, 0.976]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.9778
	best: 0.9806

Starting e_i: 1859
Model ind 665 epoch 1859 batch: 0 avg loss -2.972342 avg loss no lamb -2.972342 time 2020-06-27 21:28:08.692572
Model ind 665 epoch 1859 batch: 100 avg loss -2.893609 avg loss no lamb -2.893609 time 2020-06-27 21:28:18.542032
Model ind 665 epoch 1859 batch: 200 avg loss -2.900463 avg loss no lamb -2.900463 time 2020-06-27 21:28:28.614547
Model ind 665 epoch 1859 batch: 300 avg loss -2.859486 avg loss no lamb -2.859486 time 2020-06-27 21:28:38.615771
Model ind 665 epoch 1859 batch: 400 avg loss -2.799584 avg loss no lamb -2.799584 time 2020-06-27 21:28:48.372436
Model ind 665 epoch 1859 batch: 500 avg loss -2.847319 avg loss no lamb -2.847319 time 2020-06-27 21:28:58.213703
Model ind 665 epoch 1859 batch: 600 avg loss -2.925429 avg loss no lamb -2.925429 time 2020-06-27 21:29:08.197123
Model ind 665 epoch 1859 batch: 700 avg loss -2.875027 avg loss no lamb -2.875027 time 2020-06-27 21:29:18.032449
Model ind 665 epoch 1859 batch: 800 avg loss -2.851409 avg loss no lamb -2.851409 time 2020-06-27 21:29:27.908121
last batch sz 10
Pre: time 2020-06-27 21:29:41.028905: 
 	std: 0.0026309013
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.9788, 0.9736, 0.979, 0.9736]
	train_accs: [0.98141664, 0.98043334, 0.97548336, 0.98141664, 0.97613335]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97682
	best: 0.9791

Starting e_i: 1860
Model ind 665 epoch 1860 batch: 0 avg loss -2.945324 avg loss no lamb -2.945324 time 2020-06-27 21:29:42.304607
Model ind 665 epoch 1860 batch: 100 avg loss -2.900480 avg loss no lamb -2.900480 time 2020-06-27 21:29:52.146122
Model ind 665 epoch 1860 batch: 200 avg loss -2.830048 avg loss no lamb -2.830048 time 2020-06-27 21:30:02.175255
Model ind 665 epoch 1860 batch: 300 avg loss -2.896493 avg loss no lamb -2.896493 time 2020-06-27 21:30:11.965730
Model ind 665 epoch 1860 batch: 400 avg loss -2.862076 avg loss no lamb -2.862076 time 2020-06-27 21:30:21.855112
Model ind 665 epoch 1860 batch: 500 avg loss -2.840406 avg loss no lamb -2.840406 time 2020-06-27 21:30:31.681626
Model ind 665 epoch 1860 batch: 600 avg loss -2.856172 avg loss no lamb -2.856172 time 2020-06-27 21:30:41.639204
Model ind 665 epoch 1860 batch: 700 avg loss -2.792564 avg loss no lamb -2.792564 time 2020-06-27 21:30:51.565480
Model ind 665 epoch 1860 batch: 800 avg loss -2.928015 avg loss no lamb -2.928015 time 2020-06-27 21:31:01.553422
last batch sz 10
Pre: time 2020-06-27 21:31:14.791326: 
 	std: 0.0033958901
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9792, 0.9723, 0.9803, 0.9737]
	train_accs: [0.98151666, 0.98113334, 0.97506666, 0.98175, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.9771
	best: 0.9803

Starting e_i: 1861
Model ind 665 epoch 1861 batch: 0 avg loss -2.934356 avg loss no lamb -2.934356 time 2020-06-27 21:31:17.454844
Model ind 665 epoch 1861 batch: 100 avg loss -2.940586 avg loss no lamb -2.940586 time 2020-06-27 21:31:27.290681
Model ind 665 epoch 1861 batch: 200 avg loss -2.871246 avg loss no lamb -2.871246 time 2020-06-27 21:31:37.144833
Model ind 665 epoch 1861 batch: 300 avg loss -2.892293 avg loss no lamb -2.892293 time 2020-06-27 21:31:46.975084
Model ind 665 epoch 1861 batch: 400 avg loss -2.859818 avg loss no lamb -2.859818 time 2020-06-27 21:31:56.997268
Model ind 665 epoch 1861 batch: 500 avg loss -2.877288 avg loss no lamb -2.877288 time 2020-06-27 21:32:06.929443
Model ind 665 epoch 1861 batch: 600 avg loss -2.883003 avg loss no lamb -2.883003 time 2020-06-27 21:32:16.720902
Model ind 665 epoch 1861 batch: 700 avg loss -2.783700 avg loss no lamb -2.783700 time 2020-06-27 21:32:26.732944
Model ind 665 epoch 1861 batch: 800 avg loss -2.839486 avg loss no lamb -2.839486 time 2020-06-27 21:32:36.715934
last batch sz 10
Pre: time 2020-06-27 21:32:49.861620: 
 	std: 0.003710198
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9821, 0.9805, 0.9732, 0.982, 0.9751]
	train_accs: [0.98178333, 0.9811, 0.97568333, 0.98193336, 0.9766167]
	best_train_sub_head: 3
	worst: 0.9732
	avg: 0.97858
	best: 0.982

Starting e_i: 1862
Model ind 665 epoch 1862 batch: 0 avg loss -2.930259 avg loss no lamb -2.930259 time 2020-06-27 21:32:51.195202
Model ind 665 epoch 1862 batch: 100 avg loss -2.954863 avg loss no lamb -2.954863 time 2020-06-27 21:33:01.034399
Model ind 665 epoch 1862 batch: 200 avg loss -2.898098 avg loss no lamb -2.898098 time 2020-06-27 21:33:11.017284
Model ind 665 epoch 1862 batch: 300 avg loss -2.871111 avg loss no lamb -2.871111 time 2020-06-27 21:33:21.043570
Model ind 665 epoch 1862 batch: 400 avg loss -2.837809 avg loss no lamb -2.837809 time 2020-06-27 21:33:30.984007
Model ind 665 epoch 1862 batch: 500 avg loss -2.839937 avg loss no lamb -2.839937 time 2020-06-27 21:33:40.964875
Model ind 665 epoch 1862 batch: 600 avg loss -2.883479 avg loss no lamb -2.883479 time 2020-06-27 21:33:50.812397
Model ind 665 epoch 1862 batch: 700 avg loss -2.730118 avg loss no lamb -2.730118 time 2020-06-27 21:34:00.850198
Model ind 665 epoch 1862 batch: 800 avg loss -2.871786 avg loss no lamb -2.871786 time 2020-06-27 21:34:10.754569
last batch sz 10
Pre: time 2020-06-27 21:34:24.188811: 
 	std: 0.0038545653
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9806, 0.9802, 0.9723, 0.9814, 0.9736]
	train_accs: [0.98158336, 0.98105, 0.975, 0.9815, 0.97625]
	best_train_sub_head: 0
	worst: 0.9723
	avg: 0.97762
	best: 0.9806

Starting e_i: 1863
Model ind 665 epoch 1863 batch: 0 avg loss -2.963508 avg loss no lamb -2.963508 time 2020-06-27 21:34:25.510784
Model ind 665 epoch 1863 batch: 100 avg loss -2.960345 avg loss no lamb -2.960345 time 2020-06-27 21:34:35.520778
Model ind 665 epoch 1863 batch: 200 avg loss -2.926251 avg loss no lamb -2.926251 time 2020-06-27 21:34:45.269429
Model ind 665 epoch 1863 batch: 300 avg loss -2.848543 avg loss no lamb -2.848543 time 2020-06-27 21:34:55.063242
Model ind 665 epoch 1863 batch: 400 avg loss -2.837518 avg loss no lamb -2.837518 time 2020-06-27 21:35:05.015263
Model ind 665 epoch 1863 batch: 500 avg loss -2.848717 avg loss no lamb -2.848717 time 2020-06-27 21:35:14.936184
Model ind 665 epoch 1863 batch: 600 avg loss -2.890752 avg loss no lamb -2.890752 time 2020-06-27 21:35:24.793078
Model ind 665 epoch 1863 batch: 700 avg loss -2.765944 avg loss no lamb -2.765944 time 2020-06-27 21:35:34.593834
Model ind 665 epoch 1863 batch: 800 avg loss -2.854183 avg loss no lamb -2.854183 time 2020-06-27 21:35:44.563011
last batch sz 10
Pre: time 2020-06-27 21:35:58.064656: 
 	std: 0.0039300947
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9811, 0.9799, 0.9724, 0.981, 0.973]
	train_accs: [0.9817, 0.9810333, 0.9748, 0.9816, 0.97543335]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97747993
	best: 0.9811

Starting e_i: 1864
Model ind 665 epoch 1864 batch: 0 avg loss -2.941566 avg loss no lamb -2.941566 time 2020-06-27 21:35:59.364655
Model ind 665 epoch 1864 batch: 100 avg loss -2.883975 avg loss no lamb -2.883975 time 2020-06-27 21:36:09.365896
Model ind 665 epoch 1864 batch: 200 avg loss -2.879422 avg loss no lamb -2.879422 time 2020-06-27 21:36:19.135975
Model ind 665 epoch 1864 batch: 300 avg loss -2.872954 avg loss no lamb -2.872954 time 2020-06-27 21:36:28.915024
Model ind 665 epoch 1864 batch: 400 avg loss -2.818035 avg loss no lamb -2.818035 time 2020-06-27 21:36:38.707694
Model ind 665 epoch 1864 batch: 500 avg loss -2.876662 avg loss no lamb -2.876662 time 2020-06-27 21:36:48.540044
Model ind 665 epoch 1864 batch: 600 avg loss -2.925063 avg loss no lamb -2.925063 time 2020-06-27 21:36:58.398475
Model ind 665 epoch 1864 batch: 700 avg loss -2.792121 avg loss no lamb -2.792121 time 2020-06-27 21:37:08.251959
Model ind 665 epoch 1864 batch: 800 avg loss -2.813914 avg loss no lamb -2.813914 time 2020-06-27 21:37:18.109209
last batch sz 10
Pre: time 2020-06-27 21:37:31.071417: 
 	std: 0.0035050933
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9812, 0.9803, 0.9729, 0.981, 0.9747]
	train_accs: [0.98153335, 0.9810167, 0.9755, 0.9817333, 0.97665]
	best_train_sub_head: 3
	worst: 0.9729
	avg: 0.9780199
	best: 0.981

Starting e_i: 1865
Model ind 665 epoch 1865 batch: 0 avg loss -2.944617 avg loss no lamb -2.944617 time 2020-06-27 21:37:32.376823
Model ind 665 epoch 1865 batch: 100 avg loss -2.888852 avg loss no lamb -2.888852 time 2020-06-27 21:37:42.162903
Model ind 665 epoch 1865 batch: 200 avg loss -2.893935 avg loss no lamb -2.893935 time 2020-06-27 21:37:52.030296
Model ind 665 epoch 1865 batch: 300 avg loss -2.910891 avg loss no lamb -2.910891 time 2020-06-27 21:38:01.841926
Model ind 665 epoch 1865 batch: 400 avg loss -2.836607 avg loss no lamb -2.836607 time 2020-06-27 21:38:11.553266
Model ind 665 epoch 1865 batch: 500 avg loss -2.864279 avg loss no lamb -2.864279 time 2020-06-27 21:38:21.271667
Model ind 665 epoch 1865 batch: 600 avg loss -2.895385 avg loss no lamb -2.895385 time 2020-06-27 21:38:31.137157
Model ind 665 epoch 1865 batch: 700 avg loss -2.777711 avg loss no lamb -2.777711 time 2020-06-27 21:38:41.008472
Model ind 665 epoch 1865 batch: 800 avg loss -2.868646 avg loss no lamb -2.868646 time 2020-06-27 21:38:50.873574
last batch sz 10
Pre: time 2020-06-27 21:39:04.152678: 
 	std: 0.0037541464
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9787, 0.9789, 0.9706, 0.9786, 0.9716]
	train_accs: [0.98041666, 0.9805833, 0.9740667, 0.9805, 0.9748]
	best_train_sub_head: 1
	worst: 0.9706
	avg: 0.97568
	best: 0.9789

Starting e_i: 1866
Model ind 665 epoch 1866 batch: 0 avg loss -2.962204 avg loss no lamb -2.962204 time 2020-06-27 21:39:05.858944
Model ind 665 epoch 1866 batch: 100 avg loss -2.927795 avg loss no lamb -2.927795 time 2020-06-27 21:39:17.584671
Model ind 665 epoch 1866 batch: 200 avg loss -2.908151 avg loss no lamb -2.908151 time 2020-06-27 21:39:28.164391
Model ind 665 epoch 1866 batch: 300 avg loss -2.876566 avg loss no lamb -2.876566 time 2020-06-27 21:39:37.644264
Model ind 665 epoch 1866 batch: 400 avg loss -2.839273 avg loss no lamb -2.839273 time 2020-06-27 21:39:47.196394
Model ind 665 epoch 1866 batch: 500 avg loss -2.878150 avg loss no lamb -2.878150 time 2020-06-27 21:39:56.849074
Model ind 665 epoch 1866 batch: 600 avg loss -2.919354 avg loss no lamb -2.919354 time 2020-06-27 21:40:06.494131
Model ind 665 epoch 1866 batch: 700 avg loss -2.789024 avg loss no lamb -2.789024 time 2020-06-27 21:40:18.037409
Model ind 665 epoch 1866 batch: 800 avg loss -2.942656 avg loss no lamb -2.942656 time 2020-06-27 21:40:29.894392
last batch sz 10
Pre: time 2020-06-27 21:40:43.738510: 
 	std: 0.0030309143
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9793, 0.979, 0.973, 0.9791, 0.9729]
	train_accs: [0.98155, 0.9810167, 0.97513336, 0.98143333, 0.97578335]
	best_train_sub_head: 0
	worst: 0.9729
	avg: 0.97665995
	best: 0.9793

Starting e_i: 1867
Model ind 665 epoch 1867 batch: 0 avg loss -2.962688 avg loss no lamb -2.962688 time 2020-06-27 21:40:45.407845
Model ind 665 epoch 1867 batch: 100 avg loss -2.950629 avg loss no lamb -2.950629 time 2020-06-27 21:40:56.669020
Model ind 665 epoch 1867 batch: 200 avg loss -2.920117 avg loss no lamb -2.920117 time 2020-06-27 21:41:10.476371
Model ind 665 epoch 1867 batch: 300 avg loss -2.952885 avg loss no lamb -2.952885 time 2020-06-27 21:41:24.383366
Model ind 665 epoch 1867 batch: 400 avg loss -2.855939 avg loss no lamb -2.855939 time 2020-06-27 21:41:37.026151
Model ind 665 epoch 1867 batch: 500 avg loss -2.880639 avg loss no lamb -2.880639 time 2020-06-27 21:41:49.738612
Model ind 665 epoch 1867 batch: 600 avg loss -2.887730 avg loss no lamb -2.887730 time 2020-06-27 21:41:59.460769
Model ind 665 epoch 1867 batch: 700 avg loss -2.799243 avg loss no lamb -2.799243 time 2020-06-27 21:42:08.955674
Model ind 665 epoch 1867 batch: 800 avg loss -2.886081 avg loss no lamb -2.886081 time 2020-06-27 21:42:18.405503
last batch sz 10
Pre: time 2020-06-27 21:42:30.945700: 
 	std: 0.0035094016
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9805, 0.9795, 0.9718, 0.9802, 0.9745]
	train_accs: [0.9813, 0.981, 0.97468334, 0.9813167, 0.97601664]
	best_train_sub_head: 3
	worst: 0.9718
	avg: 0.97730005
	best: 0.9802

Starting e_i: 1868
Model ind 665 epoch 1868 batch: 0 avg loss -2.941221 avg loss no lamb -2.941221 time 2020-06-27 21:42:32.501193
Model ind 665 epoch 1868 batch: 100 avg loss -2.854312 avg loss no lamb -2.854312 time 2020-06-27 21:42:41.964321
Model ind 665 epoch 1868 batch: 200 avg loss -2.877993 avg loss no lamb -2.877993 time 2020-06-27 21:42:54.595298
Model ind 665 epoch 1868 batch: 300 avg loss -2.937423 avg loss no lamb -2.937423 time 2020-06-27 21:43:08.250601
Model ind 665 epoch 1868 batch: 400 avg loss -2.831691 avg loss no lamb -2.831691 time 2020-06-27 21:43:18.816394
Model ind 665 epoch 1868 batch: 500 avg loss -2.878989 avg loss no lamb -2.878989 time 2020-06-27 21:43:28.830363
Model ind 665 epoch 1868 batch: 600 avg loss -2.878171 avg loss no lamb -2.878171 time 2020-06-27 21:43:38.597990
Model ind 665 epoch 1868 batch: 700 avg loss -2.833710 avg loss no lamb -2.833710 time 2020-06-27 21:43:48.372147
Model ind 665 epoch 1868 batch: 800 avg loss -2.894383 avg loss no lamb -2.894383 time 2020-06-27 21:43:58.157835
last batch sz 10
Pre: time 2020-06-27 21:44:10.913457: 
 	std: 0.0026780565
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9788, 0.9737, 0.9795, 0.9742]
	train_accs: [0.9810333, 0.98048335, 0.9751667, 0.9809, 0.97585]
	best_train_sub_head: 0
	worst: 0.9737
	avg: 0.97720003
	best: 0.9798

Starting e_i: 1869
Model ind 665 epoch 1869 batch: 0 avg loss -2.950188 avg loss no lamb -2.950188 time 2020-06-27 21:44:12.205861
Model ind 665 epoch 1869 batch: 100 avg loss -2.914022 avg loss no lamb -2.914022 time 2020-06-27 21:44:21.743115
Model ind 665 epoch 1869 batch: 200 avg loss -2.922168 avg loss no lamb -2.922168 time 2020-06-27 21:44:31.420085
Model ind 665 epoch 1869 batch: 300 avg loss -2.892574 avg loss no lamb -2.892574 time 2020-06-27 21:44:41.076715
Model ind 665 epoch 1869 batch: 400 avg loss -2.816580 avg loss no lamb -2.816580 time 2020-06-27 21:44:50.703624
Model ind 665 epoch 1869 batch: 500 avg loss -2.888626 avg loss no lamb -2.888626 time 2020-06-27 21:45:00.235001
Model ind 665 epoch 1869 batch: 600 avg loss -2.892429 avg loss no lamb -2.892429 time 2020-06-27 21:45:09.861090
Model ind 665 epoch 1869 batch: 700 avg loss -2.823234 avg loss no lamb -2.823234 time 2020-06-27 21:45:19.516484
Model ind 665 epoch 1869 batch: 800 avg loss -2.906488 avg loss no lamb -2.906488 time 2020-06-27 21:45:29.167924
last batch sz 10
Pre: time 2020-06-27 21:45:42.019270: 
 	std: 0.003358923
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.9788, 0.9714, 0.9786, 0.9725]
	train_accs: [0.98073334, 0.98008335, 0.9738, 0.9806333, 0.97485]
	best_train_sub_head: 0
	worst: 0.9714
	avg: 0.97604
	best: 0.9789

Starting e_i: 1870
Model ind 665 epoch 1870 batch: 0 avg loss -2.951132 avg loss no lamb -2.951132 time 2020-06-27 21:45:43.356429
Model ind 665 epoch 1870 batch: 100 avg loss -2.879814 avg loss no lamb -2.879814 time 2020-06-27 21:45:52.946820
Model ind 665 epoch 1870 batch: 200 avg loss -2.933562 avg loss no lamb -2.933562 time 2020-06-27 21:46:02.622854
Model ind 665 epoch 1870 batch: 300 avg loss -2.861943 avg loss no lamb -2.861943 time 2020-06-27 21:46:12.168532
Model ind 665 epoch 1870 batch: 400 avg loss -2.816061 avg loss no lamb -2.816061 time 2020-06-27 21:46:21.739345
Model ind 665 epoch 1870 batch: 500 avg loss -2.869846 avg loss no lamb -2.869846 time 2020-06-27 21:46:34.237847
Model ind 665 epoch 1870 batch: 600 avg loss -2.906498 avg loss no lamb -2.906498 time 2020-06-27 21:46:45.931202
Model ind 665 epoch 1870 batch: 700 avg loss -2.813967 avg loss no lamb -2.813967 time 2020-06-27 21:46:56.721374
Model ind 665 epoch 1870 batch: 800 avg loss -2.914509 avg loss no lamb -2.914509 time 2020-06-27 21:47:07.422498
last batch sz 10
Pre: time 2020-06-27 21:47:22.159565: 
 	std: 0.002903393
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.98, 0.9736, 0.9801, 0.9747]
	train_accs: [0.9812667, 0.9809333, 0.9755333, 0.9813667, 0.97616667]
	best_train_sub_head: 3
	worst: 0.9736
	avg: 0.97768
	best: 0.9801

Starting e_i: 1871
Model ind 665 epoch 1871 batch: 0 avg loss -2.998021 avg loss no lamb -2.998021 time 2020-06-27 21:47:24.610548
Model ind 665 epoch 1871 batch: 100 avg loss -2.935940 avg loss no lamb -2.935940 time 2020-06-27 21:47:35.984718
Model ind 665 epoch 1871 batch: 200 avg loss -2.905063 avg loss no lamb -2.905063 time 2020-06-27 21:47:46.934939
Model ind 665 epoch 1871 batch: 300 avg loss -2.866232 avg loss no lamb -2.866232 time 2020-06-27 21:47:58.602718
Model ind 665 epoch 1871 batch: 400 avg loss -2.797883 avg loss no lamb -2.797883 time 2020-06-27 21:48:09.792137
Model ind 665 epoch 1871 batch: 500 avg loss -2.806911 avg loss no lamb -2.806911 time 2020-06-27 21:48:21.569599
Model ind 665 epoch 1871 batch: 600 avg loss -2.885461 avg loss no lamb -2.885461 time 2020-06-27 21:48:32.772326
Model ind 665 epoch 1871 batch: 700 avg loss -2.837807 avg loss no lamb -2.837807 time 2020-06-27 21:48:44.576383
Model ind 665 epoch 1871 batch: 800 avg loss -2.882433 avg loss no lamb -2.882433 time 2020-06-27 21:48:56.011131
last batch sz 10
Pre: time 2020-06-27 21:49:10.694207: 
 	std: 0.0025255603
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9789, 0.9783, 0.973, 0.9786, 0.974]
	train_accs: [0.98095, 0.9805667, 0.9755167, 0.98076665, 0.9759833]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97656
	best: 0.9789

Starting e_i: 1872
Model ind 665 epoch 1872 batch: 0 avg loss -2.954895 avg loss no lamb -2.954895 time 2020-06-27 21:49:12.038951
Model ind 665 epoch 1872 batch: 100 avg loss -2.886571 avg loss no lamb -2.886571 time 2020-06-27 21:49:24.401323
Model ind 665 epoch 1872 batch: 200 avg loss -2.894178 avg loss no lamb -2.894178 time 2020-06-27 21:49:35.974333
Model ind 665 epoch 1872 batch: 300 avg loss -2.892888 avg loss no lamb -2.892888 time 2020-06-27 21:49:46.484813
Model ind 665 epoch 1872 batch: 400 avg loss -2.797981 avg loss no lamb -2.797981 time 2020-06-27 21:49:55.968782
Model ind 665 epoch 1872 batch: 500 avg loss -2.848837 avg loss no lamb -2.848837 time 2020-06-27 21:50:05.421419
Model ind 665 epoch 1872 batch: 600 avg loss -2.920897 avg loss no lamb -2.920897 time 2020-06-27 21:50:14.849423
Model ind 665 epoch 1872 batch: 700 avg loss -2.864343 avg loss no lamb -2.864343 time 2020-06-27 21:50:24.314010
Model ind 665 epoch 1872 batch: 800 avg loss -2.895852 avg loss no lamb -2.895852 time 2020-06-27 21:50:33.983266
last batch sz 10
Pre: time 2020-06-27 21:50:46.448231: 
 	std: 0.0033362256
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.9783, 0.9716, 0.9798, 0.9733]
	train_accs: [0.98105, 0.98031664, 0.97473335, 0.9809667, 0.97566664]
	best_train_sub_head: 0
	worst: 0.9716
	avg: 0.9764401
	best: 0.9792

Starting e_i: 1873
Model ind 665 epoch 1873 batch: 0 avg loss -3.000070 avg loss no lamb -3.000070 time 2020-06-27 21:50:47.894331
Model ind 665 epoch 1873 batch: 100 avg loss -2.898609 avg loss no lamb -2.898609 time 2020-06-27 21:50:57.453124
Model ind 665 epoch 1873 batch: 200 avg loss -2.876793 avg loss no lamb -2.876793 time 2020-06-27 21:51:07.279601
Model ind 665 epoch 1873 batch: 300 avg loss -2.863422 avg loss no lamb -2.863422 time 2020-06-27 21:51:16.772121
Model ind 665 epoch 1873 batch: 400 avg loss -2.855702 avg loss no lamb -2.855702 time 2020-06-27 21:51:26.296175
Model ind 665 epoch 1873 batch: 500 avg loss -2.896795 avg loss no lamb -2.896795 time 2020-06-27 21:51:35.851761
Model ind 665 epoch 1873 batch: 600 avg loss -2.954381 avg loss no lamb -2.954381 time 2020-06-27 21:51:45.391424
Model ind 665 epoch 1873 batch: 700 avg loss -2.810364 avg loss no lamb -2.810364 time 2020-06-27 21:51:55.010540
Model ind 665 epoch 1873 batch: 800 avg loss -2.857850 avg loss no lamb -2.857850 time 2020-06-27 21:52:04.547177
last batch sz 10
Pre: time 2020-06-27 21:52:17.297092: 
 	std: 0.002972949
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9789, 0.973, 0.9804, 0.9746]
	train_accs: [0.9814, 0.98095, 0.9755167, 0.98146665, 0.97643334]
	best_train_sub_head: 3
	worst: 0.973
	avg: 0.97734004
	best: 0.9804

Starting e_i: 1874
Model ind 665 epoch 1874 batch: 0 avg loss -2.978180 avg loss no lamb -2.978180 time 2020-06-27 21:52:18.537399
Model ind 665 epoch 1874 batch: 100 avg loss -2.947907 avg loss no lamb -2.947907 time 2020-06-27 21:52:28.067733
Model ind 665 epoch 1874 batch: 200 avg loss -2.875233 avg loss no lamb -2.875233 time 2020-06-27 21:52:37.463705
Model ind 665 epoch 1874 batch: 300 avg loss -2.874318 avg loss no lamb -2.874318 time 2020-06-27 21:52:47.037997
Model ind 665 epoch 1874 batch: 400 avg loss -2.880823 avg loss no lamb -2.880823 time 2020-06-27 21:52:56.656162
Model ind 665 epoch 1874 batch: 500 avg loss -2.885026 avg loss no lamb -2.885026 time 2020-06-27 21:53:06.254519
Model ind 665 epoch 1874 batch: 600 avg loss -2.902363 avg loss no lamb -2.902363 time 2020-06-27 21:53:15.822775
Model ind 665 epoch 1874 batch: 700 avg loss -2.789885 avg loss no lamb -2.789885 time 2020-06-27 21:53:25.349157
Model ind 665 epoch 1874 batch: 800 avg loss -2.867744 avg loss no lamb -2.867744 time 2020-06-27 21:53:34.918840
last batch sz 10
Pre: time 2020-06-27 21:53:47.635025: 
 	std: 0.0032041413
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9779, 0.9715, 0.98, 0.9737]
	train_accs: [0.98085, 0.9804, 0.9754, 0.98153335, 0.9755833]
	best_train_sub_head: 3
	worst: 0.9715
	avg: 0.97634
	best: 0.98

Starting e_i: 1875
Model ind 665 epoch 1875 batch: 0 avg loss -2.964220 avg loss no lamb -2.964220 time 2020-06-27 21:53:48.883679
Model ind 665 epoch 1875 batch: 100 avg loss -2.888577 avg loss no lamb -2.888577 time 2020-06-27 21:53:58.470071
Model ind 665 epoch 1875 batch: 200 avg loss -2.858916 avg loss no lamb -2.858916 time 2020-06-27 21:54:08.107661
Model ind 665 epoch 1875 batch: 300 avg loss -2.888065 avg loss no lamb -2.888065 time 2020-06-27 21:54:17.572554
Model ind 665 epoch 1875 batch: 400 avg loss -2.788442 avg loss no lamb -2.788442 time 2020-06-27 21:54:27.037734
Model ind 665 epoch 1875 batch: 500 avg loss -2.897476 avg loss no lamb -2.897476 time 2020-06-27 21:54:36.625879
Model ind 665 epoch 1875 batch: 600 avg loss -2.904373 avg loss no lamb -2.904373 time 2020-06-27 21:54:46.169937
Model ind 665 epoch 1875 batch: 700 avg loss -2.824954 avg loss no lamb -2.824954 time 2020-06-27 21:54:55.623115
Model ind 665 epoch 1875 batch: 800 avg loss -2.913290 avg loss no lamb -2.913290 time 2020-06-27 21:55:05.168460
last batch sz 10
Pre: time 2020-06-27 21:55:17.959754: 
 	std: 0.0027744581
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 8), (3, 3), (4, 0), (5, 8), (6, 3), (7, 6), (8, 5), (9, 9), (10, 2), (11, 9), (12, 5), (13, 0), (14, 7), (15, 9), (16, 3), (17, 6), (18, 4), (19, 2), (20, 7), (21, 1), (22, 8), (23, 3), (24, 4)]
	test_accs: [0.9779, 0.9783, 0.9721, 0.9782, 0.9729]
	train_accs: [0.98078334, 0.9809833, 0.97536665, 0.98078334, 0.9757]
	best_train_sub_head: 1
	worst: 0.9721
	avg: 0.97587997
	best: 0.9783

Starting e_i: 1876
Model ind 665 epoch 1876 batch: 0 avg loss -2.951368 avg loss no lamb -2.951368 time 2020-06-27 21:55:19.189739
Model ind 665 epoch 1876 batch: 100 avg loss -2.911952 avg loss no lamb -2.911952 time 2020-06-27 21:55:28.618604
Model ind 665 epoch 1876 batch: 200 avg loss -2.906855 avg loss no lamb -2.906855 time 2020-06-27 21:55:38.054418
Model ind 665 epoch 1876 batch: 300 avg loss -2.845693 avg loss no lamb -2.845693 time 2020-06-27 21:55:47.523416
Model ind 665 epoch 1876 batch: 400 avg loss -2.830854 avg loss no lamb -2.830854 time 2020-06-27 21:55:57.083289
Model ind 665 epoch 1876 batch: 500 avg loss -2.846428 avg loss no lamb -2.846428 time 2020-06-27 21:56:06.761056
Model ind 665 epoch 1876 batch: 600 avg loss -2.875154 avg loss no lamb -2.875154 time 2020-06-27 21:56:20.011413
Model ind 665 epoch 1876 batch: 700 avg loss -2.817749 avg loss no lamb -2.817749 time 2020-06-27 21:56:30.474892
Model ind 665 epoch 1876 batch: 800 avg loss -2.914304 avg loss no lamb -2.914304 time 2020-06-27 21:56:40.471083
last batch sz 10
Pre: time 2020-06-27 21:56:54.515775: 
 	std: 0.0024170987
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9792, 0.9739, 0.979, 0.9747]
	train_accs: [0.9809833, 0.98088336, 0.9758, 0.98123336, 0.9766]
	best_train_sub_head: 3
	worst: 0.9739
	avg: 0.97723997
	best: 0.979

Starting e_i: 1877
Model ind 665 epoch 1877 batch: 0 avg loss -2.945940 avg loss no lamb -2.945940 time 2020-06-27 21:56:55.752450
Model ind 665 epoch 1877 batch: 100 avg loss -2.933641 avg loss no lamb -2.933641 time 2020-06-27 21:57:06.293020
Model ind 665 epoch 1877 batch: 200 avg loss -2.871132 avg loss no lamb -2.871132 time 2020-06-27 21:57:18.041169
Model ind 665 epoch 1877 batch: 300 avg loss -2.859180 avg loss no lamb -2.859180 time 2020-06-27 21:57:30.437439
Model ind 665 epoch 1877 batch: 400 avg loss -2.820060 avg loss no lamb -2.820060 time 2020-06-27 21:57:41.686439
Model ind 665 epoch 1877 batch: 500 avg loss -2.892637 avg loss no lamb -2.892637 time 2020-06-27 21:57:53.313836
Model ind 665 epoch 1877 batch: 600 avg loss -2.868607 avg loss no lamb -2.868607 time 2020-06-27 21:58:04.612737
Model ind 665 epoch 1877 batch: 700 avg loss -2.868453 avg loss no lamb -2.868453 time 2020-06-27 21:58:14.165869
Model ind 665 epoch 1877 batch: 800 avg loss -2.891604 avg loss no lamb -2.891604 time 2020-06-27 21:58:24.861359
last batch sz 10
Pre: time 2020-06-27 21:58:40.704641: 
 	std: 0.0030142323
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9789, 0.9789, 0.9721, 0.9792, 0.9738]
	train_accs: [0.9812833, 0.9808667, 0.97471666, 0.9813, 0.9752333]
	best_train_sub_head: 3
	worst: 0.9721
	avg: 0.97658
	best: 0.9792

Starting e_i: 1878
Model ind 665 epoch 1878 batch: 0 avg loss -2.939101 avg loss no lamb -2.939101 time 2020-06-27 21:58:42.178450
Model ind 665 epoch 1878 batch: 100 avg loss -2.892533 avg loss no lamb -2.892533 time 2020-06-27 21:58:53.457762
Model ind 665 epoch 1878 batch: 200 avg loss -2.903234 avg loss no lamb -2.903234 time 2020-06-27 21:59:03.800527
Model ind 665 epoch 1878 batch: 300 avg loss -2.894794 avg loss no lamb -2.894794 time 2020-06-27 21:59:14.468767
Model ind 665 epoch 1878 batch: 400 avg loss -2.761602 avg loss no lamb -2.761602 time 2020-06-27 21:59:24.983953
Model ind 665 epoch 1878 batch: 500 avg loss -2.884710 avg loss no lamb -2.884710 time 2020-06-27 21:59:34.981558
Model ind 665 epoch 1878 batch: 600 avg loss -2.939139 avg loss no lamb -2.939139 time 2020-06-27 21:59:45.611052
Model ind 665 epoch 1878 batch: 700 avg loss -2.851650 avg loss no lamb -2.851650 time 2020-06-27 21:59:56.051727
Model ind 665 epoch 1878 batch: 800 avg loss -2.857581 avg loss no lamb -2.857581 time 2020-06-27 22:00:06.532604
last batch sz 10
Pre: time 2020-06-27 22:00:20.252503: 
 	std: 0.002828006
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9778, 0.9789, 0.9723, 0.9781, 0.9728]
	train_accs: [0.98048335, 0.98053336, 0.9747667, 0.9806167, 0.9751667]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97598
	best: 0.9781

Starting e_i: 1879
Model ind 665 epoch 1879 batch: 0 avg loss -2.971541 avg loss no lamb -2.971541 time 2020-06-27 22:00:21.803435
Model ind 665 epoch 1879 batch: 100 avg loss -2.944549 avg loss no lamb -2.944549 time 2020-06-27 22:00:32.151802
Model ind 665 epoch 1879 batch: 200 avg loss -2.805420 avg loss no lamb -2.805420 time 2020-06-27 22:00:42.512854
Model ind 665 epoch 1879 batch: 300 avg loss -2.880694 avg loss no lamb -2.880694 time 2020-06-27 22:00:53.160451
Model ind 665 epoch 1879 batch: 400 avg loss -2.725550 avg loss no lamb -2.725550 time 2020-06-27 22:01:03.688424
Model ind 665 epoch 1879 batch: 500 avg loss -2.861676 avg loss no lamb -2.861676 time 2020-06-27 22:01:14.285109
Model ind 665 epoch 1879 batch: 600 avg loss -2.919443 avg loss no lamb -2.919443 time 2020-06-27 22:01:24.530146
Model ind 665 epoch 1879 batch: 700 avg loss -2.846747 avg loss no lamb -2.846747 time 2020-06-27 22:01:34.996568
Model ind 665 epoch 1879 batch: 800 avg loss -2.881004 avg loss no lamb -2.881004 time 2020-06-27 22:01:45.988927
last batch sz 10
Pre: time 2020-06-27 22:01:59.395061: 
 	std: 0.002737893
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9784, 0.9782, 0.9726, 0.9784, 0.9729]
	train_accs: [0.9806, 0.9805833, 0.9753, 0.9808, 0.97576666]
	best_train_sub_head: 3
	worst: 0.9726
	avg: 0.97609997
	best: 0.9784

Starting e_i: 1880
Model ind 665 epoch 1880 batch: 0 avg loss -2.966942 avg loss no lamb -2.966942 time 2020-06-27 22:02:01.085050
Model ind 665 epoch 1880 batch: 100 avg loss -2.963955 avg loss no lamb -2.963955 time 2020-06-27 22:02:12.800615
Model ind 665 epoch 1880 batch: 200 avg loss -2.858138 avg loss no lamb -2.858138 time 2020-06-27 22:02:24.547468
Model ind 665 epoch 1880 batch: 300 avg loss -2.881565 avg loss no lamb -2.881565 time 2020-06-27 22:02:37.031673
Model ind 665 epoch 1880 batch: 400 avg loss -2.777517 avg loss no lamb -2.777517 time 2020-06-27 22:02:48.833112
Model ind 665 epoch 1880 batch: 500 avg loss -2.807616 avg loss no lamb -2.807616 time 2020-06-27 22:03:01.048060
Model ind 665 epoch 1880 batch: 600 avg loss -2.855124 avg loss no lamb -2.855124 time 2020-06-27 22:03:12.899491
Model ind 665 epoch 1880 batch: 700 avg loss -2.815856 avg loss no lamb -2.815856 time 2020-06-27 22:03:24.663907
Model ind 665 epoch 1880 batch: 800 avg loss -2.900046 avg loss no lamb -2.900046 time 2020-06-27 22:03:37.031539
last batch sz 10
Pre: time 2020-06-27 22:03:52.344309: 
 	std: 0.0025568856
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9795, 0.9781, 0.9736, 0.9793, 0.9741]
	train_accs: [0.9811, 0.9799, 0.97571665, 0.98105, 0.9759333]
	best_train_sub_head: 0
	worst: 0.9736
	avg: 0.97692
	best: 0.9795

Starting e_i: 1881
Model ind 665 epoch 1881 batch: 0 avg loss -2.985045 avg loss no lamb -2.985045 time 2020-06-27 22:03:55.028520
Model ind 665 epoch 1881 batch: 100 avg loss -2.901650 avg loss no lamb -2.901650 time 2020-06-27 22:04:06.535018
Model ind 665 epoch 1881 batch: 200 avg loss -2.892000 avg loss no lamb -2.892000 time 2020-06-27 22:04:18.534109
Model ind 665 epoch 1881 batch: 300 avg loss -2.883015 avg loss no lamb -2.883015 time 2020-06-27 22:04:30.332033
Model ind 665 epoch 1881 batch: 400 avg loss -2.765028 avg loss no lamb -2.765028 time 2020-06-27 22:04:40.969238
Model ind 665 epoch 1881 batch: 500 avg loss -2.834439 avg loss no lamb -2.834439 time 2020-06-27 22:04:53.338522
Model ind 665 epoch 1881 batch: 600 avg loss -2.932681 avg loss no lamb -2.932681 time 2020-06-27 22:05:04.126008
Model ind 665 epoch 1881 batch: 700 avg loss -2.759156 avg loss no lamb -2.759156 time 2020-06-27 22:05:15.404234
Model ind 665 epoch 1881 batch: 800 avg loss -2.860592 avg loss no lamb -2.860592 time 2020-06-27 22:05:26.499179
last batch sz 10
Pre: time 2020-06-27 22:05:40.388668: 
 	std: 0.0029583788
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9779, 0.9771, 0.9717, 0.9781, 0.9717]
	train_accs: [0.9803333, 0.97971666, 0.97455, 0.9805833, 0.97515]
	best_train_sub_head: 3
	worst: 0.9717
	avg: 0.9753
	best: 0.9781

Starting e_i: 1882
Model ind 665 epoch 1882 batch: 0 avg loss -2.964570 avg loss no lamb -2.964570 time 2020-06-27 22:05:41.911469
Model ind 665 epoch 1882 batch: 100 avg loss -2.943769 avg loss no lamb -2.943769 time 2020-06-27 22:05:52.589343
Model ind 665 epoch 1882 batch: 200 avg loss -2.906751 avg loss no lamb -2.906751 time 2020-06-27 22:06:02.922572
Model ind 665 epoch 1882 batch: 300 avg loss -2.909867 avg loss no lamb -2.909867 time 2020-06-27 22:06:14.374270
Model ind 665 epoch 1882 batch: 400 avg loss -2.875751 avg loss no lamb -2.875751 time 2020-06-27 22:06:25.536972
Model ind 665 epoch 1882 batch: 500 avg loss -2.866232 avg loss no lamb -2.866232 time 2020-06-27 22:06:37.412993
Model ind 665 epoch 1882 batch: 600 avg loss -2.954741 avg loss no lamb -2.954741 time 2020-06-27 22:06:48.176275
Model ind 665 epoch 1882 batch: 700 avg loss -2.827711 avg loss no lamb -2.827711 time 2020-06-27 22:07:00.977785
Model ind 665 epoch 1882 batch: 800 avg loss -2.889776 avg loss no lamb -2.889776 time 2020-06-27 22:07:12.709254
last batch sz 10
Pre: time 2020-06-27 22:07:27.735029: 
 	std: 0.0024375394
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9791, 0.9789, 0.9738, 0.9789, 0.9742]
	train_accs: [0.9812, 0.98083335, 0.97595, 0.9812667, 0.9759833]
	best_train_sub_head: 3
	worst: 0.9738
	avg: 0.97698003
	best: 0.9789

Starting e_i: 1883
Model ind 665 epoch 1883 batch: 0 avg loss -2.988993 avg loss no lamb -2.988993 time 2020-06-27 22:07:29.112858
Model ind 665 epoch 1883 batch: 100 avg loss -2.920542 avg loss no lamb -2.920542 time 2020-06-27 22:07:40.726436
Model ind 665 epoch 1883 batch: 200 avg loss -2.882589 avg loss no lamb -2.882589 time 2020-06-27 22:07:51.068415
Model ind 665 epoch 1883 batch: 300 avg loss -2.902258 avg loss no lamb -2.902258 time 2020-06-27 22:08:02.424429
Model ind 665 epoch 1883 batch: 400 avg loss -2.785131 avg loss no lamb -2.785131 time 2020-06-27 22:08:13.778488
Model ind 665 epoch 1883 batch: 500 avg loss -2.878073 avg loss no lamb -2.878073 time 2020-06-27 22:08:24.685691
Model ind 665 epoch 1883 batch: 600 avg loss -2.884979 avg loss no lamb -2.884979 time 2020-06-27 22:08:36.196855
Model ind 665 epoch 1883 batch: 700 avg loss -2.764233 avg loss no lamb -2.764233 time 2020-06-27 22:08:46.503779
Model ind 665 epoch 1883 batch: 800 avg loss -2.905503 avg loss no lamb -2.905503 time 2020-06-27 22:08:55.996626
last batch sz 10
Pre: time 2020-06-27 22:09:08.840748: 
 	std: 0.0032725525
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9778, 0.9776, 0.9705, 0.9778, 0.9717]
	train_accs: [0.9802667, 0.98013335, 0.9741, 0.98045, 0.975]
	best_train_sub_head: 3
	worst: 0.9705
	avg: 0.97508
	best: 0.9778

Starting e_i: 1884
Model ind 665 epoch 1884 batch: 0 avg loss -2.961184 avg loss no lamb -2.961184 time 2020-06-27 22:09:10.092252
Model ind 665 epoch 1884 batch: 100 avg loss -2.876685 avg loss no lamb -2.876685 time 2020-06-27 22:09:19.605946
Model ind 665 epoch 1884 batch: 200 avg loss -2.872964 avg loss no lamb -2.872964 time 2020-06-27 22:09:29.137995
Model ind 665 epoch 1884 batch: 300 avg loss -2.899639 avg loss no lamb -2.899639 time 2020-06-27 22:09:38.616902
Model ind 665 epoch 1884 batch: 400 avg loss -2.841753 avg loss no lamb -2.841753 time 2020-06-27 22:09:48.106181
Model ind 665 epoch 1884 batch: 500 avg loss -2.890435 avg loss no lamb -2.890435 time 2020-06-27 22:09:57.560311
Model ind 665 epoch 1884 batch: 600 avg loss -2.932983 avg loss no lamb -2.932983 time 2020-06-27 22:10:06.970276
Model ind 665 epoch 1884 batch: 700 avg loss -2.862281 avg loss no lamb -2.862281 time 2020-06-27 22:10:16.379295
Model ind 665 epoch 1884 batch: 800 avg loss -2.901903 avg loss no lamb -2.901903 time 2020-06-27 22:10:25.797235
last batch sz 10
Pre: time 2020-06-27 22:10:38.363924: 
 	std: 0.0031409455
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9802, 0.979, 0.9724, 0.9798, 0.9745]
	train_accs: [0.98113334, 0.98055, 0.97508335, 0.9809333, 0.9759]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97718
	best: 0.9802

Starting e_i: 1885
Model ind 665 epoch 1885 batch: 0 avg loss -2.973020 avg loss no lamb -2.973020 time 2020-06-27 22:10:39.808305
Model ind 665 epoch 1885 batch: 100 avg loss -2.939967 avg loss no lamb -2.939967 time 2020-06-27 22:10:49.331619
Model ind 665 epoch 1885 batch: 200 avg loss -2.903176 avg loss no lamb -2.903176 time 2020-06-27 22:10:58.849477
Model ind 665 epoch 1885 batch: 300 avg loss -2.887197 avg loss no lamb -2.887197 time 2020-06-27 22:11:08.491532
Model ind 665 epoch 1885 batch: 400 avg loss -2.855958 avg loss no lamb -2.855958 time 2020-06-27 22:11:18.038659
Model ind 665 epoch 1885 batch: 500 avg loss -2.889084 avg loss no lamb -2.889084 time 2020-06-27 22:11:27.638355
Model ind 665 epoch 1885 batch: 600 avg loss -2.931338 avg loss no lamb -2.931338 time 2020-06-27 22:11:37.307008
Model ind 665 epoch 1885 batch: 700 avg loss -2.886503 avg loss no lamb -2.886503 time 2020-06-27 22:11:46.951789
Model ind 665 epoch 1885 batch: 800 avg loss -2.876224 avg loss no lamb -2.876224 time 2020-06-27 22:11:56.647276
last batch sz 10
Pre: time 2020-06-27 22:12:09.225331: 
 	std: 0.0028951052
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.9793, 0.973, 0.9795, 0.9746]
	train_accs: [0.98121667, 0.9807, 0.97576666, 0.98116666, 0.9763]
	best_train_sub_head: 0
	worst: 0.973
	avg: 0.97728
	best: 0.98

Starting e_i: 1886
Model ind 665 epoch 1886 batch: 0 avg loss -2.934008 avg loss no lamb -2.934008 time 2020-06-27 22:12:10.458761
Model ind 665 epoch 1886 batch: 100 avg loss -2.903363 avg loss no lamb -2.903363 time 2020-06-27 22:12:19.842139
Model ind 665 epoch 1886 batch: 200 avg loss -2.885280 avg loss no lamb -2.885280 time 2020-06-27 22:12:29.406024
Model ind 665 epoch 1886 batch: 300 avg loss -2.847047 avg loss no lamb -2.847047 time 2020-06-27 22:12:38.914069
Model ind 665 epoch 1886 batch: 400 avg loss -2.824931 avg loss no lamb -2.824931 time 2020-06-27 22:12:48.470002
Model ind 665 epoch 1886 batch: 500 avg loss -2.874504 avg loss no lamb -2.874504 time 2020-06-27 22:12:58.066634
Model ind 665 epoch 1886 batch: 600 avg loss -2.921295 avg loss no lamb -2.921295 time 2020-06-27 22:13:07.651124
Model ind 665 epoch 1886 batch: 700 avg loss -2.848135 avg loss no lamb -2.848135 time 2020-06-27 22:13:17.230606
Model ind 665 epoch 1886 batch: 800 avg loss -2.863665 avg loss no lamb -2.863665 time 2020-06-27 22:13:26.877974
last batch sz 10
Pre: time 2020-06-27 22:13:39.657086: 
 	std: 0.002582257
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9797, 0.9792, 0.974, 0.979, 0.9741]
	train_accs: [0.9812667, 0.98053336, 0.97578335, 0.9809, 0.9762833]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97720003
	best: 0.9797

Starting e_i: 1887
Model ind 665 epoch 1887 batch: 0 avg loss -2.960528 avg loss no lamb -2.960528 time 2020-06-27 22:13:41.061463
Model ind 665 epoch 1887 batch: 100 avg loss -2.882149 avg loss no lamb -2.882149 time 2020-06-27 22:13:54.450469
Model ind 665 epoch 1887 batch: 200 avg loss -2.875586 avg loss no lamb -2.875586 time 2020-06-27 22:14:04.398173
Model ind 665 epoch 1887 batch: 300 avg loss -2.889252 avg loss no lamb -2.889252 time 2020-06-27 22:14:14.127786
Model ind 665 epoch 1887 batch: 400 avg loss -2.796976 avg loss no lamb -2.796976 time 2020-06-27 22:14:23.960813
Model ind 665 epoch 1887 batch: 500 avg loss -2.908221 avg loss no lamb -2.908221 time 2020-06-27 22:14:33.746621
Model ind 665 epoch 1887 batch: 600 avg loss -2.922994 avg loss no lamb -2.922994 time 2020-06-27 22:14:43.491074
Model ind 665 epoch 1887 batch: 700 avg loss -2.859875 avg loss no lamb -2.859875 time 2020-06-27 22:14:53.261022
Model ind 665 epoch 1887 batch: 800 avg loss -2.883490 avg loss no lamb -2.883490 time 2020-06-27 22:15:03.055380
last batch sz 10
Pre: time 2020-06-27 22:15:16.232414: 
 	std: 0.0030077205
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9792, 0.978, 0.9724, 0.9791, 0.973]
	train_accs: [0.9805833, 0.9803, 0.9745333, 0.98045, 0.97515]
	best_train_sub_head: 0
	worst: 0.9724
	avg: 0.97634
	best: 0.9792

Starting e_i: 1888
Model ind 665 epoch 1888 batch: 0 avg loss -2.965372 avg loss no lamb -2.965372 time 2020-06-27 22:15:17.562998
Model ind 665 epoch 1888 batch: 100 avg loss -2.916849 avg loss no lamb -2.916849 time 2020-06-27 22:15:27.280396
Model ind 665 epoch 1888 batch: 200 avg loss -2.891319 avg loss no lamb -2.891319 time 2020-06-27 22:15:36.934057
Model ind 665 epoch 1888 batch: 300 avg loss -2.837687 avg loss no lamb -2.837687 time 2020-06-27 22:15:46.626769
Model ind 665 epoch 1888 batch: 400 avg loss -2.830276 avg loss no lamb -2.830276 time 2020-06-27 22:15:56.357556
Model ind 665 epoch 1888 batch: 500 avg loss -2.856292 avg loss no lamb -2.856292 time 2020-06-27 22:16:06.104067
Model ind 665 epoch 1888 batch: 600 avg loss -2.899894 avg loss no lamb -2.899894 time 2020-06-27 22:16:15.672342
Model ind 665 epoch 1888 batch: 700 avg loss -2.761623 avg loss no lamb -2.761623 time 2020-06-27 22:16:25.533751
Model ind 665 epoch 1888 batch: 800 avg loss -2.942182 avg loss no lamb -2.942182 time 2020-06-27 22:16:35.311204
last batch sz 10
Pre: time 2020-06-27 22:16:48.143457: 
 	std: 0.0031949994
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9788, 0.9787, 0.9719, 0.9789, 0.9727]
	train_accs: [0.9809167, 0.98025, 0.9752667, 0.9809167, 0.97603333]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.9762
	best: 0.9788

Starting e_i: 1889
Model ind 665 epoch 1889 batch: 0 avg loss -2.966654 avg loss no lamb -2.966654 time 2020-06-27 22:16:49.491422
Model ind 665 epoch 1889 batch: 100 avg loss -2.877974 avg loss no lamb -2.877974 time 2020-06-27 22:16:59.173938
Model ind 665 epoch 1889 batch: 200 avg loss -2.894990 avg loss no lamb -2.894990 time 2020-06-27 22:17:08.918438
Model ind 665 epoch 1889 batch: 300 avg loss -2.912951 avg loss no lamb -2.912951 time 2020-06-27 22:17:18.777976
Model ind 665 epoch 1889 batch: 400 avg loss -2.804040 avg loss no lamb -2.804040 time 2020-06-27 22:17:28.636526
Model ind 665 epoch 1889 batch: 500 avg loss -2.895249 avg loss no lamb -2.895249 time 2020-06-27 22:17:38.548484
Model ind 665 epoch 1889 batch: 600 avg loss -2.906972 avg loss no lamb -2.906972 time 2020-06-27 22:17:48.407869
Model ind 665 epoch 1889 batch: 700 avg loss -2.787474 avg loss no lamb -2.787474 time 2020-06-27 22:17:58.161532
Model ind 665 epoch 1889 batch: 800 avg loss -2.900613 avg loss no lamb -2.900613 time 2020-06-27 22:18:07.950435
last batch sz 10
Pre: time 2020-06-27 22:18:21.320687: 
 	std: 0.0029904032
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.98, 0.98, 0.974, 0.9803, 0.974]
	train_accs: [0.98141664, 0.98115, 0.97595, 0.9814, 0.9759333]
	best_train_sub_head: 0
	worst: 0.974
	avg: 0.97766
	best: 0.98

Starting e_i: 1890
Model ind 665 epoch 1890 batch: 0 avg loss -2.978755 avg loss no lamb -2.978755 time 2020-06-27 22:18:22.607401
Model ind 665 epoch 1890 batch: 100 avg loss -2.949619 avg loss no lamb -2.949619 time 2020-06-27 22:18:32.416771
Model ind 665 epoch 1890 batch: 200 avg loss -2.908505 avg loss no lamb -2.908505 time 2020-06-27 22:18:42.223736
Model ind 665 epoch 1890 batch: 300 avg loss -2.895555 avg loss no lamb -2.895555 time 2020-06-27 22:18:53.076610
Model ind 665 epoch 1890 batch: 400 avg loss -2.801341 avg loss no lamb -2.801341 time 2020-06-27 22:19:02.507938
Model ind 665 epoch 1890 batch: 500 avg loss -2.811546 avg loss no lamb -2.811546 time 2020-06-27 22:19:11.963213
Model ind 665 epoch 1890 batch: 600 avg loss -2.913180 avg loss no lamb -2.913180 time 2020-06-27 22:19:21.434230
Model ind 665 epoch 1890 batch: 700 avg loss -2.809505 avg loss no lamb -2.809505 time 2020-06-27 22:19:31.010497
Model ind 665 epoch 1890 batch: 800 avg loss -2.899715 avg loss no lamb -2.899715 time 2020-06-27 22:19:40.624339
last batch sz 10
Pre: time 2020-06-27 22:19:53.438601: 
 	std: 0.003286694
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9787, 0.9781, 0.9716, 0.9788, 0.9721]
	train_accs: [0.9801667, 0.9797, 0.97403336, 0.98071665, 0.97461665]
	best_train_sub_head: 3
	worst: 0.9716
	avg: 0.97586
	best: 0.9788

Starting e_i: 1891
Model ind 665 epoch 1891 batch: 0 avg loss -2.887155 avg loss no lamb -2.887155 time 2020-06-27 22:19:55.871869
Model ind 665 epoch 1891 batch: 100 avg loss -2.962417 avg loss no lamb -2.962417 time 2020-06-27 22:20:05.644893
Model ind 665 epoch 1891 batch: 200 avg loss -2.943422 avg loss no lamb -2.943422 time 2020-06-27 22:20:15.055025
Model ind 665 epoch 1891 batch: 300 avg loss -2.813794 avg loss no lamb -2.813794 time 2020-06-27 22:20:24.631659
Model ind 665 epoch 1891 batch: 400 avg loss -2.857023 avg loss no lamb -2.857023 time 2020-06-27 22:20:34.164576
Model ind 665 epoch 1891 batch: 500 avg loss -2.874930 avg loss no lamb -2.874930 time 2020-06-27 22:20:43.782460
Model ind 665 epoch 1891 batch: 600 avg loss -2.931452 avg loss no lamb -2.931452 time 2020-06-27 22:20:53.364301
Model ind 665 epoch 1891 batch: 700 avg loss -2.799869 avg loss no lamb -2.799869 time 2020-06-27 22:21:02.954037
Model ind 665 epoch 1891 batch: 800 avg loss -2.866097 avg loss no lamb -2.866097 time 2020-06-27 22:21:12.556583
last batch sz 10
Pre: time 2020-06-27 22:21:25.078332: 
 	std: 0.003307337
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9796, 0.9727, 0.9796, 0.9733]
	train_accs: [0.98146665, 0.9809167, 0.97533333, 0.98148334, 0.9758]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97704
	best: 0.9796

Starting e_i: 1892
Model ind 665 epoch 1892 batch: 0 avg loss -2.935361 avg loss no lamb -2.935361 time 2020-06-27 22:21:26.550155
Model ind 665 epoch 1892 batch: 100 avg loss -2.947460 avg loss no lamb -2.947460 time 2020-06-27 22:21:36.041977
Model ind 665 epoch 1892 batch: 200 avg loss -2.902523 avg loss no lamb -2.902523 time 2020-06-27 22:21:45.484765
Model ind 665 epoch 1892 batch: 300 avg loss -2.868884 avg loss no lamb -2.868884 time 2020-06-27 22:21:54.994200
Model ind 665 epoch 1892 batch: 400 avg loss -2.847104 avg loss no lamb -2.847104 time 2020-06-27 22:22:04.600628
Model ind 665 epoch 1892 batch: 500 avg loss -2.844738 avg loss no lamb -2.844738 time 2020-06-27 22:22:14.113190
Model ind 665 epoch 1892 batch: 600 avg loss -2.921446 avg loss no lamb -2.921446 time 2020-06-27 22:22:23.571424
Model ind 665 epoch 1892 batch: 700 avg loss -2.809871 avg loss no lamb -2.809871 time 2020-06-27 22:22:33.053429
Model ind 665 epoch 1892 batch: 800 avg loss -2.936530 avg loss no lamb -2.936530 time 2020-06-27 22:22:42.511906
last batch sz 10
Pre: time 2020-06-27 22:22:55.080480: 
 	std: 0.0027359938
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.9796, 0.9733, 0.9786, 0.9735]
	train_accs: [0.981, 0.98066664, 0.9752167, 0.98121667, 0.97546667]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97672004
	best: 0.9786

Starting e_i: 1893
Model ind 665 epoch 1893 batch: 0 avg loss -2.968982 avg loss no lamb -2.968982 time 2020-06-27 22:22:56.324102
Model ind 665 epoch 1893 batch: 100 avg loss -2.978128 avg loss no lamb -2.978128 time 2020-06-27 22:23:05.681979
Model ind 665 epoch 1893 batch: 200 avg loss -2.925233 avg loss no lamb -2.925233 time 2020-06-27 22:23:15.101791
Model ind 665 epoch 1893 batch: 300 avg loss -2.885971 avg loss no lamb -2.885971 time 2020-06-27 22:23:24.497428
Model ind 665 epoch 1893 batch: 400 avg loss -2.910343 avg loss no lamb -2.910343 time 2020-06-27 22:23:33.938819
Model ind 665 epoch 1893 batch: 500 avg loss -2.856949 avg loss no lamb -2.856949 time 2020-06-27 22:23:43.430868
Model ind 665 epoch 1893 batch: 600 avg loss -2.928671 avg loss no lamb -2.928671 time 2020-06-27 22:23:53.936577
Model ind 665 epoch 1893 batch: 700 avg loss -2.841104 avg loss no lamb -2.841104 time 2020-06-27 22:24:05.862000
Model ind 665 epoch 1893 batch: 800 avg loss -2.875202 avg loss no lamb -2.875202 time 2020-06-27 22:24:15.523856
last batch sz 10
Pre: time 2020-06-27 22:24:28.555469: 
 	std: 0.003596887
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9808, 0.979, 0.9724, 0.9815, 0.9744]
	train_accs: [0.98141664, 0.9802833, 0.9751, 0.98153335, 0.9765]
	best_train_sub_head: 3
	worst: 0.9724
	avg: 0.97762
	best: 0.9815

Starting e_i: 1894
Model ind 665 epoch 1894 batch: 0 avg loss -2.941742 avg loss no lamb -2.941742 time 2020-06-27 22:24:29.867528
Model ind 665 epoch 1894 batch: 100 avg loss -2.915031 avg loss no lamb -2.915031 time 2020-06-27 22:24:39.621397
Model ind 665 epoch 1894 batch: 200 avg loss -2.884474 avg loss no lamb -2.884474 time 2020-06-27 22:24:50.928922
Model ind 665 epoch 1894 batch: 300 avg loss -2.899633 avg loss no lamb -2.899633 time 2020-06-27 22:25:02.474728
Model ind 665 epoch 1894 batch: 400 avg loss -2.784225 avg loss no lamb -2.784225 time 2020-06-27 22:25:13.130216
Model ind 665 epoch 1894 batch: 500 avg loss -2.842191 avg loss no lamb -2.842191 time 2020-06-27 22:25:25.927163
Model ind 665 epoch 1894 batch: 600 avg loss -2.903801 avg loss no lamb -2.903801 time 2020-06-27 22:25:39.495493
Model ind 665 epoch 1894 batch: 700 avg loss -2.845660 avg loss no lamb -2.845660 time 2020-06-27 22:25:53.029008
Model ind 665 epoch 1894 batch: 800 avg loss -2.948951 avg loss no lamb -2.948951 time 2020-06-27 22:26:06.507435
last batch sz 10
Pre: time 2020-06-27 22:26:23.802727: 
 	std: 0.0036602872
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9797, 0.9723, 0.9809, 0.9736]
	train_accs: [0.98095, 0.98045, 0.9747667, 0.9810167, 0.97575]
	best_train_sub_head: 3
	worst: 0.9723
	avg: 0.97738
	best: 0.9809

Starting e_i: 1895
Model ind 665 epoch 1895 batch: 0 avg loss -2.959595 avg loss no lamb -2.959595 time 2020-06-27 22:26:25.481816
Model ind 665 epoch 1895 batch: 100 avg loss -2.908264 avg loss no lamb -2.908264 time 2020-06-27 22:26:38.863705
Model ind 665 epoch 1895 batch: 200 avg loss -2.930389 avg loss no lamb -2.930389 time 2020-06-27 22:26:52.212097
Model ind 665 epoch 1895 batch: 300 avg loss -2.891116 avg loss no lamb -2.891116 time 2020-06-27 22:27:05.624199
Model ind 665 epoch 1895 batch: 400 avg loss -2.814448 avg loss no lamb -2.814448 time 2020-06-27 22:27:19.094038
Model ind 665 epoch 1895 batch: 500 avg loss -2.868827 avg loss no lamb -2.868827 time 2020-06-27 22:27:32.378050
Model ind 665 epoch 1895 batch: 600 avg loss -2.894123 avg loss no lamb -2.894123 time 2020-06-27 22:27:45.868652
Model ind 665 epoch 1895 batch: 700 avg loss -2.831460 avg loss no lamb -2.831460 time 2020-06-27 22:27:59.295978
Model ind 665 epoch 1895 batch: 800 avg loss -2.854165 avg loss no lamb -2.854165 time 2020-06-27 22:28:12.909083
last batch sz 10
Pre: time 2020-06-27 22:28:29.817710: 
 	std: 0.0034347607
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9798, 0.9779, 0.9724, 0.9794, 0.9719]
	train_accs: [0.9806167, 0.97938335, 0.97433335, 0.9802833, 0.97436666]
	best_train_sub_head: 0
	worst: 0.9719
	avg: 0.97628003
	best: 0.9798

Starting e_i: 1896
Model ind 665 epoch 1896 batch: 0 avg loss -2.995171 avg loss no lamb -2.995171 time 2020-06-27 22:28:31.518522
Model ind 665 epoch 1896 batch: 100 avg loss -2.964643 avg loss no lamb -2.964643 time 2020-06-27 22:28:44.994213
Model ind 665 epoch 1896 batch: 200 avg loss -2.889607 avg loss no lamb -2.889607 time 2020-06-27 22:28:58.419356
Model ind 665 epoch 1896 batch: 300 avg loss -2.908935 avg loss no lamb -2.908935 time 2020-06-27 22:29:11.966641
Model ind 665 epoch 1896 batch: 400 avg loss -2.804070 avg loss no lamb -2.804070 time 2020-06-27 22:29:22.275397
Model ind 665 epoch 1896 batch: 500 avg loss -2.871699 avg loss no lamb -2.871699 time 2020-06-27 22:29:31.844624
Model ind 665 epoch 1896 batch: 600 avg loss -2.887917 avg loss no lamb -2.887917 time 2020-06-27 22:29:41.296897
Model ind 665 epoch 1896 batch: 700 avg loss -2.821939 avg loss no lamb -2.821939 time 2020-06-27 22:29:50.714875
Model ind 665 epoch 1896 batch: 800 avg loss -2.841895 avg loss no lamb -2.841895 time 2020-06-27 22:30:00.310721
last batch sz 10
Pre: time 2020-06-27 22:30:13.027062: 
 	std: 0.0035400493
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9787, 0.978, 0.9709, 0.979, 0.9719]
	train_accs: [0.98071665, 0.98006666, 0.9744, 0.98066664, 0.9751833]
	best_train_sub_head: 0
	worst: 0.9709
	avg: 0.9757
	best: 0.9787

Starting e_i: 1897
Model ind 665 epoch 1897 batch: 0 avg loss -2.992753 avg loss no lamb -2.992753 time 2020-06-27 22:30:14.663202
Model ind 665 epoch 1897 batch: 100 avg loss -2.925458 avg loss no lamb -2.925458 time 2020-06-27 22:30:25.586559
Model ind 665 epoch 1897 batch: 200 avg loss -2.865292 avg loss no lamb -2.865292 time 2020-06-27 22:30:36.144138
Model ind 665 epoch 1897 batch: 300 avg loss -2.906237 avg loss no lamb -2.906237 time 2020-06-27 22:30:46.350791
Model ind 665 epoch 1897 batch: 400 avg loss -2.823859 avg loss no lamb -2.823859 time 2020-06-27 22:30:56.958197
Model ind 665 epoch 1897 batch: 500 avg loss -2.875332 avg loss no lamb -2.875332 time 2020-06-27 22:31:09.438836
Model ind 665 epoch 1897 batch: 600 avg loss -2.910765 avg loss no lamb -2.910765 time 2020-06-27 22:31:22.773601
Model ind 665 epoch 1897 batch: 700 avg loss -2.842731 avg loss no lamb -2.842731 time 2020-06-27 22:31:35.030267
Model ind 665 epoch 1897 batch: 800 avg loss -2.938620 avg loss no lamb -2.938620 time 2020-06-27 22:31:46.234575
last batch sz 10
Pre: time 2020-06-27 22:32:00.022326: 
 	std: 0.0028869393
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9799, 0.9789, 0.9735, 0.9797, 0.9738]
	train_accs: [0.98095, 0.9802167, 0.9746, 0.98088336, 0.97533333]
	best_train_sub_head: 0
	worst: 0.9735
	avg: 0.9771601
	best: 0.9799

Starting e_i: 1898
Model ind 665 epoch 1898 batch: 0 avg loss -2.933877 avg loss no lamb -2.933877 time 2020-06-27 22:32:01.303587
Model ind 665 epoch 1898 batch: 100 avg loss -2.946015 avg loss no lamb -2.946015 time 2020-06-27 22:32:11.789066
Model ind 665 epoch 1898 batch: 200 avg loss -2.860500 avg loss no lamb -2.860500 time 2020-06-27 22:32:23.484367
Model ind 665 epoch 1898 batch: 300 avg loss -2.895570 avg loss no lamb -2.895570 time 2020-06-27 22:32:36.749693
Model ind 665 epoch 1898 batch: 400 avg loss -2.798570 avg loss no lamb -2.798570 time 2020-06-27 22:32:47.607447
Model ind 665 epoch 1898 batch: 500 avg loss -2.901862 avg loss no lamb -2.901862 time 2020-06-27 22:32:58.255929
Model ind 665 epoch 1898 batch: 600 avg loss -2.880465 avg loss no lamb -2.880465 time 2020-06-27 22:33:09.264953
Model ind 665 epoch 1898 batch: 700 avg loss -2.792245 avg loss no lamb -2.792245 time 2020-06-27 22:33:19.946120
Model ind 665 epoch 1898 batch: 800 avg loss -2.857258 avg loss no lamb -2.857258 time 2020-06-27 22:33:30.619384
last batch sz 10
Pre: time 2020-06-27 22:33:45.153741: 
 	std: 0.0028411336
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9804, 0.9794, 0.9745, 0.9802, 0.974]
	train_accs: [0.98075, 0.9802333, 0.97525, 0.98083335, 0.97606665]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97770005
	best: 0.9802

Starting e_i: 1899
Model ind 665 epoch 1899 batch: 0 avg loss -3.000324 avg loss no lamb -3.000324 time 2020-06-27 22:33:46.549810
Model ind 665 epoch 1899 batch: 100 avg loss -2.887680 avg loss no lamb -2.887680 time 2020-06-27 22:33:57.696261
Model ind 665 epoch 1899 batch: 200 avg loss -2.871977 avg loss no lamb -2.871977 time 2020-06-27 22:34:08.774385
Model ind 665 epoch 1899 batch: 300 avg loss -2.867475 avg loss no lamb -2.867475 time 2020-06-27 22:34:19.708173
Model ind 665 epoch 1899 batch: 400 avg loss -2.809252 avg loss no lamb -2.809252 time 2020-06-27 22:34:30.437750
Model ind 665 epoch 1899 batch: 500 avg loss -2.817497 avg loss no lamb -2.817497 time 2020-06-27 22:34:40.664944
Model ind 665 epoch 1899 batch: 600 avg loss -2.862891 avg loss no lamb -2.862891 time 2020-06-27 22:34:52.102441
Model ind 665 epoch 1899 batch: 700 avg loss -2.738841 avg loss no lamb -2.738841 time 2020-06-27 22:35:03.404404
Model ind 665 epoch 1899 batch: 800 avg loss -2.867725 avg loss no lamb -2.867725 time 2020-06-27 22:35:14.767532
last batch sz 10
Pre: time 2020-06-27 22:35:29.019711: 
 	std: 0.003063596
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9798, 0.9792, 0.9734, 0.9804, 0.9738]
	train_accs: [0.98083335, 0.98005, 0.97425, 0.9810167, 0.97503334]
	best_train_sub_head: 3
	worst: 0.9734
	avg: 0.97732
	best: 0.9804

Starting e_i: 1900
Model ind 665 epoch 1900 batch: 0 avg loss -2.986347 avg loss no lamb -2.986347 time 2020-06-27 22:35:30.347953
Model ind 665 epoch 1900 batch: 100 avg loss -2.957188 avg loss no lamb -2.957188 time 2020-06-27 22:35:41.904331
Model ind 665 epoch 1900 batch: 200 avg loss -2.879169 avg loss no lamb -2.879169 time 2020-06-27 22:35:53.486358
Model ind 665 epoch 1900 batch: 300 avg loss -2.901322 avg loss no lamb -2.901322 time 2020-06-27 22:36:03.954226
Model ind 665 epoch 1900 batch: 400 avg loss -2.820575 avg loss no lamb -2.820575 time 2020-06-27 22:36:14.828035
Model ind 665 epoch 1900 batch: 500 avg loss -2.873848 avg loss no lamb -2.873848 time 2020-06-27 22:36:25.146154
Model ind 665 epoch 1900 batch: 600 avg loss -2.858065 avg loss no lamb -2.858065 time 2020-06-27 22:36:36.651808
Model ind 665 epoch 1900 batch: 700 avg loss -2.771267 avg loss no lamb -2.771267 time 2020-06-27 22:36:46.844660
Model ind 665 epoch 1900 batch: 800 avg loss -2.842943 avg loss no lamb -2.842943 time 2020-06-27 22:36:58.713278
last batch sz 10
Pre: time 2020-06-27 22:37:13.695694: 
 	std: 0.0021350503
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9786, 0.979, 0.9747, 0.9784, 0.974]
	train_accs: [0.9802667, 0.97975, 0.9752833, 0.98038334, 0.9758]
	best_train_sub_head: 3
	worst: 0.974
	avg: 0.97694
	best: 0.9784

Starting e_i: 1901
Model ind 665 epoch 1901 batch: 0 avg loss -2.948334 avg loss no lamb -2.948334 time 2020-06-27 22:37:16.421069
Model ind 665 epoch 1901 batch: 100 avg loss -2.916106 avg loss no lamb -2.916106 time 2020-06-27 22:37:27.842662
Model ind 665 epoch 1901 batch: 200 avg loss -2.922600 avg loss no lamb -2.922600 time 2020-06-27 22:37:37.871012
Model ind 665 epoch 1901 batch: 300 avg loss -2.907671 avg loss no lamb -2.907671 time 2020-06-27 22:37:49.476746
Model ind 665 epoch 1901 batch: 400 avg loss -2.841599 avg loss no lamb -2.841599 time 2020-06-27 22:38:00.426591
Model ind 665 epoch 1901 batch: 500 avg loss -2.877831 avg loss no lamb -2.877831 time 2020-06-27 22:38:11.807591
Model ind 665 epoch 1901 batch: 600 avg loss -2.890114 avg loss no lamb -2.890114 time 2020-06-27 22:38:22.425691
Model ind 665 epoch 1901 batch: 700 avg loss -2.813126 avg loss no lamb -2.813126 time 2020-06-27 22:38:32.074314
Model ind 665 epoch 1901 batch: 800 avg loss -2.888829 avg loss no lamb -2.888829 time 2020-06-27 22:38:43.848476
last batch sz 10
Pre: time 2020-06-27 22:38:58.221307: 
 	std: 0.0036701008
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9793, 0.9795, 0.9708, 0.9794, 0.9734]
	train_accs: [0.98041666, 0.98015, 0.9733667, 0.98075, 0.975]
	best_train_sub_head: 3
	worst: 0.9708
	avg: 0.97648
	best: 0.9794

Starting e_i: 1902
Model ind 665 epoch 1902 batch: 0 avg loss -2.960921 avg loss no lamb -2.960921 time 2020-06-27 22:39:00.049046
Model ind 665 epoch 1902 batch: 100 avg loss -2.958349 avg loss no lamb -2.958349 time 2020-06-27 22:39:11.850335
Model ind 665 epoch 1902 batch: 200 avg loss -2.851202 avg loss no lamb -2.851202 time 2020-06-27 22:39:23.655024
Model ind 665 epoch 1902 batch: 300 avg loss -2.902948 avg loss no lamb -2.902948 time 2020-06-27 22:39:34.799196
Model ind 665 epoch 1902 batch: 400 avg loss -2.823354 avg loss no lamb -2.823354 time 2020-06-27 22:39:45.996681
Model ind 665 epoch 1902 batch: 500 avg loss -2.851785 avg loss no lamb -2.851785 time 2020-06-27 22:39:57.289231
Model ind 665 epoch 1902 batch: 600 avg loss -2.896382 avg loss no lamb -2.896382 time 2020-06-27 22:40:08.860874
Model ind 665 epoch 1902 batch: 700 avg loss -2.811875 avg loss no lamb -2.811875 time 2020-06-27 22:40:20.508631
Model ind 665 epoch 1902 batch: 800 avg loss -2.848266 avg loss no lamb -2.848266 time 2020-06-27 22:40:33.247389
last batch sz 10
Pre: time 2020-06-27 22:40:46.111472: 
 	std: 0.0023983128
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.9783, 0.9739, 0.9794, 0.9743]
	train_accs: [0.98071665, 0.98015, 0.97543335, 0.9805, 0.9759833]
	best_train_sub_head: 0
	worst: 0.9739
	avg: 0.97699994
	best: 0.9791

Starting e_i: 1903
Model ind 665 epoch 1903 batch: 0 avg loss -2.958851 avg loss no lamb -2.958851 time 2020-06-27 22:40:47.554613
Model ind 665 epoch 1903 batch: 100 avg loss -2.919918 avg loss no lamb -2.919918 time 2020-06-27 22:40:57.650656
Model ind 665 epoch 1903 batch: 200 avg loss -2.847563 avg loss no lamb -2.847563 time 2020-06-27 22:41:08.404482
Model ind 665 epoch 1903 batch: 300 avg loss -2.915226 avg loss no lamb -2.915226 time 2020-06-27 22:41:20.000809
Model ind 665 epoch 1903 batch: 400 avg loss -2.844748 avg loss no lamb -2.844748 time 2020-06-27 22:41:31.287205
Model ind 665 epoch 1903 batch: 500 avg loss -2.873965 avg loss no lamb -2.873965 time 2020-06-27 22:41:44.388389
Model ind 665 epoch 1903 batch: 600 avg loss -2.885375 avg loss no lamb -2.885375 time 2020-06-27 22:41:57.687331
Model ind 665 epoch 1903 batch: 700 avg loss -2.754027 avg loss no lamb -2.754027 time 2020-06-27 22:42:10.229576
Model ind 665 epoch 1903 batch: 800 avg loss -2.928451 avg loss no lamb -2.928451 time 2020-06-27 22:42:20.967989
last batch sz 10
Pre: time 2020-06-27 22:42:35.712482: 
 	std: 0.0029226139
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.9794, 0.9791, 0.9733, 0.9797, 0.9736]
	train_accs: [0.98085, 0.98008335, 0.9752667, 0.98106664, 0.97565]
	best_train_sub_head: 3
	worst: 0.9733
	avg: 0.97701997
	best: 0.9797

Starting e_i: 1904
Model ind 665 epoch 1904 batch: 0 avg loss -2.992931 avg loss no lamb -2.992931 time 2020-06-27 22:42:37.144634
Model ind 665 epoch 1904 batch: 100 avg loss -2.925210 avg loss no lamb -2.925210 time 2020-06-27 22:42:48.185563
Model ind 665 epoch 1904 batch: 200 avg loss -2.885386 avg loss no lamb -2.885386 time 2020-06-27 22:42:59.465985
Model ind 665 epoch 1904 batch: 300 avg loss -2.919591 avg loss no lamb -2.919591 time 2020-06-27 22:43:10.086131
Model ind 665 epoch 1904 batch: 400 avg loss -2.825019 avg loss no lamb -2.825019 time 2020-06-27 22:43:20.402260
Model ind 665 epoch 1904 batch: 500 avg loss -2.869578 avg loss no lamb -2.869578 time 2020-06-27 22:43:32.165286
Model ind 665 epoch 1904 batch: 600 avg loss -2.930777 avg loss no lamb -2.930777 time 2020-06-27 22:43:42.857647
Model ind 665 epoch 1904 batch: 700 avg loss -2.843064 avg loss no lamb -2.843064 time 2020-06-27 22:43:53.948278
Model ind 665 epoch 1904 batch: 800 avg loss -2.870017 avg loss no lamb -2.870017 time 2020-06-27 22:44:05.296542
last batch sz 10
Pre: time 2020-06-27 22:44:18.475602: 
 	std: 0.0041777464
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9791, 0.9781, 0.9696, 0.9788, 0.9708]
	train_accs: [0.9802, 0.97971666, 0.9734667, 0.9801, 0.97443336]
	best_train_sub_head: 0
	worst: 0.9696
	avg: 0.97528
	best: 0.9791

Starting e_i: 1905
Model ind 665 epoch 1905 batch: 0 avg loss -2.998471 avg loss no lamb -2.998471 time 2020-06-27 22:44:19.731158
Model ind 665 epoch 1905 batch: 100 avg loss -2.992214 avg loss no lamb -2.992214 time 2020-06-27 22:44:29.333638
Model ind 665 epoch 1905 batch: 200 avg loss -2.897922 avg loss no lamb -2.897922 time 2020-06-27 22:44:39.846884
Model ind 665 epoch 1905 batch: 300 avg loss -2.845168 avg loss no lamb -2.845168 time 2020-06-27 22:44:50.412377
Model ind 665 epoch 1905 batch: 400 avg loss -2.767905 avg loss no lamb -2.767905 time 2020-06-27 22:45:01.020882
Model ind 665 epoch 1905 batch: 500 avg loss -2.888101 avg loss no lamb -2.888101 time 2020-06-27 22:45:11.538526
Model ind 665 epoch 1905 batch: 600 avg loss -2.889569 avg loss no lamb -2.889569 time 2020-06-27 22:45:21.655819
Model ind 665 epoch 1905 batch: 700 avg loss -2.836852 avg loss no lamb -2.836852 time 2020-06-27 22:45:31.771103
Model ind 665 epoch 1905 batch: 800 avg loss -2.920655 avg loss no lamb -2.920655 time 2020-06-27 22:45:41.623908
last batch sz 10
Pre: time 2020-06-27 22:45:54.581275: 
 	std: 0.0032769504
	best_train_sub_head_match: [(0, 9), (1, 4), (2, 0), (3, 3), (4, 5), (5, 2), (6, 5), (7, 4), (8, 9), (9, 0), (10, 0), (11, 2), (12, 9), (13, 8), (14, 8), (15, 7), (16, 6), (17, 3), (18, 8), (19, 1), (20, 7), (21, 2), (22, 7), (23, 6), (24, 3)]
	test_accs: [0.98, 0.9799, 0.9727, 0.9798, 0.9738]
	train_accs: [0.98153335, 0.98076665, 0.97506666, 0.98155, 0.97603333]
	best_train_sub_head: 3
	worst: 0.9727
	avg: 0.97723997
	best: 0.9798

Starting e_i: 1906
Model ind 665 epoch 1906 batch: 0 avg loss -2.929633 avg loss no lamb -2.929633 time 2020-06-27 22:45:55.956449
Model ind 665 epoch 1906 batch: 100 avg loss -2.848048 avg loss no lamb -2.848048 time 2020-06-27 22:46:06.199119
Model ind 665 epoch 1906 batch: 200 avg loss -2.883869 avg loss no lamb -2.883869 time 2020-06-27 22:46:16.614182
Model ind 665 epoch 1906 batch: 300 avg loss -2.881316 avg loss no lamb -2.881316 time 2020-06-27 22:46:27.952230
Model ind 665 epoch 1906 batch: 400 avg loss -2.832219 avg loss no lamb -2.832219 time 2020-06-27 22:46:38.206602
Model ind 665 epoch 1906 batch: 500 avg loss -2.898227 avg loss no lamb -2.898227 time 2020-06-27 22:46:48.827525
Model ind 665 epoch 1906 batch: 600 avg loss -2.901662 avg loss no lamb -2.901662 time 2020-06-27 22:46:59.772283
Model ind 665 epoch 1906 batch: 700 avg loss -2.817141 avg loss no lamb -2.817141 time 2020-06-27 22:47:11.238365
Model ind 665 epoch 1906 batch: 800 avg loss -2.894777 avg loss no lamb -2.894777 time 2020-06-27 22:47:22.068051
last batch sz 10
Pre: time 2020-06-27 22:47:36.327092: 
 	std: 0.0031044432
	best_train_sub_head_match: [(0, 0), (1, 7), (2, 5), (3, 6), (4, 1), (5, 3), (6, 7), (7, 4), (8, 0), (9, 4), (10, 7), (11, 9), (12, 2), (13, 2), (14, 0), (15, 6), (16, 8), (17, 8), (18, 9), (19, 3), (20, 8), (21, 3), (22, 9), (23, 5), (24, 2)]
	test_accs: [0.9801, 0.9793, 0.9732, 0.98, 0.9738]
	train_accs: [0.98113334, 0.9802333, 0.97503334, 0.98113334, 0.9755167]
	best_train_sub_head: 0
	worst: 0.9732
	avg: 0.97728
	best: 0.9801

Starting e_i: 1907
Model ind 665 epoch 1907 batch: 0 avg loss -2.970643 avg loss no lamb -2.970643 time 2020-06-27 22:47:38.130178
Model ind 665 epoch 1907 batch: 100 avg loss -2.945418 avg loss no lamb -2.945418 time 2020-06-27 22:47:50.485396
